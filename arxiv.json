[
  {
    "year": 1996,
    "title": "A Growth model for DNA evolution",
    "summary": "  A simple growth model for DNA evolution is introduced which is analytically\nsolvable and reproduces the observed statistical behavior of real sequences.\n"
  },
  {
    "year": 2000,
    "title": "Symmetry Analysis Of Genetic Code And Determinative Degree (in Russian)",
    "summary": "  A three-dimensional model of the genetic vocabulary is proposed in terms of\ndeterminative degree, an introduced characteristic of nucleotide, which\nreflects absolute difference between purin and pyrimidin bases in one DNA\nstrand. In the fremework of this model manifest symmetry and group properties\nare described. It is proposed to use the determinative degree of codons to\nanalyze genetic texts, and to explain the biological sense of various\nfunctional regions of DNA.\n"
  },
  {
    "year": 2001,
    "title": "Self-organizing Approach for Automated Gene Identification in Whole\n  Genomes",
    "summary": "  An approach based on using the idea of distinguished coding phase in explicit\nform for identification of protein-coding regions (exons) in whole genome has\nbeen proposed. For several genomes an optimal window length for averaging\nGC-content function and calculating codon frequencies has been found.\nSelf-training procedure based on clustering in multidimensional space of\ntriplet frequencies is proposed. For visualization of data in the space of\ntriplet requiencies method of elastic maps was applied.\n"
  },
  {
    "year": 2002,
    "title": "On the Hypercube Structure of the Genetic Code",
    "summary": "  A representation of the genetic code as a six-dimensional Boolean hypercube\nis proposed. It is assumed here that this structure is the result of the\nhierarchical order of the interaction energies of the bases in codon-anticodon\nrecognition. The proposed structure demonstrates that in the genetic code there\nis a balance between conservatism and innovation. Comparing aligned positions\nin homologous protein sequences two different behaviors are found: a) There are\nsites in which the different amino acids present may be explained by one or two\n\"attractor nodes\" (coding for the dominating amino acid(s)) and their one-bit\nneighbors in the codon hypercube, and b) There are sites in which the amino\nacids present correspond to codons located in closed paths in the hypercube.\nThe structure of the code facilitates evolution: the variation found at the\nvariable positions of proteins do not corresponds to random jumps at the codon\nlevel, but to well defined regions of the hypercube.\n"
  },
  {
    "year": 2002,
    "title": "Entropy, Transinformation and Word Distribution of Information-Carrying\n  Sequences",
    "summary": "  We investigate correlations in information carriers, e.g. texts and pieces of\nmusic, which are represented by strings of letters. For information carrying\nstrings generated by one source (i.e. a novel or a piece of music) we find\ncorrelations on many length scales. The word distribution, the higher order\nentropies and the transinformation are calculated. The analogy to strings\ngenerated through symbolic dynamics by nonlinear systems in critical states is\ndiscussed.\n"
  },
  {
    "year": 2002,
    "title": "L\u00e9vy scaling: the Diffusion Entropy Analysis applied to DNA\n  sequences",
    "summary": "  We address the problem of the statistical analysis of a time series generated\nby complex dynamics with a new method: the Diffusion Entropy Analysis (DEA)\n(Fractals, {\\bf 9}, 193 (2001)). This method is based on the evaluation of the\nShannon entropy of the diffusion process generated by the time series imagined\nas a physical source of fluctuations, rather than on the measurement of the\nvariance of this diffusion process, as done with the traditional methods. We\ncompare the DEA to the traditional methods of scaling detection and we prove\nthat the DEA is the only method that always yields the correct scaling value,\nif the scaling condition applies. Furthermore, DEA detects the real scaling of\na time series without requiring any form of de-trending. We show that the joint\nuse of DEA and variance method allows to assess whether a time series is\ncharacterized by L\\'{e}vy or Gauss statistics. We apply the DEA to the study of\nDNA sequences, and we prove that their large-time scales are characterized by\nL\\'{e}vy statistics, regardless of whether they are coding or non-coding\nsequences. We show that the DEA is a reliable technique and, at the same time,\nwe use it to confirm the validity of the dynamic approach to the DNA sequences,\nproposed in earlier work.\n"
  },
  {
    "year": 2002,
    "title": "Minimal model for genome evolution and growth",
    "summary": "  Textual analysis of typical microbial genomes reveals that they have the\nstatistical characteristics of a DNA sequence of a much shorter length. This\npeculiar property supports an evolutionary model in which a genome evolves by\nrandom mutation but primarily grows by random segmental self-copying. That\ngenomes grew mostly by self-copying is consistent with the observation that\nrepeat sequences in all genomes are widespread and intragenomic and\nintergenomic homologous genes are preponderance across all life forms. The\nmodel predicates the coexistence of the two competing modes of evolution: the\ngradual changes of classical Darwinism and the stochastic spurts envisioned in\n``punctuated equilibrium''.\n"
  },
  {
    "year": 2002,
    "title": "Coupled Two-Way Clustering Analysis of Breast Cancer and Colon Cancer\n  Gene Expression Data",
    "summary": "  We present and review Coupled Two Way Clustering, a method designed to mine\ngene expression data. The method identifies submatrices of the total expression\nmatrix, whose clustering analysis reveals partitions of samples (and genes)\ninto biologically relevant classes. We demonstrate, on data from colon and\nbreast cancer, that we are able to identify partitions that elude standard\nclustering analysis.\n"
  },
  {
    "year": 2002,
    "title": "Simplifying the mosaic description of DNA sequences",
    "summary": "  By using the Jensen-Shannon divergence, genomic DNA can be divided into\ncompositionally distinct domains through a standard recursive segmentation\nprocedure. Each domain, while significantly different from its neighbours, may\nhowever share compositional similarity with one or more distant\n(non--neighbouring) domains. We thus obtain a coarse--grained description of\nthe given DNA string in terms of a smaller set of distinct domain labels. This\nyields a minimal domain description of a given DNA sequence, significantly\nreducing its organizational complexity. This procedure gives a new means of\nevaluating genomic complexity as one examines organisms ranging from bacteria\nto human. The mosaic organization of DNA sequences could have originated from\nthe insertion of fragments of one genome (the parasite) inside another (the\nhost), and we present numerical experiments that are suggestive of this\nscenario.\n"
  },
  {
    "year": 2003,
    "title": "Evidence for growth of microbial genomes by short segmental duplications",
    "summary": "  We show that textual analysis of microbial genomes reveal telling footprints\nof the early evolution of the genomes. The frequencies of word occurrence of\nrandom DNA sequences considered as texts in their four nucleotides are expected\nto obey Poisson distributions. It is noticed that for words less than nine\nletters the average width of the distributions for complete microbial genomes\nis many times that of a Poisson distribution. We interpret this phenomenon as\nfollows: the genome is a large system that possesses the statistical\ncharacteristics of a much smaller ``random'' system, and certain textual\nstatistical properties of genomes we now see are remnants of those of their\nancestral genomes, which were much shorter than the genomes are now. This\ninterpretation suggests a simple biologically plausible model for the growth of\ngenomes: the genome first grows randomly to an initial length of approximately\none thousand nucleotides (1k nt), or about one thousandth of its final length,\nthereafter mainly grows by random segmental duplication. We show that using\nduplicated segments averaging around 25 nt, the model sequences generated\npossess statistical properties characteristic of present day genomes. Both the\ninitial length and the duplicated segment length support an RNA world at the\ntime duplication began. Random segmental duplication would greatly enhance the\nability of a genome to use its hard-to-acquire codes repeatedly, and a genome\nthat practiced it would have evolved enormously faster than those that did not.\n"
  },
  {
    "year": 2003,
    "title": "Scaling laws in the functional content of genomes",
    "summary": "  With the number of sequenced genomes now over one hundred, and the\navailability of rough functional annotations for a substantial proportion of\ntheir genes, it has become possible to study the statistics of gene content\nacross genomes. Here I show that, for many high-level functional categories,\nthe number of genes in the category scales as a power-law in the total number\nof genes in the genome. The occurrence of such scaling laws can be explained\nwith a simple theoretical model, and this model suggests that the exponents of\nthe observed scaling laws correspond to universal constants of the evolutionary\nprocess. I discuss some consequences of these scaling laws for our\nunderstanding of organism design.\n"
  },
  {
    "year": 2003,
    "title": "Symmetry Scheme for Amino Acid Codons",
    "summary": "  Group theoretical concepts are invoked in a specific model to explain how\nonly twenty amino acids occur in nature out of a possible sixty four. The\nmethods we use enable us to justify the occurrence of the recently discovered\ntwenty first amino acid selenocysteine, and also enables us to predict the\npossible existence of two more, as yet undiscovered amino acids.\n"
  },
  {
    "year": 2003,
    "title": "Quasireplicas and universal lengths of microbial genomes",
    "summary": "  Statistical analysis of distributions of occurrence frequencies of short\nwords in 108 microbial complete genomes reveals the existence of a set of\nuniversal \"root-sequence lengths\" shared by all microbial genomes. These\nlengths and their universality give powerful clues to the way microbial genomes\nare grown. We show that the observed genomic properties are explained by a\nmodel for genome growth in which primitive genomes grew mainly by maximally\nstochastic duplications of short segments from an initial length of about 200\nnucleotides (nt) to a length of about one million nt typical of microbial\ngenomes. The relevance of the result of this study to the nature of\nsimultaneous random growth and information acquisition by genomes, to the\nso-called RNA world in which life evolved before the rise of proteins and\nenzymes and to several other topics are discussed.\n"
  },
  {
    "year": 2003,
    "title": "On the 20 canonical amino acids by a cooperative vector-addition\n  principle based on the quasi-28-gon symmetry of the genetic code",
    "summary": "  Upon the covalent-bonding hybrid of the nitrogen atoms taken as a measure for\nthe structural regularity in nucleobases, it can be identified that the\ninternal relation within the 20 amino acids follows a cooperative\nvector-in-space addition principle based on the spherical and rotational\nsymmetry of a quasi-28-gon (quasi-icosikaioctagon), with two evolutionary axes.\n"
  },
  {
    "year": 2003,
    "title": "Human housekeeping genes are compact",
    "summary": "  We identify a set of 575 human genes that are expressed in all conditions\ntested in a publicly available database of microarray results. Based on this\ncommon occurrence, the set is expected to be rich in \"housekeeping\" genes,\nshowing constitutive expression in all tissues. We compare selected aspects of\ntheir genomic structure with a set of background genes. We find that the\nintrons, untranslated regions and coding sequences of the housekeeping genes\nare shorter, indicating a selection for compactness in these genes.\n"
  },
  {
    "year": 2003,
    "title": "A covariance kernel for proteins",
    "summary": "  We propose a new kernel for biological sequences which borrows ideas and\ntechniques from information theory and data compression. This kernel can be\nused in combination with any kernel method, in particular Support Vector\nMachines for protein classification. By incorporating prior biological\nassumptions on the properties of amino-acid sequences and using a Bayesian\naveraging framework, we compute the value of this kernel in linear time and\nspace, benefiting from previous achievements proposed in the field of universal\ncoding. Encouraging classification results are reported on a standard protein\nhomology detection experiment.\n"
  },
  {
    "year": 2003,
    "title": "Evolutionary conservation of motif constituents within the yeast protein\n  interaction network",
    "summary": "  Understanding why some cellular components are conserved across species,\nwhile others evolve rapidly is a key question of modern biology. Here we\ndemonstrate that in S. cerevisiae proteins organized in cohesive patterns of\ninteractions are conserved to a significantly higher degree than those that do\nnot participate in such motifs. We find that the conservation of proteins\nwithin distinct topological motifs correlates with the motif's\ninter-connectedness and function and also depends on the structure of the\noverall interactome topology. These findings indicate that motifs may represent\nevolutionary conserved topological units of cellular networks molded in\naccordance with the specific biological function in which they participate.\n"
  },
  {
    "year": 2003,
    "title": "Random model for RNA interference yields scale free network",
    "summary": "  We introduce a random bit-string model of post-transcriptional genetic\nregulation based on sequence matching. The model spontaneously yields a scale\nfree network with power law scaling with $ \\gamma=-1$ and also exhibits\nlog-periodic behaviour. The in-degree distribution is much narrower, and\nexhibits a pronounced peak followed by a Gaussian distribution. The network is\nof the smallest world type, with the average minimum path length independent of\nthe size of the network, as long as the network consists of one giant cluster.\nThe percolation threshold depends on the system size.\n"
  },
  {
    "year": 2003,
    "title": "Statistical analysis of the spatial distribution of operons in the\n  transcriptional regulation network of Escherichia coli",
    "summary": "  We have performed a statistical analysis of the spatial distribution of\noperons in the transcriptional regulation network of Escherichia coli. The\nanalysis reveals that operons that regulate each other and operons that are\ncoregulated tend to lie next to each other on the genome. Moreover, these pairs\nof operons tend to be transcribed in diverging directions. This spatial\narrangement of operons allows the upstream regulatory regions to interfere with\neach other. This affords additional regulatory control, as illustrated by a\nmean-field analysis of a feed-forward loop. Our results suggest that regulatory\ncontrol can provide a selection pressure that drives operons together in the\ncourse of evolution.\n"
  },
  {
    "year": 2003,
    "title": "\"Junk\" DNA as a genetic decoy",
    "summary": "  We propose that the evolutionary purpose of junk DNA is to protect the gene.\nMutation agents, such as retro-viruses, hit ``decoy'' DNA most of the time.\nAlthough the argument is far from general, we propose that the percentage of\njunk DNA should correlate with the number of retroviruses attacking a given\nspecies. It should also anti-correlate with the ideal mutation rates (higher in\ninsects than in mammals).\n"
  },
  {
    "year": 2003,
    "title": "Plant defense multigene families: II Evolution of coding sequence and\n  differential expression of PR10 genes in Pisum",
    "summary": "  While it is not possible to directly the observe evolution of multigene\nfamilies, the best alternative is to compare orthologous family members among\nseveral closely-related species with varying degrees of reproductive isolation.\nUsing RT-PCR we show that in pea (Pisum sativum) each member of the\npathogenesis-related PR10 family has a distinct pattern of expression in\nresponse to the fungus Fusarium solani, and in treatment with salicylic acid,\nchitosan and abcisic acid. Sequencing reveals that PR10.1, PR10.2 and PR10.3\nexist in P. humile, P. elatius and P. fulvum, except that no PR10.2 orthologue\nwas identified in P. elatius. PR10.1, PR10.2 and PR10.3 appear to have diverged\nfrom a single gene in the common Pisum ancestor. For the recently diverged\nPR10.1 and PR10.2, the timing of fungal-induced expression differs greatly\namong species. For example, PR10.1 was strongly induced in P. sativum by F.\nsolani within 8 hours postinoculation (h.p.i.), whereas little PR10.1\nexpression was seen in pea's closest relative, P. humile, and in the more\ndistantly-related P. elatius. In P. fulvum, expression did not peak until 48\nh.p.i. Expression of the more ancient PR10.4 and PR10.5 genes is more tightly\nconserved among Pisum species. These data indicate that expression, as well as\nsequence, can evolve rapidly. We hypothesize that changes in differential\nexpression of multigene family members could provide a source of phenotypic\ndiversity in populations, which may be of particular importance to\nplant/pathogen coevolution.\n"
  },
  {
    "year": 2003,
    "title": "Computational identification of transcription factor binding sites by\n  functional analysis of sets of genes sharing overrepresented upstream motifs",
    "summary": "  BACKGROUND: Transcriptional regulation is a key mechanism in the functioning\nof the cell, and is mostly effected through transcription factors binding to\nspecific recognition motifs located upstream of the coding region of the\nregulated gene. The computational identification of such motifs is made easier\nby the fact that they often appear several times in the upstream region of the\nregulated genes, so that the number of occurrences of relevant motifs is often\nsignificantly larger than expected by pure chance. RESULTS: To exploit this\nfact, we construct sets of genes characterized by the statistical\noverrepresentation of a certain motif in their upstream regions. Then we study\nthe functional characterization of these sets by analyzing their annotation to\nGene Ontology terms. For the sets showing a statistically significant specific\nfunctional characterization, we conjecture that the upstream motif\ncharacterizing the set is a binding site for a transcription factor involved in\nthe regulation of the genes in the set. CONCLUSIONS: The method we propose is\nable to identify many known binding sites in S. cerevisiae and new candidate\ntargets of regulation by known transcription factors. Its application to less\nwell studied organisms is likely to be valuable in the exploration of their\nregulatory interaction network.\n"
  },
  {
    "year": 2003,
    "title": "Determinative degree and nucleotide sequence analysis by trianders",
    "summary": "  A new version of DNA walks, where nucleotides are regarded unequal in their\ncontribution to a walk is introduced, which allows us to study thoroughly the\n\"fine structure\" of nucleotide sequences. The approach is based on the\nassumption that nucleotides have an inner abstract characteristics, the\ndeterminative degree, which reflects phenomenological properties of genetic\ncode and is adjusted to nucleotides physical properties. We consider each\nposition in codon independently, which gives three separate walks being\ncharacterized by different angles and lengths, and such an object is called\ntriander which reflects the \"strength\" of branch. A general method of\nidentification of DNA sequence \"by triander\", which can be treated as a unique\n\"genogram\", \"gene passport\" is proposed. The two- and three-dimensional\ntrianders are considered. The difference of sequence fine structure in genes\nand the intergenic space is shown. A clear triplet signal in coding locuses is\nfound which is absent in the intergenic space and is independent from the\nsequence length. The topological classification of trianders is presented which\ncan allow us to provide a detail working out signatures of functionally\ndifferent genomic regions.\n"
  },
  {
    "year": 2003,
    "title": "Finding regulatory modules through large-scale gene-expression data\n  analysis",
    "summary": "  The use of gene microchips has enabled a rapid accumulation of\ngene-expression data. One of the major challenges of analyzing this data is the\ndiversity, in both size and signal strength, of the various modules in the gene\nregulatory networks of organisms. Based on the Iterative Signature Algorithm\n[Bergmann, S., Ihmels, J. and Barkai, N. (2002) Phys. Rev. E 67, 031902], we\npresent an algorithm - the Progressive Iterative Signature Algorithm (PISA) -\nthat, by sequentially eliminating modules, allows unsupervised identification\nof both large and small regulatory modules. We applied PISA to a large set of\nyeast gene-expression data, and, using the Gene Ontology annotation database as\na reference, found that our algorithm is much better able to identify\nregulatory modules than methods based on high-throughput transcription-factor\nbinding experiments or on comparative genomics.\n"
  },
  {
    "year": 2003,
    "title": "MAVID: Constrained ancestral alignment of multiple sequences",
    "summary": "  We describe a new global multiple alignment program capable of aligning a\nlarge number of genomic regions. Our progressive alignment approach\nincorporates the following ideas: maximum-likelihood inference of ancestral\nsequences, automatic guide-tree construction, protein based anchoring of\nab-initio gene predictions, and constraints derived from a global homology map\nof the sequences. We have implemented these ideas in the MAVID program, which\nis able to accurately align multiple genomic regions up to megabases long.\nMAVID is able to effectively align divergent sequences, as well as incomplete\nunfinished sequences. We demonstrate the capabilities of the program on the\nbenchmark CFTR region which consists of 1.8Mb of human sequence and 20\northologous regions in marsupials, birds, fish, and mammals. Finally, we\ndescribe two large MAVID alignments: an alignment of all the available HIV\ngenomes and a multiple alignment of the entire human, mouse and rat genomes.\n"
  },
  {
    "year": 2003,
    "title": "Is prokaryotic complexity limited by accelerated growth in regulatory\n  overhead?",
    "summary": "  Increased biological complexity is generally associated with the addition of\nnew genetic information, which must be integrated into the existing regulatory\nnetwork that operates within the cell. General arguments on network control, as\nwell as several recent genomic observations, indicate that regulatory gene\nnumber grows disproportionally fast with increasing genome size. We present two\nmodels for the growth of regulatory networks. Both predict that the number of\ntranscriptional regulators will scale quadratically with total gene number.\nThis appears to be in good quantitative agreement with genomic data from 89\nfully sequenced prokaryotes. Moreover, the empirical curve predicts that any\nnew non-regulatory gene will be accompanied by more than one additional\nregulator beyond a genome size of about 20,000 genes, within a factor of two of\nthe observed ceiling. Our analysis places transcriptional regulatory networks\nin the class of accelerating networks. We suggest that prokaryotic complexity\nmay have been limited throughout evolution by regulatory overhead, and\nconversely that complex eukaryotes must have bypassed this constraint by novel\nstrategies.\n"
  },
  {
    "year": 2003,
    "title": "Relevance Vector Machines for classifying points and regions in\n  biological sequences",
    "summary": "  The Relevance Vector Machine (RVM) is a recently developed machine learning\nframework capable of building simple models from large sets of candidate\nfeatures. Here, we describe a protocol for using the RVM to explore very large\nnumbers of candidate features, and a family of models which apply the power of\nthe RVM to classifying and detecting interesting points and regions in\nbiological sequence data. The models described here have been used successfully\nfor predicting transcription start sites and other features in genome\nsequences.\n"
  },
  {
    "year": 2003,
    "title": "What can we learn from noncoding regions of similarity between genomes?",
    "summary": "  Background: In addition to known protein-coding genes, large amount of\napparently non-coding sequence are conserved between the human and mouse\ngenomes. It seems reasonable to assume that these conserved regions are more\nlikely to contain functional elements than less-conserved portions of the\ngenome. Here we used a motif-oriented machine learning method to extract the\nstrongest signal from a set of non-coding conserved sequences.\n  Results: We successfully fitted models to reflect the non-coding sequences,\nand showed that the results were quite consistent for repeated training runs.\nUsing the learned model to scan genomic sequence, we found that it often made\npredictions close to the start of annotated genes. We compared this method with\nother published promoter-prediction systems, and show that the set of promoters\nwhich are detected by this method seems to be substantially similar to that\ndetected by existing methods.\n  Conclusions: The results presented here indicate that the promoter signal is\nthe strongest single motif-based signal in the non-coding functional fraction\nof the genome. They also lend support to the belief that there exists a\nsubstantial subset of promoter regions which share common features and are\ndetectable by a variety of computational methods.\n"
  },
  {
    "year": 2003,
    "title": "Clone-array pooled shotgun mapping and sequencing: design and analysis\n  of experiments",
    "summary": "  This paper studies sequencing and mapping methods that rely solely on pooling\nand shotgun sequencing of clones. First, we scrutinize and improve the recently\nproposed Clone-Array Pooled Shotgun Sequencing (CAPSS) method, which delivers a\nBAC-linked assembly of a whole genome sequence. Secondly, we introduce a novel\nphysical mapping method, called Clone-Array Pooled Shotgun Mapping (CAPS-MAP),\nwhich computes the physical ordering of BACs in a random library. Both CAPSS\nand CAPS-MAP construct subclone libraries from pooled genomic BAC clones.\n  We propose algorithmic and experimental improvements that make CAPSS a viable\noption for sequencing a set of BACs. We provide the first probabilistic model\nof CAPSS sequencing progress. The model leads to theoretical results supporting\nprevious, less formal arguments on the practicality of CAPSS. We demonstrate\nthe usefulness of CAPS-MAP for clone overlap detection with a probabilistic\nanalysis, and a simulated assembly of the Drosophila melanogaster genome. Our\nanalysis indicates that CAPS-MAP is well-suited for detecting BAC overlaps in a\nhighly redundant library, relying on a low amount of shotgun sequence\ninformation. Consequently, it is a practical method for computing the physical\nordering of clones in a random library, without requiring additional clone\nfingerprinting. Since CAPS-MAP requires only shotgun sequence reads, it can be\nseamlessly incorporated into a sequencing project with almost no experimental\noverhead.\n"
  },
  {
    "year": 2003,
    "title": "Genetic Paralog Analysis and Simulations",
    "summary": "  Using Monte Carlo methods, we simulated the effects of bias in generation and\nelimination of paralogs on the size distribution of paralog groups. It was\nfound that the function describing the decay of the number of paralog groups\nwith their size depends on the ratio between the probability of duplications of\ngenes and their deletions, which corresponds to different selection pressures\non the genome size. Slightly different slopes of curves describing the decay of\nthe number of paralog groups with their size were also observed when the\nthreshold of homology between paralogous sequences was changed.\n"
  },
  {
    "year": 2003,
    "title": "Modularity \"for free\" in genome architecture?",
    "summary": "  Background: Recent models of genome-proteome evolution have shown that some\nof the key traits displayed by the global structure of cellular networks might\nbe a natural result of a duplication-diversification (DD) process. One of the\nconsequences of such evolution is the emergence of a small world architecture\ntogether with a scale-free distribution of interactions. Here we show that the\ndomain of parameter space were such structure emerges is related to a phase\ntransition phenomenon. At this transition point, modular architecture\nspontaneously emerges as a byproduct of the DD process.\n  Results: Although the DD models lack any functionality and are thus free from\nmeeting functional constraints, they show the observed features displayed by\nthe real proteome maps when tuned close to a sharp transition point separating\na highly connected graph from a disconnected system. Close to such boundary,\nthe maps are shown to display scale-free hierarchical organization, behave as\nsmall worlds and exhibit modularity.\n  Conclusions: It is conjectured that natural selection tuned the average\nconnectivity in such a way that the network reaches a sparse graph of\nconnections. One consequence of such scenario is that the scaling laws and the\nessential ingredients for building a modular net emerge for free close to such\ntransition.\n"
  },
  {
    "year": 2003,
    "title": "RNA Binding Density on X-chromosome Differing from that on 22 Autosomes\n  in Human",
    "summary": "  To test whether X-chromosome has unique genomic characteristics, X-chromosome\nand 22 autosomes were compared for RNA binding density. Nucleotide sequences on\nthe chromosomes were divided into 50kb per segment that was recoded as a set of\nfrequency values of 7-nucleotide (7nt) strings using all possible 7nt strings\n(47=16384). 120 genes highly expressed in tonsil germinal center B cells were\nselected for calculating 7nt string frequency values of all introns (RNAs). The\nbinding density of DNA segments and RNAs was determined by the amount of\ncomplement sequences. It was shown for the first time that gene-poor and low\ngene expression X-chromosome had the lowest percentage of the DNA segments that\ncan highly bind RNAs, whereas gene-rich and high gene expression chromosome 19\nhad the highest percentage of the segments. On the basis of these results, it\nis proposed that the nonrandom properties of distribution of RNA highly binding\nDNA segments on the chromosomes provide strong evidence that lack of RNA highly\nbinding segments may be a cause of X-chromosome inactivation\n"
  },
  {
    "year": 2004,
    "title": "Increasing biological complexity is positively correlated with the\n  relative genome-wide expansion of non-protein-coding DNA sequences",
    "summary": "  Background: Prior to the current genomic era it was suggested that the number\nof protein-coding genes that an organism made use of was a valid measure of its\ncomplexity. It is now clear, however, that major incongruities exist and that\nthere is only a weak relationship between biological complexity and the number\nof protein coding genes. For example, using the protein-coding gene number as a\nbasis for evaluating biological complexity would make urochordates and insects\nless complex than nematodes, and humans less complex than rice. Results: We\nanalyzed the ratio of noncoding to total genomic DNA (ncDNA/tgDNA) for 85\nsequenced species and found that this ratio correlates well with increasing\nbiological complexity. The ncDNA/tgDNA ratio is generally contained within the\nbandwidth of 0.05-0.24 for prokaryotes, but rises to 0.26-0.52 in unicellular\neukaryotes, and to 0.62-0.985 for developmentally complex multicellular\norganisms. Significantly, prokaryotic species display a non-uniform species\ndistribution approaching the mean of 0.1177 ncDNA/tgDNA (p=1.58 x 10^-13), and\na nonlinear ncDNA/tgDNA relationship to genome size (r=0.15). Importantly, the\nncDNA/tgDNA ratio corrects for ploidy, and is not substantially affected by\nvariable loads of repetitive sequences. Conclusions: We suggest that the\nobserved noncoding DNA increases and compositional patterns are primarily a\nfunction of increased information content. It is therefore possible that\nintrons, intergenic sequences, repeat elements, and genomic DNA previously\nregarded as genetically inert may be far more important to the evolution and\nfunctional repertoire of complex organisms than has been previously\nappreciated.\n"
  },
  {
    "year": 2004,
    "title": "Complex cooperativity of ATP hydrolysis in the F1-ATPase molecular motor",
    "summary": "  F1-ATPase catalyses ATP hydrolysis and converts the cellular chemical energy\ninto mechanical rotation. The hydrolysis reaction in F1-ATPase does not follow\nthe widely believed Michaelis-Menten mechanism. Instead, the hydrolysis\nmechanism behaves in an ATP-dependent manner. We develop a model for enzyme\nkinetics and hydrolysis cooperativity of F1-ATPase which involves the\nbinding-state changes to the coupling catalytic reactions. The quantitative\nanalysis and modeling suggest the existence of complex cooperative hydrolysis\nbetween three different catalysis sites of F1-ATPase. This complexity may be\ntaken into account to resolve the arguments on the bindingchange mechanism in\nF1-ATPase.\n"
  },
  {
    "year": 2004,
    "title": "Online tool for the discrimination of equi-distributions",
    "summary": "  For many applications one wishes to decide whether a certain set of numbers\noriginates from an equiprobability distribution or whether they are unequally\ndistributed. Distributions of relative frequencies may deviate significantly\nfrom the corresponding probability distributions due to finite sample effects.\nHence, it is not trivial to discriminate between an equiprobability\ndistribution and non-equally distributed probabilities when knowing only\nfrequencies. Based on analytical results we provide a software tool which\nallows to decide whether data correspond to an equiprobability distribution.\nThe tool is available at http://bioinf.charite.de/equifreq/. Its application is\ndemonstrated for the distribution of point mutations in coding genes.\n"
  },
  {
    "year": 2004,
    "title": "Molecular Signatures from Gene Expression Data",
    "summary": "  Motivation: ``Molecular signatures'' or ``gene-expression signatures'' are\nused to predict patients' characteristics using data from coexpressed genes.\nSignatures can enhance understanding about biological mechanisms and have\ndiagnostic use. However, available methods to search for signatures fail to\naddress key requirements of signatures, especially the discovery of sets of\ntightly coexpressed genes. Results: After suggesting an operational definition\nof signature, we develop a method that fulfills these requirements, returning\nsets of tightly coexpressed genes with good predictive performance. This method\ncan also identify when the data are inconsistent with the hypothesis of a few,\nstable, easily interpretable sets of coexpressed genes. Identification of\nmolecular signatures in some widely used data sets is questionable under this\nsimple model, which emphasizes the needed for further work on the\noperationalization of the biological model and the assessment of the stability\nof putative signatures. Availability: The code (R with C++) is available from\nhttp://www.ligarto.org/rdiaz/Software/Software.html under the GNU GPL.\n"
  },
  {
    "year": 2004,
    "title": "A Fast Reconstruction Algorithm for Gene Networks",
    "summary": "  This paper deals with gene networks whose dynamics is assumed to be generated\nby a continuous-time, linear, time invariant, finite dimensional system (LTI)\nat steady state. In particular, we deal with the problem of network\nreconstruction in the typical practical situation in which the number of\navailable data is largely insufficient to uniquely determine the network. In\norder to try to remove this ambiguity, we will exploit the biologically a\npriori assumption of network sparseness, and propose a new algorithm for\nnetwork reconstruction having a very low computational complexity (linear in\nthe number of genes) so to be able to deal also with very large networks (say,\nthousands of genes). Its performances are also tested both on artificial data\n(generated with linear models) and on real data obtained by Gardner et al. from\nthe SOS pathway in Escherichia coli.\n"
  },
  {
    "year": 2004,
    "title": "Functional Bias and Spatial Organization of Genes in Mutational Hot and\n  Cold Regions in the Human Genome",
    "summary": "  The neutral mutation rate is known to vary widely along human chromosomes,\nleading to mutational hot and cold regions. We provide evidence that categories\nof functionally-related genes reside preferentially in mutationally hot or cold\nregions, the size of which we have measured. Genes in hot regions are biased\ntoward extra-cellular communication (surface receptors, cell adhesion, immune\nresponse, etc.) while those in cold regions are biased toward essential\ncellular processes (gene regulation, RNA processing, protein modification,\netc.). From a selective perspective, this organization of genes could minimize\nthe mutational load on genes that need to be conserved and allow fast evolution\nfor genes that must frequently adapt. We also analyze the effect of gene\nduplication and chromosomal recombination, which contribute significantly to\nthese biases for certain categories of hot genes. Overall, our results show\nthat genes are located non-randomly with respect to hot and cold regions,\noffering the possibility that selection acts at the level of gene location in\nthe human genome.\n"
  },
  {
    "year": 2004,
    "title": "Long-range correlation in the whole human genome",
    "summary": "  We calculate the mutual information function for each of the 24 chromosomes\nin the human genome. The same correlation pattern is observed regardless the\nindividual functional features of each chromosome. Moreover, correlations of\ndifferent scale length are detected depicting a multifractal scenario. This\nfact suggest a unique mechanism of structural evolution. We propose that such a\nmechanism could be an expansion-modification dynamical system.\n"
  },
  {
    "year": 2004,
    "title": "Solution of the Quasispecies Model for an Arbitrary Gene Network",
    "summary": "  In this paper, we study the equilibrium behavior of Eigen's quasispecies\nequations for an arbitrary gene network. We consider a genome consisting of $ N\n$ genes, so that each gene sequence $ \\sigma $ may be written as $ \\sigma =\n\\sigma_1 \\sigma_2 ... \\sigma_N $. We assume a single fitness peak (SFP) model\nfor each gene, so that gene $ i $ has some ``master'' sequence $ \\sigma_{i, 0}\n$ for which it is functioning. The fitness landscape is then determined by\nwhich genes in the genome are functioning, and which are not. The equilibrium\nbehavior of this model may be solved in the limit of infinite sequence length.\nThe central result is that, instead of a single error catastrophe, the model\nexhibits a series of localization to delocalization transitions, which we term\nan ``error cascade.'' As the mutation rate is increased, the selective\nadvantage for maintaining functional copies of certain genes in the network\ndisappears, and the population distribution delocalizes over the corresponding\nsequence spaces. The network goes through a series of such transitions, as more\nand more genes become inactivated, until eventually delocalization occurs over\nthe entire genome space, resulting in a final error catastrophe. This model\nprovides a criterion for determining the conditions under which certain genes\nin a genome will lose functionality due to genetic drift. It also provides\ninsight into the response of gene networks to mutagens. In particular, it\nsuggests an approach for determining the relative importance of various genes\nto the fitness of an organism, in a more accurate manner than the standard\n``deletion set'' method. The results in this paper also have implications for\nmutational robustness and what C.O. Wilke termed ``survival of the flattest.''\n"
  },
  {
    "year": 2004,
    "title": "Gene-history correlation and population structure",
    "summary": "  Correlation of gene histories in the human genome determines the patterns of\ngenetic variation (haplotype structure) and is crucial to understanding genetic\nfactors in common diseases. We derive closed analytical expressions for the\ncorrelation of gene histories in established demographic models for genetic\nevolution and show how to extend the analysis to more realistic (but more\ncomplicated) models of demographic structure. We identify two contributions to\nthe correlation of gene histories in divergent populations: linkage\ndisequilibrium, and differences in the demographic history of individuals in\nthe sample. These two factors contribute to correlations at different length\nscales: the former at small, and the latter at large scales. We show that\nrecent mixing events in divergent populations limit the range of correlations\nand compare our findings to empirical results on the correlation of gene\nhistories in the human genome.\n"
  },
  {
    "year": 2004,
    "title": "A Machine Learning Strategy to Identity Exonic Splice Enhancers in Human\n  Protein-coding Sequence",
    "summary": "  Background: Exonic splice enhancers are sequences embedded within exons which\npromote and regulate the splicing of the transcript in which they are located.\nA class of exonic splice enhancers are the SR proteins, which are thought to\nmediate interactions between splicing factors bound to the 5' and 3' splice\nsites. Method and results: We present a novel strategy for analysing\nprotein-coding sequence by first randomizing the codons used at each position\nwithin the coding sequence, then applying a motif-based machine learning\nalgorithm to compare the true and randomized sequences. This strategy\nidentified a collection of motifs which can successfully discriminate between\nreal and randomized coding sequence, including -- but not restricted to --\nseveral previously reported splice enhancer elements. As well as successfully\ndistinguishing coding exons from randomized sequences, we show that our model\nis able to recognize non-coding exons. Conclusions: Our strategy succeeded in\ndetecting signals in coding exons which seem to be orthogonal to the sequences'\nprimary function of coding for proteins. We believe that many of the motifs\ndetected here may represent binding sites for previously unrecognized proteins\nwhich influence RNA splicing. We hope that this development will lead to\nimproved knowledge of exonic splice enhancers, and new developments in the\nfield of computational gene prediction.\n"
  },
  {
    "year": 2004,
    "title": "Statistical analysis of the distribution of amino acids in Borrelia\n  burgdorferi genome under different genetic codes",
    "summary": "  The genetic code is considered to be universal. In order to test if some\nstatistical properties of the coding bacterial genome were due to inherent\nproperties of the genetic code, we compared the autocorrelation function, the\nscaling properties and the maximum entropy of the distribution of distances of\namino acids in sequences obtained by translating protein-coding regions from\nthe genome of Borrelia burgdorferi, under different genetic codes. Overall our\nresults indicate that these properties are very stable to perturbations made by\naltering the genetic code. We also discuss the evolutionary likely implications\nof the present results.\n"
  },
  {
    "year": 2004,
    "title": "Extreme Value Distribution Based Gene Selection Criteria for\n  Discriminant Microarray Data Analysis Using Logistic Regression",
    "summary": "  One important issue commonly encountered in the analysis of microarray data\nis to decide which and how many genes should be selected for further studies.\nFor discriminant microarray data analyses based on statistical models, such as\nthe logistic regression models, gene selection can be accomplished by a\ncomparison of the maximum likelihood of the model given the real data,\n$\\hat{L}(D|M)$, and the expected maximum likelihood of the model given an\nensemble of surrogate data with randomly permuted label, $\\hat{L}(D_0|M)$.\nTypically, the computational burden for obtaining $\\hat{L}(D_0|M)$ is immense,\noften exceeding the limits of computing available resources by orders of\nmagnitude. Here, we propose an approach that circumvents such heavy\ncomputations by mapping the simulation problem to an extreme-value problem. We\npresent the derivation of an asymptotic distribution of the extreme-value as\nwell as its mean, median, and variance. Using this distribution, we propose two\ngene selection criteria, and we apply them to two microarray datasets and three\nclassification tasks for illustration.\n"
  },
  {
    "year": 2004,
    "title": "Alternative Splicing and Genomic Stability",
    "summary": "  Alternative splicing allows an organism to make different proteins in\ndifferent cells at different times, all from the same gene. In a cell that uses\nalternative splicing, the total length of all the exons is much shorter than in\na cell that encodes the same set of proteins without alternative splicing. This\neconomical use of exons makes genes more stable during reproduction and\ndevelopment because a genome with a shorter exon length is more resistant to\nharmful mutations. Genomic stability may be the reason why higher vertebrates\nsplice alternatively. For a broad class of alternatively spliced genes, a\nformula is given for the increase in their stability.\n"
  },
  {
    "year": 2004,
    "title": "Variations in Substitution Rate in Human and Mouse Genomes",
    "summary": "  We present a method to quantify spatial fluctuations of the substitution rate\non different length scales throughout genomes of eukaryotes. The fluctuations\non large length scales are found to be predominantly a consequence of a\ncoarse-graining effect of fluctuations on shorter length scales. This is\nverified for both the mouse and the human genome. We also found that both\nspecies show similar standard deviation of fluctuations even though their mean\nsubstitution rate differs by a factor of two. Our method furthermore allows to\ndetermine time-resolved substitution rate maps from which we can compute\nauto-correlation functions in order to quantify how fast the spatial\nfluctuations in substitution rate change in time.\n"
  },
  {
    "year": 2004,
    "title": "Stacking and Hydrogen Bonding. DNA Cooperativity at Melting",
    "summary": "  By taking into account base-base stacking interactions we improve the\nGeneralized Model of Polypeptide Chain (GMPC). Based on a one-dimensional\nPotts-like model with many-particle interactions, the GMPC describes the\nhelix-coil transition in both polypeptides and polynucleotides. In the\nframework of the GMPC we show that correctly introduced nearest-neighbor\nstacking interactions against the background of hydrogen bonding lead to\nincreased stability (melting temperature) and, unexpectedly, to decreased\ncooperativity (maximal correlation length). The increase in stability is\nexplained as due to an additional stabilizing interaction (stacking) and the\nsurprising decrease in cooperativity is seen as a result of mixing of\ncontributions of hydrogen bonding and stacking.\n"
  },
  {
    "year": 2004,
    "title": "Partly melted DNA conformations obtained with a probability peak finding\n  method",
    "summary": "  Peaks in the probabilities of loops or bubbles, helical segments, and\nunzipping ends in melting DNA are found in this article using a peak finding\nmethod that maps the hierarchical structure of certain energy landscapes. The\npeaks indicate the alternative conformations that coexist in equilibrium and\nthe range of their fluctuations. This yields a representation of the\nconformational ensemble at a given temperature, which is illustrated in a\nsingle diagram called a stitch profile. This article describes the methodology\nand discusses stitch profiles vs. the ordinary probability profiles using the\nphage lambda genome as an example.\n"
  },
  {
    "year": 2004,
    "title": "Modulation of Base Specific Mutation and Recombination Rates Enables\n  Functional Adaptation within the Context of the Genetic Code",
    "summary": "  The persistence of life requires populations to adapt at a rate commensurate\nwith the dynamics of their environment. Successful populations that inhabit\nhighly variable environments have evolved mechanisms to increase the likelihood\nof successful adaptation. We introduce a $64 \\times 64$ matrix to quantify\nbase-specific mutation potential, analyzing four different replicative systems,\nerror-prone PCR, mouse antibodies, a nematode, and Drosophila. Mutational\ntendencies are correlated with the structural evolution of proteins. In systems\nunder strong selective pressure, mutational biases are shown to favor the\nadaptive search of space, either by base mutation or by recombination. Such\nadaptability is discussed within the context of the genetic code at the levels\nof replication and codon usage.\n"
  },
  {
    "year": 2004,
    "title": "Statistical analysis of Gene and Intergenic DNA Sequences",
    "summary": "  Much of the on-going statistical analysis of DNA sequences is focused on the\nestimation of characteristics of coding and non-coding regions that would\npossibly allow discrimination of these regions. In the current approach, we\nconcentrate specifically on genes and intergenic regions. To estimate the level\nand type of correlation in these regions we apply various statistical methods\ninspired from nonlinear time series analysis, namely the probability\ndistribution of tuplets, the Mutual Information and the Identical Neighbour\nFit. The methods are suitably modified to work on symbolic sequences and they\nare first tested for validity on sequences obtained from well--known simple\ndeterministic and stochastic models. Then they are applied to the DNA sequence\nof chromosome 1 of {\\em arabidopsis thaliana}. The results suggest that\ncorrelations do exist in the DNA sequence but they are weak and that intergenic\nsequences tend to be more correlated than gene sequences. The use of\nstatistical tests with surrogate data establish these findings in a rigorous\nstatistical manner.\n"
  },
  {
    "year": 2004,
    "title": "Metabolic Rate Calibrates the Molecular Clock: Reconciling Molecular and\n  Fossil Estimates of Evolutionary Divergence",
    "summary": "  Observations that rates of molecular evolution vary widely within and among\nlineages have cast doubts upon the existence of a single molecular clock.\nDifferences in the timing of evolutionary events estimated from genetic and\nfossil evidence have raised further questions about the existence of molecular\nclocks and their use. Here we present a model of nucleotide substitution that\ncombines new theory on metabolic rate with the now classic neutral theory of\nmolecular evolution. The model quantitatively predicts rate heterogeneity, and\nreconciles differences in molecular- and fossil-estimated dates of evolutionary\nevents. Model predictions are supported by extensive data from mitochondrial\nand nuclear genomes. By accounting for the effects of body size and temperature\non metabolic rate, a single molecular clock explains heterogeneity in rates of\nnucleotide substitution in different genes, taxa, and thermal environments.\nThis model suggests that there is indeed a general molecular clock, as\noriginally proposed by Zuckerkandl and Pauling, but that it ticks at a constant\nsubstitution rate per unit mass-specific metabolic energy rather than per unit\ntime. More generally, the model suggests that body size and temperature combine\nto control the overall rate of evolution through their effects on metabolism.\n"
  },
  {
    "year": 2004,
    "title": "On \"Strong control, conservative point estimation and simultaneous\n  conservative consistency of false discovery rates\": Does a large number of\n  tests obviate confidence intervals of the FDR?",
    "summary": "  A previously proved theorem gives sufficient conditions for an estimator of\nthe false discovery rate (FDR) to conservatively converge to the FDR with\nprobability 1 as the number of hypothesis tests increases, even for small\nsample sizes. It does not follow that several thousand tests ensure that the\nestimator has moderate variance under those conditions. In fact, they can hold\neven if the test statistics have long-range correlations, which yield\nunacceptably wide confidence intervals, as observed in genomic data when there\nare 8 or 16 individuals (microarrays) per group. Thus, informative FDR\nestimation will include some measure of its reliability.\n"
  },
  {
    "year": 2004,
    "title": "Probabilities of spurious connections in gene networks: Application to\n  expression time series",
    "summary": "  Motivation: The reconstruction of gene networks from gene expression\nmicroarrays is gaining popularity as methods improve and as more data become\navailable. The reliability of such networks could be judged by the probability\nthat a connection between genes is spurious, resulting from chance fluctuations\nrather than from a true biological relationship. Results: Unlike the false\ndiscovery rate and positive false discovery rate, the decisive false discovery\nrate (dFDR) is exactly equal to a conditional probability without assuming\nindependence or the randomness of hypothesis truth values. This property is\nuseful not only in the common application to the detection of differential gene\nexpression, but also in determining the probability of a spurious connection in\na reconstructed gene network. Estimators of the dFDR can estimate each of three\nprobabilities: 1. The probability that two genes that appear to be associated\nwith each other lack such association. 2. The probability that a time ordering\nobserved for two associated genes is misleading. 3. The probability that a time\nordering observed for two genes is misleading, either because they are not\nassociated or because they are associated without a lag in time. The first\nprobability applies to both static and dynamic gene networks, and the other two\nonly apply to dynamic gene networks. Availability: Cross-platform software for\nnetwork reconstruction, probability estimation, and plotting is free from\nhttp://www.davidbickel.com as R functions and a Java application.\n"
  },
  {
    "year": 2004,
    "title": "Protein Structure and Evolutionary History Determine Sequence Space\n  Topology",
    "summary": "  Understanding the observed variability in the number of homologs of a gene is\na very important, unsolved problem that has broad implications for research\ninto co-evolution of structure and function, gene duplication, pseudogene\nformation and possibly for emerging diseases. Here we attempt to define and\nelucidate the reasons behind this observed unevenness in sequence space. We\npresent evidence that sequence variability and functional diversity of a gene\nor fold family is influenced by certain quantitative characteristics of the\nprotein structure that reflect potential for sequence plasticity i.e. the\nability to accept mutation without losing thermodynamic stability.\n"
  },
  {
    "year": 2004,
    "title": "A statistical framework for the design of microarray experiments and\n  effective detection of differential gene expression",
    "summary": "  Four reasons why you might wish to read this paper: 1. We have devised a new\nstatistical T test to determine differentially expressed genes (DEG) in the\ncontext of microarray experiments. This statistical test adds a new member to\nthe traditional T-test family. 2. An exact formula for calculating the\ndetection power of this T test is presented, which can also be fairly easily\nmodified to cover the traditional T tests. 3. We have presented an accurate yet\ncomputationally very simple method to estimate the fraction of non-DEGs in a\nset of genes being tested. This method is superior to an existing one which is\ncomputationally much involved. 4. We approach the multiple testing problem from\na fresh angle, and discuss its relation to the classical Bonferroni procedure\nand to the FDR (false discovery rate) approach. This is most useful in the\nanalysis of microarray data, where typically several thousands of genes are\nbeing tested simultaneously.\n"
  },
  {
    "year": 2004,
    "title": "Scaling laws in the functional content of genomes: Fundamental constants\n  of evolution?",
    "summary": "  With the number of fully-sequenced genomes now well over a hundred it has\nbecome possible to start investigating if there are any quantitative\nregularities in the genetic make-up of genomes. In (physics/0307001), I\noriginally showed that the numbers of genes in different functional categories\nscale as power laws in the total number of genes in the genome. In this chapter\nI revisit these results with more recent data and go into considerable more\ndepth regarding the implications of these scaling laws for our understanding of\nthe regulatory design of cells. In addition, I further develop the evolutionary\nmodel first proposed in (physics/0307001), which suggests that the exponents of\nthe observed scaling laws correspond to fundamental constants of the\nevolutionary process. In particular, I put forward an hypothesis for the\napproximately quadratic scaling of regulatory and signal transducing genes with\ngenome size.\n"
  },
  {
    "year": 2004,
    "title": "Conserved network motifs allow protein-protein interaction prediction",
    "summary": "  High-throughput protein interaction detection methods are strongly affected\nby false positive and false negative results. Focused experiments are needed to\ncomplement the large-scale methods by validating previously detected\ninteractions but it is often difficult to decide which proteins to probe as\ninteraction partners. Developing reliable computational methods assisting this\ndecision process is a pressing need in bioinformatics. We show that we can use\nthe conserved properties of the protein network to identify and validate\ninteraction candidates. We apply a number of machine learning algorithms to the\nprotein connectivity information and achieve a surprisingly good overall\nperformance in predicting interacting proteins. Using a 'leave-one-out'\napproach we find average success rates between 20-50% for predicting the\ncorrect interaction partner of a protein. We demonstrate that the success of\nthese methods is based on the presence of conserved interaction motifs within\nthe network. A reference implementation and a table with candidate interacting\npartners for each yeast protein are available at http://www.protsuggest.org\n"
  },
  {
    "year": 2004,
    "title": "Semiconservative replication in the quasispecies model II:\n  Generalization to arbitrary lesion repair probabilities",
    "summary": "  This paper extends the semiconservative quasispecies equations to account for\narbitrary post-replication lesion repair efficiency. Such an extension could be\nan important tool for understanding processes such as cancer development and\nstem cell growth. Starting from the quasispecies dynamics over the space of\ngenomes, we derive an equivalent dynamics over the space of ordered sequence\npairs. From this set of equations, we are able to derive the infinite sequence\nlength form of the dynamics for a class of ``master-genome''-based fitness\nlandscapes. We use these equations to solve for a ``generalized''\nsingle-fitness-peak landscape, where the master genome can sustain a maximum\nnumber of lesions and remain viable. The central pattern that emerges from our\nstudies is that imperfect lesion repair often leads to increased mutational\nrobustness over semiconservative replication with completely efficient lesion\nrepair. The reason for this is that imperfect lesion repair breaks some of the\ncorrelation between the parent and daughter strands, thereby preventing\nreplication errors from destroying the information in the original genome. The\nresult is a delayed error catastrophe over that expected from the original\nsemiconservative quasispecies model. In particular, we show that when only of\nthe strands is necessary for conferring viability, then, when lesion repair is\nturned off, a semiconservatively replicating system becomes an effectively\nconservatively replicating one.\n"
  },
  {
    "year": 2004,
    "title": "Biological Profiling of Gene Groups utilizing Gene Ontology",
    "summary": "  Increasingly used high throughput experimental techniques, like DNA or\nprotein microarrays give as a result groups of interesting, e.g. differentially\nregulated genes which require further biological interpretation. With the\nsystematic functional annotation provided by the Gene Ontology the information\nrequired to automate the interpretation task is now accessible. However, the\ndetermination of statistical significant e.g. molecular functions within these\ngroups is still an open question. In answering this question, multiple testing\nissues must be taken into account to avoid misleading results. Here we present\na statistical framework that tests whether functions, processes or locations\ndescribed in the Gene Ontology are significantly enriched within a group of\ninteresting genes when compared to a reference group. First we define an exact\nanalytical expression for the expected number of false positives that allows us\nto calculate adjusted p-values to control the false discovery rate. Next, we\ndemonstrate and discuss the capabilities of our approach using publicly\navailable microarray data on cell-cycle regulated genes. Further, we analyze\nthe robustness of our framework with respect to the exact gene group\ncomposition and compare the performance with earlier approaches. The software\npackage GOSSIP implements our method and is made freely available at\nhttp://gossip.gene-groups.net/\n"
  },
  {
    "year": 2004,
    "title": "Relational patterns of gene expression via nonmetric multidimensional\n  scaling analysis",
    "summary": "  Motivation:Microarray experiments result in large scale data sets that\nrequire extensive mining and refining to extract useful information. We\ndemonstrate the usefulness of (nonmetric) multidimensional scaling (MDS) method\nin analyzing a large number of genes. Applying MDS to the microarray data is\ncertainly not new, but the existing works are all on small numbers\n  (< 100) of points to be analyzed. We have been developing an efficient novel\nalgorithm for nonmetric multidimensional scaling (nMDS) analysis for very large\ndata sets as a maximally unsupervised data mining device. We wish to\ndemonstrate its usefulness in the context of bioinformatics (unraveling\nrelational patterns among genes from time series data in this paper).\n  Results: The Pearson correlation coefficient with its sign flipped is used to\nmeasure the dissimilarity of the gene activities in transcriptional response of\ncell-cycle-synchronized human fibroblasts to serum [Iyer {\\it et al}., Science\n{\\bf 283}, 83 (1999)]. These dissimilarity data have been analyzed with our\nnMDS algorithm to produce an almost circular relational pattern of the genes.\nThe obtained pattern expresses a temporal order in the data in this example;\nthe temporal expression pattern of the genes rotates along this circular\narrangement and is related to the cell cycle. For the data we analyze in this\npaper we observe the following. If an appropriate preparation procedure is\napplied to the original data set, linear methods such as the principal\ncomponent analysis (PCA) could achieve reasonable results, but without data\npreprocessing linear methods such as PCA cannot achieve a useful picture.\nFurthermore, even with an appropriate data preprocessing, the outcomes of\nlinear procedures are not as clearcut as those by nMDS without preprocessing.\n"
  },
  {
    "year": 2004,
    "title": "Unidentifiable divergence times in rates-across-sites models",
    "summary": "  The rates-across-sites assumption in phylogenetic inference posits that the\nrate matrix governing the Markovian evolution of a character on an edge of the\nputative phylogenetic tree is the product of a character-specific scale factor\nand a rate matrix that is particular to that edge. Thus, evolution follows\nbasically the same process for all characters, except that it occurs faster for\nsome characters than others. To allow estimation of tree topologies and edge\nlengths for such models, it is commonly assumed that the scale factors are not\narbitrary unknown constants, but rather unobserved, independent, identically\ndistributed draws from a member of some parametric family of distributions. A\npopular choice is the gamma family. We consider an example of a clock-like tree\nwith three taxa, one unknown edge length, and a parametric family of scale\nfactor distributions that contain the gamma family. This model has the property\nthat, for a generic choice of unknown edge length and scale factor\ndistribution, there is another edge length and scale factor distribution which\ngenerates data with exactly the same distribution, so that even with infinitely\nmany data it will be typically impossible to make correct inferences about the\nunknown edge length.\n"
  },
  {
    "year": 2004,
    "title": "Statistical properties of DNA sequences revisited: the role of inverse\n  bilateral symmetry in bacterial chromosomes",
    "summary": "  Herein it is shown that in order to study the statistical properties of DNA\nsequences in bacterial chromosomes it suffices to consider only one half of the\nchromosome because they are similar to its corresponding complementary sequence\nin the other half. This is due to the inverse bilateral symmetry of bacterial\nchromosomes. Contrary to the classical result that DNA coding regions of\nbacterial genomes are purely uncorrelated random sequences, here it is shown,\nvia a renormalization group approach, that DNA random fluctuations of single\nbases are modulated by log-periodic variations. Distance series of triplets\ndisplay long-range correlations in each half of the intact chromosome and in\nintronless protein-coding sequences, or both long-range correlations and\nlog-periodic modulations along the whole chromosome. Hence scaling analyses of\ndistance series of DNA sequences have to consider the functional units of\nbacterial chromosomes.\n"
  },
  {
    "year": 2004,
    "title": "Monte Carlo Simulation and Statistical Analysis of Genetic Information\n  Coding",
    "summary": "  The rules that specify how the information contained in DNA codes amino\nacids, is called \"the genetic code\". Using a simplified version of the Penna\nnodel, we are using computer simulations to investigate the importance of the\ngenetic code and the number of amino acids in Nature on population dynamics. We\nfind that the genetic code is not a random pairing of codons to amino acids and\nthe number of amino acids in Nature is an optimum under mutations.\n"
  },
  {
    "year": 2004,
    "title": "SUMO Substrates and Sites Prediction Combining Pattern Recognition and\n  Phylogenetic Conservation",
    "summary": "  Small Ubiquitin-related modifier (SUMO) proteins are widely expressed in\neukaryotic cells, which are reversibly coupled to their substrates by motif\nrecognition, called sumoylation. Two interesting questions are 1) how many\npotential SUMO substrates may be included in mammalian proteomes, such as human\nand mouse, 2) and given a SUMO substrate, can we recognize its sumoylation\nsites? To answer these two questions, previous prediction systems of SUMO\nsubstrates mainly adopted the pattern recognition methods, which could get high\nsensitivity with relatively too many potential false positives. So we use\nphylogenetic conservation between mouse and human to reduce the number of\npotential false positives.\n"
  },
  {
    "year": 2004,
    "title": "Outcome signature genes in breast cancer: is there a unique set?",
    "summary": "  Motivation: Predicting the metastatic potential of primary malignant tissues\nhas direct bearing on choice of therapy. Several microarray studies yielded\ngene sets whose expression profiles successfully predicted survival (Ramaswamy\net al 2003; Sorlie et al 2001; van't Veer et al 2003). Nevertheless, the\noverlap between these gene sets is almost zero. Such small overlaps were\nobserved also in other complex diseases (Lossos et al 2003; Miklos and Maleszka\n2004), and the variables that could account for the differences had evoked a\nwide interest. One of the main open questions in this context is whether the\ndisparity can be attributed only to trivial reasons such as different\ntechnologies, different patients and different types of analysis. Results: To\nanswer this question we concentrated on one single breast cancer dataset, and\nanalyzed it by one single method, the one which was used by van't Veer et al to\nproduce a set of outcome predictive genes. We showed that in fact the resulting\nset of genes is not unique; it is strongly influenced by the subset of patients\nused for gene selection. Many equally predictive lists could have been produced\nfrom the same analysis. Three main properties of the data explain this\nsensitivity: (a) many genes are correlated with survival; (b) the differences\nbetween these correlations are small; (c) the correlations fluctuate strongly\nwhen measured over different subsets of patients. A possible biological\nexplanation for these properties is discussed.\n"
  },
  {
    "year": 2004,
    "title": "The Shift-Match Number and String Matching Probabilities for Binary\n  Sequences",
    "summary": "  We define the ``shift-match number'' for a binary string and we compute the\nprobability of occurrence of a given string as a subsequence in longer strings\nin terms of its shift-match number. We thus prove that the string matching\nprobabilities depend not only on the length of shorter strings, but also on the\nequivalence class of the shorter string determined by its shift-match number.\n"
  },
  {
    "year": 2004,
    "title": "Characterizing large scale base composition structures of genomes",
    "summary": "  Intermittent density fluctuations of nucleotide molecules (adenine, guanine,\ncytosine and thymine) along DNA sequences are studied in the framework of a\nhierarchical structure (HS) model originally proposed for the study of fully\ndeveloped turbulence [She and Leque, Phys. Rev. Lett. 72}, 336 (1994)]. Large\nscale (10^3 < \\ell < 10^5 bp) base density fluctuation is shown to satisfy the\nHS similarity. The derived values of a HS parameter $\\beta$ from a large number\nof genome data (including Bacteria, Archaea, human chromosomes and viruses)\ncharacterize different biological properties such as strand symmetry,\nphylogenetic relations and horizontal gene transfer. It is suggested that the\nHS analysis offers a useful quantitative description for heterogeneity,\nsequence complexity and large scale structures of genomes.\n"
  },
  {
    "year": 2004,
    "title": "Probabilistic pairwise sequence alignment",
    "summary": "  We describe an new algorithm for visualizing an alignment of biological\nsequences according to a probabilistic model of evolution. The resulting data\narray is readily interpreted by the human eye and amenable to digital image\ntechniques. We present examples using mRNA sequences from mouse and rat: three\ncytochromes, Rattus norvegicus Cyp2a1, Cyp2a2, (Medline: 90212624) and Mus\nmusculus Cyp2a12 (Medline: 93249380); and two zinc finger proteins, Mus\nmusculus zfp111 and zfp235 (Medline: 22683274). The underlying evolutionary\nmodel is derived from one proposed by Thorne, Kishino, and Felsenstein and\nimproved by Hein and others. The demonstration implementation aligns two\nsequences using time and memory quadratic in the mean sequence length. The\nalgorithm is extensible, after Hein, to multiple sequences. We mention a basic\nmethod to reduce time and memory demands.\n"
  },
  {
    "year": 2004,
    "title": "A modular Fibonacci sequence in proteins",
    "summary": "  Protein-fragment seqlets typically feature about 10 amino acid residue\npositions that are fixed to within conservative substitutions but usually\nseparated by a number of prescribed gaps with arbitrary residue content. By\nquantifying a general amino acid residue sequence in terms of the associated\ncodon number sequence, we have found a precise modular Fibonacci sequence in a\ncontinuous gap-free 10-residue seqlet with either 3 or 4 conservative amino\nacid substitutions. This modular Fibonacci sequence is genuinely biophysical,\nfor it occurs nine times in the SWISS-Prot/TrEMBL database of natural proteins.\n"
  },
  {
    "year": 2004,
    "title": "Needed for completion of the human genome: hypothesis driven experiments\n  and biologically realistic mathematical models",
    "summary": "  With the sponsorship of ``Fundacio La Caixa'' we met in Barcelona, November\n21st and 22nd, to analyze the reasons why, after the completion of the human\ngenome sequence, the identification all protein coding genes and their variants\nremains a distant goal. Here we report on our discussions and summarize some of\nthe major challenges that need to be overcome in order to complete the human\ngene catalog.\n"
  },
  {
    "year": 2004,
    "title": "Synonymous codon usage and selection on proteins",
    "summary": "  Selection pressures on proteins are usually measured by comparing homologous\nnucleotide sequences (Zuckerkandl and Pauling 1965). Recently we introduced a\nnovel method, termed `volatility', to estimate selection pressures on protein\nsequences from their synonymous codon usage (Plotkin and Dushoff 2003, Plotkin\net al 2004a). Here we provide a theoretical foundation for this approach. We\nderive the expected frequencies of synonymous codons as a function of the\nstrength of selection, the mutation rate, and the effective population size. We\nanalyze the conditions under which we can expect to draw inferences from biased\ncodon usage, and we estimate the time scales required to establish and maintain\nsuch a signal. Our results indicate that, over a broad range of parameters,\nsynonymous codon usage can reliably distinguish between negative selection,\npositive selection, and neutrality. While the power of volatility to detect\nnegative selection depends on the population size, there is no such dependence\nfor the detection of positive selection. Furthermore, we show that phenomena\nsuch as transient hyper-mutators in microbes can improve the power of\nvolatility to detect negative selection, even when the typical observed neutral\nsite heterozygosity is low.\n"
  },
  {
    "year": 2004,
    "title": "Two Major Paths of Gene-Duplicates Evolution",
    "summary": "  Evolving genomes increase a number of their genes by gene duplications. To\nescape degradation in a functionless pseudogene, any gene duplicate needs to be\nguarded by negative (purifying) selection from otherwise inevitable fixation of\ndegenerative mutations. In the present study we focus on the evolutionary stage\nat which new duplicates come under such surveillance.\n  Our analyses of several genomes indicate that in about 10% gene pairs,\nselection begins to guard a new gene copy very soon after a duplication event\nwhereas the vast majority (90%) of extra genes remain redundant and\nunrecognised by selection. Such duplicates accumulate all mutations (including\ndegenerative) in neutral fashion and are actually destined to become\npseudogenes. We revealed this \"two-stream\" evolutionary pattern by the analysis\nof mutations in 2nd versus 3rd codon positions but not by the routinely used\nratio of amino acid replacements (R) versus silent substitutions (S), i.e. the\n'2nd vs. 3rd' metric proved to be more resolving than the traditional 'R vs. S'\none for distinguishing neutrally evolving future pseudogenes from their\nfunctional counterparts controlled by negative selection.\n  In gene databases for large genomes, hundreds of future pseudogenes are\nannotated as functional genes because they do look like intact and valuable by\nstandard criteria, including even active transcription and translation.\nApparently, these \"pseudogenes-to-be\" over-cloud and mimic those very\ninfrequent gene duplicates with increased sequence evolution rates driven by\npositive selection.\n"
  },
  {
    "year": 2004,
    "title": "Multiple, weak hits confuse complex systems: A transcriptional\n  regulatory network as an example",
    "summary": "  Robust systems, like the molecular networks of living cells are often\nresistant to single hits such as those caused by high-specificity drugs. Here\nwe show that partial weakening of the Escherichia coli and Saccharomyces\ncerevisiae transcriptional regulatory networks at a small number (3-5) selected\nnodes can have a greater impact than the complete elimination of a single\nselected node. In both cases, the targeted nodes have the greatest possible\nimpact; still the results suggest that in some cases broad specificity\ncompounds or multitarget drug therapies may be more effective than individual\nhigh-affinity, high-specificity ones. Multiple but partial attacks mimic well a\nnumber of in vivo scenarios and may be useful in the efficient modification of\nother complex systems.\n"
  },
  {
    "year": 2004,
    "title": "An Unusual 500,000 Bases Long Oscillation of Guanine and Cytosine\n  Content in Human Chromosome 21",
    "summary": "  An oscillation with a period of around 500 kb in guanine and cytosine content\n(GC%) is observed in the DNA sequence of human chromosome 21. This oscillation\nis localized in the rightmost one-eighth region of the chromosome, from 43.5 Mb\nto 46.5 Mb. Five cycles of oscillation are observed in this region with six\nGC-rich peaks and five GC-poor valleys. The GC-poor valleys comprise regions\nwith low density of CpG islands and, alternating between the two DNA strands,\nlow gene density regions. Consequently, the long-range oscillation of GC%\nresult in spacing patterns of both CpG island density, and to a lesser extent,\ngene densities.\n"
  },
  {
    "year": 2004,
    "title": "Universal 1/f noise, cross-overs of scaling exponents, and chromosome\n  specific patterns of GC content in DNA sequences of the human genome",
    "summary": "  Spatial fluctuations of guanine and cytosine base content (GC%) are studied\nby spectral analysis for the complete set of human genomic DNA sequences. We\nfind that (i) the 1/f^alpha decay is universally observed in the power spectra\nof all twenty-four chromosomes, and that (ii) the exponent alpha \\approx 1\nextends to about 10^7 bases, one order of magnitude longer than what has\npreviously been observed. We further find that (iii) almost all human\nchromosomes exhibit a cross-over from alpha_1 \\approx 1 (1/f^alpha_1) at lower\nfrequency to alpha_2 < 1 (1/f^alpha_2) at higher frequency, typically occurring\nat around 30,000--100,000 bases, while (iv) the cross-over in this frequency\nrange is virtually absent in human chromosome 22. In addition to the universal\n1/f^alpha noise in power spectra, we find (v) several lines of evidence for\nchromosome-specific correlation structures, including a 500,000 bases long\noscillation in human chromosome 21. The universal 1/f^alpha spectrum in human\ngenome is further substantiated by a resistance to variance reduction in\nguanine and cytosine content when the window size is increased.\n"
  },
  {
    "year": 2004,
    "title": "Spectral Analysis of Guanine and Cytosine Fluctuations of Mouse Genomic\n  DNA",
    "summary": "  We study global fluctuations of the guanine and cytosine base content (GC%)\nin mouse genomic DNA using spectral analyses. Power spectra S(f) of GC%\nfluctuations in all nineteen autosomal and two sex chromosomes are observed to\nhave the universal functional form S(f) \\sim 1/f^alpha (alpha \\approx 1) over\nseveral orders of magnitude in the frequency range 10^-7< f < 10^-5 cycle/base,\ncorresponding to long-ranging GC% correlations at distances between 100 kb and\n10 Mb. S(f) for higher frequencies (f > 10^-5 cycle/base) shows a flattened\npower-law function with alpha < 1 across all twenty-one chromosomes. The\nsubstitution of about 38% interspersed repeats does not affect the functional\nform of S(f), indicating that these are not predominantly responsible for the\nlong-ranged multi-scale GC% fluctuations in mammalian genomes. Several\nbiological implications of the large-scale GC% fluctuation are discussed,\nincluding neutral evolutionary history by DNA duplication, chromosomal bands,\nspatial distribution of transcription units (genes), replication timing, and\nrecombination hot spots.\n"
  },
  {
    "year": 2004,
    "title": "Expected number of inversions after a sequence of random adjacent\n  transpositions",
    "summary": "  In the evolution of a genome, the gene sequence is sometimes rearranged, for\nexample by transposition of two adjacent gene blocks. In biocombinatorics, one\ntries to reconstruct these rearrangement incidents from the resulting\npermutation. It seems that the algorithms used are too effective and find a\nshorter path than the real one. For the simplified case of adjacent\ntranspositions, we give expressions for the expected number of inversions after\nt random moves. This average can be much smaller than t, a fact that has\nlargely been neglected so far.\n"
  },
  {
    "year": 2004,
    "title": "Gene splice sites correlate with nucleosome positions",
    "summary": "  Gene sequences in the vicinity of splice sites are found to possess\ndinucleotide periodicities, especially RR and YY, with the period close to the\npitch of nucleosome DNA. This confirms previously reported finding about\npreferential positioning of splice junctions within the nucleosomes. The RR and\nYY dinucleotides oscillate counterphase, i.e., their respective preferred\npositions are shifted about half-period one from another, as it was observed\nearlier for AA and TT dinucleotides. Species specificity of nucleosome\npositioning DNA pattern is indicated by predominant use of the periodical\nGG(CC) dinucleotides in human and mouse genes, as opposed to predominant AA(TT)\ndinucleotides in Arabidopsis and C.elegans.\n  Keywords: chromatin; gene splicing; intron; exon; dinucleotide; periodical\npattern\n"
  },
  {
    "year": 2004,
    "title": "Systematic identification of abundant A-to-I editing sites in the human\n  transcriptome",
    "summary": "  RNA editing by members of the double-stranded RNA-specific ADAR family leads\nto site-specific conversion of adenosine to inosine (A-to-I) in precursor\nmessenger RNAs. Editing by ADARs is believed to occur in all metazoa, and is\nessential for mammalian development. Currently, only a limited number of human\nADAR substrates are known, while indirect evidence suggests a substantial\nfraction of all pre-mRNAs being affected. Here we describe a computational\nsearch for ADAR editing sites in the human transcriptome, using millions of\navailable expressed sequences. 12,723 A-to-I editing sites were mapped in 1,637\ndifferent genes, with an estimated accuracy of 95%, raising the number of known\nediting sites by two orders of magnitude. We experimentally validated our\nmethod by verifying the occurrence of editing in 26 novel substrates. A-to-I\nediting in humans primarily occurs in non-coding regions of the RNA, typically\nin Alu repeats. Analysis of the large set of editing sites indicates the role\nof editing in controlling dsRNA stability.\n"
  },
  {
    "year": 2004,
    "title": "Subtree power analysis finds optimal species for comparative genomics",
    "summary": "  Sequence comparison across multiple organisms aids in the detection of\nregions under selection. However, resource limitations require a prioritization\nof genomes to be sequenced. This prioritization should be grounded in two\nconsiderations: the lineal scope encompassing the biological phenomena of\ninterest, and the optimal species within that scope for detecting functional\nelements. We introduce a statistical framework for optimal species subset\nselection, based on maximizing power to detect conserved sites. In a study of\nvertebrate species, we show that the optimal species subset is not in general\nthe most evolutionarily diverged subset. Our results suggest that marsupials\nare prime sequencing candidates.\n"
  },
  {
    "year": 2004,
    "title": "A New DNA Sequences Vector Space on a Genetic Code Galois Field",
    "summary": "  A new n-dimensional vector space of the DNA sequences on the Galois field of\nthe 64 codons (GF(64)) is proposed. In this vector space gene mutations can be\nconsidered linear transformations or translations of the wild type gene. In\nparticular, the set of translations that preserve the chemical type of the\nthird base position in the codon is a subgroup which describes the most\nfrequent mutations observed in mutational variants of four genes: human\nphenylalanine hydroxylase (PAH), human beta globin (HBG), HIV-1 Protease (HIVP)\nand HIV-1 Reverse transcriptase (HIVRT). Furthermore, an inner pseudo-product\ndefined between codons tends to have a positive value when the codons code to\nsimilar amino acids and a negative value when the codons code to amino acids\nwith extreme hydrophobic properties. Consequently, it is found that the inner\npseudo-product between the wild type and the mutant codons tends to have a\npositive value in the mutational variants of the genes: PAH, HBG, HIVP, HIVRT.\n"
  },
  {
    "year": 2004,
    "title": "Gene Algebra from a Genetic Code Algebraic Structure",
    "summary": "  The biological distinction between the base positions in the codon, the\nchemical types of bases (purine and pyrimidine) and their hydrogen bond number\nhave been the most relevant codon properties used in the genetic code analysis.\nNow, these properties have allowed us to build a Genetic Code ring isomorphic\nto the ring (Z64, +,*) of the integer module 64. On the Z64-algebra of the set\nof 64^N codon sequences of length N, gene mutations are described by means of\nendomorphisms F: (Z64)^N->(Z64)^N. Endomorphisms and automorphisms helped us\ndescribe the gene mutation pathways. For instance, 77.7% mutations in 749 HIV\nprotease gene sequences correspond to unique diagonal endomorphisms of the wild\ntype strain HXB2. In particular, most of the reported mutations that confer\ndrug resistance to the HIV protease gene correspond to diagonal automorphisms\nof the wild type. What is more, in the human beta-globin gene a similar\nsituation appears where most of the single codon mutations correspond to\nautomorphisms. Hence, in the analyses of molecular evolution process on the DNA\nsequence set of length N, the Z64-algebra will help us explain the quantitative\nrelationships between genes.\n"
  },
  {
    "year": 2004,
    "title": "The Genetic Code Boolean Lattice",
    "summary": "  The algebraic structures of the genetic code are most important to obtain\nadditional information about the semantic code and its applications. In this\npaper we define two dual Boolean codon lattices of the genetic code using\nhydrogen bond numbers and the chemical types of bases: purines and pyrimidines.\nThe Boolean lattices reflect the role of hydrophobicity in the distribution of\ncodon assignments to each amino acid. Particularly, the symmetric images of\ncodons with adenine as second base coding to hydrophilic amino acids are always\ncodons with uracil as second base coding to hydrophobic amino acids as they\nrepresented in the Hasse diagrams. The Hamming distance between two codons in\nthe Hasse diagram reflects the different hydrophobicities between their\nrespective coded amino acids. Our experiments have demonstrated a small Hamming\ndistance to the wild type HXB2 of almost all the drug-resistant reported\nmutations in HIV protease gene. The human beta-globin mutant genes have also\nexhibited similar results. Our research suggests that the Hamming distance\nbetween two genes in the molecular evolution process have a minimal value.\n"
  },
  {
    "year": 2004,
    "title": "Divergence and Shannon information in genomes",
    "summary": "  Shannon information (SI) and its special case, divergence, are defined for a\nDNA sequence in terms of probabilities of chemical words in the sequence and\nare computed for a set of complete genomes highly diverse in length and\ncomposition. We find the following: SI (but not divergence) is inversely\nproportional to sequence length for a random sequence but is length-independent\nfor genomes; the genomic SI is always greater and, for shorter words and longer\nsequences, hundreds to thousands times greater than the SI in a random sequence\nwhose length and composition match those of the genome; genomic SIs appear to\nhave word-length dependent universal values. The universality is inferred to be\nan evolution footprint of a universal mode for genome growth.\n"
  },
  {
    "year": 2004,
    "title": "Is abundant A-to-I RNA editing primate-specific?",
    "summary": "  A-To-I RNA editing is common to all eukaryotes, associated with various\nneurological functions. Recently, A-to-I editing was found to occur abundantly\nin the human transcriptome. Here we show that the frequency of A-to-I editing\nin humans is at least an order of magnitude higher as that of mouse, rat,\nchicken or fly. The extraordinary frequency of RNA editing in human is\nexplained by the dominance of the primate-specific Alu element in the human\ntranscriptome, which increases the number of double-stranded RNA substrates.\n"
  },
  {
    "year": 2004,
    "title": "Modeling Genetic Networks from Clonal Analysis",
    "summary": "  In this report a systematic approach is used to determine the approximate\ngenetic network and robust dependencies underlying differentiation. The data\nconsidered is in the form of a binary matrix and represent the expression of\nthe nine genes across the ninety-nine colonies. The report is divided into two\nparts: the first part identifies significant pair-wise dependencies from the\ngiven binary matrix using linear correlation and mutual information. A new\nmethod is proposed to determine statistically significant dependencies\nestimated using the mutual information measure. In the second, a Bayesian\napproach is used to obtain an approximate description (equivalence class) of\nnetwork structures. The robustness of linear correlation, mutual information\nand the equivalence class of networks is investigated with perturbation and\ndecreasing colony number. Perturbation of the data was achieved by generating\nbootstrap realizations. The results are refined with biological knowledge. It\nwas found that certain dependencies in the network are immune to perturbation\nand decreasing colony number and may represent robust features, inherent in the\ndifferentiation program of osteoblast progenitor cells. The methods to be\ndiscussed are generic in nature and not restricted to the experimental paradigm\naddressed in this study.\n"
  },
  {
    "year": 2005,
    "title": "The posterior-Viterbi: a new decoding algorithm for hidden Markov models",
    "summary": "  Background: Hidden Markov models (HMM) are powerful machine learning tools\nsuccessfully applied to problems of computational Molecular Biology. In a\npredictive task, the HMM is endowed with a decoding algorithm in order to\nassign the most probable state path, and in turn the class labeling, to an\nunknown sequence. The Viterbi and the posterior decoding algorithms are the\nmost common. The former is very efficient when one path dominates, while the\nlatter, even though does not guarantee to preserve the automaton grammar, is\nmore effective when several concurring paths have similar probabilities. A\nthird good alternative is 1-best, which was shown to perform equal or better\nthan Viterbi. Results: In this paper we introduce the posterior-Viterbi (PV) a\nnew decoding which combines the posterior and Viterbi algorithms. PV is a two\nstep process: first the posterior probability of each state is computed and\nthen the best posterior allowed path through the model is evaluated by a\nViterbi algorithm.\n  Conclusions: We show that PV decoding performs better than other algorithms\nfirst on toy models and then on the computational biological problem of the\nprediction of the topology of beta-barrel membrane proteins.\n"
  },
  {
    "year": 2005,
    "title": "A Solvable Sequence Evolution Model and Genomic Correlations",
    "summary": "  We study a minimal model for genome evolution whose elementary processes are\nsingle site mutation, duplication and deletion of sequence regions and\ninsertion of random segments. These processes are found to generate long-range\ncorrelations in the composition of letters as long as the sequence length is\ngrowing, i.e., the combined rates of duplications and insertions are higher\nthan the deletion rate. For constant sequence length, on the other hand, all\ninitial correlations decay exponentially. These results are obtained\nanalytically and by simulations. They are compared with the long-range\ncorrelations observed in genomic DNA, and the implications for genome evolution\nare discussed.\n"
  },
  {
    "year": 2005,
    "title": "Information Capacity of Biological Macromoleculae Reloaded",
    "summary": "  Information capacity of a symbol sequence is a measure of the unexpectedness\nof a continuation of given string of symbols. Continuation of a string is\ndetermined through the maximum entropy of the reconstructed frequency\ndictionary; the capacity, in turn, is determined through the calculation of\nmutual entropy of a real frequency dictionary of a sequence with respect to the\nreconstructed one. The capacity does not depend on the length of strings in a\ndictionary. The capacity calculated for various genomes exhibits a multi-minima\npattern reflecting an order observed within a sequence.\n"
  },
  {
    "year": 2005,
    "title": "A New Simulated Annealing Algorithm for the Multiple Sequence Alignment\n  Problem: The approach of Polymers in a Random Media",
    "summary": "  We proposed a probabilistic algorithm to solve the Multiple Sequence\nAlignment problem. The algorithm is a Simulated Annealing (SA) that exploits\nthe representation of the Multiple Alignment between $D$ sequences as a\ndirected polymer in $D$ dimensions. Within this representation we can easily\ntrack the evolution in the configuration space of the alignment through local\nmoves of low computational cost. At variance with other probabilistic\nalgorithms proposed to solve this problem, our approach allows for the creation\nand deletion of gaps without extra computational cost. The algorithm was tested\naligning proteins from the kinases family. When D=3 the results are consistent\nwith those obtained using a complete algorithm. For $D>3$ where the complete\nalgorithm fails, we show that our algorithm still converges to reasonable\nalignments. Moreover, we study the space of solutions obtained and show that\ndepending on the number of sequences aligned the solutions are organized in\ndifferent ways, suggesting a possible source of errors for progressive\nalgorithms.\n"
  },
  {
    "year": 2005,
    "title": "Identification and Measurement of Neighbor Dependent Nucleotide\n  Substitution Processes",
    "summary": "  The presence of neighbor dependencies generated a specific pattern of\ndinucleotide frequencies in all organisms. Especially, the\nCpG-methylation-deamination process is the predominant substitution process in\nvertebrates and needs to be incorporated into a more realistic model for\nnucleotide substitutions. Based on a general framework of nucleotide\nsubstitutions we develop a method that is able to identify the most relevant\nneighbor dependent substitution processes, measure their strength, and judge\ntheir importance to be included into the modeling. Starting from a model for\nneighbor independent nucleotide substitution we successively add neighbor\ndependent substitution processes in the order of their ability to increase the\nlikelihood of the model describing given data. The analysis of neighbor\ndependent nucleotide substitutions in human, zebrafish and fruit fly is\npresented. A web server to perform the presented analysis is publicly\navailable.\n"
  },
  {
    "year": 2005,
    "title": "Substantial regional variation in substitution rates in the human\n  genome: importance of GC content, gene density and telomere-specific effects",
    "summary": "  This study presents the first global, 1 Mbp level analysis of patterns of\nnucleotide substitutions along the human lineage. The study is based on the\nanalysis of a large amount of repetitive elements deposited into the human\ngenome since the mammalian radiation, yielding a number of results that would\nhave been difficult to obtain using the more conventional comparative method of\nanalysis. This analysis revealed substantial and consistent variability of\nrates of substitution, with the variability ranging up to 2-fold among\ndifferent regions. The rates of substitutions of C or G nucleotides with A or T\nnucleotides vary much more sharply than the reverse rates suggesting that much\nof that variation is due to differences in mutation rates rather than in the\nprobabilities of fixation of C/G vs. A/T nucleotides across the genome. For all\ntypes of substitution we observe substantially more hotspots than coldspots,\nwith hotspots showing substantial clustering over tens of Mbp's. Our analysis\nrevealed that GC-content of surrounding sequences is the best predictor of the\nrates of substitution. The pattern of substitution appears very different near\ntelomeres compared to the rest of the genome and cannot be explained by the\ngenome-wide correlations of the substitution rates with GC content or exon\ndensity. The telomere pattern of substitution is consistent with natural\nselection or biased gene conversion acting to increase the GC-content of the\nsequences that are within 10-15 Mbp away from the telomere.\n"
  },
  {
    "year": 2005,
    "title": "Are scale-free regulatory networks larger than random ones?",
    "summary": "  Network of packages with regulatory interactions (dependences and conflicts)\nfrom Debian GNU/Linux operating system is compiled and used as analogy of a\ngene regulatory network. Using a trace-back algorithm we assembly networks from\nthe potential pool of packages for both scale-free and exponential topology\nfrom real and a null model data, respectively. We calculate the maximum number\nof packages that can be functionally installed in the system (i.e., the active\nnetwork size). We show that scale-free regulatory networks allow a larger\nactive network size than random ones. Small genomes with scale-free regulatory\ntopology could allow much more functionality than large genomes with an\nexponential one, with implications on its dynamics, robustness and evolution.\n"
  },
  {
    "year": 2005,
    "title": "Molecular Phylogenetic Analyses and Real Life Data",
    "summary": "  In molecular phylogeny, relationships among organisms are reconstructed using\nDNA or protein sequences and are displayed as trees. A linear increase in the\nnumber of sequences results in an exponential increase of possible trees. Thus,\ninferring trees from molecular data was shown to be NP-hard. This causes\nproblems, if large data sets are used. This review gives an introduction to\nmolecular phylogenetic methods and to the problems biologists are facing in\nmolecular phylogenetic analyses.\n"
  },
  {
    "year": 2005,
    "title": "Global divergence of microbial genome sequences mediated by propagating\n  fronts",
    "summary": "  We model the competition between recombination and point mutation in\nmicrobial genomes, and present evidence for two distinct phases, one uniform,\nthe other genetically diverse. Depending on the specifics of homologous\nrecombination, we find that global sequence divergence can be mediated by\nfronts propagating along the genome, whose characteristic signature on genome\nstructure is elucidated, and apparently observed in closely-related {\\it\nBacillus} strains. Front propagation provides an emergent, generic mechanism\nfor microbial \"speciation\", and suggests a classification of microorganisms on\nthe basis of their propensity to support propagating fronts.\n"
  },
  {
    "year": 2005,
    "title": "A Novel Lie Algebra of the Genetic Code over the Galois Field of Four\n  DNA Bases",
    "summary": "  By starting from the four DNA bases order in the Boolean lattice, a novel Lie\nAlgebra of the genetic code is proposed. Here, the principal partitions of the\ngenetic code table were obtained as equivalent classes of quotient subspaces of\nthe genetic code vector space over the Galois field of the four DNA bases. The\nnew algebraic structure shows strong connections among algebraic relationships,\ncodon assignment and physicochemical properties of amino acids. Moreover, a\ndistance function defined between codons in the Lie algebra was demonstrated to\nhave a linear behavior respect to physical variables such as the mean of amino\nacids energy in proteins. It was also noticed that the distance between wild\ntype and mutant codons approach smaller values in mutational variants of four\ngenes, i.e, human phenylalanine hydroxylase, human beta-globin, HIV-1 protease\nand HIV-1 reverse transcriptase. These results strongly suggest that\ndeterministic rules in genetic code origin must be involved.\n"
  },
  {
    "year": 2005,
    "title": "Statistical analysis of simple repeats in the human genome",
    "summary": "  The human genome contains repetitive DNA at different level of sequence\nlength, number and dispersion. Highly repetitive DNA is particularly rich in\nhomo-- and di--nucleotide repeats, while middle repetitive DNA is rich of\nfamilies of interspersed, mobile elements hundreds of base pairs (bp) long,\namong which the Alu families. A link between homo- and di-polymeric tracts and\nmobile elements has been recently highlighted. In particular, the mobility of\nAlu repeats, which form 10% of the human genome, has been correlated with the\nlength of poly(A) tracts located at one end of the Alu. These tracts have a\nrigid and non-bendable structure and have an inhibitory effect on nucleosomes,\nwhich normally compact the DNA. We performed a statistical analysis of the\ngenome-wide distribution of lengths and inter--tract separations of poly(X) and\npoly(XY) tracts in the human genome. Our study shows that in humans the length\ndistributions of these sequences reflect the dynamics of their expansion and\nDNA replication. By means of general tools from linguistics, we show that the\nlatter play the role of highly-significant content-bearing terms in the DNA\ntext. Furthermore, we find that such tracts are positioned in a non-random\nfashion, with an apparent periodicity of 150 bases. This allows us to extend\nthe link between repetitive, highly mobile elements such as Alus and\nlow-complexity words in human DNA. More precisely, we show that Alus are\nsources of poly(X) tracts, which in turn affect in a subtle way the combination\nand diversification of gene expression and the fixation of multigene families.\n"
  },
  {
    "year": 2005,
    "title": "Evolutionarily conserved human targets of adenosine to inosine RNA\n  editing",
    "summary": "  A-to-I RNA editing by ADARs is a post-transcriptional mechanism for expanding\nthe proteomic repertoire. Genetic recoding by editing was so far observed for\nonly a few mammalian RNAs that are predominantly expressed in nervous tissues.\nHowever, as these editing targets fail to explain the broad and severe\nphenotypes of ADAR1 knockout mice, additional targets for editing by ADARs were\nalways expected. Using comparative genomics and expressed sequence analysis, we\nidentified and experimentally verified four additional candidate human\nsubstrates for ADAR-mediated editing: FLNA, BLCAP, CYFIP2 and IGFBP7.\nAdditionally, editing of three of these substrates was verified in the mouse\nwhile two of them were validated in chicken. Interestingly, none of these\nsubstrates encodes a receptor protein but two of them are strongly expressed in\nthe CNS and seem important for proper nervous system function. The editing\npattern observed suggests that some of the affected proteins might have altered\nphysiological properties leaving the possibility that they can be related to\nthe phenotypes of ADAR1 knockout mice.\n"
  },
  {
    "year": 2005,
    "title": "New symmetry in nucleotide sequences",
    "summary": "  Information valuable words are the strings with the significant deviation of\nreal frequency from the expected one. The expected frequency is determined\nthrough the maximum entropy principle of the reconstructed (extended) frequency\ndictionary of strings composed from the shorter words. The information valuable\nwords are found to be the complementary palindromes: they are read equally in\nopposite directions, if nucleotides are changed for the complementary ones (A\n<--> T; C <--> G) in one of them. Some properties of such symmetric words are\ndiscussed.\n"
  },
  {
    "year": 2005,
    "title": "Principles in the Evolution of Metabolic Networks",
    "summary": "  Understanding design principles of complex cellular organization is one of\nthe major challenges in biology. Recent analysis of the large-scale cellular\norganization has revealed the scale-free nature and robustness of metabolic and\nprotein networks. However, the underlying evolutional process that creates such\na cellular organization is not fully elucidated. To approach this problem, we\nanalyzed the metabolic networks of 126 organisms, whose draft or complete\ngenome sequences have been published. This analysis has revealed that the\nevolutional process of metabolic networks follows the same and surprisingly\nsimple principles in Archaea, Bacteria and Eukaryotes; where highly linked\nmetabolites change their chemical links more dynamically than less linked\nmetabolites. Here we demonstrate that this rich-travel-more mechanism rather\nthan the previously proposed rich-get-richer mechanism can generate the\nobserved scale-free organization of metabolic networks. These findings\nillustrate universal principles in evolution of metabolic networks and suggest\nmarked flexibility of metabolic network throughout evolution.\n"
  },
  {
    "year": 2005,
    "title": "Confidence intervals for nonhomogeneous branching processes and\n  polymerase chain reactions",
    "summary": "  We extend in two directions our previous results about the sampling and the\nempirical measures of immortal branching Markov processes. Direct applications\nto molecular biology are rigorous estimates of the mutation rates of polymerase\nchain reactions from uniform samples of the population after the reaction.\nFirst, we consider nonhomogeneous processes, which are more adapted to real\nreactions. Second, recalling that the first moment estimator is analytically\nknown only in the infinite population limit, we provide rigorous confidence\nintervals for this estimator that are valid for any finite population. Our\nbounds are explicit, nonasymptotic and valid for a wide class of nonhomogeneous\nbranching Markov processes that we describe in detail. In the setting of\npolymerase chain reactions, our results imply that enlarging the size of the\nsample becomes useless for surprisingly small sizes. Establishing confidence\nintervals requires precise estimates of the second moment of random samples.\nThe proof of these estimates is more involved than the proofs that allowed us,\nin a previous paper, to deal with the first moment. On the other hand, our\nmethod uses various, seemingly new, monotonicity properties of the harmonic\nmoments of sums of exchangeable random variables.\n"
  },
  {
    "year": 2005,
    "title": "Mimivirus Gene Promoters Exhibit an Unprecedented Conservation among all\n  Eukaryotes",
    "summary": "  The initial analysis of the recently sequenced genome of Acanthamoeba\npolyphaga Mimivirus, the largest known double-stranded DNA virus, predicted a\nproteome of size and complexity more akin to small parasitic bacteria than to\nother nucleo-cytoplasmic large DNA viruses, and identified numerous functions\nnever before described in a virus. It has been proposed that the Mimivirus\nlineage could have emerged before the individualization of cellular organisms\nfrom the 3 domains of life. An exhaustive in silico analysis of the non-coding\nmoiety of all known viral genomes, now uncovers the unprecedented perfect\nconservation of a AAAATTGA motif in close to 50% of the Mimivirus genes. This\nmotif preferentially occurs in genes transcribed from the predicted leading\nstrand and is associated with functions required early in the viral infectious\ncycle, such as transcription and protein translation. A comparison with the\nknown promoter of unicellular eukaryotes, in particular amoebal protists,\nstrongly suggests that the AAAATTGA motif is the structural equivalent of the\nTATA box core promoter element. This element is specific to the Mimivirus\nlineage, and may correspond to an ancestral promoter structure predating the\nradiation of the eukaryotic kingdoms. This unprecedented conservation of core\npromoter regions is another exceptional features of Mimivirus, that again\nraises the question of its evolutionary origin.\n"
  },
  {
    "year": 2005,
    "title": "PCA and K-Means decipher genome",
    "summary": "  In this paper, we aim to give a tutorial for undergraduate students studying\nstatistical methods and/or bioinformatics. The students will learn how data\nvisualization can help in genomic sequence analysis. Students start with a\nfragment of genetic text of a bacterial genome and analyze its structure. By\nmeans of principal component analysis they ``discover'' that the information in\nthe genome is encoded by non-overlapping triplets. Next, they learn how to find\ngene positions. This exercise on PCA and K-Means clustering enables active\nstudy of the basic bioinformatics notions. Appendix 1 contains program listings\nthat go along with this exercise. Appendix 2 includes 2D PCA plots of triplet\nusage in moving frame for a series of bacterial genomes from GC-poor to GC-rich\nones. Animated 3D PCA plots are attached as separate gif files. Topology\n(cluster structure) and geometry (mutual positions of clusters) of these plots\ndepends clearly on GC-content.\n"
  },
  {
    "year": 2005,
    "title": "tRNA-alike in Nanoarchaeum equitans ?",
    "summary": "  The recent algorithm for five split tRNA-genes in N.equitans is new . It\nlocates missing tRNA-trp, tRNA-imet, tRNA-glu and tRNA-his . But the split\ntRNA-trp(CCA) solution is anomalous ; the tRNA-imet lacks cognition elements\nfor aminoacylation . In view therefore we present here alternate non-split\ncomposite solutions for tRNA-trp, tRNA-imet, tRNA-glu and tRNA-his .\n"
  },
  {
    "year": 2005,
    "title": "Limitations of Markov chain Monte Carlo algorithms for Bayesian\n  Inference of phylogeny",
    "summary": "  Markov chain Monte Carlo algorithms play a key role in the Bayesian approach\nto phylogenetic inference. In this paper, we present the first theoretical work\nanalyzing the rate of convergence of several Markov chains widely used in\nphylogenetic inference. We analyze simple, realistic examples where these\nMarkov chains fail to converge quickly. In particular, the data studied are\ngenerated from a pair of trees, under a standard evolutionary model. We prove\nthat many of the popular Markov chains take exponentially long to reach their\nstationary distribution. Our construction is pertinent since it is well known\nthat phylogenetic trees for genes may differ within a single organism. Our\nresults shed a cautionary light on phylogenetic analysis using Bayesian\ninference and highlight future directions for potential theoretical work.\n"
  },
  {
    "year": 2005,
    "title": "Ab initio identification of putative human transcription factor binding\n  sites by comparative genomics",
    "summary": "  We discuss a simple and powerful approach for the ab initio identification of\ncis-regulatory motifs involved in transcriptional regulation. The method we\npresent integrates several elements: human-mouse comparison, statistical\nanalysis of genomic sequences and the concept of coregulation. We apply it to a\ncomplete scan of the human genome. By using the catalogue of conserved upstream\nsequences collected in the CORG database we construct sets of genes sharing the\nsame overrepresented motif (short DNA sequence) in their upstream regions both\nin human and in mouse. We perform this construction for all possible motifs\nfrom 5 to 8 nucleotides in length and then filter the resulting sets looking\nfor two types of evidence of coregulation: first, we analyze the Gene Ontology\nannotation of the genes in the set, searching for statistically significant\ncommon annotations; second, we analyze the expression profiles of the genes in\nthe set as measured by microarray experiments, searching for evidence of\ncoexpression. The sets which pass one or both filters are conjectured to\ncontain a significant fraction of coregulated genes, and the upstream motifs\ncharacterizing the sets are thus good candidates to be the binding sites of the\nTF's involved in such regulation. In this way we find various known motifs and\nalso some new candidate binding sites.\n"
  },
  {
    "year": 2005,
    "title": "On the Complexity of Several Haplotyping Problems",
    "summary": "  In this paper we present a collection of results pertaining to haplotyping.\nThe first set of results concerns the combinatorial problem of reconstructing\nhaplotypes from incomplete and/or imperfectly sequenced haplotype data. More\nspecifically, we show that an interesting, restricted case of Minimum Error\nCorrection (MEC) is NP-hard, point out problems in earlier claims about a\nrelated problem, and present a polynomial-time algorithm for the ungapped case\nof Longest Haplotype Reconstruction (LHR). Secondly, we present a polynomial\ntime algorithm for the problem of resolving genotype data using as few\nhaplotypes as possible (the Pure Parsimony Haplotyping Problem, PPH) where each\ngenotype has at most two ambiguous positions, thus solving an open problem\nposed by Lancia et al in \"Haplotyping Populations by Pure Parsimony: Complexity\nof Exact and Approximation Algorithms.\"\n"
  },
  {
    "year": 2005,
    "title": "Prediction of transcription factor binding to DNA using rule induction\n  methods",
    "summary": "  The transcription of DNA into mRNA is initiated and aided by a number of\ntranscription factors (TFs), proteins with DNA-binding regions that attach\nthemselves to binding sites in the DNA (transcription factor binding sites,\nTFBSs). As it has become apparent that both TFs and TFBSs are highly variable,\ntools are needed to quantify the strength of the interaction resulting from a\ncertain TF variant binding to a certain TFBS. We used a simple way to predict\ninteractions between protein and DNA: given experimental cases from the\nliterature where the interaction strength between two sequences has been\nquantified, we created training vectors for rule induction by regarding each\namino acid and nucleotide position as a single feature in the example vector.\nThe resulting interaction strength was used as the target class or value. These\ntraining vectors were then used to build a rule induction model. We applied the\nrule induction method to three protein families - transcription factors from\nthe NF-kappaB, the early-growth-response (EGR), and the paired domain groups -\nand their corresponding DNA targets. These three prediction problems increase\nin complexity with regard to model building, and thus gave us a good range for\nvalidation. The main focus of the study was the most complex problem: paired\ndomain-DNA binding. For this problem, we also found sequence/binding strength\ncorrelations using measures from information theory. Prediction results were\nuniformly good: the rule induction approach was able to correctly order all of\nnine unseen examples for one NF-kappaB protein, achieved a correlation\ncoefficient of 0.52 on unseen (and noisy) examples in the EGR case, and reached\na classification accuracy of 69.7% for the Paired domain as evaluated by\ncross-validation.\n"
  },
  {
    "year": 2005,
    "title": "Gene & Genome Duplication in Acanthamoeba Polyphaga Mimivirus",
    "summary": "  Gene duplication is key to molecular evolution in all three domains of life\nand may be the first step in the emergence of new gene function. It is a well\nrecognized feature in large DNA viruses, but has not been studied extensively\nin the largest known virus to date, the recently discovered Acanthamoeba\nPolyphaga Mimivirus. Here we present a systematic analysis of gene and genome\nduplication events in the Mimivirus genome. We find that one third of the\nMimivirus genes are related to at least one other gene in the Mimivirus genome,\neither through a large segmental genome duplication event that occurred in the\nmore remote past, either through more recent gene duplication events, which\noften occur in tandem. This shows that gene and genome duplication played a\nmajor role in shaping the Mimivirus genome. Using multiple alignments together\nwith remote homology detection methods based on Hidden Markov Model comparison,\nwe assign putative functions to some of the paralogous gene families. We\nsuggest that a large part of the duplicated Mimivirus gene families are likely\nto interfere with important host cell processes, such as transcription control,\nprotein degradation, and cell regulatory processes. Our findings support the\nview that large DNA viruses are complex evolving organisms, possibly deeply\nrooted within the tree of life, and oppose the paradigm that viral evolution is\ndominated by lateral gene acquisition, at least in what concerns large DNA\nviruses.\n"
  },
  {
    "year": 2005,
    "title": "Why highly expressed proteins evolve slowly",
    "summary": "  Much recent work has explored molecular and population-genetic constraints on\nthe rate of protein sequence evolution. The best predictor of evolutionary rate\nis expression level, for reasons which have remained unexplained. Here, we\nhypothesize that selection to reduce the burden of protein misfolding will\nfavor protein sequences with increased robustness to translational missense\nerrors. Pressure for translational robustness increases with expression level\nand constrains sequence evolution. Using several sequenced yeast genomes,\nglobal expression and protein abundance data, and sets of paralogs traceable to\nan ancient whole-genome duplication in yeast, we rule out several confounding\neffects and show that expression level explains roughly half the variation in\nSaccharomyces cerevisiae protein evolutionary rates. We examine causes for\nexpression's dominant role and find that genome-wide tests favor the\ntranslational robustness explanation over existing hypotheses that invoke\nconstraints on function or translational efficiency. Our results suggest that\nproteins evolve at rates largely unrelated to their functions, and can explain\nwhy highly expressed proteins evolve slowly across the tree of life.\n"
  },
  {
    "year": 2005,
    "title": "Weighted-Codon-Usage Based Phylogeny In Ectocarpales",
    "summary": "  We analyse forty seven chloroplastid genes of the large subunit of RuBisCO,\nfrom the Algal order Ectocarpales, sourced from GenBank. Codon-usage weighted\nby the nucleotide base bias defines our score called the\nCodon-Impact-Parameter. This score is used to obtain phylogenetic relations\namongst the 47 Ectocarpales. We compare our classification with the ones done\nearlier.\n"
  },
  {
    "year": 2005,
    "title": "Identity Elements of Archaeal tRNA",
    "summary": "  Features unique to a transfer-RNA are recognized by the corresponding\ntRNA-synthetase. Keeping this in view we isolate the discriminating features of\nall archaeal tRNA. These are our identity elements. Further, we investigate\ntRNA-characteristics that delineate the different orders of archaea.\n"
  },
  {
    "year": 2005,
    "title": "Evidence for abundant transcription of non-coding regions in the\n  Saccharomyces cerevisiae genome",
    "summary": "  Background: Recent studies in a growing number of organisms have yielded\naccumulating evidence that a significant portion of the non-coding region in\nthe genome is transcribed. We address this issue in the yeast Saccharomyces\ncerevisiae.\n  Results: Taking into account the absence of a significantly large yeast EST\ndatabase, we use microarray expression data collected for genomic regions\nerroneously believed to be coding to study the expression pattern of non-coding\nregions in the Saccharomyces cerevisiae genome. We find that at least 164 out\nof 589 (28%) such regions are expressed under specific biological conditions.\nIn particular, looking at the probes that are located opposing other known\ngenes at the same genomic locus, we find that 88 out of 341 (26%) of these\ngenes support antisense transcription. The expression patterns of these\nantisense genes are positively correlated. We validate these results using\nRT-PCR on a sample of 6 non-coding transcripts.\n  Conclusions: 1. The yeast genome is transcribed on a scale larger than\npreviously assumed. 2. Correlated transcription of antisense genes is abundant\nin the yeast genome. 3. Antisense genes in yeast are non-coding.\n"
  },
  {
    "year": 2005,
    "title": "Is there any sense in antisense editing?",
    "summary": "  A number of recent studies have hypothesized that sense-antisense RNA\ntranscript pairs create dsRNA duplexes that undergo extensive A-to-I RNA\nediting. Here we studied human and mouse genomic antisense regions, and found\nthat the editing level in these areas is negligible. This observation puts in\nquestion the scope of sense-antisense duplexes formation in-vivo, which is the\nbasis for a number of proposed regulatory mechanisms.\n"
  },
  {
    "year": 2005,
    "title": "Introns Restructure tRNA Genes of Archaea",
    "summary": "  This paper has been withdrawn by the author.\n"
  },
  {
    "year": 2005,
    "title": "Modeling genome evolution with a diffusion approximation of a\n  birth-and-death process",
    "summary": "  In our previous studies, we developed discrete-space Birth, Death and\nInnovation Models (BDIM) of genome evolution. These models explain the origin\nof the characteristic Pareto distribution of paralogous gene family sizes in\ngenomes, and model parameters that provide for the evolution of these\ndistributions within a realistic timeframe have been identified. Here we\ndevelop the diffusion version of BDIM whose dynamics is described by the\nFokker-Plank equation and the stationary solution could be any specified Pareto\nfunction. The diffusion models have time-dependent solutions of a special kind,\nnamely, the generalized self-similar solutions, which describe the transition\nfrom one stationary distribution of the system to another; this provides for\nthe possibility of examining the temporal dynamics of genome evolution.\nAnalysis of the generalized self-similar solutions of the diffusion BDIM\nreveals a biphasic curve of genome growth in which the initial, relatively\nshort, self-accelerating phase is followed by a prolonged phase of slow\ndeceleration. In biological terms, this regime of evolution can be tentatively\ninterpreted as a punctuated-equilibrium-like phenomenon such that whereby\nevolutionary transitions are accompanied by rapid gene amplification and\ninnovation, followed by slow relaxation to a new stationary state.\n"
  },
  {
    "year": 2005,
    "title": "Simple stochastic birth and death models of genome evolution: Was there\n  enough time for us to evolve?",
    "summary": "  We show that simple stochastic models of genome evolution lead to power law\nasymptotics of protein domain family size distribution. These models, called\nBirth, Death and Innovation Models (BDIM), represent a special class of\nbalanced birth-and-death processes, in which domain duplication and deletion\nrates are asymptotically equal up to the second order. The simplest, linear\nBDIM shows an excellent fit to the observed distributions of domain family size\nin diverse prokaryotic and eukaryotic genomes. However, the stochastic version\nof the linear BDIM explored here predicts that the actual size of large\nparalogous families is reached on an unrealistically long timescale. We show\nthat introduction of non-linearity, which might be interpreted as interaction\nof a particular order between individual family members, allows the model to\nachieve genome evolution rates that are much better compatible with the current\nestimates of the rates of individual duplication/loss events.\n"
  },
  {
    "year": 2005,
    "title": "Biological applications of the theory of birth-and-death processes",
    "summary": "  In this review, we discuss the applications of the theory of birth-and-death\nprocesses to problems in biology, primarily, those of evolutionary genomics.\nThe mathematical principles of the theory of these processes are briefly\ndescribed. Birth-and-death processes, with some straightforward additions such\nas innovation, are a simple, natural formal framework for modeling a vast\nvariety of biological processes such as population dynamics, speciation, genome\nevolution, including growth of paralogous gene families and horizontal gene\ntransfer, and somatic evolution of cancers. We further describe how empirical\ndata, e.g., distributions of paralogous gene family size, can be used to choose\nthe model that best reflects the actual course of evolution among different\nversions of birth-death-and-innovation models. It is concluded that\nbirth-and-death processes, thanks to their mathematical transparency,\nflexibility and relevance to fundamental biological process, are going to be an\nindispensable mathematical tool for the burgeoning field of systems biology.\n"
  },
  {
    "year": 2005,
    "title": "Conspiracy in bacterial genomes",
    "summary": "  The rank ordered distribution of the codon usage frequencies for 123\nbacteriae is best fitted by a three parameters function that is the sum of a\nconstant, an exponential and a linear term in the rank n. The parameters depend\n(two parabolically) from the total GC content. The rank ordered distribution of\nthe amino acids is fitted by a straight line. The Shannon entropy computed over\nall the codons is well fitted by a parabola in the GC content, while the\npartial entropies computed over subsets of the codons show peculiar different\nbehavior, exhibiting therefore a first conspiracy effect. Moreover the sum of\nthe codon usage frequencies over particular sets, e.g. with C and A\n(respectively G and U) as i-th nucleotide, shows a clear linear dependence from\nthe GC content, exhibiting another conspiracy effect.\n"
  },
  {
    "year": 2005,
    "title": "Parameters of proteome evolution from histograms of amino-acid sequence\n  identities of paralogous proteins",
    "summary": "  The evolution of the full repertoire of proteins encoded in a given genome is\nmostly driven by gene duplications, deletions, and sequence modifications of\nexisting proteins. Indirect information about relative rates and other\nintrinsic parameters of these three basic processes is contained in the\nproteome-wide distribution of sequence identities of pairs of paralogous\nproteins. We introduce a simple mathematical framework based on a stochastic\nbirth-and-death model that allows one to extract some of this information and\napply it to the set of all pairs of paralogous proteins in seven model\norganisms. It was found that the histogram of sequence identities p generated\nby an all-to-all alignment of all protein sequences encoded in a genome is well\nfitted with a power-law form ~p^(-gamma) with the value of the exponent gamma\naround 4 for the majority of organisms used in this study. This implies that\nthe intra-protein variability of substitution rates is best described by the\nGamma-distribution with the exponent alpha ~ 0.33. We separately measure the\nshort-term (``raw'') duplication and deletion rates r*_dup, r*_del which\ninclude gene copies that will be removed soon after the duplication event and\ntheir dramatically reduced long-term counterparts r_dup, r_del. Systematic\ntrends of each of the four duplication/deletion rates with the total number of\ngenes in the genome were analyzed. All but the deletion rate of recent\nduplicates r*_del were shown to systematically increase with N_genes.\nAbnormally flat shapes of sequence identity histograms observed for yeast and\nhuman are consistent with lineages leading to these organisms undergoing one or\nmore whole-genome duplications.\n"
  },
  {
    "year": 2005,
    "title": "On the Complexity of the Single Individual SNP Haplotyping Problem",
    "summary": "  We present several new results pertaining to haplotyping. These results\nconcern the combinatorial problem of reconstructing haplotypes from incomplete\nand/or imperfectly sequenced haplotype fragments. We consider the complexity of\nthe problems Minimum Error Correction (MEC) and Longest Haplotype\nReconstruction (LHR) for different restrictions on the input data.\nSpecifically, we look at the gapless case, where every row of the input\ncorresponds to a gapless haplotype-fragment, and the 1-gap case, where at most\none gap per fragment is allowed. We prove that MEC is APX-hard in the 1-gap\ncase and still NP-hard in the gapless case. In addition, we question earlier\nclaims that MEC is NP-hard even when the input matrix is restricted to being\ncompletely binary. Concerning LHR, we show that this problem is NP-hard and\nAPX-hard in the 1-gap case (and thus also in the general case), but is\npolynomial time solvable in the gapless case.\n"
  },
  {
    "year": 2005,
    "title": "Positioning Crenarchaeal tRNA-Introns",
    "summary": "  We precisely position a noncanonical intron in the odd second copy of\ntRNAAsp(GTC) gene in the newly sequenced crenarchaea S.acidocaldarius. The\nuniform assortment of some features from normal aspartate tDNA and some from\nthose corresponding to non-standard amino acids conduce us to conjecture it to\nbe a novel tRNA gene, probably coding for a modified aspartate residue. Further\nwe reposition intron in tRNAHis(GUG) gene in P.aerophilum.The BHB motif at the\nexon-intron boundaries are re-analyzed and found to support our conjectures.\n"
  },
  {
    "year": 2005,
    "title": "Mathematical modeling of evolution of horizontally transferred genes",
    "summary": "  We describe a stochastic birth-and-death model of evolution of horizontally\ntransferred genes in microbial populations. The model is a generalization of\nthe stochastic model described by Berg and Kurland and includes five\nparameters: the rate of mutational inactivation, selection coefficient,\nimmigration rate (i.e., rate of arrival of a novel sequence from outside of the\nrecipient population), within-population horizontal transmission rate, and\npopulation size. The model of Berg and Kurland included four parameters,\nnamely, mutational inactivation, selection coefficient, population size, and\ntransmission rate. However, the effect of transmission was disregarded in the\ninterpretation of the results, and the overall conclusion was that horizontally\nacquired sequences can be fixed in a population only when they confer a\nsubstantial selective advantage onto the recipient and therefore are subject to\nstrong positive selection. By contrast, analysis of the present model in\ndifferent domains of parameter values shows that, as long as the rate of\nwithin-population horizontal transmission is comparable to the mutational\ninactivation rate and there is even a low rate of immigration, horizontally\nacquired sequences can be fixed in the population or at least persist for a\nlong time even when they are neutral or slightly deleterious. The available\nbiological data strongly suggest that intense within-population and even\nbetween-populations gene flows are realistic for at least some prokaryotic\nspecies and environments. Therefore our modeling results are compatible with\nthe notion of a pivotal role of horizontal gene transfer in the evolution of\nprokaryotes.\n"
  },
  {
    "year": 2005,
    "title": "In silicio stretching of chromatin",
    "summary": "  We present Monte-Carlo (MC) simulations of the stretching of a single 30 nm\nchromatin fiber. The model approximates the DNA by a flexible polymer chain\nwith Debye-H\\\"uckel electrostatics and uses a two-angle zig-zag model for the\ngeometry of the linker DNA connecting the nucleosomes. The latter are\nrepresented by flat disks interacting via an attractive Gay-Berne potential.\nOur results show that the stiffness of the chromatin fiber strongly depends on\nthe linker DNA length. Furthermore, changing the twisting angle between\nnucleosomes from 90 deg to 130 deg increases the stiffness significantly. An\nincrease in the opening angle from 22 deg to 34 deg leads to softer fibers for\nsmall linker lengths. We observe that fibers containing a linker histone at\neach nucleosome are stiffer compared to those without the linker histone. The\nsimulated persistence lengths and elastic moduli agree with experimental data.\nFinally, we show that the chromatin fiber does not behave as an isotropic\nelastic rod, but its rigidity depends on the direction of deformation:\nchromatin is much more resistant to stretching than to bending.\n"
  },
  {
    "year": 2005,
    "title": "Embedded transfer RNA Genes",
    "summary": "  In euryarchaeal methanogen M.kandleri and in Nanoarchaea N. equitans some of\nthe missing tRNA genes are embedded in others. We argue from bioinformatic\nevidence that position specific intron splicing is the key behind co-location\nof these tRNA genes.\n"
  },
  {
    "year": 2005,
    "title": "Universality of Long-Range Correlations in Expansion-Randomization\n  Systems",
    "summary": "  We study the stochastic dynamics of sequences evolving by single site\nmutations, segmental duplications, deletions, and random insertions. These\nprocesses are relevant for the evolution of genomic DNA. They define a\nuniversality class of non-equilibrium 1D expansion-randomization systems with\ngeneric stationary long-range correlations in a regime of growing sequence\nlength. We obtain explicitly the two-point correlation function of the sequence\ncomposition and the distribution function of the composition bias in sequences\nof finite length. The characteristic exponent $\\chi$ of these quantities is\ndetermined by the ratio of two effective rates, which are explicitly calculated\nfor several specific sequence evolution dynamics of the universality class.\nDepending on the value of $\\chi$, we find two different scaling regimes, which\nare distinguished by the detectability of the initial composition bias. All\nanalytic results are accurately verified by numerical simulations. We also\ndiscuss the non-stationary build-up and decay of correlations, as well as more\ncomplex evolutionary scenarios, where the rates of the processes vary in time.\nOur findings provide a possible example for the emergence of universality in\nmolecular biology.\n"
  },
  {
    "year": 2005,
    "title": "Domesticated P elements in the Drosophila montium species subgroup have\n  a new function related to a DNA binding property",
    "summary": "  Molecular domestication of a transposable element is defined as its\nfunctional recruitment by the host genome. To date, two independent events of\nmolecular domestication of the P transposable element have been described: in\nthe Drosophila obscura species group and in the Drosophila montium species\nsubgroup. These P neogenes consist to stationary, non repeated sequences,\npotentially encoding 66 kDa repressor-like proteins (RLs). Here we investigate\nthe function of the montium P neogenes. We provide evidence for the presence of\nRLs proteins in two montium species (D. tsacasi and D. bocqueti) specifically\nexpressed in adult and larval brain and gonads. We tested the hypothesis that\nthe montium P neogenes function is related to the repression of the\ntransposition of distant related mobile P elements which coexist in the genome.\nOur results strongly suggest that the montium P neogenes are not recruited to\ndown regulate the P element transposition. Given that all the proteins encoded\nby mobile or stationary P homologous sequences show a strong conservation of\nthe DNA Binding Domain, we tested the capacity of the RLs proteins to bind DNA\nin vivo. Immunstaining of polytene chromosomes in D. melanogaster transgenic\nlines strongly suggest that montium P neogenes encode proteins that bind DNA in\nvivo. RLs proteins show multiple binding to the chromosomes. We suggest that\nthe property recruited in the case of the montium P neoproteins is their DNA\nbinding property. The possible functions of these neogenes are discussed.\n"
  },
  {
    "year": 2005,
    "title": "The response of amino acid frequencies to directional mutation pressure\n  in mitochondrial genome sequences is related to the physical properties of\n  the amino acids and to the structure of the genetic code",
    "summary": "  The frequencies of A, C, G and T in mitochondrial DNA vary among species due\nto unequal rates of mutation between the bases. The frequencies of bases at\nfour-fold degenerate sites respond directly to mutation pressure. At 1st and\n2nd positions, selection reduces the degree of frequency variation. Using a\nsimple evolutionary model, we show that 1st position sites are less constrained\nby selection than 2nd position sites, and therefore that the frequencies of\nbases at 1st position are more responsive to mutation pressure than those at\n2nd position. We define a similarity measure between amino acids that is a\nfunction of 8 measured physical properties. We define a proximity measure for\neach amino acid, which is the average similarity between an amino acid and all\nothers that are accessible via single point mutations in the genetic code. We\nalso define a responsiveness for each amino acid, which measures how rapidly an\namino acid frequency changes as a result of mutation pressure acting on the\nbase frequencies. There is a strong correlation between responsiveness and\nproximity, and both these quantities are also correlated with the mutability of\namino acids estimated from the mtREV substitution rate matrix. We also consider\nthe variation of base frequencies between strands and between genes on a\nstrand. These trends are consistent with the patterns expected from analysis of\nthe variation among genomes\n"
  },
  {
    "year": 2005,
    "title": "Statistical Indicators of Collective Behavior and Functional Clusters in\n  Gene Networks of Yeast",
    "summary": "  We analyze gene expression time-series data of yeast S. cerevisiae measured\nalong two full cell-cycles. We quantify these data by using q-exponentials,\ngene expression ranking and a temporal mean-variance analysis. We construct\ngene interaction networks based on correlation coefficients and study the\nformation of the corresponding giant components and minimum spanning trees. By\ncoloring genes according to their cell function we find functional clusters in\nthe correlation networks and functional branches in the associated trees. Our\nresults suggest that a percolation point of functional clusters can be\nidentified on these gene expression correlation networks.\n"
  },
  {
    "year": 2005,
    "title": "Solvable models of neighbor-dependent nucleotide substitution processes",
    "summary": "  We prove that a wide class of models of Markov neighbor-dependent\nsubstitution processes on the integer line is solvable. This class contains\nsome models of nucleotide substitutions recently introduced and studied\nempirically by molecular biologists. We show that the polynucleotide\nfrequencies at equilibrium solve explicit finite-size linear systems. Finally,\nthe dynamics of the process and the distribution at equilibrium exhibit some\nstringent, rather unexpected, independence properties. For example, nucleotide\nsites at distance at least three evolve independently, and the sites, if\nencoded as purines and pyrimidines, evolve independently.\n"
  },
  {
    "year": 2005,
    "title": "Invariance principle for the coverage rate of genomic physical mappings",
    "summary": "  We study some stochastic models of physical mapping of genomic sequences. Our\nstarting point is a global construction of the process of the clones and of the\nprocess of the anchors which are used to map the sequence. This yields explicit\nformulas for the moments of the proportion occupied by the anchored clones,\neven in inhomogeneous models. This also allows to compare, in this respect,\ninhomogeneous models to homogeneous ones. Finally, for homogeneous models, we\nprovide nonasymptotic bounds of the variance and we prove functional invariance\nresults.\n"
  },
  {
    "year": 2005,
    "title": "Asymptotics of iterated branching processes",
    "summary": "  We study the iterated Galton-Watson process (IGW), possibly with thinning,\nintroduced by Gawe{\\l}and Kimmel to model the number of repeats of DNA triplets\nduring some genetic disorders. If the process involves some thinning, then\nextinction and explosion can have positive probability simultaneously. If the\nunderlying (simple) Galton-Watson process is nondecreasing with mean m, then,\nconditionally on the explosion, the logarithm of the population of the IGW at\ntime n+1 is equivalent to log(m) times the population at time n, almost surely.\nThis simplifies arguments of Gawe{\\l}and Kimmel, and confirms and extends a\nconjecture of Pakes.\n"
  },
  {
    "year": 2005,
    "title": "The protein map of Synechococcus sp. PCC 7942 - the first overlook",
    "summary": "  The unicellular cyanobacterium Synechococcus PCC 7942 has been used as a\nmodel organism for studies of prokaryotic circadian rhythms,\ncarbon-concentrating mechanisms, response to a variety of nutrient and\nenvironmental stresses, and cell division. This paper presents the results of\nthe first proteomic exploratory study of Synechococcus PCC 7942. The proteome\nwas analyzed using two-dimensional gel electrophoresis followed by MALDI-TOF\nmass spectroscopy, and database searching. Of 140 analyzed protein spots, 110\nwere successfully identified as 62 different proteins, many of which occurred\nas multiple spots on the gel. The identified proteins were organized into 18\ndifferent functional categories reflecting the major metabolic and cellular\nprocesses occurring in the cyanobacterial cells in the exponential growth\nphase. Among the identified proteins, 14 previously unknown or considered to be\nhypothetical are here shown to be true gene products in Synechococcus sp. PCC\n7942, and may be helpful for annotation of the newly sequenced genome.\n"
  },
  {
    "year": 2005,
    "title": "Non-extensive Trends in the Size Distribution of Coding and Non-coding\n  DNA Sequences in the Human Genome",
    "summary": "  We study the primary DNA structure of four of the most completely sequenced\nhuman chromosomes (including chromosome 19 which is the most dense in coding),\nusing Non-extensive Statistics. We show that the exponents governing the decay\nof the coding size distributions vary between $5.2 \\le r \\le 5.7$ for the short\nscales and $1.45 \\le q \\le 1.50$ for the large scales. On the contrary, the\nexponents governing the decay of the non-coding size distributions in these\nfour chromosomes, take the values $2.4 \\le r \\le 3.2$ for the short scales and\n$1.50 \\le q \\le 1.72$ for the large scales. This quantitative difference, in\nparticular in the tail exponent $q$, indicates that the non-coding (coding)\nsize distributions have long (short) range correlations. This non-trivial\ndifference in the DNA statistics is attributed to the non-conservative\n(conservative) evolution dynamics acting on the non-coding (coding) DNA\nsequences.\n"
  },
  {
    "year": 2005,
    "title": "Effect of pooling samples on the efficiency of comparative studies using\n  microarrays",
    "summary": "  Many biomedical experiments are carried out by pooling individual biological\nsamples. However, pooling samples can potentially hide biological variance and\ngive false confidence concerning the data significance. In the context of\nmicroarray experiments for detecting differentially expressed genes, recent\npublications have addressed the problem of the efficiency of sample-pooling,\nand some approximate formulas were provided for the power and sample size\ncalculations. It is desirable to have exact formulas for these calculations and\nhave the approximate results checked against the exact ones. We show that the\ndifference between the approximate and exact results can be large. In this\nstudy, we have characterized quantitatively the effect of pooling samples on\nthe efficiency of microarray experiments for the detection of differential gene\nexpression between two classes. We present exact formulas for calculating the\npower of microarray experimental designs involving sample pooling and technical\nreplications. The formulas can be used to determine the total numbers of arrays\nand biological subjects required in an experiment to achieve the desired power\nat a given significance level. The conditions under which pooled design becomes\npreferable to non-pooled design can then be derived given the unit cost\nassociated with a microarray and that with a biological subject. This paper\nthus serves to provide guidance on sample pooling and cost effectiveness. The\nformulation in this paper is outlined in the context of performing microarray\ncomparative studies, but its applicability is not limited to microarray\nexperiments. It is also applicable to a wide range of biomedical comparative\nstudies where sample pooling may be involved.\n"
  },
  {
    "year": 2005,
    "title": "Vector space of DNA genomic sequences on a Genetic Code Galois Field",
    "summary": "  A new N-dimensional vector space of DNA sequences over the Galois field of\nthe 64 codons (GF(64)) was recently presented. Now, in order to include\ndeletions and insertions (indel mutations), we have defined a new Galois field\nover the set of elements X1X2X3 (C125), where Xi belong to {O, A, C, G, C}. We\nhave called this set, the extended triplet set and the elements X1X2X3, the\nextended triplets. The order of the bases is derived from the Z64-algebra of\nthe genetic code -recently published-. Starting from the natural bijection phi:\nGF(5^3)-> C125 between the polynomial representation of elements from GF(5^3)\nand the elements X1X2X3, a novel Galois field over the set of elements X1X2X3\nis defined. Taking the polynomial coefficients a0, a1, a2 belong to GF(5) and\nthe bijective function f: GF(5) ->{O, A, C, G, C}, where f(0) = O, f(1) = A,\nf(2) = C, f(3) = G, f(4) = U, bijection phi is induced such that phi(a0 + a1x +\na2x^2) = (f(a1), f(a2), f(a0)) = (X1X2X3). Next, by means of the bijection phi\nwe define sum \"+\" and product \"*\" operations in the set of codons C125, in such\na way that the resultant field (C125, +, *) turns isomorphic to the Galois\nField GF(5^3). This field allows the definition of a novel N-dimensional vector\nspace (S) over the field GF (5^3) on the set of all 125^N sequences of extended\ntriplets in which all possible DNA sequence alignments of length N are\nincluded. Here the \"classical gap\" produced by alignment algorithms corresponds\nto the neutral element \"O\". It is verified that the homologous (generalized)\nrecombination between two homologous DNA duplexes involving a reciprocal\nexchange of DNA sequences -e.g. between two chromosomes that carry the same\ngenetic loci- algebraically corresponds to the action of two automorphism pairs\n(or two translation pairs) over two paired DNA duplexes.\n"
  },
  {
    "year": 2005,
    "title": "Impact of Tandem Repeats on the Scaling of Nucleotide Sequences",
    "summary": "  Techniques such as detrended fluctuation analysis (DFA) and its extensions\nhave been widely used to determine the nature of scaling in nucleotide\nsequences. In this brief communication we show that tandem repeats which are\nubiquitous in nucleotide sequences can prevent reliable estimation of possible\nlong-range correlations. Therefore, it is important to investigate the presence\nof tandem repeats prior to scaling exponent estimation.\n"
  },
  {
    "year": 2005,
    "title": "An Algorithm for Missing Value Estimation for DNA Microarray Data",
    "summary": "  Gene expression data matrices often contain missing expression values. In\nthis paper, we describe a new algorithm, named improved fixed rank\napproximation algorithm (IFRAA), for missing values estimations of the large\ngene expression data matrices. We compare the present algorithm with the two\nexisting and widely used methods for reconstructing missing entries for DNA\nmicroarray gene expression data: the Bayesian principal component analysis\n(BPCA) and the local least squares imputation method (LLS). The three\nalgorithms were applied to four microarray data sets and two synthetic low-rank\ndata matrices. Certain percentages of the elements of these data sets were\nrandomly deleted, and the three algorithms were used to recover them. In\nconclusion IFRAA appears to be the most reliable and accurate approach for\nrecovering missing DNA microarray gene expression data, or any other noisy data\nmatrices that are effectively low rank.\n"
  },
  {
    "year": 2005,
    "title": "tRNA-isoleucine-tryptophan Composite Gene",
    "summary": "  Transfer-RNA genes in archaea often have introns intervening between exon\nsequences. The structural motif at the boundary between exon and intron is the\nbulge-helix-bulge. Computational investigations of these boundary structures in\nH. marismortui lead us to propose that tRNA-isoleucine and tRNA-tryptophan\ngenes are co-located. Precise insilico identification of the splice-sites on\nthe bulges at the exon-intron boundaries conduce us to infer that a single\nintron-containing composite tRNA-gene can give rise to more than one gene\nproduc.\n"
  },
  {
    "year": 2005,
    "title": "Sequential and asynchronous processes driven by stochastic or quantum\n  grammars and their application to genomics: a survey",
    "summary": "  We present the formalism of sequential and asynchronous processes defined in\nterms of random or quantum grammars and argue that these processes have\nrelevance in genomics. To make the article accessible to the\nnon-mathematicians, we keep the mathematical exposition as elementary as\npossible, focusing on some general ideas behind the formalism and stating the\nimplications of the known mathematical results. We close with a set of open\nchallenging problems.\n"
  },
  {
    "year": 2005,
    "title": "Gene expression analysis reveals a strong signature of an interferon\n  induced pathway in childhood lymphoblastic leukemia as well as in breast and\n  ovarian cancer",
    "summary": "  On the basis of epidemiological studies, infection was suggested to play a\nrole in the etiology of human cancer. While for some cancers such a role was\nindeed demonstrated, there is no direct biological support for the role of\nviral pathogens in the pathogenesis of childhood leukemia. Using a novel\nbioinformatic tool, that alternates between clustering and standard statistical\nmethods of analysis, we performed a \"double blind\" search of published gene\nexpression data of subjects with different childhood ALL subtypes, looking for\nunanticipated partitions of patients, induced by unexpected groups of genes\nwith correlated expression. We discovered a group of about thirty genes,\nrelated to the interferon response pathway, whose expression levels divide the\nALL samples into two subgroups; high in 50, low in 285 patients. Leukemic\nsubclasses prevalent in early childhood (the age most susceptible to infection)\nare over-represented in the high expression subgroup. Similar partitions,\ninduced by the same genes, were found also in breast and ovarian cancer but not\nin lung cancer, prostate cancer and lymphoma. About 40% of breast cancer\nsamples expressed the \"interferon- related\" signature. It is of interested that\nseveral studies demonstrated MMTV-like sequences in about 40% of breast cancer\nsamples. Our discovery of an unanticipated strong signature of an interferon\ninduced pathway provides molecular support for a role for either inflammation\nor viral infection in the pathogenesis of childhood leukemia as well as breast\nand ovarian cancer.\n"
  },
  {
    "year": 2005,
    "title": "Elucidation of Directionality for Co-Expressed Genes: Predicting\n  Intra-Operon Termination Sites",
    "summary": "  We present a novel framework for inferring regulatory and sequence-level\ninformation from gene co-expression networks. The key idea of our methodology\nis the systematic integration of network inference and network topological\nanalysis approaches for uncovering biological insights. We determine the gene\nco-expression network of Bacillus subtilis using Affymetrix GeneChip time\nseries data and show how the inferred network topology can be linked to\nsequence-level information hard-wired in the organism's genome. We propose a\nsystematic way for determining the correlation threshold at which two genes are\nassessed to be co-expressed by using the clustering coefficient and we expand\nthe scope of the gene co-expression network by proposing the slope ratio metric\nas a means for incorporating directionality on the edges. We show through\nspecific examples for B. subtilis that by incorporating expression level\ninformation in addition to the temporal expression patterns, we can uncover\nsequence-level biological insights. In particular, we are able to identify a\nnumber of cases where (i) the co-expressed genes are part of a single\ntranscriptional unit or operon and (ii) the inferred directionality arises due\nto the presence of intra-operon transcription termination sites.\n"
  },
  {
    "year": 2005,
    "title": "Correlation Statistics for cDNA Microarray Image Analysis",
    "summary": "  In this report, correlation of the pixels comprising a microarray spot is\ninvestigated. Subsequently, correlation statistics namely: Pearson correlation\nand Spearman rank correlation are used to segment the foreground and background\nintensity of microarray spots. The performance of correlation-based\nsegmentation is compared to clustering-based (PAM, k-means) and seeded-region\ngrowing techniques (SPOT). It is shown that correlation-based segmentation is\nuseful in flagging poorly hybridized spots, thus minimizes false-positives. The\npresent study also raises the intriguing question of whether a change in\ncorrelation can be an indicator of differential gene expression.\n"
  },
  {
    "year": 2005,
    "title": "Quasispecies and recombination",
    "summary": "  Recombination is introduced into Eigen's theory of quasispecies evolution.\nComparing numerical simulations of the rate equations in the non-recombining\nand recombining cases show that recombination has a strong effect on the error\nthreshold and, for a wide range of mutation rates, gives rise to two stable\nfixed points in the dynamics. This bi-stability results in the existence of two\nerror thresholds. We prove that, under some assumptions on the fitness\nlandscape but for general crossover probability, a fixed point localized about\nthe sequence with superior fitness is globally stable for low mutation rates.\n"
  },
  {
    "year": 2005,
    "title": "Quantitative modeling and data analysis of SELEX experiments",
    "summary": "  SELEX (Systematic Evolution of Ligands by Exponential Enrichment) is an\nexperimental procedure that allows extracting, from an initially random pool of\nDNA, those oligomers with high affinity for a given DNA-binding protein. We\naddress what is a suitable experimental and computational procedure to infer\nparameters of transcription factor-DNA interaction from SELEX experiments. To\nanswer this, we use a biophysical model of transcription factor-DNA\ninteractions to quantitatively model SELEX. We show that a standard procedure\nis unsuitable for obtaining accurate interaction parameters. However, we\ntheoretically show that a modified experiment in which chemical potential is\nfixed through different rounds of the experiment allows robust generation of an\nappropriate data set. Based on our quantitative model, we propose a novel\nbioinformatic method of data analysis for such modified experiment and apply it\nto extract the interaction parameters for a mammalian transcription factor\nCTF/NFI. From a practical point of view, our method results in a significantly\nimproved false positive/false negative trade-off, as compared to both the\nstandard information theory based method and a widely used empirically\nformulated procedure.\n"
  },
  {
    "year": 2005,
    "title": "Parametric inference of recombination in HIV genomes",
    "summary": "  Recombination is an important event in the evolution of HIV. It affects the\nglobal spread of the pandemic as well as evolutionary escape from host immune\nresponse and from drug therapy within single patients. Comprehensive\ncomputational methods are needed for detecting recombinant sequences in large\ndatabases, and for inferring the parental sequences.\n  We present a hidden Markov model to annotate a query sequence as a\nrecombinant of a given set of aligned sequences. Parametric inference is used\nto determine all optimal annotations for all parameters of the model. We show\nthat the inferred annotations recover most features of established hand-curated\nannotations. Thus, parametric analysis of the hidden Markov model is feasible\nfor HIV full-length genomes, and it improves the detection and annotation of\nrecombinant forms.\n  All computational results, reference alignments, and C++ source code are\navailable at http://bio.math.berkeley.edu/recombination/.\n"
  },
  {
    "year": 2005,
    "title": "Cloning, expression and purification of the general stress protein Yhbo\n  from Escherichia coli",
    "summary": "  We cloned, expressed and purified the Escherichia coli yhbO gene product,\nwhich is homolog to the Bacillus subtilis general stress protein 18 (the yfkM\ngene product), the Pyrococcus furiosus intracellular protease PfpI, and the\nhuman Parkinson disease protein DJ-1. The gene coding for YhbO was generated by\namplifying the yhbO gene from E. coli by polymerase chain reaction. It was\ninserted in the expression plasmid pET-21a, under the transcriptional control\nof the bacteriophage T7 promoter and lac operator. A BL21(DE3) E. coli strain\ntransformed with the YhbO-expression vector pET-21a-yhbO, accumulates large\namounts of a soluble protein of 20 kDa in SDS-PAGE that matches the expected\nYhbO molecular weight. YhbO was purified to homogeneity by HPLC DEAE ion\nexchange chromatography and hydroxylapatite chromatography and its identity was\nconfirmed by N-terminal sequencing and mass spectrometry analysis. The native\nprotein exists in monomeric, trimeric and hexameric forms.\n"
  },
  {
    "year": 2005,
    "title": "High-Throughput SNP Genotyping by SBE/SBH",
    "summary": "  Despite much progress over the past decade, current Single Nucleotide\nPolymorphism (SNP) genotyping technologies still offer an insufficient degree\nof multiplexing when required to handle user-selected sets of SNPs. In this\npaper we propose a new genotyping assay architecture combining multiplexed\nsolution-phase single-base extension (SBE) reactions with sequencing by\nhybridization (SBH) using universal DNA arrays such as all $k$-mer arrays. In\naddition to PCR amplification of genomic DNA, SNP genotyping using SBE/SBH\nassays involves the following steps: (1) Synthesizing primers complementing the\ngenomic sequence immediately preceding SNPs of interest; (2) Hybridizing these\nprimers with the genomic DNA; (3) Extending each primer by a single base using\npolymerase enzyme and dideoxynucleotides labeled with 4 different fluorescent\ndyes; and finally (4) Hybridizing extended primers to a universal DNA array and\ndetermining the identity of the bases that extend each primer by hybridization\npattern analysis. Our contributions include a study of multiplexing algorithms\nfor SBE/SBH genotyping assays and preliminary experimental results showing the\nachievable tradeoffs between the number of array probes and primer length on\none hand and the number of SNPs that can be assayed simultaneously on the\nother. Simulation results on datasets both randomly generated and extracted\nfrom the NCBI dbSNP database suggest that the SBE/SBH architecture provides a\nflexible and cost-effective alternative to genotyping assays currently used in\nthe industry, enabling genotyping of up to hundreds of thousands of\nuser-specified SNPs per assay.\n"
  },
  {
    "year": 2005,
    "title": "Abelian Finite Group of DNA Genomic Sequences",
    "summary": "  The Z_64-algebra of the genetic code and DNA sequences of length N was\nrecently stated. In order to beat the limits of this structure such as the\nimpossibility of non-coding region analysis in genomes and the impossibility of\nthe insertions and deletions analysis (indel mutations), we have develop a\ncycle group structure over the of extended base triplets of DNA X_1X_2X_3, X_i\nbelong to {O, A, C, G, U}, where the letter O denote the base omission\n(deletion) in the codon. The obtained group is isomorphic to the abelian\n5-group Z_125 of integer module 125. Next, it is defined the abelian finite\ngroup S over a set of DNA alignment sequences of length N. The group S could be\nrepresented as the direct sum of homocyclic groups: 2-group and 5-group. In\nparticular, DNA subsequences without indel mutation could be considered\nbuilding block of genes represented by homocyclic 2-groups (described in the\nprevious Z_64-algebra). While those DNA subsequences affected by indel\nmutations are described by means of homocyclic 5-groups. This representation\nsuggests identify genome block structures by way of a regular grammar capable\nof recognize it. In addition, this novel structure allows us a general analysis\nof the mutational pathways follow by genes and isofunctional genome regions by\nmeans of the automorphism group on S.\n"
  },
  {
    "year": 2006,
    "title": "Regularization Strategies for Hyperplane Classifiers: Application to\n  Cancer Classification with Gene Expression Data",
    "summary": "  Linear discrimination, from the point of view of numerical linear algebra,\ncan be treated as solving an ill-posed system of linear equations. In order to\ngenerate a solution that is robust in the presence of noise, these problems\nrequire regularization. Here, we examine the ill-posedness involved in the\nlinear discrimination of cancer gene expression data with respect to outcome\nand tumor subclasses. We show that a filter factor representation, based upon\nSingular Value Decomposition, yields insight into the numerical ill-posedness\nof the hyperplane-based separation when applied to gene expression data. We\nalso show that this representation yields useful diagnostic tools for guiding\nthe selection of classifier parameters, thus leading to improved performance.\n"
  },
  {
    "year": 2006,
    "title": "Universal core modules govern the dynamics of cellular regulatory\n  networks",
    "summary": "  This paper contains some immature results. It have been withdrawn.\n"
  },
  {
    "year": 2006,
    "title": "Reducibility of Gene Patterns in Ciliates using the Breakpoint Graph",
    "summary": "  Gene assembly in ciliates is one of the most involved DNA processings going\non in any organism. This process transforms one nucleus (the micronucleus) into\nanother functionally different nucleus (the macronucleus). We continue the\ndevelopment of the theoretical models of gene assembly, and in particular we\ndemonstrate the use of the concept of the breakpoint graph, known from another\nbranch of DNA transformation research. More specifically: (1) we characterize\nthe intermediate gene patterns that can occur during the transformation of a\ngiven micronuclear gene pattern to its macronuclear form; (2) we determine the\nnumber of applications of the loop recombination operation (the most basic of\nthe three molecular operations that accomplish gene assembly) needed in this\ntransformation; (3) we generalize previous results (and give elegant\nalternatives for some proofs) concerning characterizations of the micronuclear\ngene patterns that can be assembled using a specific subset of the three\nmolecular operations.\n"
  },
  {
    "year": 2006,
    "title": "Strategies of Loop Recombination in Ciliates",
    "summary": "  Gene assembly in ciliates is an extremely involved DNA transformation\nprocess, which transforms a nucleus, the micronucleus, to another functionally\ndifferent nucleus, the macronucleus. In this paper we characterize which loop\nrecombination operations (one of the three types of molecular operations that\naccomplish gene assembly) can possibly be applied in the transformation of a\ngiven gene from its micronuclear form to its macronuclear form. We also\ncharacterize in which order these loop recombination operations are applicable.\nThis is done in the abstract and more general setting of so-called legal\nstrings.\n"
  },
  {
    "year": 2006,
    "title": "CFinder: Locating cliques and overlapping modules in biological networks",
    "summary": "  Summary: Most cellular tasks are performed not by individual proteins, but by\ngroups of functionally associated proteins, often referred to as modules. In a\nprotein assocation network modules appear as groups of densely interconnected\nnodes, also called communities or clusters. These modules often overlap with\neach other and form a network of their own, in which nodes (links) represent\nthe modules (overlaps). We introduce CFinder, a fast program locating and\nvisualizing overlapping, densely interconnected groups of nodes in undirected\ngraphs, and allowing the user to easily navigate between the original graph and\nthe web of these groups. We show that in gene (protein) association networks\nCFinder can be used to predict the function(s) of a single protein and to\ndiscover novel modules. CFinder is also very efficient for locating the cliques\nof large sparse graphs.\n  Availability: CFinder (for Windows, Linux, and Macintosh) and its manual can\nbe downloaded from http://angel.elte.hu/clustering.\n  Contact: cfinder@angel.elte.hu\n"
  },
  {
    "year": 2006,
    "title": "PFMFind: a system for discovery of peptide homology and function",
    "summary": "  Protein Fragment Motif Finder (PFMFind) is a system that enables efficient\ndiscovery of relationships between short fragments of protein sequences using\nsimilarity search. It supports queries based on score matrices and PSSMs\nobtained through an iterative procedure similar to PSI-BLAST. PSSM construction\nis customisable through plugins written in Python. PFMFind consists of a GUI\nclient, an algorithm using an index for fast similarity search and a relational\ndatabase for storing search results and sequence annotations. It is written\nmostly in Python. All components communicate between themselves using TCP/IP\nsockets and can be located on different physical machines.\n"
  },
  {
    "year": 2006,
    "title": "Qualitative Assessment of Gene Expression in Affymetrix Genechip Arrays",
    "summary": "  Affymetrix Genechip microarrays are used widely to determine the simultaneous\nexpression of genes in a given biological paradigm. Probes on the Genechip\narray are atomic entities which by definition are randomly distributed across\nthe array and in turn govern the gene expression. In the present study, we make\nseveral interesting observations. We show that there is considerable\ncorrelation between the probe intensities across the array which defy the\nindependence assumption. While the mechanism behind such correlations is\nunclear, we show that scaling behavior and the profiles of perfect match (PM)\nas well as mismatch (MM) probes are similar and immune to background\nsubtraction. We believe that the observed correlations are possibly an outcome\nof inherent non-stationarities or patchiness in the array devoid of biological\nsignificance. This is demonstrated by inspecting their scaling behavior and\nprofiles of the PM and MM probe intensities obtained from publicly available\nGenechip arrays from three eukaryotic genomes, namely: Drosophila Melanogaster,\nHomo Sapiens and Mus musculus across distinct biological paradigms and across\nlaboratories, with and without background subtraction. The fluctuation\nfunctions were estimated using detrended fluctuation analysis (DFA) with fourth\norder polynomial detrending. The results presented in this study provide new\ninsights into correlation signatures of PM and MM probe intensities and\nsuggests the choice of DFA as a tool for qualitative assessment of Affymetrix\nGenechip microarrays prior to their analysis. A more detailed investigation is\nnecessary in order to understand the source of these correlations.\n"
  },
  {
    "year": 2006,
    "title": "Effects of Growth on Dinitrogen on the Transcriptome and Predicted\n  Proteome of Nostoc PCC 7120",
    "summary": "  Upon growth on dinitrogen, the filamentous cyanobacterium Nostoc PCC 7120\ninitiates metabolic and morphological changes. We analyzed the expression of\n1249 genes from major metabolic categories under nitrogen fixing and\nnon-nitrogen fixing growth. The expression data were correlated with potential\ntarget secondary structures, probe GC-content, predicted operon structures, and\nnitrogen content of gene products. Of the selected genes, 494 show a more than\n2-fold difference in the two conditions analyzed. Under nitrogen-fixing\nconditions 465 genes, mainly involved in energy metabolism, photosynthesis,\nrespiration and nitrogen-fixation, were found to be stronger expressed, whereas\n29 genes showed a stronger expression under non-nitrogen fixing conditions.\nAnalysis of the nitrogen content of regulated genes shows that Nostoc PCC 7120\ngrowing on dinitrogen is freed from any constraints to save nitrogen. For the\nfirst time the expression of high light-induced stress proteins (HLIP-family)\nis shown to be linked to the nitrogen availability.\n"
  },
  {
    "year": 2006,
    "title": "Microarray Data Management. An Enterprise Information Approach:\n  Implementations and Challenges",
    "summary": "  The extraction of information form high-throughput experiments is a key\naspect of modern biology. Early in the development of microarray technology,\nresearchers recognized that the size of the datasets and the limitations of\nboth computational and visualization techniques restricted their ability to\nfind the biological meaning hidden in the data. In addition, most researchers\nwanted to make their datasets accessible to others. This resulted in the\ndevelopment of new and advanced data storage, analysis, and visualization tools\nenabling the cross-platform validation of the experiments and the\nidentification of previously undetected patterns. In order to reap the benefits\nof this microarray data, researchers have needed to implement database\nmanagement systems providing integration of different experiments and data\ntypes. Moreover, it was necessary to standardize the basic data structure and\nexperimental techniques for the standardization of microarray platforms. In\nthis chapter, we introduce the reader to the major concepts related to the use\nof controlled vocabularies (ontologies), the definition of Minimum Information\nAbout a Microarray Experiment (MIAME) and provide an overview of different\nmicroarray data management strategies in use today. We summarize the main\ncharacteristics of microarray data storage and sharing strategies including\nwarehouses, datamarts, and federations. The fundamental challenges involved in\nthe distribution, and retrieval of microarray data are presented, along with an\noverview of some emerging technologies.\n"
  },
  {
    "year": 2006,
    "title": "Gene Function Classification Using Bayesian Models with Hierarchy-Based\n  Priors",
    "summary": "  We investigate the application of hierarchical classification schemes to the\nannotation of gene function based on several characteristics of protein\nsequences including phylogenic descriptors, sequence based attributes, and\npredicted secondary structure. We discuss three Bayesian models and compare\ntheir performance in terms of predictive accuracy. These models are the\nordinary multinomial logit (MNL) model, a hierarchical model based on a set of\nnested MNL models, and a MNL model with a prior that introduces correlations\nbetween the parameters for classes that are nearby in the hierarchy. We also\nprovide a new scheme for combining different sources of information. We use\nthese models to predict the functional class of Open Reading Frames (ORFs) from\nthe E. coli genome. The results from all three models show substantial\nimprovement over previous methods, which were based on the C5 algorithm. The\nMNL model using a prior based on the hierarchy outperforms both the\nnon-hierarchical MNL model and the nested MNL model. In contrast to previous\nattempts at combining these sources of information, our approach results in a\nhigher accuracy rate when compared to models that use each data source alone.\nTogether, these results show that gene function can be predicted with higher\naccuracy than previously achieved, using Bayesian models that incorporate\nsuitable prior information.\n"
  },
  {
    "year": 2006,
    "title": "Large-scale Oscillation of Structure-Related DNA Sequence Features in\n  Human Chromosome 21",
    "summary": "  Human chromosome 21 is the only chromosome in human genome that exhibits\noscillation of (G+C)-content of cycle length of hundreds kilobases (500 kb near\nthe right telomere). We aim at establishing the existence of similar\nperiodicity in structure-related sequence features in order to relate this\n(G+C)% oscillation to other biological phenomena. The following quantities are\nshown to oscillate with the same 500kb periodicity in human chromosome 21:\nbinding energy calculated by two sets of dinucleotide-based thermodynamic\nparameters, AA/TT and AAA/TTT bi-/tri-nucleotide density, 5'-TA-3' dinucleotide\ndensity, and signal for 10/11-base periodicity of AA/TT or AAA/TTT. These\nintrinsic quantities are related to structural features of the double helix of\nDNA molecules, such as base-pair binding, untwisting/unwinding, stiffness, and\na putative tendency for nucleosome formation.\n"
  },
  {
    "year": 2006,
    "title": "Mean Field Model of Genetic Regulatory Networks",
    "summary": "  In this paper, we propose a mean-field model which attempts to bridge the gap\nbetween random Boolean networks and more realistic stochastic modeling of\ngenetic regulatory networks. The main idea of the model is to replace all\nregulatory interactions to any one gene with an average or effective\ninteraction, which takes into account the repression and activation mechanisms.\nWe find that depending on the set of regulatory parameters, the model exhibits\nrich nonlinear dynamics. The model also provides quantitative support to the\nearlier qualitative results obtained for random Boolean networks.\n"
  },
  {
    "year": 2006,
    "title": "Conservation rules, their breakdown, and optimality in Caenorhabditis\n  sinusoidal locomotion",
    "summary": "  Undulatory locomotion is common to nematodes as well as to limbless\nvertebrates, but its control is not understood in spite of the identification\nof hundred of genes involved in Caenorhabditis elegans locomotion. To reveal\nthe mechanisms of nematode undulatory locomotion, we quantitatively analyzed\nthe movement of C. elegans with genetic perturbations to neurons, muscles, and\nskeleton (cuticle). We also compared locomotion of different Caenorhabditis\nspecies. We constructed a theoretical model that combines mechanics and\nbiophysics, and that is constrained by the observations of propulsion and\nmuscular velocities, as well as wavelength and amplitude of undulations. We\nfind that normalized wavelength is a conserved quantity among wild-type C.\nelegans individuals, across mutants, and across different species. The velocity\nof forward propulsion scales linearly with the velocity of the muscular wave\nand the corresponding slope is also a conserved quantity and almost optimal;\nthe exceptions are in some mutants affecting cuticle structure. In theoretical\nterms, the optimality of the slope is equivalent to the exact balance between\nmuscular and visco-elastic body reaction bending moments. We find that the\namplitude and frequency of undulations are inversely correlated and provide a\ntheoretical explanation for this fact. These experimental results are valid\nboth for young adults and for all larval stages of wild type C. elegans. In\nparticular, during development, the amplitude scales linearly with the\nwavelength, consistent with our theory. We also investigated the influence of\nsubstrate firmness on motion parameters, and found that it does not affect the\nabove invariants. In general, our biomechanical model can explain the observed\nrobustness of the mechanisms controlling nematode undulatory locomotion.\n"
  },
  {
    "year": 2006,
    "title": "Is the intrinsic disorder of proteins the cause of the scale-free\n  architecture of protein-protein interaction networks?",
    "summary": "  In protein-protein interaction networks certain topological properties appear\nto be recurrent: networks maps are considered scale-free. It is possible that\nthis topology is reflected in the protein structure. In this paper we\ninvestigate the role of protein disorder in the network topology. We find that\nthe disorder of a protein (or of its neighbors) is independent of its number of\nprotein-protein interactions. This result suggests that protein disorder does\nnot play a role in the scale-free architecture of protein networks.\n"
  },
  {
    "year": 2006,
    "title": "Probabilistic Regulatory Networks: Modeling Genetic Networks",
    "summary": "  We describe here the new concept of $\\epsilon$-Homomorphisms of Probabilistic\nRegulatory Gene Networks(PRN). The $\\epsilon$-homomorphisms are special\nmappings between two probabilistic networks, that consider the algebraic action\nof the iteration of functions and the probabilistic dynamic of the two\nnetworks. It is proved here that the class of PRN, together with the\nhomomorphisms, form a category with products and coproducts. Projections are\nspecial homomorphisms, induced by invariant subnetworks. Here, it is proved\nthat an $\\epsilon$-homomorphism for 0 <$\\epsilon$< 1 produces simultaneous\nMarkov Chains in both networks, that permit to introduce the concepts of\n$\\epsilon$-isomorphism of Markov Chains, and similar networks.\n"
  },
  {
    "year": 2006,
    "title": "The Genetic Code Degeneration I: Rules Governing the Code Degeneration\n  and the Spatial Organization of the Codon Informative Properties",
    "summary": "  The present work is devoted to describe a set of rules explaining the\ndiscriminating versus non-discriminating behavior of the di-basic stages and to\ncharacterize the role of each base in determining such a behavior. Bases are\nanalyze as dual entities characterized by its chemical type and the number of H\nbonds involved in the codon-anticodon interaction. A codon is characterized as\nan asymmetric informative entity whose global informative capacity results from\nthe spatially organized combinatory of the 6 proper-ties assigned by the 3\nbases.\n"
  },
  {
    "year": 2006,
    "title": "Protein and DNA sequence determinants of thermophilic adaptation",
    "summary": "  Prokaryotes living at extreme environmental temperatures exhibit pronounced\nsignatures in the amino acid composition of their proteins and nucleotide\ncompositions of their genomes reflective of adaptation to their thermal\nenvironments. However, despite significant efforts, the definitive answer of\nwhat are the genomic and proteomic compositional determinants of Optimal Growth\nTemperature of prokaryotic organisms remained elusive. Here the authors\nperformed a comprehensive analysis of amino acid and nucleotide compositional\nsignatures of thermophylic adaptation by exhaustively evaluating all\ncombinations of amino acids and nucleotides as possible determinants of Optimal\nGrowth Temperature for all prokaryotic organisms with fully sequences genomes..\nThe authors discovered that total concentration of seven amino acids in\nproteomes, IVYWREL, serves as a universal proteomic predictor of Optimal Growth\nTemperature in prokaryotes. Resolving the old-standing controversy the authors\ndetermined that the variation in nucleotide composition (increase of purine\nload, or A+G content with temperature) is largely a consequence of thermal\nadaptation of proteins. However, the frequency with which A and G nucleotides\nappear as nearest neighbors in genome sequences is strongly and independently\ncorrelated with Optimal Growth Temperature. as a result of codon bias in\ncorresponding genomes. Together these results provide a complete picture of\nproteomic and genomic determinants of thermophilic adaptation.\n"
  },
  {
    "year": 2006,
    "title": "Distance based Inference for Gene-Ontology Analysis of Microarray\n  Experiments",
    "summary": "  The increasing availability of high throughput data arising from gene\nexpression studies leads to the necessity of methods for summarizing the\navailable information. As annotation quality improves it is becoming common to\nrely on the Gene Ontology (GO) to build functional profiles that characterize a\nset of genes using the frequency of use of each GO term or group of terms in\nthe array. In this work we describe a statistical model for such profiles,\nprovide methods to compare profiles and develop inferential procedures to\nassess this comparison. An R-package implementing the methods is available.\n"
  },
  {
    "year": 2006,
    "title": "Modeling gene's length distribution in genomes",
    "summary": "  We show, that the specific distribution of gene's length, which is observed\nin natural genomes, might be a result of a growth process, in which a single\nlength scale $L(t)$ develops that grows with time as $t^{1/3}$. This length\nscale could be associated with the length of the longest gene in an evolving\ngenome. The growth kinetics of the genes resembles the one observed in physical\nsystems with conserved ordered parameter. We show, that in genome this\nconservation is guaranteed by compositional compensation along DNA strands of\nthe purine-like trends introduced by genes. The presented mathematical model is\nthe modified Bak-Sneppen model of critical self-organization applied to the\none-dimensional system of $N$ spins. The spins take discrete values, which\nrepresent gene's length.\n"
  },
  {
    "year": 2006,
    "title": "Mathematic principles underlying genetic structures",
    "summary": "  Many people are familiar with the physico-chemical properties of gene\nsequences. In this paper I present a mathematical perspective: how do\nmathematical principles such as information theory, coding theory, and\ncombinatorics influence the beginnings of life and the formation of the genetic\ncodes we observe today? What constraints on possible life forms are imposed by\ninformation-theoretical concepts? Further, I detail how mathematical principles\ncan help us to analyse the genetic sequences we observe in the world today.\n"
  },
  {
    "year": 2006,
    "title": "Invertibility of the TKF model of sequence evolution",
    "summary": "  We consider character sequences evolving on a phylogenetic tree under the\nTKF91 model. We show that as the sequence lengths tend to infinity the the\ntopology of the phylogenetic tree and the edge lengths are determined by any\none of (a) the alignment of sequences (b) the collection of sequence lengths.\nWe also show that the probability of any homology structure on a collection of\nsequences related by a TKF91 process on a tree is independent of the root\nlocation.\n  Keywords: phylogenetics, DNA sequence evolution models, identifiability,\nalignment\n"
  },
  {
    "year": 2006,
    "title": "Nearest neighbour spacing distribution of basis in some intron-less and\n  intron-containing DNA sequences",
    "summary": "  We show that the nearest neighbour distribution of distances between basis\npairs of some intron-less and intron-containing coding regions are the same\nwhen a procedure, called {\\em unfolding}, is applied. Such a procedure consists\nin separating the secular variations from the oscillatory terms. The form of\nthe distribution obtained is quite similar to that of a random, i.e.\nPoissonian, sequence. This is done for the HUMBMYH7CD, DROMYONMA, HUMBMYH7 and\nDROMHC sequences. The first two correspond to highly coding regions while the\nlast two correspond to non-coding regions. We also show that the distributions\nbefore the unfolding procedure depend on the secular part but, after the\nunfolding procedure we obtain an striking result: all distributions are similar\nto each other. The result becomes independent of the content of introns or the\nspecies we have chosen. This is in contradiction with the results obtained with\nthe detrended fluctuation analysis in which the correlations yield different\nresults for intron-less and intron-containing regions.\n"
  },
  {
    "year": 2006,
    "title": "Maximum-frequency gene tree: a simplified genome-scale approach to\n  overcoming incongruence in molecular phylogenies",
    "summary": "  Genomes and genes diversify during evolution; however, it is unclear to what\nextent genes still retain the relationship among species. Model species for\nmolecular phylogenetic studies include yeasts and viruses whose genomes were\nsequenced as well as plants that have the fossil-supported true phylogenetic\ntrees available. In this study, we generated single gene trees of seven yeast\nspecies as well as single gene trees of nine baculovirus species using all the\northologous genes among the species compared. Homologous genes among seven\nknown plants were used for validation of the fi nding. Four algorithms: maximum\nparsimony, minimum evolution, maximum likelihood, and neighbor-joining, were\nused. Trees were reconstructed before and after weighting the DNA and protein\nsequence lengths among genes. Rarely a gene can always generate the \"true tree\"\nby all the four algorithms. However, the most frequent gene tree, termed\n\"maximum gene-support tree\" (MGS tree, or WMGS tree for the weighted one), in\nyeasts, baculoviruses, or plants was consistently found to be the \"true tree\"\namong the species. The results provide insights into the overall degree of\ndivergence of orthologous genes of the genomes analyzed and suggest the\nfollowing: 1) The true tree relationship among the species studied is still\nmaintained by the largest group of orthologous genes; 2) There are usually more\northologous genes with higher similarities between genetically closer species\nthan between genetically more distant ones; and 3) The maximum gene-support\ntree refl ects the phylogenetic relationship among species in comparison.\n  Keywords: genome, gene evolution, molecular phylogeny, true tree\n"
  },
  {
    "year": 2006,
    "title": "Genome-wide EST data mining approaches to resolving incongruence of\n  molecular phylogenies",
    "summary": "  36 single genes of six plants inferred 18 unique trees using maximum\nparsimony. Such incongruence is an important issue and how to reconstruct the\ncongruent tree still is one of the most challenges in molecular phylogenetics.\nFor resolving this problem, a genome-wide EST data mining approach was\nsystematically investigated by retrieving a large size of EST data of 144\nshared genes of six green plants from GenBank. The results show that the\nconcatenated alignments approach overcame incongruence among single-gene\nphylogenies and successfully reconstructed the congruent tree of six species\nwith 100% jackknife support across each branch when 144 genes was used.\nJackknife supports of correct branches increased with number of genes linearly,\nbut those of wrong branches also increased linearly. For inferring the\ncongruent tree, the minimum 30 genes were required. This approach may provide\npotential power in resolving conflictions of phylogenies.\n  Keywords: Genome-wide; Data mining; EST; Phylogeny; Congruent tree; Jackknife\nsupport; Plants.\n"
  },
  {
    "year": 2006,
    "title": "Expression of MHC II genes",
    "summary": "  Innate and adaptive immunity are connected via antigen processing and\npresentation (APP), which results in the presentation of antigenic peptides to\nT cells in the complex with the major histocompatibility (MHC) determinants.\nMHC class II (MHC II) determinants present antigens to CD4+ T cells, which are\nthe main regulators of the immune response. Their genes are transcribed from\ncompact promoters that form first the MHC II enhanceosome, which contains\nDNA-bound activators and then the MHC II transcriptosome with the addition of\nthe class II transactivator (CIITA). CIITA is the master regulator of MHC II\ntranscription. It is expressed constitutively in dendritic cells (DC) and\nmature B cells and is inducible in most other cell types. Three isoforms of\nCIITA exist, depending on cell type and inducing signals. CIITA is regulated at\nthe levels of transcription and post-translational modifications, which are\nstill not very clear. Inappropriate immune responses are found in several\ndiseases, including cancer and autoimmunity. Since CIITA regulates the\nexpression of MHC II genes, it is involved directly in the regulation of the\nimmune response. The knowledge of CIITA will facilitate the manipulation of the\nimmune response and might contribute to the treatment of these diseases.\n"
  },
  {
    "year": 2006,
    "title": "Reconsidering the significance of genomic word frequency",
    "summary": "  We propose that the distribution of DNA words in genomic sequences can be\nprimarily characterized by a double Pareto-lognormal distribution, which\nexplains lognormal and power-law features found across all known genomes. Such\na distribution may be the result of completely random sequence evolution by\nduplication processes. The parametrization of genomic word frequencies allows\nfor an assessment of significance for frequent or rare sequence motifs.\n"
  },
  {
    "year": 2006,
    "title": "Evaluation of cell wall preparations for proteomics: a new procedure for\n  purifying cell walls from Arabidopsis hypocotyls",
    "summary": "  The ultimate goal of proteomic analysis of a cell compartment should be the\nexhaustive identification of resident proteins; excluding proteins from other\ncell compartments. Plant cell walls possess specific difficulties. Several\nreported procedures to isolate cell walls for proteomic analyses led to the\nisolation of a high proportion (more than 50%) of predicted intracellular\nproteins. The rationales of several published procedures to isolate cell walls\nfor proteomics were analyzed, with regard to the bioinformatic-predicted\nsubcellular localization of the identified proteins. A new procedure was\ndeveloped to prepare cell walls from etiolated hypocotyls of Arabidopsis\nthaliana. After salt extraction, a high proportion of proteins predicted to be\nsecreted was released (73%), belonging to the same functional classes as\nproteins identified using previously described protocols. The new cell wall\npreparation described in this paper gives the lowest proportion of proteins\npredicted to be intracellular when compared to available protocols. The\napplication of its principles should lead to a more realistic view of the cell\nwall proteome, at least for the weakly bound CWP extractable by salts. In\naddition, it offers a clean cell wall preparation for subsequent extraction of\nstrongly bound CWP.\n"
  },
  {
    "year": 2006,
    "title": "Cell wall proteins: a new insight through proteomics",
    "summary": "  Cell wall proteins are essential constituents of plant cell walls; they are\ninvolved in modifications of cell wall components, wall structure, signaling\nand interactions with plasma membrane proteins at the cell surface. The\napplication of proteomic approaches to the cell wall compartment raises\nimportant questions: are there technical problems specific to cell wall\nproteomics? What kinds of proteins can be found in Arabidopsis walls? Are some\nof them unexpected? What sort of post-translational modifications have been\ncharacterized in cell wall proteins to date? The purpose of this review is to\ndiscuss the experimental results obtained to date using proteomics, as well as\nsome of the new questions challenging future research.\n"
  },
  {
    "year": 2006,
    "title": "Transcriptional Interactions During Smallpox Infection and\n  Identification of Early Infection Biomarkers",
    "summary": "  Smallpox is a deadly disease that can be intentionally reintroduced into the\nhuman population as a bioweapon. While host gene expression microarray\nprofiling can be used to detect infection, the analysis of this information\nusing unsupervised and supervised classification techniques can produce\ncontradictory results. Here, we present a novel computational approach to\nincorporate molecular genome annotation features that are key for identifying\nearly infection biomarkers (EIB). Our analysis identified 58 EIBs expressed in\nperipheral blood mononuclear cells (PBMCs) collected from 21 cynomolgus\nmacaques (Macaca fascicularis) infected with two variola strains via aerosol\nand intravenous exposure. The level of expression of these EIBs was correlated\nwith disease progression and severity. No overlap between the EIBs\nco-expression and protein interaction data reported in public databases was\nfound. This suggests that a pathogen-specific re-organization of the gene\nexpression and protein interaction networks occurs during infection. To\nidentify potential genome-wide protein interactions between variola and humans,\nwe performed a protein domain analysis of all smallpox and human proteins. We\nfound that only 55 of the 161 protein domains in smallpox are also present in\nthe human genome. These co-occurring domains are mostly represented in proteins\ninvolved in blood coagulation, complement activation, angiogenesis,\ninflammation, and hormone transport. Several of these proteins are within the\nEIBs category and suggest potential new targets for the development of\ntherapeutic countermeasures.\n"
  },
  {
    "year": 2006,
    "title": "The riddle of the plant vacuolar sorting receptors",
    "summary": "  Proteins synthesized on membrane-bound ribosomes are sorted at the Golgi\napparatus level for delivery to various cellular destinations: the plasma\nmembrane or the extracellular space, and the lytic vacuole or lysosome. Sorting\ninvolves the assembly of vesicles, which preferentially package soluble\nproteins with a common destination. The selection of proteins for a particular\nvesicle type involves the recognition of proteins by specific receptors, such\nas the vacuolar sorting receptors for vacuolar targeting. Most eukaryotic\norganisms have one or two receptors to target proteins to the lytic vacuole.\nSurprisingly, plants have several members of the same family, seven in\nArabidopsis thaliana. Why do plants have so many proteins to sort soluble\nproteins to their respective destinations? The presence of at least two types\nof vacuoles, lytic and storage, seems to be a partial answer. In this review we\nanalyze the last experimental evidence supporting the presence of different\nsubfamilies of plant vacuolar sorting receptors.\n"
  },
  {
    "year": 2006,
    "title": "Asterias: a parallelized web-based suite for the analysis of expression\n  and aCGH data",
    "summary": "  Asterias (\\url{http://www.asterias.info}) is an integrated collection of\nfreely-accessible web tools for the analysis of gene expression and aCGH data.\nMost of the tools use parallel computing (via MPI). Most of our applications\nallow the user to obtain additional information for user-selected genes by\nusing clickable links in tables and/or figures. Our tools include:\nnormalization of expression and aCGH data; converting between different types\nof gene/clone and protein identifiers; filtering and imputation; finding\ndifferentially expressed genes related to patient class and survival data;\nsearching for models of class prediction; using random forests to search for\nminimal models for class prediction or for large subsets of genes with\npredictive capacity; searching for molecular signatures and predictive genes\nwith survival data; detecting regions of genomic DNA gain or loss. The\ncapability to send results between different applications, access to additional\nfunctional information, and parallelized computation make our suite unique and\nexploit features only available to web-based applications.\n"
  },
  {
    "year": 2006,
    "title": "Correlated fragile site expression allows the identification of\n  candidate fragile genes involved in immunity and associated with\n  carcinogenesis",
    "summary": "  Common fragile sites (cfs) are specific regions in the human genome that are\nparticularly prone to genomic instability under conditions of replicative\nstress. Several investigations support the view that common fragile sites play\na role in carcinogenesis. We discuss a genome-wide approach based on graph\ntheory and Gene Ontology vocabulary for the functional characterization of\ncommon fragile sites and for the identification of genes that contribute to\ntumour cell biology. CFS were assembled in a network based on a simple measure\nof correlation among common fragile site patterns of expression. By applying\nrobust measurements to capture in quantitative terms the non triviality of the\nnetwork, we identified several topological features clearly indicating\ndeparture from the Erdos-Renyi random graph model. The most important outcome\nwas the presence of an unexpected large connected component far below the\npercolation threshold. Most of the best characterized common fragile sites\nbelonged to this connected component. By filtering this connected component\nwith Gene Ontology, statistically significant shared functional features were\ndetected. Common fragile sites were found to be enriched for genes associated\nto the immune response and to mechanisms involved in tumour progression such as\nextracellular space remodeling and angiogenesis. Our results support the\nhypothesis that fragile sites serve a function; we propose that fragility is\nlinked to a coordinated regulation of fragile genes expression.\n"
  },
  {
    "year": 2006,
    "title": "Perfect Sampling of the Master Equation for Gene Regulatory Networks",
    "summary": "  We present a Perfect Sampling algorithm that can be applied to the Master\nEquation of Gene Regulatory Networks (GRNs). The method recasts Gillespie's\nStochastic Simulation Algorithm (SSA) in the light of Markov Chain Monte Carlo\nmethods and combines it with the Dominated Coupling From The Past (DCFTP)\nalgorithm to provide guaranteed sampling from the stationary distribution. We\nshow how the DCFTP-SSA can be generically applied to genetic networks with\nfeedback formed by the interconnection of linear enzymatic reactions and\nnonlinear Monod- and Hill-type elements. We establish rigorous bounds on the\nerror and convergence of the DCFTP-SSA, as compared to the standard SSA,\nthrough a set of increasingly complex examples. Once the building blocks for\nGRNs have been introduced, the algorithm is applied to study properly averaged\ndynamic properties of two experimentally relevant genetic networks: the toggle\nswitch, a two-dimensional bistable system, and the repressilator, a\nsix-dimensional genetic oscillator.\n"
  },
  {
    "year": 2006,
    "title": "Topological basis of signal integration in the\n  transcriptional-regulatory network of the yeast, Saccharomyces cerevisiae",
    "summary": "  BACKGROUND. Signal recognition and information processing is a fundamental\ncellular function, which in part involves comprehensive transcriptional\nregulatory (TR) mechanisms carried out in response to complex environmental\nsignals in the context of the cell's own internal state. However, the network\ntopological basis of developing such integrated responses remains poorly\nunderstood.\n  RESULTS. By studying the TR network of the yeast Saccharomyces cerevisiae we\nshow that an intermediate layer of transcription factors naturally segregates\ninto distinct subnetworks. In these topological units transcription factors are\ndensely interlinked in a largely hierarchical manner and respond to external\nsignals by utilizing a fraction of these subnets.\n  CONCLUSIONS. As transcriptional regulation represents the \"slow\" component of\noverall information processing, the identified topology suggests a model in\nwhich successive waves of transcriptional regulation originating from distinct\nfractions of the TR network control robust integrated responses to complex\nstimuli.\n"
  },
  {
    "year": 2006,
    "title": "Proteomic nonlinear waves in networks of transcriptional regulators",
    "summary": "  A chain of connected genes with activation-repression links is analysed. It\nis shown that for various promoter activity functions (parametrised by Hill\ncoefficient) the equations describing the concentrations of transcription\nfactors, are differential-difference KdV-type with perturbations. In the case\nof large Hill coefficient the proteomic signal along the gene network is given\nby a superposition of perturbed dark solitons of defocusing\ndifferential-difference mKdV equation. Biological implications are discussed.\n"
  },
  {
    "year": 2006,
    "title": "Networks from gene expression time series: characterization of\n  correlation patterns",
    "summary": "  This paper describes characteristic features of networks reconstructed from\ngene expression time series data. Several null models are considered in order\nto discriminate between informations embedded in the network that are related\nto real data, and features that are due to the method used for network\nreconstruction (time correlation).\n"
  },
  {
    "year": 2006,
    "title": "How much non-coding DNA do eukaryotes require?",
    "summary": "  Despite tremendous advances in the field of genomics, the amount and function\nof the large non-coding part of the genome in higher organisms remains poorly\nunderstood. Here we report an observation, made for 37 fully sequenced\neukaryotic genomes, which indicates that eukaryotes require a certain minimum\namount of non-coding DNA (ncDNA). This minimum increases quadratically with the\namount of DNA located in exons. Based on a simple model of the growth of\nregulatory networks, we derive a theoretical prediction of the required\nquantity of ncDNA and find it to be in excellent agreement with the data. The\namount of additional ncDNA (in basepairs) which eukaryotes require obeys Ndef =\n1/2 (Nc / Np) (Nc - Np), where Nc is the amount of exonic DNA, and Np is a\nconstant of about 10Mb. This value Ndef corresponds to a few percent of the\ngenome in Homo sapiens and other mammals, and up to half the genome in simpler\neukaryotes. Thus our findings confirm that eukaryotic life depends on a\nsubstantial fraction of ncDNA and also make a prediction of the size of this\nfraction, which matches the data closely.\n"
  },
  {
    "year": 2006,
    "title": "Di-nucleotide Entropy as a Measure of Genomic Sequence Functionality",
    "summary": "  Considering vast amounts of genomic sequences of mostly unknown\nfunctionality, in-silico prediction of functional regions is an important\nenterprise. Many genomic browsers employ GC content, which was observed to be\nelevated in gene-rich functional regions. This report shows that the entropy of\ndi- and tri-nucleotides distributions provides a superior measure of genomic\nsequence functionality, and proposes an explanation on why the GC content must\nbe elevated (closer to 50%) in functional regions. Regions with high entropy\nstrongly co-localize with exons and provide genome-wide evidences of purifying\nselection acting on non-coding regions, such as decreased SNPs density. The\nobservations suggest that functional non-coding regions are optimised for\nmutation load in a way, that transition mutations have less impact on\nfunctionality than transversions, leading to the decrease in transversions to\ntransitions ratio in functional regions.\n"
  },
  {
    "year": 2006,
    "title": "Genetic Variability of Splicing Sites",
    "summary": "  Splicing sites provide unique statistics in human genome due to their large\nnumber and reasonably complete annotation. Analyses of the cumulative SNPs\ndistribution in splicing sites reveal a few interesting observations. While a\ndegree of the nucleotide conservation reflects on the SNPs density\nmonotonically, no detectable changes in the SNPs frequencies spectrum were\nfound. Semi-conserved nucleotide sites harbor transition mutations\npredominantly. We propose that such transition preference is caused by\nco-evolution of a site with corresponding binding agents. Since transitions in\nhumans and similarly in other organisms are almost twice as frequent as\ntransversions, this adaptation significantly lowers the mutation load.\n"
  },
  {
    "year": 2006,
    "title": "Alterations of the mitochondrial proteome caused by the absence of\n  mitochondrial DNA: A proteomic view",
    "summary": "  The proper functioning of mitochondria requires that both the mitochondrial\nand the nuclear genome are functional. To investigate the importance of the\nmitochondrial genome, which encodes only 13 subunits of the respiratory\ncomplexes, the mitochondrial rRNAs and a few tRNAs, we performed a comparative\nstudy on the 143B cell line and on its Rho-0 counterpart, i.e., devoid of\nmitochondrial DNA. Quantitative differences were found, of course in the\nrespiratory complexes subunits, but also in the mitochondrial translation\napparatus, mainly mitochondrial ribosomal proteins, and in the ion and protein\nimport system, i.e., including membrane proteins. Various mitochondrial\nmetabolic processes were also altered, especially electron transfer proteins\nand some dehydrogenases, but quite often on a few proteins for each pathway.\nThis study also showed variations in some hypothetical or poorly characterized\nproteins, suggesting a mitochondrial localization for these proteins. Examples\ninclude a stomatin-like protein and a protein sharing homologies with bacterial\nproteins implicated in tyrosine catabolism. Proteins involved in apoptosis\ncontrol are also found modulated in Rho-0 mitochondria.\n"
  },
  {
    "year": 2006,
    "title": "High expression of antioxidant proteins in dendritic cells: possible\n  implications in atherosclerosis",
    "summary": "  Dendritic cells (DCs) display the unique ability to activate naive T cells\nand to initiate primary T cell responses revealed in DC-T cell alloreactions.\nDCs frequently operate under stress conditions. Oxidative stress enhances the\nproduction of inflammatory cytokines by DCs. We performed a proteomic analysis\nto see which major changes occur, at the protein expression level, during DC\ndifferentiation and maturation. Comparative two-dimensional gel analysis of the\nmonocyte, immature DC, and mature DC stages was performed. Manganese superoxide\ndismutase (Mn-SOD) reached 0.7% of the gel-displayed proteins at the mature DC\nstage. This important amount of Mn-SOD is a primary antioxidant defense system\nagainst superoxide radicals, but its product, H(2)O(2), is also deleterious for\ncells. Peroxiredoxin (Prx) enzymes play an important role in eliminating such\nperoxide. Prx1 expression level continuously increased during DC\ndifferentiation and maturation, whereas Prx6 continuously decreased, and Prx2\npeaked at the immature DC stage. As a consequence, DCs were more resistant than\nmonocytes to apoptosis induced by high amounts of oxidized low density\nlipoproteins containing toxic organic peroxides and hydrogen peroxide.\nFurthermore DC-stimulated T cells produced high levels of receptor activator of\nnuclear factor kappaB ligand, a chemotactic and survival factor for monocytes\nand DCs. This study provides insights into the original ability of DCs to\nexpress very high levels of antioxidant enzymes such as Mn-SOD and Prx1, to\ndetoxify oxidized low density lipoproteins, and to induce high levels of\nreceptor activator of nuclear factor kappaB ligand by the T cells they activate\nand further emphasizes the role that DCs might play in atherosclerosis, a\npathology recognized as a chronic inflammatory disorder.\n"
  },
  {
    "year": 2006,
    "title": "Improved mass spectrometry compatibility is afforded by ammoniacal\n  silver staining",
    "summary": "  Sequence coverage in MS analysis of protein digestion-derived peptides is a\nkey issue for detailed characterization of proteins or identification at low\nquantities. In gel-based proteomics studies, the sequence coverage greatly\ndepends on the protein detection method. It is shown here that ammoniacal\nsilver detection methods offer improved sequence coverage over standard silver\nnitrate methods, while keeping the high sensitivity of silver staining. With\nthe development of 2D-PAGE-based proteomics, another burden is placed on the\ndetection methods used for protein detection on 2-D-gels. Besides the classical\nrequirements of linearity, sensitivity, and homogeneity from one protein to\nanother, detection methods must now take into account another aspect, namely\ntheir compatibility with MS. This compatibility is evidenced by two different\nand complementary aspects, which are (i) the absence of adducts and artefactual\nmodifications on the peptides obtained after protease digestion of a protein\ndetected and digested in - gel, and (ii) the quantitative yield of peptides\nrecovered after digestion and analyzed by the mass spectrometer. While this\nquantitative yield is not very important per se, it is however a crucial\nparameter as it strongly influences the S/N of the mass spectrum and thus the\nnumber of peptides that can be detected from a given protein input, especially\nat low protein amounts. This influences in turn the sequence coverage and thus\nthe detail of the analysis provided by the mass spectrometer.\n"
  },
  {
    "year": 2006,
    "title": "About thiol derivatization and resolution of basic proteins in\n  two-dimensional electrophoresis",
    "summary": "  The influence of thiol blocking on the resolution of basic proteins by\ntwo-dimensional electrophoresis was investigated. Cysteine blocking greatly\nincreased resolution and decreased streaking, especially in the basic region of\nthe gels. Two strategies for cysteine blocking were found to be efficient:\nclassical alkylation with maleimide derivatives and mixed disulfide exchange\nwith an excess of a low molecular weight disulfide. The effect on resolution\nwas significant enough to allow correct resolution of basic proteins with\nin-gel rehydration on wide gradients (e.g. 3-10 and 4-12), but anodic\ncup-loading was still required for basic gradients (e.g. 6-12 or 8-12). These\nresults demonstrate that thiol-related problems are not solely responsible for\nstreaking of basic proteins on two-dimensional gels.\n"
  },
  {
    "year": 2006,
    "title": "Evaluation of nonionic and zwitterionic detergents as membrane protein\n  solubilizers in two-dimensional electrophoresis",
    "summary": "  The solubilizing power of various nonionic and zwitterionic detergents as\nmembrane protein solubilizers for two-dimensional electrophoresis was\ninvestigated. Human red blood cell ghosts and Arabidopsis thaliana leaf\nmembrane proteins were used as model systems. Efficient detergents could be\nfound in each class, i.e. with oligooxyethylene, sugar or sulfobetaine polar\nheads. Among the commercially available nonionic detergents, dodecyl maltoside\nand decaethylene glycol mono hexadecyl ether proved most efficient. They\ncomplement the more classical sulfobetaine detergents to widen the scope of\nuseful detergents for the solubilization of membrane proteins in proteomics.\n"
  },
  {
    "year": 2006,
    "title": "Progress in the definition of a reference human mitochondrial proteome",
    "summary": "  Owing to the complexity of higher eukaryotic cells, a complete proteome is\nlikely to be very difficult to achieve. However, advantage can be taken of the\ncell compartmentalization to build organelle proteomes, which can moreover be\nviewed as specialized tools to study specifically the biology and \"physiology\"\nof the target organelle. Within this frame, we report here the construction of\nthe human mitochondrial proteome, using placenta as the source tissue. Protein\nidentification was carried out mainly by peptide mass fingerprinting. The\noptimization steps in two-dimensional electrophoresis needed for proteome\nresearch are discussed. However, the relative paucity of data concerning\nmitochondrial proteins is still the major limiting factor in building the\ncorresponding proteome, which should be a useful tool for researchers working\non human mitochondria and their deficiencies.\n"
  },
  {
    "year": 2006,
    "title": "Detergents and Chaotropes for Protein Solubilization before\n  Two-Dimensional Electrophoresis",
    "summary": "  Because of the outstanding separating capabilities of two-dimensional\nelectrophoresis for complete proteins, it would be advantageous to be able to\napply it to all types of proteins. Unfortunately, severe solubility problems\nhamper the analysis of many classes of proteins, but especially membrane\nproteins. These problems arise mainly in the extraction and isoelectric\nfocusing steps, and solutions are sought to improve protein solubility under\nthe conditions prevailing during isoelectric focusing. These solutions deal\nmainly with chaotropes and new detergents, which are both able to enhance\nprotein solubility. The input of these compounds in proteomics analysis of\nmembrane proteins is discussed, as well as future directions.\n"
  },
  {
    "year": 2006,
    "title": "A versatile electrophoresis system for the analysis of high- and\n  low-molecular-weight proteins",
    "summary": "  A new, versatile, multiphasic buffer system for high-resolution sodium\ndodecyl sulfate-polyacrylamide gel electrophoresis of proteins in the relative\nmolecular weight range of 300 000-3000 Da is described. The system, based on\nthe theory of multiphasic zone electrophoresis, allows complete stacking and\ndestacking of proteins in the above M(r) range. The buffer system uses taurine\nand chloride as trailing and leading ion, respectively, and Tris, at a pH close\nto its pK(a), as the buffering counterion. Coupled with limited variation in\nthe acrylamide concentration, this electrophoresis system allows to tailor the\nresolution in the 6-200 kDa M(r) range, with minimal difficulties in the post\nelectrophoretic identification processes.\n"
  },
  {
    "year": 2006,
    "title": "Improvement of the solubilization of proteins in two-dimensional\n  electrophoresis with immobilized pH gradients",
    "summary": "  Membrane and nuclear proteins of poor solubility have been separated by high\nresolution two-dimensional (2-D) gel electrophoresis. Isoelectric focusing with\nimmobilized pH gradients leads to severe quantitative losses of proteins in the\nresulting 2-D map, although the resolution is usually high. Protein solubility\ncould be improved by using denaturing solutions containing various detergents\nand chaotropes. Best results were obtained with a denaturing solution\ncontaining urea, thiourea, and detergents (both nonionic and zwitterionic). The\nusefulness of thiourea-containing denaturing mixtures is shown for microsomal\nand nuclear proteins as well as for tubulin, a protein highly prone to\naggregation.\n"
  },
  {
    "year": 2006,
    "title": "Gene induction during differentiation of human monocytes into dendritic\n  cells: an integrated study at the RNA and protein levels",
    "summary": "  Changes in gene expression occurring during differentiation of human\nmonocytes into dendritic cells were studied at the RNA and protein levels.\nThese studies showed the induction of several gene classes corresponding to\nvarious biological functions. These functions encompass antigen processing and\npresentation, cytoskeleton, cell signalling and signal transduction, but also\nan increase in mitochondrial function and in the protein synthesis machinery,\nincluding some, but not all, chaperones. These changes put in perspective the\nevents occurring during this differentiation process. On a more technical\npoint, it appears that the studies carried out at the RNA and protein levels\nare highly complementary.\n"
  },
  {
    "year": 2006,
    "title": "Linguistic mechanism of the evolution of amino acid frequencies and\n  genomic GC content",
    "summary": "  Much information is stored in amino acid composition of protein and base\ncomposition of DNA. We simulated the evolution of amino acid frequencies and\ngenomic GC content by a linguistic model. It is showed that the evolution of\ngenetic code determines the evolution of amino acid frequencies and genomic GC\ncontent. We explained the relationships among amino acid frequencies, genomic\nGC content and protein length distribution in a unified theoretical framework.\nEspecially, the simulations of the evolution of amino acid frequencies and the\ncodon position GC content agree dramatically with the results based on the data\nof all known genomes so far. Furthermore, we found that the space of average\nprotein length in proteome and ratio of amino acid frequencies is useful to\ndescribe the phylogeny and evolution. Amazingly, the dots of all the species in\nthis space form an evolutionary flow. We believe that the amino acid gain and\nloss is motivated by the established pattern of the variation of amino acid\nfrequencies. The linguistic mechanism is helpful to unveil the origin of the\ngenetic code.\n"
  },
  {
    "year": 2006,
    "title": "Beneficial mutation-selection balance and the effect of linkage on\n  positive selection",
    "summary": "  When beneficial mutations are rare, they accumulate by a series of selective\nsweeps. But when they are common, many beneficial mutations will occur before\nany can fix, so there will be many different mutant lineages in the population\nconcurrently. In an asexual population, these different mutant lineages\ninterfere and not all can fix simultaneously. In addition, further beneficial\nmutations can accumulate in mutant lineages while these are still a minority of\nthe population. In this paper, we analyze the dynamics of such multiple\nmutations and the interplay between multiple mutations and interference between\nclones. These result in substantial variation in fitness accumulating within a\nsingle asexual population. The amount of variation is determined by a balance\nbetween selection, which destroys variation, and beneficial mutations, which\ncreate more. The behavior depends in a subtle way on the population parameters:\nthe population size, the beneficial mutation rate, and the distribution of the\nfitness increments of the potential beneficial mutations. The\nmutation-selection balance leads to a continually evolving population with a\nsteady-state fitness variation. This variation increases logarithmically with\nboth population size and mutation rate and sets the rate at which the\npopulation accumulates beneficial mutations, which thus also grows only\nlogarithmically with population size and mutation rate. These results imply\nthat mutator phenotypes are less effective in larger asexual populations. They\nalso have consequences for the advantages (or disadvantages) of sex via the\nFisher-Muller effect; these are discussed briefly.\n"
  },
  {
    "year": 2006,
    "title": "Principles of microRNA regulation of a human cellular signaling network",
    "summary": "  MicroRNAs (miRNAs) are endogenous 22-nucleotide RNAs, which suppress gene\nexpression by selectively binding to the 3-noncoding region of specific message\nRNAs through base-pairing. Given the diversity and abundance of miRNA targets,\nmiRNAs appear to functionally interact with various components of many cellular\nnetworks. By analyzing the interactions between miRNAs and a human cellular\nsignaling network, we found that miRNAs predominantly target positive\nregulatory motifs, highly connected scaffolds and most downstream network\ncomponents such as signaling transcription factors, but less frequently target\nnegative regulatory motifs, common components of basic cellular machines and\nmost upstream network components such as ligands. In addition, when an adaptor\nhas potential to recruit more downstream components, these components are more\nfrequently targeted by miRNAs. This work uncovers the principles of miRNA\nregulation of signal transduction networks and implies a potential function of\nmiRNAs for facilitating robust transitions of cellular response to\nextracellular signals and maintaining cellular homeostasis.\n"
  },
  {
    "year": 2006,
    "title": "Information Theory of Genomes",
    "summary": "  Relation of genome sizes to organisms complexity is still described rather\nequivocally. Neither the number of genes (G-value), nor the total amount of DNA\n(C-value) correlates consistently with phenotype complexity. Using information\ntheory considerations we developed a model that allows a quantative estimate\nfor the amount of functional information in a genomic sequence. This model\neasily answers the long-standing question of why GC content is increased in\nfunctional regions. The model allows consistent estimate of genome\ncomplexities, resolving the major discrepancies of G- and C-values. For related\norganisms with similarly complex phenotypes, this estimate provides biological\ninsights into their niches complexities. This theoretical framework suggests\nthat biological information can rapidly evolve on demand from environment,\nmainly in non-coding genomic sequence and explains the role of duplications in\nthe evolution of biological information. Knowing the approximate amount of\nfunctionality in a genomic sequence is useful for many applications such as\nphylogenetics analyses, in-silico functional elements discovery or prioritising\ntargets for genotyping and sequencing.\n"
  },
  {
    "year": 2006,
    "title": "Hybridization to surface-bound oligonucleotide probes: Influence of\n  point defects",
    "summary": "  Microarray-based genotyping is based on the high discrimination capability of\noligonucleotide probes. For detection of Single Nucleotide Polymorphisms (SNPs)\nsingle-base discrimination is required. We investigate how various\npoint-mutations, comprising single base mismatches (MMs), insertions and\ndeletions, affect hybridization of DNA-DNA oligonucleotide duplexes. Employing\nlight-directed in situ synthesis we fabricate DNA microarrays with\ncomprehensive sets of cognate point-mutated probes, allowing us to\nsystematically investigate the influence of defect type, position and nearest\nneighbor effects. Defect position has been identified as the dominating\ninfluential factor. This positional effect which is almost identical for the\ndifferent point-mutation types, is biased from the local sequence environment.\nThe impact of the MM type is largely determined by the type of base pair\n(either AT or CG) affected by the mismatch. We observe that single base\ninsertions next to like-bases result in considerably larger hybridization\nsignals than insertions next to nonidentical bases. The latter as well as the\ndistinct position dependence could be explained by a kinetic zipper model in\nwhich point defects represent a barrier for the rapid closure of the DNA\nduplex.\n"
  },
  {
    "year": 2006,
    "title": "An introduction to reconstructing ancestral genomes",
    "summary": "  Recent advances in high-throughput genomics technologies have resulted in the\nsequencing of large numbers of (near) complete genomes. These genome sequences\nare being mined for important functional elements, such as genes. They are also\nbeing compared and contrasted in order to identify other functional sequences,\nsuch as those involved in the regulation of genes. In cases where DNA sequences\nfrom different organisms can be determined to have originated from a common\nancestor, it is natural to try to infer the an- cestral sequences. The\nreconstruction of ancestral genomes can lead to insights about genome\nevolution, and the origins and diversity of function. There are a number of\ninteresting foundational questions associated with reconstructing ancestral\ngenomes: Which statistical models for evolution should be used for making\ninferences about ancestral sequences? How should extant genomes be compared in\norder to facilitate ancestral reconstruction? Which portions of ancestral\ngenomes can be reconstructed reliably, and what are the limits of ancestral\nreconstruction? We discuss recent progress on some of these questions, offer\nsome of our own opinions, and highlight interesting mathematics, statistics,\nand computer science problems.\n"
  },
  {
    "year": 2006,
    "title": "MicroRNAs preferentially target the genes with high transcriptional\n  regulation complexity",
    "summary": "  Over the past few years, microRNAs (miRNAs) have emerged as a new prominent\nclass of gene regulatory factors that negatively regulate expression of\napproximately one-third of the genes in animal genomes at post-transcriptional\nlevel. However, it is still unclear why some genes are regulated by miRNAs but\nothers are not, i.e. what principles govern miRNA regulation in animal genomes.\nIn this study, we systematically analyzed the relationship between\ntranscription factors (TFs) and miRNAs in gene regulation. We found that the\ngenes with more TF-binding sites have a higher probability of being targeted by\nmiRNAs and have more miRNA-binding sites on average. This observation reveals\nthat the genes with higher cis-regulation complexity are more coordinately\nregulated by TFs at the transcriptional level and by miRNAs at the\npost-transcriptional level. This is a potentially novel discovery of mechanism\nfor coordinated regulation of gene expression. Gene ontology analysis further\ndemonstrated that such coordinated regulation is more popular in the\ndevelopmental genes.\n"
  },
  {
    "year": 2007,
    "title": "Motif Discovery through Predictive Modeling of Gene Regulation",
    "summary": "  We present MEDUSA, an integrative method for learning motif models of\ntranscription factor binding sites by incorporating promoter sequence and gene\nexpression data. We use a modern large-margin machine learning approach, based\non boosting, to enable feature selection from the high-dimensional search space\nof candidate binding sequences while avoiding overfitting. At each iteration of\nthe algorithm, MEDUSA builds a motif model whose presence in the promoter\nregion of a gene, coupled with activity of a regulator in an experiment, is\npredictive of differential expression. In this way, we learn motifs that are\nfunctional and predictive of regulatory response rather than motifs that are\nsimply overrepresented in promoter sequences. Moreover, MEDUSA produces a model\nof the transcriptional control logic that can predict the expression of any\ngene in the organism, given the sequence of the promoter region of the target\ngene and the expression state of a set of known or putative transcription\nfactors and signaling molecules. Each motif model is either a $k$-length\nsequence, a dimer, or a PSSM that is built by agglomerative probabilistic\nclustering of sequences with similar boosting loss. By applying MEDUSA to a set\nof environmental stress response expression data in yeast, we learn motifs\nwhose ability to predict differential expression of target genes outperforms\nmotifs from the TRANSFAC dataset and from a previously published candidate set\nof PSSMs. We also show that MEDUSA retrieves many experimentally confirmed\nbinding sites associated with environmental stress response from the\nliterature.\n"
  },
  {
    "year": 2007,
    "title": "Marker enzyme phenotype ratios in agamospermous sugarbeet progenies as a\n  demonstration of multidimensional encoding of inherited information in plants",
    "summary": "  It has been demonstrated that the observed ratio of phenotypes of marker\nenzymes in some sugarbeet plants produced by mitotic agamospermy can be\nexplained by different degrees of endoreduplication of chromosomes carrying\ndifferent alleles of the enzyme loci. In these plants, different patterns of\nvariability of the enzymes controlled by the linked loci suggest different\ndegrees of endoreduplication of different chromosomal regions. A concept of\nmultidimensional encoding of inherited information in eukaryotes has been\nproposed.\n"
  },
  {
    "year": 2007,
    "title": "In search of lost introns",
    "summary": "  Many fundamental questions concerning the emergence and subsequent evolution\nof eukaryotic exon-intron organization are still unsettled. Genome-scale\ncomparative studies, which can shed light on crucial aspects of eukaryotic\nevolution, require adequate computational tools.\n  We describe novel computational methods for studying spliceosomal intron\nevolution. Our goal is to give a reliable characterization of the dynamics of\nintron evolution. Our algorithmic innovations address the identification of\northologous introns, and the likelihood-based analysis of intron data. We\ndiscuss a compression method for the evaluation of the likelihood function,\nwhich is noteworthy for phylogenetic likelihood problems in general. We prove\nthat after $O(nL)$ preprocessing time, subsequent evaluations take $O(nL/\\log\nL)$ time almost surely in the Yule-Harding random model of $n$-taxon\nphylogenies, where $L$ is the input sequence length.\n  We illustrate the practicality of our methods by compiling and analyzing a\ndata set involving 18 eukaryotes, more than in any other study to date. The\nstudy yields the surprising result that ancestral eukaryotes were fairly\nintron-rich. For example, the bilaterian ancestor is estimated to have had more\nthan 90% as many introns as vertebrates do now.\n"
  },
  {
    "year": 2007,
    "title": "Finding Sequence Features in Tissue-specific Sequences",
    "summary": "  The discovery of motifs underlying gene expression is a challenging one. Some\nof these motifs are known transcription factors, but sequence inspection often\nprovides valuable clues, even discovery of novel motifs with uncharacterized\nfunction in gene expression. Coupled with the complexity underlying\ntissue-specific gene expression, there are several motifs that are putatively\nresponsible for expression in a certain cell type. This has important\nimplications in understanding fundamental biological processes, such as\ndevelopment and disease progression. In this work, we present an approach to\nthe principled selection of motifs (not necessarily transcription factor sites)\nand examine its application to several questions in current bioinformatics\nresearch.\n  There are two main contributions of this work: Firstly, we introduce a new\nmetric for variable selection during classification, and secondly, we\ninvestigate a problem of finding specific sequence motifs that underlie tissue\nspecific gene expression. In conjunction with the SVM classifier we find these\nmotifs and discover several novel motifs which have not yet been attributed\nwith any particular functional role (eg: TFBS binding motifs). We hypothesize\nthat the discovery of these motifs would enable the large-scale investigation\nfor the tissue specific regulatory potential of any conserved sequence element\nidentified from genome-wide studies.\n  Finally, we propose the utility of this developed framework to not only aid\ndiscovery of discriminatory motifs, but also to examine the role of any motif\nof choice in co-regulation or co-expression of gene groups.\n"
  },
  {
    "year": 2007,
    "title": "Bacteria are not Lamarckian",
    "summary": "  Instructive influence of environment on heredity has been a debated topic for\ncenturies. Darwin's identification of natural selection coupled to chance\nvariation as the driving force for evolution, against a formal interpretation\nproposed by Lamarck, convinced most scientists that environment does not\nspecifically instruct evolution in an oriented direction. This is true for\nmulticellular organisms. In contrast, bacteria were long thought of as prone to\nreceive oriented influences from their environment, although much was in favour\nof the Darwinian route (1). In this context Cairns et al. raised a passionate\ndebate by suggesting that bacteria generate mutations oriented by the\nenvironmental conditions (2). Several independent pieces of work subsequently\ndemonstrated that mutations overcoming specific defects arised as a consequence\nof cultivation on specific media (3-7). Two diametrically opposed\ninterpretations were proposed to explain these observations : either induction\nof mutations instructed by the environment (e.g. by a process involving a\nputative reverse transcription) or selection of variants among a large set of\nmutant bacteria generated when stress conditions are present. The experiments\npresented below indicate that the Darwinian paradigm is the most plausible.\n"
  },
  {
    "year": 2007,
    "title": "Nucleotide Distribution Patterns in Insect Genomes",
    "summary": "  This work analyzed genome-wide nucleotide distribution patterns in ten insect\ngenomes. Two internal measures were applied: (i) GC variation and (ii) third\ncodon nucleotide preference. Although the genome size and overall GC level did\nnot show any correlation with insect order, the internal measures usually\ndisplayed higher levels of consistency. GC variations in genomes of\nhymenopteran insects, honeybee and wasp, ranked highest among all eukaryotic\ngenomes analyzed by us. Genomes of honeybee and beetle, insects of different\norders with similar overall GC levels, showed significant internal differences.\nHoneybee genome stood out as unusual due to its high GC variation and\n'left-handed' gene locations.\n"
  },
  {
    "year": 2007,
    "title": "Ultraconserved Sequences in the Honeybee Genome - Are GC-rich Regions\n  Preferred?",
    "summary": "  Among all insect genomes, honeybee displays one of the most unusual patterns\nwith interspersed long AT and GC-rich segments. Nearly 75% of the\nprotein-coding genes are located in the AT-rich segments of the genome, but the\nbiological significance of the GC-rich regions is not well understood. Based on\nan observation that the bee miRNAs, actins and tubulins are located in the\nGC-rich segments, this work investigated whether other highly conserved genomic\nregions show similar preferences. Sequences ultraconserved between the genomes\nof honeybee and Nasonia, another hymenopteran insect, were determined. They\nshowed strong preferences towards locating in the GC-rich regions of the bee\ngenome.\n"
  },
  {
    "year": 2007,
    "title": "A windowed local fdr estimator providing higher resolution and robust\n  thresholds",
    "summary": "  Motivation: In microarray analysis, special consideration must be given to\nthe issues of multiple statistical tests and typically p-values are adjusted to\ncontrol family-wise error rate (FWER) or false discovery rate (FDR). FDR\nmetrics have been suggested for controlling false positives, however, genes\nwith p-values close to the threshold typically have a higher chance of being\nfalse positives than genes with very low p-values. The local FDR (fdr) metric\ngives the number of false positives in the vicinity of a test statistic. We\npropose a new fdr estimator that uses windows instead of binsand define\nheuristics that use the fluctuations in the estimator to determine robust\nthresholds for classifying differential expression.\n  Results: Our fdr approach estimates the false discovery rate within a window\nof p-values. We present heuristics that derive robust fdr thresholds such that\na significant change in the fdr threshold yields a small change in the number\nof rejected hypotheses. We compare these thresholds with thresholds from other\napproaches using two simulated datasets and one cancer microarray dataset. In\nthe latter, our estimator finds two robust thresholds. Since our fdr estimator\nis an extension of the FDR metric, it can be used with many FDR estimation\nmethods.\n  Availability: An R function implementing the proposed estimator is available\nat http://www.dbi.tju.edu/dbi/tools/fdr\n  Contact: james.schwaber@jefferson.edu\n  Supplementary Information: Supplementary figures and code are available at\nhttp://www.dbi.tju.edu/dbi/tools/fdr\n"
  },
  {
    "year": 2007,
    "title": "Toward a better analysis of secreted proteins: the example of the\n  myeloid cells secretome",
    "summary": "  The analysis of secreted proteins represents a challenge for current\nproteomics techniques. Proteins are usually secreted at low concentrations in\nthe culture media, which makes their recovery difficult. In addition, culture\nmedia are rich in salts and other compounds interfering with most proteomics\ntechniques, which makes selective precipitation of proteins almost mandatory\nfor a correct subsequent proteomics analysis. Last but not least, the\nnon-secreted proteins liberated in the culture medium upon lysis of a few dead\ncells heavily contaminate the so-called secreted proteins preparations. Several\ntechniques have been used in the past for concentration of proteins secreted in\nculture media. These techniques present several drawbacks, such as\ncoprecipitation of salts or poor yields at low protein concentrations. Improved\ntechniques based on carrier-assisted trichloroacetic acid precipitation are\ndescribed and discussed in this paper. These techniques have been used to\nanalyse the secretome of myeloid cells (macrophages, dendritic cells) and\nenabled to analyze proteins secreted at concentrations close to 1 ng/ml,\nthereby allowing to detect some of the cytokines (TNF, IL-12) secreted by the\nmyeloid cells upon activation by bacterial products.\n"
  },
  {
    "year": 2007,
    "title": "A New Genetic Code Table",
    "summary": "  In this paper it is shown that within a Combined Genetic Code Table, realized\nthrough a combination of Watson-Crick Table and Codon Path Cube it exists,\nwithout an exception, a strict distinction between two classes of enzymes\naminoacyl-tRNA synthetases, corresponding two classes of amino acids and\nbelonging codons. By this, the distinction itself is followed by a strict\nbalance of atom number within two subclasses of class I as well as two\nsubclasses of class II of amino acids.\n"
  },
  {
    "year": 2007,
    "title": "A Pattern Discovery-Based Method for Detecting Multi-Locus Genetic\n  Association",
    "summary": "  Methods to effectively detect multi-locus genetic association are becoming\nincreasingly relevant in the genetic dissection of complex trait in humans.\nCurrent approaches typically consider a limited number of hypotheses, most of\nwhich are related to the effect of a single locus or of a relatively small\nnumber of neighboring loci on a chromosomal region. We have developed a novel\nmethod that is specifically designed to detect genetic association involving\nmultiple disease-susceptibility loci, possibly on different chromosomes. Our\napproach relies on the efficient discovery of patterns comprising spatially\nunrestricted polymorphic markers and on the use of appropriate test statistics\nto evaluate pattern-trait association. Power calculations using multi-locus\ndisease models demonstrate significant gain of power by using this method in\ndetecting multi-locus genetic association when compared to a standard single\nmarker analysis method. When analyzing a Schizophrenia dataset, we confirmed a\npreviously identified gene-gene interaction. In addition, a less conspicuous\nassociation involving different markers on the same two genes was also\nidentified, implicating genetic heterogeneity.\n"
  },
  {
    "year": 2007,
    "title": "Noise-filtering features of transcription regulation in the yeast S.\n  cerevisiae",
    "summary": "  Transcription regulation is largely governed by the profile and the dynamics\nof transcription factors' binding to DNA. Stochastic effects are intrinsic to\nthis dynamics and the binding to functional sites must be controled with a\ncertain specificity for living organisms to be able to elicit specific cellular\nresponses. Specificity stems here from the interplay between binding affinity\nand cellular abundancy of transcription factor proteins and the binding of such\nproteins to DNA is thus controlled by their chemical potential.\n  We combine large-scale protein abundance data in the budding yeast with\nbinding affinities for all transcription factors with known DNA binding site\nsequences to assess the behavior of their chemical potentials. A sizable\nfraction of transcription factors is apparently bound non-specifically to DNA\nand the observed abundances are marginally sufficient to ensure high\noccupations of the functional sites. We argue that a biological cause of this\nfeature is related to its noise-filtering consequences: abundances below\nphysiological levels do not yield significant binding of functional targets and\nmis-expressions of regulated genes are thus tamed.\n"
  },
  {
    "year": 2007,
    "title": "The Mechanisms of Codon Reassignments in Mitochondrial Genetic Codes",
    "summary": "  Many cases of non-standard genetic codes are known in mitochondrial genomes.\nWe carry out analysis of phylogeny and codon usage of organisms for which the\ncomplete mitochondrial genome is available, and we determine the most likely\nmechanism for codon reassignment in each case. Reassignment events can be\nclassified according to the gain-loss framework. The gain represents the\nappearance of a new tRNA for the reassigned codon or the change of an existing\ntRNA such that it gains the ability to pair with the codon. The loss represents\nthe deletion of a tRNA or the change in a tRNA so that it no longer translates\nthe codon. One possible mechanism is Codon Disappearance, where the codon\ndisappears from the genome prior to the gain and loss events. In the\nalternative mechanisms the codon does not disappear. In the Unassigned Codon\nmechanism, the loss occurs first, whereas in the Ambiguous Intermediate\nmechanism, the gain occurs first. Codon usage analysis gives clear evidence of\ncases where the codon disappeared at the point of the reassignment and also\ncases where it did not disappear. Codon disappearance is the probable\nexplanation for stop to sense reassignments and a small number of reassignments\nof sense codons. However, the majority of sense to sense reassignments cannot\nbe explained by codon disappearance. In the latter cases, by analysis of the\npresence or absence of tRNAs in the genome and of the changes in tRNA\nsequences, it is sometimes possible to distinguish between the Unassigned Codon\nand Ambiguous Intermediate mechanisms. We emphasize that not all reassignments\nfollow the same scenario and that it is necessary to consider the details of\neach case carefully.\n"
  },
  {
    "year": 2007,
    "title": "Mismatch Repair Error Implies Chargaff's Second Parity Rule",
    "summary": "  Chargaff's second parity rule holds empirically for most types of DNA that\nalong single strands of DNA the base contents are equal for complimentary\nbases, A = T, G = C. A Markov chain model is constructed to track the evolution\nof any single base position along single strands of genomes whose organisms are\nequipped with replication mismatch repair. Under the key assumptions that\nmismatch error rates primarily depend the number of hydrogen bonds of\nnucleotides and that the mismatch repairing process itself makes strand\nrecognition error, the model shows that the steady state probabilities for any\nbase position to take on one of the 4 nucleotide bases are equal for\ncomplimentary bases. As a result, Chargaff's second parity rule is the\nmanifestation of the Law of Large Number acting on the steady state\nprobabilities. More importantly, because the model pinpoints mismatch repair as\na basis of the rule, it is suitable for experimental verification.\n"
  },
  {
    "year": 2007,
    "title": "Chromatin Folding in Relation to Human Genome Function",
    "summary": "  Three-dimensional (3D) chromatin structure is closely related to genome\nfunction, in particular transcription. However, the folding path of the\nchromatin fiber in the interphase nucleus is unknown. Here, we systematically\nmeasured the 3D physical distance between pairwise labeled genomic positions in\ngene-dense, highly transcribed domains and gene-poor less active areas on\nchromosomes 1 and 11 in G1 nuclei of human primary fibroblasts, using\nfluorescence in situ hybridization. Interpretation of our results and those\npublished by others, based on polymer physics, shows that the folding of the\nchromatin fiber can be described as a polymer in a globular state (GS),\nmaintained by intra-polymer attractive interactions that counteract\nself-avoidance forces. The GS polymer model is able to describe chromatin\nfolding in as well the highly expressed domains as the lowly expressed ones,\nindicating that they differ in Kuhn length and chromatin compaction. Each type\nof genomic domain constitutes an ensemble of relatively compact globular\nfolding states, resulting in a considerable cellto- cell variation between\notherwise identical cells. We present evidence for different polymer folding\nregimes of the chromatin fiber on the length scale of a few mega base pairs and\non that of complete chromosome arms (several tens of Mb). Our results present a\nnovel view on the folding of the chromatin fiber in interphase and open the\npossibility to explore the nature of the intra-chromatin fiber interactions.\n"
  },
  {
    "year": 2007,
    "title": "Inverted and mirror repeats in model nucleotide sequences",
    "summary": "  We analytically and numerically study the probabilistic properties of\ninverted and mirror repeats in model sequences of nucleic acids. We consider\nboth perfect and non-perfect repeats, i.e. repeats with mismatches and gaps.\nThe considered sequence models are independent identically distributed (i.i.d.)\nsequences, Markov processes and long range sequences. We show that the number\nof repeats in correlated sequences is significantly larger than in i.i.d.\nsequences and that this discrepancy increases exponentially with the repeat\nlength for long range sequences.\n"
  },
  {
    "year": 2007,
    "title": "In silico evidence of the relationship between miRNAs and siRNAs",
    "summary": "  Both short interfering RNAs (siRNAs) and microRNAs (miRNAs) mediate the\nrepression of specific sequences of mRNA through the RNA interference pathway.\nIn the last years several experiments have supported the hypothesis that siRNAs\nand miRNAs may be functionally interchangeable, at least in cultured cells. In\nthis work we verify that this hypothesis is also supported by a computational\nevidence. We show that a method specifically trained to predict the activity of\nthe exogenous siRNAs assigns a high silencing level to experimentally\ndetermined human miRNAs. This result not only supports the idea of siRNAs and\nmiRNAs equivalence but indicates that it is possible to use computational tools\ndeveloped using synthetic small interference RNAs to investigate endogenous\nmiRNAs.\n"
  },
  {
    "year": 2007,
    "title": "The influence of horizontal gene transfer on the mean fitness of\n  unicellular populations in static environments",
    "summary": "  This paper develops a mathematical model describing the influence that\nconjugation-mediated Horizontal Gene Transfer (HGT) has on the\nmutation-selection balance in an asexually reproducing population of\nunicellular, prokaryotic organisms. It is assumed that mutation-selection\nbalance is reached in the presence of a fixed background concentration of\nantibiotic, to which the population must become resistant in order to survive.\nWe analyze the behavior of the model in the limit of low and high\nantibiotic-induced first-order death rate constants, and find that the highest\nmean fitness is obtained at low rates of bacterial conjugation. As the rate of\nconjugation crosses a threshold, the mean fitness decreases to a minimum, and\nthen rises asymptotically to a limiting value as the rate of conjugation\nbecomes infinitely large. However, this limiting value is smaller than the mean\nfitness obtained in the limit of low conjugation rate. This dependence of the\nmean fitness on the conjugation rate is fairly small for the parameter ranges\nwe have considered, and disappears as the first-order death rate constant due\nto the presence of antibiotic approaches zero. For large values of the\nantibiotic death rate constant, we have obtained an analytical solution for the\nbehavior of the mean fitness that agrees well with the results of simulations.\nThe results of this paper suggest that conjugation-mediated HGT has a slightly\ndeleterious effect on the mean fitness of a population at mutation-selection\nbalance. Therefore, we argue that HGT confers a selective advantage by allowing\nfor faster adaptation to a new or changing environment. The results of this\npaper are consistent with the observation that HGT can be promoted by\nenvironmental stresses on a population.\n"
  },
  {
    "year": 2007,
    "title": "Reciprocal best hits are not a logically sufficient condition for\n  orthology",
    "summary": "  It is common to use reciprocal best hits, also known as a boomerang\ncriterion, for determining orthology between sequences. The best hits may be\nfound by blast, or by other more recently developed algorithms. Previous work\nseems to have assumed that reciprocal best hits is a sufficient but not\nnecessary condition for orthology. In this article, I explain why reciprocal\nbest hits cannot logically be a sufficient condition for orthology. If\nreciprocal best hits is neither sufficient nor necessary for orthology, it\nwould seem worthwhile to examine further the logical foundations of some\nunsupervised algorithms that are used to identify orthologs.\n"
  },
  {
    "year": 2007,
    "title": "Codon Usage Bias Measured Through Entropy Approach",
    "summary": "  Codon usage bias measure is defined through the mutual entropy calculation of\nreal codon frequency distribution against the quasi-equilibrium one. This\nlatter is defined in three manners: (1) the frequency of synonymous codons is\nsupposed to be equal (i.e., the arithmetic mean of their frequencies); (2) it\ncoincides to the frequency distribution of triplets; and, finally, (3) the\nquasi-equilibrium frequency distribution is defined as the expected frequency\nof codons derived from the dinucleotide frequency distribution. The measure of\nbias in codon usage is calculated for 125 bacterial genomes.\n"
  },
  {
    "year": 2007,
    "title": "RNA polymerase motors on DNA track: effects of traffic congestion on RNA\n  synthesis",
    "summary": "  RNA polymerase (RNAP) is an enzyme that synthesizes a messenger RNA (mRNA)\nstrand which is complementary to a single-stranded DNA template. From the\nperspective of physicists, an RNAP is a molecular motor that utilizes chemical\nenergy input to move along the track formed by a ssDNA. In some circumstances,\nwhich are described in this paper, a large number of RNAPs move simultaneously\nalong the same track. We refer to such collective movements of the RNAPs as\nRNAP traffic because of the similarities between the collective dynamics of the\nRNAPs on ssDNA track and that of vehicles in highway traffic. In this paper we\ndevelop a theoretical model for RNAP traffic by incorporating the steric\ninteractions between RNAPs as well as the mechano-chemical cycle of individual\nRNAPs during the elongation of the mRNA. By a combination of analytical and\nnumerical techniques, we calculate the rates of mRNA synthesis and the average\ndensity profile of the RNAPs on the ssDNA track. We also suggest novel\nexperiments for testing our theoretical predictions.\n"
  },
  {
    "year": 2007,
    "title": "OrfMapper: A Web-Based Application for Visualizing Gene Clusters on\n  Metabolic Pathway Maps",
    "summary": "  Computational analyses of, e.g., genomic, proteomic, or metabolomic data,\ncommonly result in one or more sets of candidate genes, proteins, or enzymes.\nThese sets are often the outcome of clustering algorithms. Subsequently, it has\nto be tested if, e.g., the candidate gene-products are members of known\nmetabolic processes. With OrfMapper we provide a powerful but easy-to-use,\nweb-based database application, that supports such analyses. All services\nprovided by OrfMapper are freely available at http://www.orfmapper.com\n"
  },
  {
    "year": 2007,
    "title": "Silver staining of proteins in polyacrylamide gels",
    "summary": "  Silver staining is used to detect proteins after electrophoretic separation\non polyacrylamide gels. It combines excellent sensitivity (in the low nanogram\nrange) with the use of very simple and cheap equipment and chemicals. It is\ncompatible with downstream processing, such as mass spectrometry analysis after\nprotein digestion. The sequential phases of silver staining are protein\nfixation, then sensitization, then silver impregnation and finally image\ndevelopment. Several variants of silver staining are described here, which can\nbe completed in a time range from 2 h to 1 d after the end of the\nelectrophoretic separation. Once completed, the stain is stable for several\nweeks.\n"
  },
  {
    "year": 2007,
    "title": "Identification of candidate regulatory sequences in mammalian 3' UTRs by\n  statistical analysis of oligonucleotide distributions",
    "summary": "  3' untranslated regions (3' UTRs) contain binding sites for many regulatory\nelements, and in particular for microRNAs (miRNAs). The importance of\nmiRNA-mediated post-transcriptional regulation has become increasingly clear in\nthe last few years.\n  We propose two complementary approaches to the statistical analysis of\noligonucleotide frequencies in mammalian 3' UTRs aimed at the identification of\ncandidate binding sites for regulatory elements. The first method is based on\nthe identification of sets of genes characterized by evolutionarily conserved\noverrepresentation of an oligonucleotide. The second method is based on the\nidentification of oligonucleotides showing statistically significant strand\nasymmetry in their distribution in 3' UTRs.\n  Both methods are able to identify many previously known binding sites located\nin 3'UTRs, and in particular seed regions of known miRNAs. Many new candidates\nare proposed for experimental verification.\n"
  },
  {
    "year": 2007,
    "title": "Global regulation of genome duplication in eukaryotes: an overview from\n  the epifluorescence microscope",
    "summary": "  In eukaryotes, DNA replication is initiated along each chromosome at multiple\nsites called replication origins. Locally, each replication origin is\n\"licensed\", or specified, at the end of the M and the beginning of G1 phases of\nthe cell cycle. During S phase when DNA synthesis takes place, origins are\nactivated in stages corresponding to early and late replicating domains. The\nstaged and progressive activation of replication origins reflects the need to\nmaintain a strict balance between the number of active replication forks and\nthe rate at which DNA synthesis procedes. This suggests that origin densities\n(frequency of intiation) and replication fork movement (rates of elongation)\nmust be co-regulated in order to guarantee the efficient and complete\nduplication of each subchromosomal domain. Emerging evidence supports this\nproposal and suggests that the ATM/ATR intra-S phase checkpoint plays an\nimportant role in the co-regulation of initiation frequencies and rates of\nelongation. In the following, we review recent results concerning the\nmechanisms governing the global regulation of DNA replication and discuss the\nroles these mechanisms play in maintaining genome stability during both a\nnormal and perturbed S phase.\n"
  },
  {
    "year": 2007,
    "title": "Genomes: at the edge of chaos with maximum information capacity",
    "summary": "  We propose an order index, phi, which quantifies the notion of ``life at the\nedge of chaos'' when applied to genome sequences. It maps genomes to a number\nfrom 0 (random and of infinite length) to 1 (fully ordered) and applies\nregardless of sequence length. The 786 complete genomic sequences in GenBank\nwere found to have phi values in a very narrow range, 0.037+/-0.027. We show\nthis implies that genomes are halfway towards being completely random, namely,\nat the edge of chaos. We argue that this narrow range represents the\nneighborhood of a fixed-point in the space of sequences, and genomes are driven\nthere by the dynamics of a robust, predominantly neutral evolution process.\n"
  },
  {
    "year": 2007,
    "title": "Systems level circuit model of C. elegans undulatory locomotion:\n  mathematical modeling and molecular genetics",
    "summary": "  To establish the relationship between locomotory behavior and dynamics of\nneural circuits in the nematode C. elegans we combined molecular and\ntheoretical approaches. In particular, we quantitatively analyzed the motion of\nC. elegans with defective synaptic GABA and acetylcholine transmission,\ndefective muscle calcium signaling, and defective muscles and cuticle\nstructures, and compared the data with our systems level circuit model. The\nmajor experimental findings are: (i) anterior-to-posterior gradients of body\nbending flex for almost all strains both for forward and backward motion, and\nfor neuronal mutants, also analogous weak gradients of undulatory frequency,\n(ii) existence of some form of neuromuscular (stretch receptor) feedback, (iii)\ninvariance of neuromuscular wavelength, (iv) biphasic dependence of frequency\non synaptic signaling, and (v) decrease of frequency with increase of the\nmuscle time constant. Based on (i) we hypothesize that the Central Pattern\nGenerator (CPG) is located in the head both for forward and backward motion.\nPoints (i) and (ii) are the starting assumptions for our theoretical model,\nwhose dynamical patterns are qualitatively insensitive to the details of the\nCPG design if stretch receptor feedback is sufficiently strong and slow. The\nmodel reveals that stretch receptor coupling in the body wall is critical for\ngeneration of the neuromuscular wave. Our model agrees with our behavioral\ndata(iii), (iv), and (v), and with other pertinent published data, e.g., that\nfrequency is an increasing function of muscle gap-junction coupling.\n"
  },
  {
    "year": 2007,
    "title": "Genome landscapes and bacteriophage codon usage",
    "summary": "  Across all kingdoms of biological life, protein-coding genes exhibit unequal\nusage of synonmous codons. Although alternative theories abound, translational\nselection has been accepted as an important mechanism that shapes the patterns\nof codon usage in prokaryotes and simple eukaryotes. Here we analyze patterns\nof codon usage across 74 diverse bacteriophages that infect E. coli, P.\naeruginosa and L. lactis as their primary host. We introduce the concept of a\n`genome landscape,' which helps reveal non-trivial, long-range patterns in\ncodon usage across a genome. We develop a series of randomization tests that\nallow us to interrogate the significance of one aspect of codon usage, such a\nGC content, while controlling for another aspect, such as adaptation to\nhost-preferred codons. We find that 33 phage genomes exhibit highly non-random\npatterns in their GC3-content, use of host-preferred codons, or both. We show\nthat the head and tail proteins of these phages exhibit significant bias\ntowards host-preferred codons, relative to the non-structural phage proteins.\nOur results support the hypothesis of translational selection on viral genes\nfor host-preferred codons, over a broad range of bacteriophages.\n"
  },
  {
    "year": 2007,
    "title": "CompostBin: A DNA composition-based algorithm for binning environmental\n  shotgun reads",
    "summary": "  A major hindrance to studies of microbial diversity has been that the vast\nmajority of microbes cannot be cultured in the laboratory and thus are not\namenable to traditional methods of characterization. Environmental shotgun\nsequencing (ESS) overcomes this hurdle by sequencing the DNA from the organisms\npresent in a microbial community. The interpretation of this metagenomic data\ncan be greatly facilitated by associating every sequence read with its source\norganism. We report the development of CompostBin, a DNA composition-based\nalgorithm for analyzing metagenomic sequence reads and distributing them into\ntaxon-specific bins. Unlike previous methods that seek to bin assembled contigs\nand often require training on known reference genomes, CompostBin has the\nability to accurately bin raw sequence reads without need for assembly or\ntraining. It applies principal component analysis to project the data into an\ninformative lower-dimensional space, and then uses the normalized cut\nclustering algorithm on this filtered data set to classify sequences into\ntaxon-specific bins. We demonstrate the algorithm's accuracy on a variety of\nsimulated data sets and on one metagenomic data set with known species\nassignments. CompostBin is a work in progress, with several refinements of the\nalgorithm planned for the future.\n"
  },
  {
    "year": 2007,
    "title": "Power-law Signatures and Patchiness in Genechip Oligonucleotide\n  Microarrays",
    "summary": "  . Genechip oligonucleotide microarrays have been used widely for\ntranscriptional profiling of a large number of genes in a given paradigm. Gene\nexpression estimation precedes biological inference and is given as a complex\ncombination of atomic entities on the array called probes. These probe\nintensities are further classified into perfect-match (PM) and mis-match (MM)\nprobes. While former is a measure of specific binding, the lat-ter is a measure\nof non-specific binding. The behavior of the MM probes has especially proven to\nbe elusive. The present study investigates qualita-tive similarities in the\ndistributional signatures and local correlation struc-tures/patchiness between\nthe PM and MM probe intensities. These qualita-tive similarities are\nestablished on publicly available microarrays generated across laboratories\ninvestigating the same paradigm. Persistence of these similarities across raw\nas well as background subtracted probe intensities is also investigated. The\nresults presented raise fundamental concerns in inter-preting Genechip\noligonucleotide microarray data.\n"
  },
  {
    "year": 2007,
    "title": "Ultrafast coelectrophoretic fluorescent staining of proteins with\n  carbocyanines",
    "summary": "  Protein detection on SDS gels or on 2-D gels must combine several features,\nsuch as sensitivity, homogeneity from one protein to another, speed, low cost,\nand user-friendliness. For some applications, it is also interesting to have a\nnonfixing stain, so that proteins can be mobilized from the gel for further use\n(electroelution, blotting). We show here that coelectrophoretic staining by\nfluorophores of the oxacarbocyanine family, and especially\ndiheptyloxacarbocyanine, offers several positive features. The sensitivity is\nintermediate between the one of colloidal CBB and the one of fluroescent\nruthenium complexes. Detection is achieved within 1 h after the end of the\nelectrophoretic process and does not use any fixing or toxic agent. The\nfluorescent SDS-carbocyanine-protein complexes can be detected either with a\nlaser scanner with an excitation wavelength of 488 nm or with a UV table\noperating at 302 nm. Excellent sequence coverage in subsequent MS analysis of\nproteolytic peptides is also achieved with this detection method.\n"
  },
  {
    "year": 2007,
    "title": "DrosOCB: a high resolution map of conserved non coding sequences in\n  Drosophila",
    "summary": "  Comparative genomics methods are widely used to aid the functional annotation\nof non coding DNA regions. However, aligning non coding sequences requires new\nalgorithms and strategies, in order to take into account extensive\nrearrangements and turnover during evolution. Here we present a novel large\nscale alignment strategy which aims at drawing a precise map of conserved non\ncoding regions between genomes, even when these regions have undergone small\nscale rearrangments events and a certain degree of sequence variability. We\napplied our alignment approach to obtain a genome-wide catalogue of conserved\nnon coding blocks (CNBs) between Drosophila melanogaster and 11 other\nDrosophila species. Interestingly, we observe numerous small scale\nrearrangement events, such as local inversions, duplications and\ntranslocations, which are not observable in the whole genome alignments\ncurrently available. The high rate of observed low scale reshuffling show that\nthis database of CNBs can constitute the starting point for several\ninvestigations, related to the evolution of regulatory DNA in Drosophila and\nthe in silico identification of unannotated functional elements.\n"
  },
  {
    "year": 2007,
    "title": "Electric Transport Properties of the p53 Gene and the Effects of Point\n  Mutations",
    "summary": "  In this work, charge transport (CT) properties of the p53 gene are\nnumerically studied by the transfer matrix method, and using either single or\ndouble strand effective tight-binding models. A statistical analysis of the\nconsequences of known p53 point mutations on CT features is performed. It is\nfound that in contrast to other kind of mutation defects, cancerous mutations\nresult in much weaker changes of CT efficiency. Given the envisioned role\nplayed by CT in the DNA-repairing mechanism, our theoretical results suggest an\nunderlying physical explanation at the origin of carcinogenesis.\n"
  },
  {
    "year": 2007,
    "title": "Understanding Transcriptional Regulation Using De-novo Sequence Motif\n  Discovery, Network Inference and Interactome Data",
    "summary": "  Gene regulation is a complex process involving the role of several genomic\nelements which work in concert to drive spatio-temporal expression. The\nexperimental characterization of gene regulatory elements is a very complex and\nresource-intensive process. One of the major goals in computational biology is\nthe \\textit{in-silico} annotation of previously uncharacterized elements using\nresults from the subset of known, previously annotated, regulatory elements.\n  The recent results of the ENCODE project (\\emph{http://encode.nih.gov})\npresented in-depth analysis of such functional (regulatory) non-coding elements\nfor 1% of the human genome. It is hoped that the results obtained on this\nsubset can be scaled to the rest of the genome. This is an extremely important\neffort which will enable faster dissection of other functional elements in key\nbiological processes such as disease progression and organ development\n(\\cite{Kleinjan2005},\\cite{Lieb2006}. The computational annotation of these\nhitherto uncharacterized regions would require an identification of features\nthat have good predictive value.\n  In this work, we study transcriptional regulation as a problem in\nheterogeneous data integration, across sequence, expression and interactome\nlevel attributes. Using the example of the \\textit{Gata2} gene and its recently\ndiscovered urogenital enhancers \\cite{Khandekar2004} as a case study, we\nexamine the predictive value of various high throughput functional genomic\nassays (from projects like ENCODE and SymAtlas) in characterizing these\nenhancers and their regulatory role. Observing results from the application of\nmodern statistical learning methodologies for each of these data modalities, we\npropose a set of features that are most discriminatory to find these enhancers.\n"
  },
  {
    "year": 2007,
    "title": "From DNA sequence analysis to modeling replication in the human genome",
    "summary": "  We explore the large-scale behavior of nucleotide compositional strand\nasymmetries along human chromosomes. As we observe for 7 of 9 origins of\nreplication experimentally identified so far, the (TA+GC) skew displays rather\nsharp upward jumps, with a linear decreasing profile in between two successive\njumps. We present a model of replication with well positioned replication\norigins and random terminations that accounts for the observed characteristic\nserrated skew profiles. We succeed in identifying 287 pairs of putative\nadjacent replication origins with an origin spacing approximately 1-2 Mbp that\nare likely to correspond to replication foci observed in interphase nuclei and\nrecognized as stable structures that persist throughout subsequent cell\ngenerations.\n"
  },
  {
    "year": 2007,
    "title": "A mechanistic model for +1 frameshifts in eubacteria",
    "summary": "  This work applies the methods of signal processing and the concepts of\ncontrol system design to model the maintenance and modulation of reading frame\nin the process of protein synthesis. The model shows how translational speed\ncan modulate translational accuracy to accomplish programmed +1 frameshifts and\ncould have implications for the regulation of translational efficiency. A\nseries of free energy estimates were calculated from the ribosome's interaction\nwith mRNA sequences during the process of translation elongation in eubacteria.\nA sinusoidal pattern of roughly constant phase was detected in these free\nenergy signals. Signal phase was identified as a useful parameter for locating\nprogrammed +1 frameshifts encoded in bacterial genes for release factor 2. A\ndisplacement model was developed that captures the mechanism of frameshift\nbased on the information content of the signal parameters and the relative\nabundance of tRNA in the bacterial cell. Results are presented using\nexperimentally verified frameshift genes across eubacteria.\n"
  },
  {
    "year": 2008,
    "title": "Classification of arrayCGH data using a fused SVM",
    "summary": "  Motivation: Array-based comparative genomic hybridization (arrayCGH) has\nrecently become a popular tool to identify DNA copy number variations along the\ngenome. These profiles are starting to be used as markers to improve prognosis\nor diagnosis of cancer, which implies that methods for automated supervised\nclassification of arrayCGH data are needed. Like gene expression profiles,\narrayCGH profiles are characterized by a large number of variables usually\nmeasured on a limited number of samples. However, arrayCGH profiles have a\nparticular structure of correlations between variables, due to the spatial\norganization of BACs along the genome. This suggests that classical\nclassification methods, often based on the selection of a small number of\ndiscriminative features, may not be the most accurate methods and may not\nproduce easily interpretable prediction rules.\n  Results: We propose a new method for supervised classification of arrayCGH\ndata. The method is a variant of support vector machine (SVM) that incorporates\nthe biological specificities of DNA copy number variations along the genome as\nprior knowledge. The resulting classifier is a sparse linear classifier based\non a limited number of regions automatically selected on the chromosomes,\nleading to easy interpretation and identification of discriminative regions of\nthe genome. We test this method on three classification problems for bladder\nand uveal cancer, involving both diagnosis and prognosis. We demonstrate that\nthe introduction of the new prior on the classifier leads not only to more\naccurate predictions, but also to the identification of known and new regions\nof interest in the genome.\n  Availability: All data and algorithms are publicly available.\n"
  },
  {
    "year": 2008,
    "title": "Zinc adaptation and resistance to cadmium toxicity in mammalian cells.\n  Molecular insight by proteomic analysis",
    "summary": "  To identify proteins involved in cellular adaptive responses to zinc, a\ncomparative proteome analysis between a previously developed high zinc- and\ncadmium- resistant human epithelial cell line (HZR) and the parental HeLa cells\nhas been carried out. Differentially produced proteins included co-chaperones,\nproteins associated with oxido-reductase activities, and ubiquitin. Biochemical\npathways to which these proteins belong were probed for their involvement in\nthe resistance of both cell lines against cadmium toxicity. Among endoplasmic\nreticulum stressors, thapsigargin sensitized HZR cells, but not HeLa cells, to\ncadmium toxicity more acutely than tunicamycin, implying that these cells\nheavily relied on proper intracellular calcium distribution. The similar\nsensitivity of both HeLa and HZR cells to inhibitors of the proteasome, such as\nMG-132 or lactacystin, excluded improved proteasome activity as a mechanism\nassociated with zinc adaptation of HZR cells. The enzyme\n4-hydroxyphenylpyruvate dioxygenase was overproduced in HZR cells as compared\nto HeLa cells. It transforms 4-hydroxyphenylpyruvate to homogentisate in the\nsecond step of tyrosine catabolism. Inhibition of 4-hydroxyphenylpyruvate\ndioxygenase decreased the resistance of HZR cells against cadmium, but not that\nof HeLa cells, suggesting that adaptation to zinc overload and increased\n4-hydroxyphenylpyruvate removal are linked in HZR cells\n"
  },
  {
    "year": 2008,
    "title": "Interchromatidal central ridge and transversal symmetry in early\n  metaphasic human chromosome one",
    "summary": "  The topographic structure of Giemsa banded (G-banded) early metaphase human\nchromosomes adsorbed on glass was analyzed by atomic force microscope using\namplitude modulation mode [AM-AFM]. Longitudinal height measurements for early\nmetaphasic human chromosomes showed a central ridge that was further\ncharacterized by transversal height measurements. The heterochromatic regions\ndisplayed a high level of transversal symmetry, while the euchromatic ones\npresented several peaks across the transversal height measurements. We suggest\nthat this central ridge and symmetry patterns point out a transitional\narrangement of the early metaphase chromosome and support evidence for\ninterchromatidal interactions prior to disjunction.\n"
  },
  {
    "year": 2008,
    "title": "Chance and necessity in chromosomal gene distributions",
    "summary": "  By analyzing the spacing of genes on chromosomes, we find that\ntranscriptional and RNA-processing regulatory sequences outside coding regions\nleave footprints on the distribution of intergenic distances. Using analogies\nbetween genes on chromosomes and one-dimensional gases, we constructed a\nstatistical null model. We have used this to estimate typical upstream and\ndownstream regulatory sequence sizes in various species. Deviations from this\nmodel reveal bi-directional transcriptional regulatory regions in S. cerevisiae\nand bi-directional terminators in E. coli.\n"
  },
  {
    "year": 2008,
    "title": "Identifying short motifs by means of extreme value analysis",
    "summary": "  The problem of detecting a binding site -- a substring of DNA where\ntranscription factors attach -- on a long DNA sequence requires the recognition\nof a small pattern in a large background. For short binding sites, the matching\nprobability can display large fluctuations from one putative binding site to\nanother. Here we use a self-consistent statistical procedure that accounts\ncorrectly for the large deviations of the matching probability to predict the\nlocation of short binding sites. We apply it in two distinct situations: (a)\nthe detection of the binding sites for three specific transcription factors on\na set of 134 estrogen-regulated genes; (b) the identification, in a set of 138\npossible transcription factors, of the ones binding a specific set of nine\ngenes. In both instances, experimental findings are reproduced (when available)\nand the number of false positives is significantly reduced with respect to the\nother methods commonly employed.\n"
  },
  {
    "year": 2008,
    "title": "A limiting rule for the variability of coding sequence length in\n  microbial genomes",
    "summary": "  The mean length and the variability of coding sequences for 48 genomes of\nbacteria and archaea were analyzed. It was found that the plotted data can be\ndescribed by an angular area. This suggests the followings: a) The variability\nof a genome increases as the mean length increases; b) There is an upper and a\nlower limit for variability for a given mean length; c) Extrapolation of the\nupper and lower limits to lower mean values converges to a single point which\nmight be assimilated to a primordial cell. The whole picture is reminding of a\nprocess which starts from a single cell and evolves into more and more species\nwhich, in turn, show more and more variability.\n"
  },
  {
    "year": 2008,
    "title": "Microbial genome as a fluctuating system: Distribution and correlation\n  of coding sequence lengths",
    "summary": "  The length of coding sequence series in microbial genomes were regarded as a\nfluctuating system and characterized by the methods of statistical physics. The\ndistribution and the correlatin properties of 50 genomes including bacteria and\nseveral archaea were investigated. The distribution was investigated by\nrank-size analysis (Zipf's law. We found that coding sequence lengths series do\nnot obey Zipf's law contrary to natural languages. The distribution was found\nto be more closely to an exponential distribution. The correlation appeared to\nbe similar to natural languages. Segmentation analysis of the series showed to\nbe short range memory systems.\n"
  },
  {
    "year": 2008,
    "title": "A standalone version of IsoFinder for the computational prediction of\n  isochores in genome sequences",
    "summary": "  Isochores are long genome segments relatively homogeneous in G+C. A heuristic\nalgorithm based on entropic segmentation has been developed by our group, and a\nweb server implementing all the required components is available. However, a\nresearcher may want to perform batch processing of many sequences\nsimultaneously in its local machine, instead of analyzing them on one by one\nbasis through the web. To this end, standalone versions are required. We report\nhere the implementation of two standalone programs, able to predict isochores\nat the sequence level: 1) a command-line version (IsoFinder) for Windows and\nLinux systems; and 2) a user-friendly version (IsoFinderWin) running under\nWindows.\n"
  },
  {
    "year": 2008,
    "title": "Comment of Global dynamics of biological systems",
    "summary": "  In a recent study, (Grigorov, 2006) analyzed temporal gene expression\nprofiles (Arbeitman et al., 2002) generated in a Drosophila experiment using\nSSA in conjunction with Monte-Carlo SSA. The author (Grigorov, 2006) makes\nthree important claims in his article, namely:\n  Claim1: A new method based on the theory of nonlinear time series analysis is\nused to capture the global dynamics of the fruit-fly cycle temporal gene\nexpression profiles.\n  Claim 2: Flattening of a significant part of the eigen-spectrum confirms the\nhypothesis about an underly-ing high-dimensional chaotic generating process.\n  Claim 3: Monte-Carlo SSA can be used to establish whether a given time series\nis distinguishable from any well-defined process including deterministic chaos.\n  In this report we present fundamental concerns with respect to the above\nclaims (Grigorov, 2006) in a systematic manner with simple examples. The\ndiscussion provided especially discourages the choice of SSA for inferring\nnonlinear dynamical structure form time series obtained in any biological\nparadigm.\n"
  },
  {
    "year": 2008,
    "title": "Keynotes on membrane proteomics",
    "summary": "  This review article deals with the specificities of the proteomics analysis\nof membrane proteins.\n"
  },
  {
    "year": 2008,
    "title": "Oxidative stress response: a proteomic view",
    "summary": "  The oxidative stress response is characterized by various effects on a range\nof biologic molecules. When examined at the protein level, both expression\nlevels and protein modifications are altered by oxidative stress. While these\neffects have been studied in the past by classic biochemical methods, the\nrecent onset of proteomics methods has allowed the oxidative stress response to\nbe studied on a much wider scale. The input of proteomics in the study of\noxidative stress response and in the evidence of an oxidative stress component\nin biologic phenomena is reviewed in this paper.\n"
  },
  {
    "year": 2008,
    "title": "How shall we use the proteomics toolbox for biomarker discovery?",
    "summary": "  Biomarker discovery for clinical purposes is one of the major areas in which\nproteomics is used. However, despite considerable effort, the successes have\nbeen relatively scarce. In this perspective paper, we try to highlight and\nanalyze the main causes for this limited success, and to suggest alternate\nstrategies, which will avoid them, without eluding the foreseeable weak points\nof these strategies. Two major strategies are analyzed, namely, the switch from\nbody fluids to cell and tissues for the initial biomarker discovery step or, if\nbody fluids must be analyzed, the implementation of highly selective protein\nselection strategies.\n"
  },
  {
    "year": 2008,
    "title": "Mitochondrial proteomics: analysis of a whole mitochondrial extract with\n  two-dimensional electrophoresis",
    "summary": "  Mitochondria are complex organelles, and their proteomics analysis requires a\ncombination of techniques. The emphasis in this chapter is made first on\nmitochondria preparation from cultured mammalian cells, then on the separation\nof the mitochondrial proteins with two-dimensional electrophoresis (2DE),\nshowing some adjustment over the classical techniques to improve resolution of\nthe mitochondrial proteins. This covers both the protein solubilization, the\nelectrophoretic part per se, and the protein detection on the gels, which makes\nthe interface with the protein identification part relying on mass\nspectrometry.\n"
  },
  {
    "year": 2008,
    "title": "Genetic code evolution as an initial driving force for molecular\n  evolution",
    "summary": "  There is an intrinsic relationship between the molecular evolution in\nprimordial period and the properties of genomes and proteomes of contemporary\nspecies. The genomic data may help us understand the driving force of evolution\nof life at molecular level. In absence of evidence, numerous problems in\nmolecular evolution had to fall into a twilight zone of speculation and\ncontroversy in the past. Here we show that delicate structures of variations of\ngenomic base compositions and amino acid frequencies resulted from the genetic\ncode evolution. And the driving force of evolution of life also originated in\nthe genetic code evolution. The theoretical results on the variations of amino\nacid frequencies and genomic base compositions agree with the experimental\nobservations very well, not only in the variation trends but also in some fine\nstructures. Inversely, the genomic data of contemporary species can help\nreconstruct the genetic code chronology and amino acid chronology in primordial\nperiod. Our results may shed light on the intrinsic mechanism of molecular\nevolution and the genetic code evolution.\n"
  },
  {
    "year": 2008,
    "title": "Computation of Maximal Resolution of Copy Number Variation on a\n  Nanofluidic Device using Digital PCR",
    "summary": "  Copy Number Variations (CNVs) of regions of the human genome are important in\ndisease association studies.The digital array is a nanofluidic biochip which\nutilizes integrated channels and valves that partition mixtures of sample and\nreagents into 765 nanovolume reaction chambers. It was recently shown how one\ncan perform statistical analysis of CNV in a DNA sample the digital array. In\nparticular, it was shown how one can accurately estimate the true concentration\nof the molecules in the DNA sample and then determine the ratios of different\nsequences along with statistical confidence intervals on these estimations. In\nthis paper we perform computation of maximum number of copies which can be\ndistinguished using the digital array which gives its resolution in terms of\nits ability to determine CNV. Then, we demonstrate the usefulness of the\nmathematical analysis to solve an important real-world problem of determination\nof the copy number of X chromosome as our example application.\n"
  },
  {
    "year": 2008,
    "title": "Sequence alignment and mutual information",
    "summary": "  Background: Alignment of biological sequences such as DNA, RNA or proteins is\none of the most widely used tools in computational bioscience. All existing\nalignment algorithms rely on heuristic scoring schemes based on biological\nexpertise. Therefore, these algorithms do not provide model independent and\nobjective measures for how similar two (or more) sequences actually are.\nAlthough information theory provides such a similarity measure -- the mutual\ninformation (MI) -- previous attempts to connect sequence alignment and\ninformation theory have not produced realistic estimates for the MI from a\ngiven alignment.\n  Results: Here we describe a simple and flexible approach to get robust\nestimates of MI from {\\it global} alignments. For mammalian mitochondrial DNA,\nour approach gives pairwise MI estimates for commonly used global alignment\nalgorithms that are strikingly close to estimates obtained by an entirely\nunrelated approach -- concatenating and zipping the sequences.\n  Conclusions: This remarkable consistency may help establish MI as a reliable\ntool for evaluating the quality of global alignments, judging the relative\nmerits of different alignment algorithms, and estimating the significance of\nspecific alignments. We expect that our approach can be extended to establish\nfurther connections between information theory and sequence alignment,\nincluding applications to local and multiple alignment procedures.\n"
  },
  {
    "year": 2008,
    "title": "SERpredict: Detection of tissue- or tumor-specific isoforms generated\n  through exonization of transposable elements",
    "summary": "  Background: Transposed elements (TEs) are known to affect transcriptomes,\nbecause either new exons are generated from intronic transposed elements (this\nis called exonization), or the element inserts into the exon, leading to a new\ntranscript. Several examples in the literature show that isoforms generated by\nan exonization are specific to a certain tissue (for example the heart muscle)\nor inflict a disease. Thus, exonizations can have negative effects for the\ntranscriptome of an organism. Results: As we aimed at detecting other tissue-\nor tumor-specific isoforms in human and mouse genomes which were generated\nthrough exonization of a transposed element, we designed the automated analysis\npipeline SERpredict (SER = Specific Exonized Retroelement) making use of\nBayesian Statistics. With this pipeline, we found several genes in which a\ntransposed element formed a tissue- or tumor-specific isoform. Conclusion: Our\nresults show that SERpredict produces relevant results, demonstrating the\nimportance of transposed elements in shaping both the human and the mouse\ntranscriptomes. The effect of transposed elements on the human transcriptome is\nseveral times higher than the effect on the mouse transcriptome, due to the\ncontribution of the primate-specific Alu elements\n"
  },
  {
    "year": 2008,
    "title": "Combining chromosomal arm status and significantly aberrant genomic\n  locations reveals new cancer subtypes",
    "summary": "  Many types of tumors exhibit chromosomal losses or gains, as well as local\namplifications and deletions. Within any given tumor type, sample specific\namplifications and deletionsare also observed. Typically, a region that is\naberrant in more tumors,or whose copy number change is stronger, would be\nconsidered as a more promising candidate to be biologically relevant to cancer.\nWe sought for an intuitive method to define such aberrations and prioritize\nthem. We define V, the volume associated with an aberration, as the product of\nthree factors: a. fraction of patients with the aberration, b. the aberrations\nlength and c. its amplitude. Our algorithm compares the values of V derived\nfrom real data to a null distribution obtained by permutations, and yields the\nstatistical significance, p value, of the measured value of V. We detected\ngenetic locations that were significantly aberrant and combined them with\nchromosomal arm status to create a succint fingerprint of the tumor genome.\nThis genomic fingerprint is used to visualize the tumors, highlighting events\nthat are co ocurring or mutually exclusive. We allpy the method on three\ndifferent public array CGH datasets of Medulloblastoma and Neuroblastoma, and\ndemonstrate its ability to detect chromosomal regions that were known to be\naltered in the tested cancer types, as well as to suggest new genomic locations\nto be tested. We identified a potential new subtype of Medulloblastoma, which\nis analogous to Neuroblastoma type 1.\n"
  },
  {
    "year": 2008,
    "title": "Sweet silver: A formaldehyde-free silver staining using aldoses as\n  developing agents, with enhanced compatibility with mass spectrometry",
    "summary": "  Protein detection methods after electrophoresis have to be sensitive,\nhomogeneous, and not to impair downstream analysis of proteins by MS. Speed,\nlow cost, and user friendliness are also favored features. Silver staining\ncombines many of these features, but its compatibility with MS is limited. We\ndescribe here, a new variant of silver staining that is completely\nformaldehyde-free. Reducing sugars in alkaline borate buffer are used as\ndevelopers. While keeping the benefits of silver staining, this method is shown\nto afford a much better performance in terms of compatibility with MS, both in\nPMF by MALDI and in LC/ESI/MS/MS.\n"
  },
  {
    "year": 2008,
    "title": "Fully denaturing two-dimensional electrophoresis of membrane proteins: a\n  critical update",
    "summary": "  The quality and ease of proteomics analysis depends on the performance of the\nanalytical tools used, and thus of the performances of the protein separation\ntools used to deconvolute complex protein samples. Among protein samples,\nmembrane proteins are one of the most difficult sample classes, because of\ntheir hydrophobicity and embedment in the lipid bilayers. This review deals\nwith the recent progresses and advances made in the separation of membrane\nproteins by 2-DE separating only denatured proteins. Traditional 2-D methods,\ni.e., methods using IEF in the first dimension are compared to methods using\nonly zone electrophoresis in both dimensions, i.e., electrophoresis in the\npresence of cationic or anionic detergents. The overall performances and fields\nof application of both types of method is critically examined, as are future\nprospects for this field.\n"
  },
  {
    "year": 2009,
    "title": "Multiple Sequence Alignment System for Pyrosequencing Reads",
    "summary": "  Pyrosequencing is among the emerging sequencing techniques, capable of\ngenerating upto 100,000 overlapping reads in a single run. This technique is\nmuch faster and cheaper than the existing state of the art sequencing technique\nsuch as Sanger. However, the reads generated by pyrosequencing are short in\nsize and contain numerous errors. Furthermore, each read has a specific\nposition in the reference genome. In order to use these reads for any\nsubsequent analysis, the reads must be aligned . Existing multiple sequence\nalignment methods cannot be used as they do not take into account the specific\npositions of the sequences with respect to the genome, and are highly\ninefficient for large number of sequences. Therefore, the common practice has\nbeen to use either simple pairwise alignment despite its poor accuracy for\nerror prone pyroreads, or use computationally expensive techniques based on\nsequential gap propagation. In this paper, we develop a computationally\nefficient method based on domain decomposition, referred to as pyro-align, to\nalign such large number of reads. The proposed alignment algorithm accurately\naligns the erroneous reads in a short period of time, which is orders of\nmagnitude faster than any existing method. The accuracy of the alignment is\nconfirmed from the consensus obtained from the multiple alignments.\n"
  },
  {
    "year": 2009,
    "title": "p-Adic numbers in bioinformatics: from genetic code to PAM-matrix",
    "summary": "  In this paper we denonstrate that the use of the system of 2-adic numbers\nprovides a new insight to some problems of genetics, in particular, generacy of\nthe genetic code and the structure of the PAM matrix in bioinformatics. The\n2-adic distance is an ultrametric and applications of ultrametrics in\nbioinformatics are not surprising. However, by using the 2-adic numbers we\nmatch ultrametric with a number theoretic structure. In this way we find new\napplications of an ultrametric which differ from known up to now in\nbioinformatics.\n  We obtain the following results. We show that the PAM matrix A allows the\nexpansion into the sum of the two matrices A=A^{(2)}+A^{(\\infty)}, where the\nmatrix A^{(2)} is 2-adically regular (i.e. matrix elements of this matrix are\nclose to locally constant with respect to the discussed earlier by the authors\n2-adic parametrization of the genetic code), and the matrix A^{(\\infty)} is\nsparse. We discuss the structure of the matrix A^{(\\infty)} in relation to the\nside chain properties of the corresponding amino acids.\n"
  },
  {
    "year": 2009,
    "title": "Organelle proteomics",
    "summary": "  This unit describes strategies for studying the proteomes of organelles,\nwhich is one example of targeted proteomics. It relies heavily on previously\npublished units dealing with organelle preparation, protein solubilization, and\nproteomics techniques. A specific commentary for organelle proteomics is\nprovided. Specific protocols for the isolation of nuclei from various sources\n(cell cultures, tissues) are also provided.\n"
  },
  {
    "year": 2009,
    "title": "Non-equilibrium thermodynamics of gene expression and transcriptional\n  regulation",
    "summary": "  In recent times whole-genome gene expression analysis has turned out to be a\nhighly important tool to study the coordinated function of a very large number\nof genes within their corresponding cellular environment, especially in\nrelation to phenotypic diversity and disease. A wide variety of methods of\nquantitative analysis have been developed to cope with high throughput data\nsets generated by gene expression profiling experiments. Due to the complexity\nassociated with transcriptomics, specially in the case of gene regulation\nphenomena, most of these methods are of a probabilistic or statistical nature.\nEven if these methods have reached a central status in the development of an\nintegrative, systematic understanding of the associated biological processes,\nthey very rarely constitute a concrete guide to the actual physicochemical\nmechanisms behind biological function and the role of these methods is more on\na hypotheses generating line. An important improvement could be done with the\ndevelopment of a thermodynamic theory for gene expression and transcriptional\nregulation that will build the foundations for a proper integration of the vast\namount of molecular biophysical data and could lead, in the future, to a\nsystemic view of genetic transcription and regulation.\n"
  },
  {
    "year": 2009,
    "title": "Detergents and chaotropes for protein solubilization before\n  two-dimensional electrophoresis",
    "summary": "  Because of the outstanding ability of two-dimensional electrophoresis to\nseparate complex mixtures of intact proteins, it would be advantageous to apply\nit to all types of proteins, including hydrophobic and membrane proteins.\nUnfortunately, poor solubility hampers the analysis of these molecules. As\nthese problems arise mainly in the extraction and isoelectric focusing steps,\nthe solution is to improve protein solubility under the conditions prevailing\nduring isoelectric focusing. This chapter describes the use of chaotropes and\nnovel detergents to enhance protein solubility during sample extraction and\nisoelectric focussing, and discusses the contribution of these compounds to\nimproving proteomic analysis of membrane proteins.\n"
  },
  {
    "year": 2009,
    "title": "Guidelines for reporting the use of gel electrophoresis in proteomics",
    "summary": "  the MIAPE Gel Electrophoresis (MIAPE-GE) guidelines specify the minimum\ninformation that should be provided when reporting the use of n-dimensional gel\nelectrophoresis in a proteomics experiment. Developed through a joint effort\nbetween the gel-based analysis working group of the Human Proteome\nOrganisation's Proteomics Standards Initiative (HUPO-PSI;\nhttp://www.psidev.info/) and the wider proteomics community, they constitute\none part of the overall Minimum Information about a Proteomics Experiment\n(MIAPE) documentation system published last August in Nature Biotechnology\n"
  },
  {
    "year": 2009,
    "title": "Guidelines for the next 10 years of proteomics",
    "summary": "  In the last ten years, the field of proteomics has expanded at a rapid rate.\nA range of exciting new technology has been developed and enthusiastically\napplied to an enormous variety of biological questions. However, the degree of\nstringency required in proteomic data generation and analysis appears to have\nbeen underestimated. As a result, there are likely to be numerous published\nfindings that are of questionable quality, requiring further confirmation\nand/or validation. This manuscript outlines a number of key issues in proteomic\nresearch, including those associated with experimental design, differential\ndisplay and biomarker discovery, protein identification and analytical\nincompleteness. In an effort to set a standard that reflects current thinking\non the necessary and desirable characteristics of publishable manuscripts in\nthe field, a minimal set of guidelines for proteomics research is then\ndescribed. These guidelines will serve as a set of criteria which editors of\nPROTEOMICS will use for assessment of future submissions to the Journal.\n"
  },
  {
    "year": 2009,
    "title": "Extending the Recursive Jensen-Shannon Segmentation of Biological\n  Sequences",
    "summary": "  In this paper, we extend a previously developed recursive entropic\nsegmentation scheme for applications to biological sequences. Instead of\nBernoulli chains, we model the statistically stationary segments in a\nbiological sequence as Markov chains, and define a generalized Jensen-Shannon\ndivergence for distinguishing between two Markov chains. We then undertake a\nmean-field analysis, based on which we identify pitfalls associated with the\nrecursive Jensen-Shannon segmentation scheme. Following this, we explain the\nneed for segmentation optimization, and describe two local optimization schemes\nfor improving the positions of domain walls discovered at each recursion stage.\nWe also develop a new termination criterion for recursive Jensen-Shannon\nsegmentation based on the strength of statistical fluctuations up to a minimum\nstatistically reliable segment length, avoiding the need for unrealistic null\nand alternative segment models of the target sequence. Finally, we compare the\nextended scheme against the original scheme by recursively segmenting the\nEscherichia coli K-12 MG1655 genome.\n"
  },
  {
    "year": 2009,
    "title": "The Context Sensitivity Problem in Biological Sequence Segmentation",
    "summary": "  In this paper, we describe the context sensitivity problem encountered in\npartitioning a heterogeneous biological sequence into statistically homogeneous\nsegments. After showing signatures of the problem in the bacterial genomes of\nEscherichia coli K-12 MG1655 and Pseudomonas syringae DC3000, when these are\nsegmented using two entropic segmentation schemes, we clarify the contextual\norigins of these signatures through mean-field analyses of the segmentation\nschemes. Finally, we explain why we believe all sequence segmentation schems\nare plagued by the context sensitivity problem.\n"
  },
  {
    "year": 2009,
    "title": "Solubilization of Proteins in 2DE: An Outline",
    "summary": "  Protein solubilization for two-dimensional electrophoresis (2DE) has to break\nmolecular interactions to separate the biological contents of the material of\ninterest into isolated and intact polypeptides. This must be carried out in\nconditions compatible with the first dimension of 2DE, namely isoelectric\nfocusing. In addition, the extraction process must enable easy removal of any\nnonprotein component interfering with the isoelectric focusing. The constraints\nbrought in this process by the peculiar features of isoelectric focusing are\ndiscussed, as well as their consequences in terms of possible solutions and\nlimits for the solubilization process.\n"
  },
  {
    "year": 2009,
    "title": "Silver Staining of Proteins in 2DE Gels",
    "summary": "  Silver staining detects proteins after electrophoretic separation on\npolyacrylamide gels. Its main positive features are its excellent sensitivity\n(in the low nanogram range) and the use of very simple and cheap equipment and\nchemicals. The sequential phases of silver staining are protein fixation, then\nsensitization, then silver impregnation, and finally image development. Several\nvariants of silver staining are described here, which can be completed in a\ntime range from 2 h to 1 day after the end of the electrophoretic separation.\nOnce completed, the stain is stable for several weeks.\n"
  },
  {
    "year": 2009,
    "title": "Membrane proteins and proteomics: Love is possible, but so difficult",
    "summary": "  Despite decades of extensive research, the large-scale analysis of membrane\nproteins remains a difficult task. This is due to the fact that membrane\nproteins require a carefully balanced hydrophilic and lipophilic environment,\nwhich optimum varies with different proteins, while most protein chemistry\nmethods work mainly, if not only, in water-based media. Taking this review\n[Santoni, Molloy and Rabilloud, Membrane proteins and proteomics: un amour\nimpossible? Electrophoresis 2000, 21, 1054-1070] as a pivotal paper, the\ncurrent paper analyzes how the field of membrane proteomics exacerbated the\ntrend in proteomics, i.e. developing alternate methods to the historical\ntwo-dimensional electrophoresis, and thus putting more and more pressure on the\nmass spectrometry side. However, in the case of membrane proteins, the\nincentive in doing so is due to the poor solubility of membrane proteins. This\nreview also shows that in some situations, where this solubility problem is\nless acute, two-dimensional electrophoresis remains a method of choice. Last\nbut not least, this review also critically examines the alternate approaches\nthat have been used for the proteomic analysis of membrane proteins.\n"
  },
  {
    "year": 2009,
    "title": "From secretome analysis to immunology: chitosan induces major\n  alterations in the activation of dendritic cells via a TLR4-dependent\n  mechanism",
    "summary": "  Dendritic cells are known to be activated by a wide range of microbial\nproducts, leading to cytokine production and increased levels of membrane\nmarkers such as major histocompatibility complex class II molecules. Such\nactivated dendritic cells possess the capacity to activate na\\\"ive T cells. In\nthe present study we demonstrated that immature dendritic cells secrete both\nthe YM1 lectin and lipocalin-2. By testing the ligands of these two proteins,\nchitosan and siderophores, respectively, we also demonstrated that chitosan, a\ndegradation product of various fungal and protozoal cell walls, induces an\nactivation of dendritic cells at the membrane level, as shown by the\nup-regulation of membrane proteins such as class II molecules, CD80 and CD86\nvia a TLR4-dependent mechanism, but is not able to induce cytokine production.\nThis led to the production of activated dendritic cells unable to stimulate T\ncells. However, costimulation with other microbial products overcame this\npartial activation and restored the capacity of these activated dendritic cells\nto stimulate T cells. In addition, successive stimulation with chitosan and\nthen by lipopolysaccharide induced a dose-dependent change in the cytokinic\nIL-12/IL-10 balance produced by the dendritic cells.\n"
  },
  {
    "year": 2009,
    "title": "Autoregressive Modeling of Coding Sequence Lengths in Bacterial Genome",
    "summary": "  Previous investigation of coding sequence lengths (CDS) in the bacterial\ncircular chromosome revealed short range correlation in the series of these\ndata. We have further analyzed the averaged periodograms of these series and we\nfound that the organization of CDS can be well described by first order\nautoregressive processes. This involves interaction between the neighboring\nterms. The autoregressive analysis may have great potential in modeling various\nphysical and biological processes like light emission of galaxies, protein\norganization, cell flickering, cognitive processes and perhaps others.\n"
  },
  {
    "year": 2009,
    "title": "Compressed Genotyping",
    "summary": "  Significant volumes of knowledge have been accumulated in recent years\nlinking subtle genetic variations to a wide variety of medical disorders from\nCystic Fibrosis to mental retardation. Nevertheless, there are still great\nchallenges in applying this knowledge routinely in the clinic, largely due to\nthe relatively tedious and expensive process of DNA sequencing. Since the\ngenetic polymorphisms that underlie these disorders are relatively rare in the\nhuman population, the presence or absence of a disease-linked polymorphism can\nbe thought of as a sparse signal. Using methods and ideas from compressed\nsensing and group testing, we have developed a cost-effective genotyping\nprotocol. In particular, we have adapted our scheme to a recently developed\nclass of high throughput DNA sequencing technologies, and assembled a\nmathematical framework that has some important distinctions from 'traditional'\ncompressed sensing ideas in order to address different biological and technical\nconstraints.\n"
  },
  {
    "year": 2009,
    "title": "Power and limitations of electrophoretic separations in proteomics\n  strategies",
    "summary": "  Proteomics can be defined as the large-scale analysis of proteins. Due to the\ncomplexity of biological systems, it is required to concatenate various\nseparation techniques prior to mass spectrometry. These techniques, dealing\nwith proteins or peptides, can rely on chromatography or electrophoresis. In\nthis review, the electrophoretic techniques are under scrutiny. Their\nprinciples are recalled, and their applications for peptide and protein\nseparations are presented and critically discussed. In addition, the features\nthat are specific to gel electrophoresis and that interplay with mass\nspectrometry (i.e., protein detection after electrophoresis, and the process\nleading from a gel piece to a solution of peptides) are also discussed.\n"
  },
  {
    "year": 2009,
    "title": "Identification of possible differences in coding and non-coding\n  fragments of DNA sequences by using the method of the Recurrence\n  Quantification Analysis",
    "summary": "  Starting with the results of Li et al. in 1992 there is valuable interest in\nfinding long range correlations in dna sequences since it raises questions\nabout the role of introns and intron-containing genes. In the present paper we\nstudied two sequences. We applied the method of the recurrence quantification\nanalysis (rqa) that was introduced by Zbilut and Webber in 1994. The\nsignificant result that we have here is that both Lmax and Laminarity exhibit\nvery large values in non coding respect to coding sequences. Therefore we\nsuggest that there the claimed higher long range correlations of introns\nrespect to exons from many authors may be explained here in reason of such\nfound higher values of Lmax and of Laminarity.\n"
  },
  {
    "year": 2009,
    "title": "Age, Sex, and Genetic Architecture of Human Gene Expression in EBV\n  Transformed Cell Lines",
    "summary": "  Individual expression profiles from EBV transformed cell lines are an\nemerging resource for genomic investigation. In this study we characterize the\neffects of age, sex, and genetic variation on gene expression by surveying\npublic datasets of such profiles. We establish that the expression space of\ncell lines maintains genetic as well as non-germline information, in an\nindividual-specific and cross-tissue manner. Age of donor is associated with\nthe expression of 949 genes in the derived cell line. Age-associated genes\ninclude over-representation of immune-related genes, specifically MHC Class I\ngenes, a phenomenon that replicates across tissues and organisms. Sex\nassociated genes in these cell lines include likely candidates, such as genes\nthat escape X-inactivation,testis specific expressed genes, androgen and\nestrogen specific genes, but also gene families previously unknown to be sex\nassociated such as common microRNA targets (MIR-490, V_ARP1_01, MIR-489).\nFinally, we report 494 transcripts whose expression levels are associated with\na genetic variant in cis, overlapping and validating previous reports.\nIncorporating age in analysis of association facilitates additional discovery\nof trans-acting regulatory genetic variants. Our findings promote expression\nprofiling of transformed cell lines as a vehicle for understanding cellular\nsystems beyond the specific lines.\n"
  },
  {
    "year": 2009,
    "title": "Silver-staining of proteins in polyacrylamide gels: a general overview",
    "summary": "  On the basis of the physico-chemical principles underlying silver-staining of\nproteins, which are recalled in this paper, several methods of silver-staining\nof proteins after SDS electrophoresis in polyacrylamide gels or isoelectric\nfocusing were tested. The most valuable protocols are presented in this report,\nincluding standard methods for unsupported gels and new methods devised for\nthin (0.5 mm) supported gels for SDS electrophoresis or isoelectric focusing\nand for staining of small peptides. Generally speaking, the most rapid methods\nwere found to be less sensitive and less reproducible than more time-consuming\nones. Among the long methods, those using silver-diammine complex gave the most\nuniform sensitivity. They require however special home-made gels and cannot be\napplied to several electrophoretic systems (e.g. systems using tricine or\nbicine as the trailing ion, or isoelectric focusing in immobilized pH\ngradients). For these reasons, protocols based on silver nitrate are of a more\ngeneral use and might be favored. Future trends for silver-staining will also\nbe discussed.\n"
  },
  {
    "year": 2010,
    "title": "Information-theoretic View of Sequence Organization in a Genome",
    "summary": "  Sequence organizations are viewed from two points: one is from informational\nredundancy or informational correlation (IC) and another is from k-mer\nfrequency statistics. Two problems are investigated. The first is how the ICs\nexceed the fluctuation bound and the order emerges from fluctuation in a genome\nwhen the sequence length attains some critical value. We demonstrated that the\ntransition from fluctuation to order takes place at about sequence length\n200-300 thousands bases for human and E coli genome. It means that the life\nemerges from a region between macroscopic and microscopic. The second is about\nthe statistical law of the k-mer organization in a genome under the\nevolutionary pressure and functional selection. We deduced a sum rule Q(k,N) on\nthe k-mer frequency deviations from the randomness in a N-long sequence of\ngenome and deduced the relations of Q(k,N) with k and N. We found that Q(k,N)\nincreases with length N at a constant rate for most genome sequences and\ndemonstrated that when the functional selection of k-mers is accumulated to\nsome critical value the ordering takes place. An important finding is the sum\nrule correlated with the evolutionary complexity of the genome.\n"
  },
  {
    "year": 2010,
    "title": "Shape-based peak identification for ChIP-Seq",
    "summary": "  We present a new algorithm for the identification of bound regions from\nChIP-seq experiments. Our method for identifying statistically significant\npeaks from read coverage is inspired by the notion of persistence in\ntopological data analysis and provides a non-parametric approach that is robust\nto noise in experiments. Specifically, our method reduces the peak calling\nproblem to the study of tree-based statistics derived from the data. We\ndemonstrate the accuracy of our method on existing datasets, and we show that\nit can discover previously missed regions and can more clearly discriminate\nbetween multiple binding events. The software T-PIC (Tree shape Peak\nIdentification for ChIP-Seq) is available at\nhttp://math.berkeley.edu/~vhower/tpic.html\n"
  },
  {
    "year": 2010,
    "title": "Variations on a theme: Changes to electrophoretic separations that can\n  make a difference",
    "summary": "  Electrophoretic separations of proteins are widely used in proteomic\nanalyses, and rely heavily on SDS electrophoresis. This mode of separation is\nalmost exclusively used when a single dimension separation is performed, and\ngenerally represents the second dimension of two-dimensional separations.\nElectrophoretic separations for proteomics use robust, well-established\nprotocols. However, many variations in almost all possible parameters have been\ndescribed in the literature over the years, and they may bring a decisive\nadvantage when the limits of the classical protocols are reached. The purpose\nof this article is to review the most important of these variations, so that\nthe readers can be aware of how they can improve or tune protein separations\naccording to their needs. The chemical variations reviewed in this paper\nencompass gel structure, buffer systems and detergents for SDS electrophoresis,\ntwo-dimensional electrophoresis based on isoelectric focusing and\ntwo-dimensional electrophoresis based on cationic zone electrophoresis.\n"
  },
  {
    "year": 2010,
    "title": "PIntron: a Fast Method for Gene Structure Prediction via Maximal\n  Pairings of a Pattern and a Text",
    "summary": "  Current computational methods for exon-intron structure prediction from a\ncluster of transcript (EST, mRNA) data do not exhibit the time and space\nefficiency necessary to process large clusters of over than 20,000 ESTs and\ngenes longer than 1Mb. Guaranteeing both accuracy and efficiency seems to be a\ncomputational goal quite far to be achieved, since accuracy is strictly related\nto exploiting the inherent redundancy of information present in a large\ncluster. We propose a fast method for the problem that combines two ideas: a\nnovel algorithm of proved small time complexity for computing spliced\nalignments of a transcript against a genome, and an efficient algorithm that\nexploits the inherent redundancy of information in a cluster of transcripts to\nselect, among all possible factorizations of EST sequences, those allowing to\ninfer splice site junctions that are highly confirmed by the input data. The\nEST alignment procedure is based on the construction of maximal embeddings that\nare sequences obtained from paths of a graph structure, called Embedding Graph,\nwhose vertices are the maximal pairings of a genomic sequence T and an EST P.\nThe procedure runs in time linear in the size of P, T and of the output.\nPIntron, the software tool implementing our methodology, is able to process in\na few seconds some critical genes that are not manageable by other gene\nstructure prediction tools. At the same time, PIntron exhibits high accuracy\n(sensitivity and specificity) when compared with ENCODE data. Detailed\nexperimental data, additional results and PIntron software are available at\nhttp://www.algolab.eu/PIntron.\n"
  },
  {
    "year": 2010,
    "title": "The role of transposable elements in the evolution of non-mammalian\n  vertebrates and invertebrates",
    "summary": "  Background: Transposable elements (TEs) have played an important role in the\ndiversification and enrichment of mammalian transcriptomes through various\nmechanisms such as exonization and intronization (the birth of new\nexons/introns from previously intronic/exonic sequences, respectively), and\ninsertion into first and last exons. However, no extensive analysis has\ncompared the effects of TEs on the transcriptomes of mammalian, non-mammalian\nvertebrates and invertebrates. Results: We analyzed the influence of TEs on the\ntranscriptomes of five species, three invertebrates and two non-mammalian\nvertebrates. Compared to previously analyzed mammals, there were lower levels\nof TE introduction into introns, significantly lower numbers of exonizations\noriginating from TEs and a lower percentage of TE insertion within the first\nand last exons. Although the transcriptomes of vertebrates exhibit a\nsignificant level of exonizations of TEs, only anecdotal cases were found in\ninvertebrates. In vertebrates, as in mammals, the exonized TEs are mostly\nalternatively spliced, indicating selective pressure maintains the original\nmRNA product generated from such genes. Conclusions: Exonization of TEs is\nwide-spread in mammals, less so in non- mammalian vertebrates, and very low in\ninvertebrates. We assume that the exonization process depends on the length of\nintrons. Vertebrates, unlike invertebrates, are characterized by long introns\nand short internal exons. Our results suggest that there is a direct link\nbetween the length of introns and exonization of TEs and that this process\nbecame more prevalent following the appearance of mammals.\n"
  },
  {
    "year": 2010,
    "title": "Characteristics of transposable element exonization within human and\n  mouse",
    "summary": "  Insertion of transposed elements within mammalian genes is thought to be an\nimportant contributor to mammalian evolution and speciation. Insertion of\ntransposed elements into introns can lead to their activation as alternatively\nspliced cassette exons, an event called exonization. Elucidation of the\nevolutionary constraints that have shaped fixation of transposed elements\nwithin human and mouse protein coding genes and subsequent exonization is\nimportant for understanding of how the exonization process has affected\ntranscriptome and proteome complexities. Here we show that exonization of\ntransposed elements is biased towards the beginning of the coding sequence in\nboth human and mouse genes. Analysis of single nucleotide polymorphisms (SNPs)\nrevealed that exonization of transposed elements can be population-specific,\nimplying that exonizations may enhance divergence and lead to speciation. SNP\ndensity analysis revealed differences between Alu and other transposed\nelements. Finally, we identified cases of primate-specific Alu elements that\ndepend on RNA editing for their exonization. These results shed light on TE\nfixation and the exonization process within human and mouse genes.\n"
  },
  {
    "year": 2010,
    "title": "Is a gene-centric human proteome project the best way for proteomics to\n  serve biology?",
    "summary": "  With the recent developments in proteomic technologies, a complete human\nproteome project (HPP) appears feasible for the first time. However, there is\nstill debate as to how it should be designed and what it should encompass. In\n\"proteomics speak\", the debate revolves around the central question as to\nwhether a gene-centric or a protein-centric proteomics approach is the most\nappropriate way forward. In this paper, we try to shed light on what these\ndefinitions mean, how large-scale proteomics such as a HPP can insert into the\nlarger omics chorus, and what we can reasonably expect from a HPP in the way it\nhas been proposed so far.\n"
  },
  {
    "year": 2010,
    "title": "Two-dimensional gel electrophoresis in proteomics: past, present and\n  future",
    "summary": "  Two-dimensional gel electrophoresis has been instrumental in the birth and\ndevelopments of proteomics, although it is no longer the exclusive separation\ntool used in the field of proteomics. In this review, a historical perspective\nis made, starting from the days where two-dimensional gels were used and the\nword proteomics did not even exist. The events that have led to the birth of\nproteomics are also recalled, ending with a description of the now well-known\nlimitations of two-dimensional gels in proteomics. However, the\noften-underestimated advantages of two-dimensional gels are also underlined,\nleading to a description of how and when to use two-dimensional gels for the\nbest in a proteomics approach. Taking support of these advantages (robustness,\nresolution, and ability to separate entire, intact proteins), possible future\napplications of this technique in proteomics are also mentioned.\n"
  },
  {
    "year": 2010,
    "title": "Sequence alignment, mutual information, and dissimilarity measures for\n  constructing phylogenies",
    "summary": "  Existing sequence alignment algorithms use heuristic scoring schemes which\ncannot be used as objective distance metrics. Therefore one relies on measures\nlike the p- or log-det distances, or makes explicit, and often simplistic,\nassumptions about sequence evolution. Information theory provides an\nalternative, in the form of mutual information (MI) which is, in principle, an\nobjective and model independent similarity measure. MI can be estimated by\nconcatenating and zipping sequences, yielding thereby the \"normalized\ncompression distance\". So far this has produced promising results, but with\nuncontrolled errors. We describe a simple approach to get robust estimates of\nMI from global pairwise alignments. Using standard alignment algorithms, this\ngives for animal mitochondrial DNA estimates that are strikingly close to\nestimates obtained from the alignment free methods mentioned above. Our main\nresult uses algorithmic (Kolmogorov) information theory, but we show that\nsimilar results can also be obtained from Shannon theory. Due to the fact that\nit is not additive, normalized compression distance is not an optimal metric\nfor phylogenetics, but we propose a simple modification that overcomes the\nissue of additivity. We test several versions of our MI based distance measures\non a large number of randomly chosen quartets and demonstrate that they all\nperform better than traditional measures like the Kimura or log-det (resp.\nparalinear) distances. Even a simplified version based on single letter Shannon\nentropies, which can be easily incorporated in existing software packages, gave\nsuperior results throughout the entire animal kingdom. But we see the main\nvirtue of our approach in a more general way. For example, it can also help to\njudge the relative merits of different alignment algorithms, by estimating the\nsignificance of specific alignments.\n"
  },
  {
    "year": 2010,
    "title": "Identification of 11 potential malaria vaccine candidates using\n  Bioinformatics",
    "summary": "  In this paper, we suggested eleven protein targets to be used as possible\nvaccines against Plasmodium falciparum causative agent of almost two to three\nmillion deaths per year. A comprehensive analysis of protein target have been\nselected from the small experimental fragment of antigen in the P. falciparum\ngenome, all of them common to the four stages of the parasite life cycle (i.e.,\nsporozoites, merozoites, trophozoites and gametocytes). The potential vaccine\ncandidates should be analyzed in silico technique using various bioinformatics\ntools. Finally, the possible protein target according to PlasmoDB gene ID are\nPFC0975c, PFE0660c, PF08_0071, PF10_0084, PFI0180w, MAL13P1.56, PF14_0192,\nPF13_0141, PF14_0425, PF13_0322, y PF14_0598.\n"
  },
  {
    "year": 2010,
    "title": "Classification of Saccharomyces cerevisiae promoter regions into\n  distinct chromatin classes reveals the existence of nucleosome-depleted\n  hotspots of transcription factor occupancy",
    "summary": "  Transcription factors (TF) play an essential role in the cell as locus- and\ncondition-specific recruiters of transcriptional machinery or\nchromatin-modifying complexes. However, predicting the in vivo profile of TF\noccupancy along the genome, which depends on complex interactions with other\nchromatin-associated proteins, from the DNA sequence remains a major challenge.\nThrough careful reanalysis of ChIP-chip data for 138 TFs obtained in rich\nmedia, we were able to classify the upstream promoter regions of S. cerevisiae\ninto 15 distinct chromatin types. One of these encompasses 5% of all promoters\nand is unique in that it is highly occupied by (essentially) all TFs expressed\nin rich media. These \"hotspots\" of TF occupancy are strongly\nnucleosome-depleted and preferentially targeted by chromatin-remodeling\ncomplexes and the origin-of-replication complex (ORC). They are also the only\nchromatin type enriched for predicted Rap1p and Pdr1p binding sites, which we\nfound to work cooperatively with AAA/TTT motifs, known to affect local DNA\nstructure, to reduce nucleosome occupancy. Taken together, our results reveal\nand characterize a new type of local chromatin structure in yeast.\n"
  },
  {
    "year": 2010,
    "title": "Stochastic modeling of gene activation and application to cell\n  regulation",
    "summary": "  Transcription factors (TFs) are key regulators of gene expression. Based on\nthe classical scenario in which the TF search process switches between\none-dimensional motion along the DNA molecule and free Brownian motion in the\nnucleus, we study the arrival time of several TFs to multiple binding sites and\nderive, in the presence of competitive binding ligands, the probability that\nseveral target sites are bound. We then apply our results to the hunchback\nregulation by bicoid in the fly embryo and we propose a general mechanism that\nallows cells to read a morphogenetic gradient and specialize according to their\nposition in the embryo.\n"
  },
  {
    "year": 2010,
    "title": "Mapping Dynamic Histone Acetylation Patterns to Gene Expression in\n  Nanog-depleted Murine Embryonic Stem Cells",
    "summary": "  Embryonic stem cells (ESC) have the potential to self-renew indefinitely and\nto differentiate into any of the three germ layers. The molecular mechanisms\nfor self-renewal, maintenance of pluripotency and lineage specification are\npoorly understood, but recent results point to a key role for epigenetic\nmechanisms. In this study, we focus on quantifying the impact of histone 3\nacetylation (H3K9,14ac) on gene expression in murine embryonic stem cells. We\nanalyze genome-wide histone acetylation patterns and gene expression profiles\nmeasured over the first five days of cell differentiation triggered by\nsilencing Nanog, a key transcription factor in ESC regulation. We explore the\ntemporal and spatial dynamics of histone acetylation data and its correlation\nwith gene expression using supervised and unsupervised statistical models. On a\ngenome-wide scale, changes in acetylation are significantly correlated to\nchanges in mRNA expression and, surprisingly, this coherence increases over\ntime. We quantify the predictive power of histone acetylation for gene\nexpression changes in a balanced cross-validation procedure. In an in-depth\nstudy we focus on genes central to the regulatory network of Mouse ESC,\nincluding those identified in a recent genome-wide RNAi screen and in the\nPluriNet, a computationally derived stem cell signature. We find that compared\nto the rest of the genome, ESC-specific genes show significantly more\nacetylation signal and a much stronger decrease in acetylation over time, which\nis often not reflected in an concordant expression change. These results shed\nlight on the complexity of the relationship between histone acetylation and\ngene expression and are a step forward to dissect the multilayer regulatory\nmechanisms that determine stem cell fate.\n"
  },
  {
    "year": 2010,
    "title": "Large Tandem, Higher Order Repeats and Regularly Dispersed Repeat Units\n  Contribute Substantially to Divergence Between Human and Chimpanzee Y\n  Chromosomes",
    "summary": "  Comparison of human and chimpanzee genomes has received much attention,\nbecause of paramount role for understanding evolutionary step distinguishing us\nfrom our closest living relative. In order to contribute to insight into Y\nchromosome evolutionary history, we study and compare tandems, higher order\nrepeats (HORs), and regularly dispersed repeats in human and chimpanzee Y\nchromosome contigs, using robust Global Repeat Map algorithm. We find a new\ntype of long-range acceleration, human-accelerated HOR regions. In peripheral\ndomains of 35mer human alphoid HORs, we find riddled features with ten\nadditional repeat monomers. In chimpanzee, we identify 30mer alphoid HOR. We\nconstruct alphoid HOR schemes showing significant human-chimpanzee difference,\nrevealing rapid evolution after human-chimpanzee separation. We identify and\nanalyze over 20 large repeat units, most of them reported here for the first\ntime as: chimpanzee and human ~1.6 kb 3mer secondary repeat unit (SRU) and\n~23.5 kb tertiary repeat unit (~0.55 kb primary repeat unit, PRU); human 10848,\n15775, 20309, 60910, and 72140 bp PRUs; human 3mer SRU (~2.4 kb PRU); 715mer\nand 1123mer SRUs (5mer PRU); chimpanzee 5096, 10762, 10853, 60523 bp PRUs; and\nchimpanzee 64624 bp SRU (10853 bp PRU). We show that substantial\nhuman-chimpanzee differences are concentrated in large repeat structures, at\nthe level of as much as ~70% divergence, sizably exceeding previous numerical\nestimates for some selected noncoding sequences. Smeared over the whole\nsequenced assembly (25 Mb) this gives ~14% human--chimpanzee divergence. This\nis significantly higher estimate of divergence between human and chimpanzee\nthan previous estimates.\n"
  },
  {
    "year": 2010,
    "title": "Towards a theoretical understanding of false positives in DNA motif\n  finding",
    "summary": "  Detection of false-positive motifs is one of the main causes of low\nperformance in motif finding methods. It is generally assumed that\nfalse-positives are mostly due to algorithmic weakness of motif-finders. Here,\nhowever, we derive the theoretical dependence of false positives on dataset\nsize and find that false positives can arise as a result of large dataset size,\nirrespective of the algorithm used. Interestingly, the false-positive strength\ndepends more on the number of sequences in the dataset than it does on the\nsequence length. As expected, false-positives can be reduced by decreasing the\nsequence length or by adding more sequences to the dataset. The dependence on\nnumber of sequences, however, diminishes and reaches a plateau after which\nadding more sequences to the dataset does not reduce the false-positive rate\nsignificantly. Based on the theoretical results presented here, we provide a\nnumber of intuitive rules of thumb that may be used to enhance motif-finding\nresults in practice.\n"
  },
  {
    "year": 2010,
    "title": "Is the unfoldome widespread in proteomes?",
    "summary": "  The term unfoldome has been recently used to indicate the universe of\nintrinsically disordered proteins. These proteins are characterized by an\nensemble of high-flexible interchangeable conformations and therefore they can\ninteract with many targets without requiring pre-existing stereo-chemical\ncomplementarity. It has been suggested that intrinsically disordered proteins\nare frequent in proteomes and disorder is widespread also in structured\nproteins. However, several studies raise some doubt about these views. It this\npaper we estimate the frequency of intrinsically disordered proteins in several\nliving organisms by using the ratio S between the likelihood, for a protein\nsequence, of being composed mainly by order-promoting or disorder-promoting\nresidues. We scan several proteomes from Archaea, Bacteria and Eukarya. We find\nthe following figures: 1.63% for Archaea, 3.91% for Bacteria, 16.35% for\nEukarya. The frequencies we found can be considered an upper bound to the real\nfrequency of intrinsically disordered proteins in proteomes. Our estimates are\nlower than those previously reported in several studies. A scanning of proteins\nin the Protein Data Bank (PDB) searching for segments of non-observed residues\nreveals that segments of non-observed residues longer than 30 amino acids, are\nrare. Our observations support the idea that the spread of the unfoldome has\nbeen often overestimated. If we exclude some exceptions, the structure-function\nparadigm is generally valid and pre-existing stereo-chemical complementarity\namong structures remains an important requisite for interactions between\nbiological macromolecules.\n"
  },
  {
    "year": 2010,
    "title": "Amino acid composition and thermal stability of protein structures: the\n  free energy geography of the Protein Data Bank",
    "summary": "  We study the combined influence of amino acid composition and chain length on\nthe thermal stability of protein structures. A new parameterization of the\ninternal free energy is considered, as the sum of hydrophobic effect,\nhydrogen-bond and de-hydration energy terms. We divided a non-redundant\nselection of protein structures from the Protein Data Bank into three groups:\ni) rich in order-promoting residues (OPR proteins); ii) rich in\ndisorder-promoting residues (DPR proteins); iii) belonging to a twilight zone\n(TZ proteins). We observe a partition of PDB in several groups with different\ninternal free energies, amino acid compositions and protein lengths. Internal\nfree energy of 96% of the proteins analyzed ranges from -2 to -6.5 kJ/mol/res.\nWe found many DPR and OPR proteins with the same relative thermal stability.\nOnly OPR proteins with internal energy between -4 and -6.5 kJ/mol/res are\nobserved to have chains longer than 200 residues, with a high de-hydration\nenergy compensated by the hydrophobic effect. DPR and TZ proteins are shorter\nthan 200 residues and they have an internal energy above -4 kJ/mol/res, with a\nfew exceptions among TZ proteins. Hydrogen-bonds play an important role in the\nstabilization of these DPR folds, often higher than contact energy. The new\nparameterization of internal free energy let emerge a geography of thermal\nstabilities of PDB structures. Amino acid composition per se is not sufficient\nto determine the stability of protein folds, since. DPR and TZ proteins\ngenerally have a relatively high internal free energy, and they are stabilized\nby hydrogen-bonds. Long DPR proteins are not observed in the PDB, because their\nlow hydrophobicity cannot compensate the high de-hydration energy necessary to\naccommodate residues within a highly packed globular fold.\n"
  },
  {
    "year": 2010,
    "title": "Relationships among the nucleotide content of human genome sequence,\n  gene structure, and gene expression features (PhD synopsis)",
    "summary": "  The Dissertation is focused on the studies of associations between functional\nelements in human genome and their nucleotide structure. The asymmetry in\nnucleotide content (skew, bias) was chosen as the main feature for nucleotide\nstructure. A significant difference in nucleotide content asymmetry was found\nfor human exons vs. introns. Specifically, exon sequences display bias for\npurines (i.e., excess of A and G over C and T), while introns exhibit\nketo-amino skew (i.e. excess of G and T over A and C). The extents of these\nbiases depend upon gene expression patterns. The highest intronic keto-amino\nskew is found in the introns of housekeeping genes. In the case of introns,\nwhose sequences are under weak repair system, the AT->GC and CG->TA\nsubstitutions are preferentially accumulated. A comparative analysis of gene\nsequences encoding cytochrome P450 2E1 of Homo sapiens and representative\nmammals was done. The cladistic tree on the basis of coding sequences\nsimilarity of the gene Cyp2e1 was constructed. A new programming tools of NCBI\ndatabase sequence mining and analysis was developed, resulting in construction\nof a own database.\n"
  },
  {
    "year": 2011,
    "title": "The Genomic HyperBrowser: inferential genomics at the sequence level",
    "summary": "  The immense increase in the generation of genomic scale data poses an unmet\nanalytical challenge, due to a lack of established methodology with the\nrequired flexibility and power. We propose a first principled approach to\nstatistical analysis of sequence-level genomic information. We provide a\ngrowing collection of generic biological investigations that query pairwise\nrelations between tracks, represented as mathematical objects, along the\ngenome. The Genomic HyperBrowser implements the approach and is available at\nhttp://hyperbrowser.uio.no.\n"
  },
  {
    "year": 2011,
    "title": "Beyond the consensus: dissecting within-host viral population diversity\n  of foot-and-mouth disease virus using next-generation genome sequencing",
    "summary": "  The sequence diversity of viral populations within individual hosts is the\nstarting material for selection and subsequent evolution of RNA viruses such as\nfoot-and-mouth disease virus (FMDV). Using next-generation sequencing (NGS)\nperformed on a Genome Analyzer platform (Illumina), this study compared the\nviral populations within two bovine epithelial samples (foot lesions) from a\nsingle animal with the Inoculum used to initiate experimental infection.\nGenomic sequences were determined in duplicate sequencing runs, and the\nconsensus sequence determined by NGS, for the Inoculum, was identical to that\npreviously determined using the Sanger method. However, NGS reveals the fine\npolymorphic sub-structure of the viral population, from nucleotide variants\npresent at just below 50% frequency to those present at fractions of 1%. Some\nof the higher frequency polymorphisms identified encoded changes within codons\nassociated with heparan sulphate binding and were present in both feet lesions\nrevealing intermediate stages in the evolution of a tissue-culture adapted\nvirus replicating within a mammalian host. We identified 2,622, 1,434 and 1,703\npolymorphisms in the Inoculum, and in the two foot lesions respectively: most\nof the substitutions occurred only in a small fraction of the population and\nrepresent the progeny from recent cellular replication prior to onset of any\nselective pressures. We estimated an upper limit for the genome-wide mutation\nrate of the virus within a cell to be 7.8 x 10-4 per nt. The greater depth of\ndetection, achieved by NGS, demonstrates that this method is a powerful and\nvaluable tool for the dissection of FMDV populations within-hosts.\n"
  },
  {
    "year": 2011,
    "title": "Improvements and simplifications in in-gel fluorescent detection of\n  proteins using ruthenium II tris-(bathophenanthroline disulfonate): the poor\n  man's fluorescent detection method",
    "summary": "  Fluorescent detection of proteins is a popular method of detection allying\nsensitivity, linearity and compatibility with mass spectrometry. Among the\nnumerous methods described in the literature, staining with ruthenium II\ntris(bathophenanthroline disulfonate) is particularly cost-effective, but\nslightly cumbersome owing to difficulties in the preparation of the complex and\ncomplexity of staining protocols. We describe here the modifications on both\naspects that allow to perform a higher contrast staining and offer a more\nrobust method of complex preparation, thereby maximizing the advantages of the\nmethod.\n"
  },
  {
    "year": 2011,
    "title": "Low-pass Genomewide Sequencing and Variant Imputation Using\n  Identity-by-descent in an Isolated Human Population",
    "summary": "  Whole-genome sequencing in an isolated population with few founders directly\nascertains variants from the population bottleneck that may be rare elsewhere.\nIn such populations, shared haplotypes allow imputation of variants in\nunsequenced samples without resorting to statistical methods, as in studies of\noutbred cohorts. We focus on an isolated population cohort from the Pacific\nIsland of Kosrae, Micronesia, where we previously collected SNP array and rich\nphenotype data for the majority of the population. We report identification of\nlong regions with haplotypes co-inherited between pairs of individuals and\nmethodology to leverage such shared genetic content for imputation. Our\nestimates show that sequencing as few as 40 personal genomes allows for\nimputation in up to 60% of the 3,000-person cohort at the average locus. We\nascertained a pilot data-set of whole-genome sequences from seven Kosraean\nindividuals, with average 5X coverage. This dataset identified 5,735,306 unique\nsites of which 1,212,831 were previously unknown. Additionally, these Kosraen\nvariants are unusually enriched for alleles that are rare in other populations\nwhen compared to geographic neighbors. We were able to use the presence of\nshared haplotypes between the seven individuals to estimate imputation accuracy\nof known and novel variants and achieved levels of 99.6% and 97.3%,\nrespectively. This study presents the first whole-genome analysis of a\nhomogenous isolate population with emphasis on rare variant inference.\n"
  },
  {
    "year": 2011,
    "title": "dMotifGreedy: a novel tool for de novo discovery of DNA motifs with\n  enhanced power of reporting distinct motifs",
    "summary": "  De novo discovery of over-represented DNA motifs is one of the major\nchallenges in computational biology. Although numerous tools have been\navailable for de novo motif discovery, many of these tools are subject to local\noptima phenomena, which may hinder detection of multiple distinct motifs. A\ngreedy algorithm based tool named dMotifGreedy was developed. dMotifGreedy\nbegins by searching for candidate motifs from pair-wise local alignments of\ninput sequences and then computes an optimal global solution for each candidate\nmotif through a greedy algorithm. dMotifGreedy has competitive performance in\ndetecting a true motif and greatly enhanced performance in detecting multiple\ndistinct true motifs. dMotifGreedy is freely available via a stand-alone\nprogram at http://lambchop.ads.uga.edu/dmotifgreedy/download.php.\n"
  },
  {
    "year": 2011,
    "title": "Matrix eQTL: Ultra fast eQTL analysis via large matrix operations",
    "summary": "  Expression quantitative trait loci (eQTL) mapping aims to determine genomic\nregions that regulate gene transcription. Expression QTL is used to study the\nregulatory structure of normal tissues and to search for genetic factors in\ncomplex diseases such as cancer, diabetes, and cystic fibrosis. A modern eQTL\ndataset contains millions of SNPs and thousands of transcripts measured for\nhundreds of samples. This makes the analysis computationally complex as it\ninvolves independent testing for association for every transcript-SNP pair. The\nheavy computational burden makes eQTL analysis less popular, often forces\nanalysts to restrict their attention to just a subset of transcripts and SNPs.\nAs larger genotype and gene expression datasets become available, the demand\nfor fast tools for eQTL analysis increases. We present a new method for fast\neQTL analysis via linear models, called Matrix eQTL. Matrix eQTL can model and\ntest for association using both linear regression and ANOVA models. The models\ncan include covariates to account for such factors as population structure,\ngender, and clinical variables. It also supports testing of heteroscedastic\nmodels and models with correlated errors. In our experiment on large datasets\nMatrix eQTL was thousands of times faster than the existing popular software\nfor QTL/eQTL analysis. Matrix eQTL is implemented as both Matlab and R packages\nand thus can easily be run on Windows, Mac OS, and Linux systems. The software\nis freely available at the following address:\nhttp://www.bios.unc.edu/research/genomic_software/Matrix_eQTL\n"
  },
  {
    "year": 2011,
    "title": "More Mouldy Data: Another mycoplasma gene jumps the silicon barrier into\n  the human genome",
    "summary": "  The human genome sequence database contains DNA sequences very like those of\nmycoplasma molds. It appears such moulds infect not only molecular Biology\nlaboratories but were picked up by experimenters from contaminated samples and\ninserted into GenBank as if they were human. At least one mouldy EST (Expressed\nSequence Tag) has transferred from public databases to commercial tools\n(Affymetrix HG-U133 plus 2.0 microarrays). We report a second example\n(DA466599) and suggest there is a need to clean up genomic databases but fear\ncurrent tools will be inadequate to catch genes which have jumped the silicon\nbarrier.\n"
  },
  {
    "year": 2011,
    "title": "Effects of nanoparticles on murine macrophages",
    "summary": "  Metallic nanoparticles are more and more widely used in an increasing number\nof applications. Consequently, they are more and more present in the\nenvironment, and the risk that they may represent for human health must be\nevaluated. This requires to increase our knowledge of the cellular responses to\nnanoparticles. In this context, macrophages appear as an attractive system.\nThey play a major role in eliminating foreign matter, e.g. pathogens or\ninfectious agents, by phagocytosis and inflammatory responses, and are thus\nhighly likely to react to nanoparticles. We have decided to study their\nresponses to nanoparticles by a combination of classical and wide-scope\napproaches such as proteomics. The long term goal of this study is the better\nunderstanding of the responses of macrophages to nanoparticles, and thus to\nhelp to assess their possible impact on human health. We chose as a model\nsystem bone marrow-derived macrophages and studied the effect of commonly used\nnanoparticles such as TiO2 and Cu. Classical responses of macrophage were\ncharacterized and proteomic approaches based on 2D gels of whole cell extracts\nwere used. Preliminary proteomic data resulting from whole cell extracts showed\ndifferent effects for TiO2-NPs and Cu-NPs. Modifications of the expression of\nseveral proteins involved in different pathways such as, for example, signal\ntransduction, endosome-lysosome pathway, Krebs cycle, oxidative stress response\nhave been underscored. These first results validate our proteomics approach and\nopen a new wide field of investigation for NPs impact on macrophages\n"
  },
  {
    "year": 2011,
    "title": "Rare coding SNP in DZIP1 gene associated with late-onset sporadic\n  Parkinson's disease",
    "summary": "  We present the first application of the hypothesis-rich mathematical theory\nto genome-wide association data. The Hamza et al. late-onset sporadic\nParkinson's disease genome-wide association study dataset was analyzed. We\nfound a rare, coding, non-synonymous SNP variant in the gene DZIP1 that confers\nincreased susceptibility to Parkinson's disease. The association of DZIP1 with\nParkinson's disease is consistent with a Parkinson's disease stem-cell ageing\ntheory.\n"
  },
  {
    "year": 2011,
    "title": "High-order chromatin architecture determines the landscape of\n  chromosomal alterations in cancer",
    "summary": "  The rapid growth of cancer genome structural information provides an\nopportunity for a better understanding of the mutational mechanisms of genomic\nalterations in cancer and the forces of selection that act upon them. Here we\ntest the evidence for two major forces, spatial chromosome structure and\npurifying (or negative) selection, that shape the landscape of somatic\ncopy-number alterations (SCNAs) in cancer1. Using a maximum likelihood\nframework we compare SCNA maps and three-dimensional genome architecture as\ndetermined by genome-wide chromosome conformation capture (HiC) and described\nby the proposed fractal-globule (FG) model2. This analysis provides evidence\nthat the distribution of chromosomal alterations in cancer is spatially related\nto three-dimensional genomic architecture and additionally suggests that\npurifying selection as well as positive selection shapes the landscape of SCNAs\nduring somatic evolution of cancer cells.\n"
  },
  {
    "year": 2011,
    "title": "Two-dimensional gel electrophoresis in proteomics: A tutorial",
    "summary": "  Two-dimensional electrophoresis of proteins has preceded, and accompanied,\nthe birth of proteomics. Although it is no longer the only experimental scheme\nused in modern proteomics, it still has distinct features and advantages. The\npurpose of this tutorial paper is to guide the reader through the history of\nthe field, then through the main steps of the process, from sample preparation\nto in-gel detection of proteins, commenting the constraints and caveats of the\ntechnique. Then the limitations and positive features of two-dimensional\nelectrophoresis are discussed (e.g. its unique ability to separate complete\nproteins and its easy interfacing with immunoblotting techniques), so that the\noptimal type of applications of this technique in current and future proteomics\ncan be perceived. This is illustrated by a detailed example taken from the\nliterature and commented in detail. This Tutorial is part of the International\nProteomics Tutorial Programme (IPTP 2).\n"
  },
  {
    "year": 2011,
    "title": "Quantifying uniformity of mapped reads",
    "summary": "  Summary: We describe a tool for quantifying the uniformity of mapped reads in\nhigh-throughput sequencing experiments. Our statistic directly measures the\nuniformity of both read position and fragment length, and we explain how to\ncompute a p-value that can be used to quantify biases arising from experimental\nprotocols and mapping procedures. Our method is useful for comparing different\nprotocols in experiments such as RNA-Seq.\n  Availability and Implementation: We provide a freely available and open\nsource python script that can be used to analyze raw read data or reads mapped\nto transcripts in BAM format at http://www.math.miami.edu/~vhower/ReadSpy.html .\n  Contact: lpachter@math.berkeley.edu\n"
  },
  {
    "year": 2011,
    "title": "Inference of Natural Selection from Interspersed Genomic Elements Based\n  on Polymorphism and Divergence",
    "summary": "  Complete genome sequences contain valuable information about natural\nselection, but extracting this information for short, widely scattered\nnoncoding elements remains a challenging problem. Here we introduce a new\ncomputational method for addressing this problem called Inference of Natural\nSelection from Interspersed Genomically coHerent elemenTs (INSIGHT). INSIGHT\nuses a generative probabilistic model to contrast patterns of polymorphism and\ndivergence in the elements of interest with those in flanking neutral sites,\npooling weak information from many short elements in a manner that accounts for\nvariation among loci in mutation rates and genealogical backgrounds. The method\nis able to disentangle the contributions of weak negative, strong negative, and\npositive selection based on their distinct effects on patterns of polymorphism\nand divergence. Information about divergence is obtained from multiple outgroup\ngenomes using a full phylogenetic model. The model is efficiently fitted to\ngenome-wide data by decomposing the maximum likelihood estimation procedure\ninto three straightforward stages. The key selection-related parameters are\nestimated by expectation maximization. Using simulations, we show that INSIGHT\ncan accurately estimate several parameters of interest even in complex\ndemographic scenarios. We apply our methods to noncoding RNAs, promoter\nregions, and transcription factor binding sites in the human genome, and find\nclear evidence of natural selection. We also present a detailed analysis of\nparticular nucleotide positions within GATA2 binding sites and primary\nmicro-RNA transcripts.\n"
  },
  {
    "year": 2011,
    "title": "Non-alignment comparison of human and high primate genomes",
    "summary": "  Compositional spectra (CS) analysis based on k-mer scoring of DNA sequences\nwas employed in this study for dot-plot comparison of human and primate\ngenomes. The detection of extended conserved synteny regions was based on\ncontinuous fuzzy similarity rather than on chains of discrete anchors (genes or\nhighly conserved noncoding elements). In addition to the high correspondence\nfound in the comparisons of whole-genome sequences, a good similarity was also\nfound after masking gene sequences, indicating that CS analysis manages to\nreveal phylogenetic signal in the organization of noncoding part of the genome\nsequences, including repetitive DNA and the genome \"dark matter\". Obviously,\nthe possibility to reveal parallel ordering depends on the signal of common\nancestor sequence organization varying locally along the corresponding segments\nof the compared genomes. We explored two sources contributing to this signal:\nsequence composition (GC content) and sequence organization (abundances of\nk-mers in the usual A,T,G,C or purine-pyrimidine alphabets). Whole-genome\ncomparisons based on GC distribution along the analyzed sequences indeed gives\nreasonable results, but combining it with k-mer abundances dramatically\nimproves the ordering quality, indicating that compositional and organizational\nheterogeneity comprise complementary sources of information on evolutionary\nconserved similarity of genome sequences.\n"
  },
  {
    "year": 2011,
    "title": "RNASEQR - A streamlined and accurate RNA-seq sequence analysis program",
    "summary": "  The paper has been withdrawn by the authors.\n"
  },
  {
    "year": 2011,
    "title": "Scaling metagenome sequence assembly with probabilistic de Bruijn graphs",
    "summary": "  Deep sequencing has enabled the investigation of a wide range of\nenvironmental microbial ecosystems, but the high memory requirements for {\\em\nde novo} assembly of short-read shotgun sequencing data from these complex\npopulations are an increasingly large practical barrier. Here we introduce a\nmemory-efficient graph representation with which we can analyze the k-mer\nconnectivity of metagenomic samples. The graph representation is based on a\nprobabilistic data structure, a Bloom filter, that allows us to efficiently\nstore assembly graphs in as little as 4 bits per k-mer, albeit inexactly. We\nshow that this data structure accurately represents DNA assembly graphs in low\nmemory. We apply this data structure to the problem of partitioning assembly\ngraphs into components as a prelude to assembly, and show that this reduces the\noverall memory requirements for {\\em de novo} assembly of metagenomes. On one\nsoil metagenome assembly, this approach achieves a nearly 40-fold decrease in\nthe maximum memory requirements for assembly. This probabilistic graph\nrepresentation is a significant theoretical advance in storing assembly graphs\nand also yields immediate leverage on metagenomic assembly.\n"
  },
  {
    "year": 2012,
    "title": "DnaA and the timing of chromosome replication in Escherichia coli as a\n  function of growth rate",
    "summary": "  Background: In Escherichia coli, overlapping rounds of DNA replication allow\nthe bacteria to double in faster times than the time required to copy the\ngenome. The precise timing of initiation of DNA replication is determined by a\nregulatory circuit that depends on the binding of a critical number of\nATP-bound DnaA proteins at the origin of replication. The synthesis of DnaA in\nthe cell is controlled by a growth-rate dependent, negatively autoregulated\ngene found near the origin of replication. Both the regulatory and initiation\nactivity of DnaA depend on its nucleotide bound state and its availability.\n  Results: In order to investigate the contributions of the different\nregulatory processes to the timing of initiation of DNA replication at varying\ngrowth rates, we formulate a minimal quantitative model of the initiator\ncircuit that includes the key ingredients known to regulate the activity of the\nDnaA protein. This model describes the average-cell oscillations in\nDnaA-ATP/DNA during the cell cycle, for varying growth rates. We evaluate the\nconditions under which this ratio attains the same threshold value at the time\nof initiation, independently of the growth rate.\n  Conclusions: We find that a quantitative description of replication\ninitiation by DnaA must rely on the dependency of the basic parameters on\ngrowth rate, in order to account for the timing of initiation of DNA\nreplication at different cell doubling times. We isolate two main possible\nscenarios for this. One possibility is that the basal rate of regulatory\ninactivation by ATP hydrolysis must vary with growth rate. Alternatively, some\nparameters defining promoter activity need to be a function of the growth rate.\nIn either case, the basal rate of gene expression needs to increase with the\ngrowth rate, in accordance with the known characteristics of the dnaA promoter.\n"
  },
  {
    "year": 2012,
    "title": "Bayesian hierarchical reconstruction of protein profiles including a\n  digestion model",
    "summary": "  Introduction : Mass spectrometry approaches are very attractive to detect\nprotein panels in a sensitive and high speed way. MS can be coupled to many\nproteomic separation techniques. However, controlling technological variability\non these analytical chains is a critical point. Adequate information processing\nis mandatory for data analysis to take into account the complexity of the\nanalysed mixture, to improve the measurement reliability and to make the\ntechnology user friendly. Therefore we develop a hierarchical parametric\nprobabilistic model of the LC-MS analytical chain including the technological\nvariability. We introduce a Bayesian reconstruction methodology to recover the\nprotein biomarkers content in a robust way. We will focus on the digestion step\nsince it brings a major contribution to technological variability. Method : In\nthis communication, we introduce a hierarchical model of the LC-MS analytical\nchain. Such a chain is a cascade of molecular events depicted by a graph\nstructure, each node being associated to a molecular state such as protein,\npeptide and ion and each branch to a molecular processing such as digestion,\nionisation and LC-MS separation. This molecular graph defines a hierarchical\nmixture model. We extend the Bayesian statistical framework we have introduced\npreviously [1] to this hierarchical description. As an example, we will\nconsider the digestion step. We describe the digestion process on a pair of\npeptides within the targeted protein as a Bernoulli random process associated\nwith a cleavage probability controlled by the digestion kinetic law.\n"
  },
  {
    "year": 2012,
    "title": "In Silico Genome-Genome Hybridization Values Accurately and Precisely\n  Predict Empirical DNA-DNA Hybridization Values for Classifying Prokaryotes",
    "summary": "  For nearly 50 years microbiologists have been determining prokaryotic genome\nrelatedness by means of nucleic acid reassociation kinetics. These methods,\nhowever, are technically challenging, difficult to reproduce, and - given the\ntime and resources it takes to generate a single data-point - not cost\neffective. In the post genomic era, with the cost of sequencing whole\nprokaryotic genomes no longer a limiting factor, we believed that\ncomputationally predicting the output value from a traditional DNA-DNA\nhybridization experiment using pair-wise comparisons of whole genome sequences\nto be of value. While other computational whole-genome classification methods\nexist, they predict values on widely different scales than DNA-DNA\nhybridization, introducing yet another metric into the polyphasic approach of\ndefining microbial species. Our goal was to develop an in silico BLAST based\npipeline that would predict with a high level of certainty the value of the wet\nlab-based DNA-DNA hybridization values. Here we report on one such method that\nproduces estimates that are both accurate and precise with respect to the\nDNA-DNA hybridization values they are designed to emulate.\n"
  },
  {
    "year": 2012,
    "title": "Evaluation of the Genome Mixture Contents by Means of the Compositional\n  Spectra Method",
    "summary": "  In this research, we consider a mixture of genome fragments of a certain\nbacteria set. The problem of mixture separation is studied under the assumption\nthat all the genomes present in the mixture are completely sequenced or are\nclose to those already sequenced. Such assumption is relevant, e.g., in regular\nobservations of ecological or biomedical objects, where the possible set of\nmicroorganisms is known and it is only necessary to follow their\nconcentrations.\n"
  },
  {
    "year": 2012,
    "title": "GC3 Biology in Eukaryotes and Prokaryotes",
    "summary": "  We describe the distribution of Guanine and Cytosine (GC) content in the\nthird codon position (GC3) distributions in different species, analyze\nevolutionary trends and discuss differences between genes and organisms with\ndistinct GC3 levels. We scrutinize previously published theoretical frameworks\nand construct a unified view of GC3 biology in eukaryotes and prokaryotes.\n"
  },
  {
    "year": 2012,
    "title": "A Reference-Free Algorithm for Computational Normalization of Shotgun\n  Sequencing Data",
    "summary": "  Deep shotgun sequencing and analysis of genomes, transcriptomes, amplified\nsingle-cell genomes, and metagenomes has enabled investigation of a wide range\nof organisms and ecosystems. However, sampling variation in short-read data\nsets and high sequencing error rates of modern sequencers present many new\ncomputational challenges in data interpretation. These challenges have led to\nthe development of new classes of mapping tools and {\\em de novo} assemblers.\nThese algorithms are challenged by the continued improvement in sequencing\nthroughput. We here describe digital normalization, a single-pass computational\nalgorithm that systematizes coverage in shotgun sequencing data sets, thereby\ndecreasing sampling variation, discarding redundant data, and removing the\nmajority of errors. Digital normalization substantially reduces the size of\nshotgun data sets and decreases the memory and time requirements for {\\em de\nnovo} sequence assembly, all without significantly impacting content of the\ngenerated contigs. We apply digital normalization to the assembly of microbial\ngenomic data, amplified single-cell genomic data, and transcriptomic data. Our\nimplementation is freely available for use and modification.\n"
  },
  {
    "year": 2012,
    "title": "Exploring single-sample SNP and INDEL calling with whole-genome de novo\n  assembly",
    "summary": "  Motivation: Eugene Myers in his string graph paper (Myers, 2005) suggested\nthat in a string graph or equivalently a unitig graph, any path spells a valid\nassembly. As a string/unitig graph also encodes every valid assembly of reads,\nsuch a graph, provided that it can be constructed correctly, is in fact a\nlossless representation of reads. In principle, every analysis based on\nwhole-genome shotgun sequencing (WGS) data, such as SNP and insertion/deletion\n(INDEL) calling, can also be achieved with unitigs.\n  Results: To explore the feasibility of using de novo assembly in the context\nof resequencing, we developed a de novo assembler, fermi, that assembles\nIllumina short reads into unitigs while preserving most of information of the\ninput reads. SNPs and INDELs can be called by mapping the unitigs against a\nreference genome. By applying the method on 35-fold human resequencing data, we\nshowed that in comparison to the standard pipeline, our approach yields similar\naccuracy for SNP calling and better results for INDEL calling. It has higher\nsensitivity than other de novo assembly based methods for variant calling. Our\nwork suggests that variant calling with de novo assembly be a beneficial\ncomplement to the standard variant calling pipeline for whole-genome\nresequencing. In the methodological aspects, we proposed FMD-index for\nforward-backward extension of DNA sequences, a fast algorithm for finding all\nsuper-maximal exact matches and one-pass construction of unitigs from an\nFMD-index.\n  Availability: http://github.com/lh3/fermi\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2012,
    "title": "A statistical framework for SNP calling, mutation discovery, association\n  mapping and population genetical parameter estimation from sequencing data",
    "summary": "  Motivation: Most existing methods for DNA sequence analysis rely on accurate\nsequences or genotypes. However, in applications of the next-generation\nsequencing (NGS), accurate genotypes may not be easily obtained (e.g.\nmulti-sample low-coverage sequencing or somatic mutation discovery). These\napplications press for the development of new methods for analyzing sequence\ndata with uncertainty.\n  Results: We present a statistical framework for calling SNPs, discovering\nsomatic mutations, inferring population genetical parameters and performing\nassociation tests directly based on sequencing data without explicit genotyping\nor linkage-based imputation. On real data, we demonstrate that our method\nachieves comparable accuracy to alternative methods for estimating site allele\ncount, for inferring allele frequency spectrum and for association mapping. We\nalso highlight the necessity of using symmetric datasets for finding somatic\nmutations and confirm that for discovering rare events, mismapping is\nfrequently the leading source of errors.\n  Availability: http://samtools.sourceforge.net.\n  Contact: hengli@broadinstitute.org.\n"
  },
  {
    "year": 2012,
    "title": "Phenomenon of irreducible genetic markers for TATAAA motifs in human\n  chromosome 1",
    "summary": "  It is well known that the general transcription factors (GTF) specifically\nrecognize correct TATA boxes, distinguishing them from many others. Employing\nthe principles of determinacy analysis (mathematical theory of rules) we\nanalyzed a fragment of human chromosome 1 DNA sequence and identified specific\ngenetic markers (IG-markers = Irreducible Genetic markers) in the nearest\nproximity to TATAAA motifs. The IG-markers enable determining the exact\nlocation of any TATAAA motif within the investigated DNA fragment. Based on our\ndata we hypothesize that the GTF recognize the\n{\\guillemotleft}true{\\guillemotright} transcriptional start TATA box by means\nof IG-markers. The math method described here is universal and can be used to\nfind IG-markers that will provide, like a global navigation satellite system,\nfor the specific location of any distinct sequence motif within larger DNA\nsequence content.\n"
  },
  {
    "year": 2012,
    "title": "The Whereabouts of 2D Gels in Quantitative Proteomics",
    "summary": "  Two-dimensional gel electrophoresis has been instrumental in the development\nof proteomics. Although it is no longer the exclusive scheme used for\nproteomics, its unique features make it a still highly valuable tool,\nespecially when multiple quantitative comparisons of samples must be made, and\neven for large samples series. However, quantitative proteomics using 2D gels\nis critically dependent on the performances of the protein detection methods\nused after the electrophoretic separations. This chapter therefore examines\ncritically the various detection methods (radioactivity, dyes, fluorescence,\nand silver) as well as the data analysis issues that must be taken into account\nwhen quantitative comparative analysis of 2D gels is performed.\n"
  },
  {
    "year": 2012,
    "title": "Silver Staining of 2D Electrophoresis Gels",
    "summary": "  Silver staining is used to detect proteins after electrophoretic separation\non polyacrylamide gels. It -combines excellent sensitivity (in the low nanogram\nrange) with the use of very simple and cheap equipment and chemicals. For its\nuse in proteomics, two important additional features must be considered,\ncompatibility with mass spectrometry and quantitative response. Both features\nare discussed in this chapter, and optimized silver staining protocols are\nproposed.\n"
  },
  {
    "year": 2012,
    "title": "About the mechanism of interference of silver staining with peptide mass\n  spectrometry",
    "summary": "  The mechanism by which silver staining of proteins in polyacrylamide gels\ninterferes with mass spectrometry of peptides produced by proteolysis has been\ninvestigated. It was demonstrated that this interference increases with time\nbetween silver staining and gel processing, although the silver image is\nconstant. This suggested an important role of the formaldehyde used in silver\nstaining development in this interference process. Consequently, a\nformaldehyde-free staining protocol has been devised, using carbohydrazide as\nthe developing agent. This protocol showed much increased peptide coverage and\nretained the sensitivity of silver staining. These results were however\nobtained at the expense of an increased background in the stained gels and of a\nreduced staining homogeneity.\n"
  },
  {
    "year": 2012,
    "title": "Hidden breakpoints in genome alignments",
    "summary": "  During the course of evolution, an organism's genome can undergo changes that\naffect the large-scale structure of the genome. These changes include gene\ngain, loss, duplication, chromosome fusion, fission, and rearrangement. When\ngene gain and loss occurs in addition to other types of rearrangement,\nbreakpoints of rearrangement can exist that are only detectable by comparison\nof three or more genomes. An arbitrarily large number of these \"hidden\"\nbreakpoints can exist among genomes that exhibit no rearrangements in pairwise\ncomparisons.\n  We present an extension of the multichromosomal breakpoint median problem to\ngenomes that have undergone gene gain and loss. We then demonstrate that the\nmedian distance among three genomes can be used to calculate a lower bound on\nthe number of hidden breakpoints present. We provide an implementation of this\ncalculation including the median distance, along with some practical\nimprovements on the time complexity of the underlying algorithm.\n  We apply our approach to measure the abundance of hidden breakpoints in\nsimulated data sets under a wide range of evolutionary scenarios. We\ndemonstrate that in simulations the hidden breakpoint counts depend strongly on\nrelative rates of inversion and gene gain/loss. Finally we apply current\nmultiple genome aligners to the simulated genomes, and show that all aligners\nintroduce a high degree of error in hidden breakpoint counts, and that this\nerror grows with evolutionary distance in the simulation. Our results suggest\nthat hidden breakpoint error may be pervasive in genome alignments.\n"
  },
  {
    "year": 2012,
    "title": "Context-specific transcriptional regulatory network inference from\n  global gene expression maps using double two-way t-tests",
    "summary": "  Transcriptional regulatory network inference methods have been studied for\nyears. Most of them relie on complex mathematical and algorithmic concepts,\nmaking them hard to adapt, re-implement or integrate with other methods. To\naddress this problem, we introduce a novel method based on a minimal\nstatistical model for observing transcriptional regulatory interactions in\nnoisy expression data, which assumes that transcription factors (TFs) and their\ntargets are both differentially expressed in a gene-specific, critical sample\ncontrast, as measured by repeated two-way t-tests. This method is conceptually\nsimple and easy to implement and integrate in any statistical software\nenvironment. Benchmarking on standard E. coli and yeast reference datasets\nshowed that it performs equally well as the best existing methods. Analysis of\nthe predicted interactions suggested that it works best to infer\ncontext-specific TF-target interactions which only co-express locally. We\nconfirmed this hypothesis on a dataset of more than 1,000 normal human tissue\nsamples, where we found that our method predicts highly tissue-specific and\nfunctionally relevant interactions, whereas a global co-expression method only\nassociates general TFs to non-specific biological processes.\n"
  },
  {
    "year": 2012,
    "title": "Finding the sources of missing heritability in a yeast cross",
    "summary": "  For many traits, including susceptibility to common diseases in humans,\ncausal loci uncovered by genetic mapping studies explain only a minority of the\nheritable contribution to trait variation. Multiple explanations for this\n\"missing heritability\" have been proposed. Here we use a large cross between\ntwo yeast strains to accurately estimate different sources of heritable\nvariation for 46 quantitative traits and to detect underlying loci with high\nstatistical power. We find that the detected loci explain nearly the entire\nadditive contribution to heritable variation for the traits studied. We also\nshow that the contribution to heritability of gene-gene interactions varies\namong traits, from near zero to 50%. Detected two-locus interactions explain\nonly a minority of this contribution. These results substantially advance our\nunderstanding of the missing heritability problem and have important\nimplications for future studies of complex and quantitative traits.\n"
  },
  {
    "year": 2012,
    "title": "Binary Interval Search (BITS): A Scalable Algorithm for Counting\n  Interval Intersections",
    "summary": "  Motivation: The comparison of diverse genomic datasets is fundamental to\nunderstanding genome biology. Researchers must explore many large datasets of\ngenome intervals (e.g., genes, sequence alignments) to place their experimental\nresults in a broader context and to make new discoveries. Relationships between\ngenomic datasets are typically measured by identifying intervals that\nintersect: that is, they overlap and thus share a common genome interval. Given\nthe continued advances in DNA sequencing technologies, efficient methods for\nmeasuring statistically significant relationships between many sets of genomic\nfeatures is crucial for future discovery.\n  Results: We introduce the Binary Interval Search (BITS) algorithm, a novel\nand scalable approach to interval set intersection. We demonstrate that BITS\noutperforms existing methods at counting interval intersections. Moreover, we\nshow that BITS is intrinsically suited to parallel computing architectures such\nas Graphics Processing Units (GPUs) by illustrating its utility for efficient\nMonte-Carlo simulations measuring the significance of relationships between\nsets of genomic intervals.\n"
  },
  {
    "year": 2012,
    "title": "Differential Expression Analysis for A Mouse p53KO Microarray Dataset",
    "summary": "  Affymetrix GeneChip technology is used to detect gene expression levels in\nsamples of cells under different conditions. In this project, we analyzed the\ngene expression profiling data for mouse induced pluripotent stem cell (iPSCs)\n(Takahashi, 2006) on Affymetrix Mouse 430 2.0 GeneChip. Three biological\nconditions were present: p53KO, microRNA mir34aKO, and wild type, each with\nthree biological replicates. The first part was devoted to identifying\ndifferentially expressed genes from around 45,000 of them, and looking into\ntheir biological meanings by pathway analysis. The second part dealt with\nrepetitive elements represented in the pool of mRNAs. We identified repetitive\nelements that show a significant difference between two biological conditions.\nBoth the comparison of p53KO versus WT and mir34aKO versus WT were done.\nHowever, the emphasis was on the former. Laboratory validation with qPCR\nconfirmed our findings. This work was done under the Overseas Research\nFellowship (ORF) Scheme 2012 for Science Students by the Faculty of Science,\nThe University of Hong Kong. Many thanks are due to the University for the\nfellowship, and to Professors Terry Speed and Lin He and Drs Chao-po Lin and\nAnne Biton of the University of California at Berkeley for their supervision\nand generous support.\n"
  },
  {
    "year": 2012,
    "title": "SSW Library: An SIMD Smith-Waterman C/C++ Library for Use in Genomic\n  Applications",
    "summary": "  Summary: The Smith Waterman (SW) algorithm, which produces the optimal\npairwise alignment between two sequences, is frequently used as a key component\nof fast heuristic read mapping and variation detection tools, but current\nimplementations are either designed as monolithic protein database searching\ntools or are embedded into other tools. To facilitate easy integration of the\nfast Single Instruction Multiple Data (SIMD) SW algorithm into third party\nsoftware, we wrote a C/C++ library, which extends Farrars Striped SW (SSW) to\nreturn alignment information in addition to the optimal SW score. Availability:\nSSW is available both as a C/C++ software library, as well as a stand alone\nalignment tool wrapping the librarys functionality at\nhttps://github.com/mengyao/Complete- Striped-Smith-Waterman-Library Contact:\nmarth@bc.edu\n"
  },
  {
    "year": 2012,
    "title": "An explanation of unexpected Hoxd expressions in mutant mice",
    "summary": "  The Hox gene collinearity enigma has often been approached using models based\non biomolecular mechanisms. The biophysical model, is an alternative approach,\nspeculating that collinearity is caused by physical forces pulling the Hox\nclusters from a territory where they are inactive to a distinct spatial domain\nwhere they are activated in a step by step manner.\n  Hox gene translocations have recently been observed in support of the\nbiophysical model. Furthermore, genetic engineering experiments, performed in\nembryonic mice, gave rise to some unexpected mutant expressions that\nbiomolecular models could not predict. In several cases when anterior Hoxd\ngenes are deleted, the expression of the genes whose expression is probed in\nthe mutants are impossible to anticipate. On the contrary, the biophysical\nmodel offers convincing explanation.\n  All these experimental results support the idea of physical forces being\nresponsible for Hox gene collinearity. In order to test the validity of the\nvarious models further, certain experiment involving gene deletions are\nproposed. The biophysical and biomolecular models predict different results for\nthese experiments, hence the expected outcome will confirm or question the\nvalidity of these models.\n"
  },
  {
    "year": 2012,
    "title": "Genome-wide analysis points to roles for extracellular matrix\n  remodeling, the visual cycle, and neuronal development in myopia",
    "summary": "  Myopia, or nearsightedness, is the most common eye disorder, resulting\nprimarily from excess elongation of the eye. The etiology of myopia, although\nknown to be complex, is poorly understood. Here we report the largest ever\ngenome-wide association study (43,360 participants) on myopia in Europeans. We\nperformed a survival analysis on age of myopia onset and identified 19\nsignificant associations (p < 5e-8), two of which are replications of earlier\nassociations with refractive error. These 19 associations in total explain 2.7%\nof the variance in myopia age of onset, and point towards a number of different\nmechanisms behind the development of myopia. One association is in the gene\nPRSS56, which has previously been linked to abnormally small eyes; one is in a\ngene that forms part of the extracellular matrix (LAMA2); two are in or near\ngenes involved in the regeneration of 11-cis-retinal (RGR and RDH5); two are\nnear genes known to be involved in the growth and guidance of retinal ganglion\ncells (ZIC2, SFRP1); and five are in or near genes involved in neuronal\nsignaling or development. These novel findings point towards multiple genetic\nfactors involved in the development of myopia and suggest that complex\ninteractions between extracellular matrix remodeling, neuronal development, and\nvisual signals from the retina may underlie the development of myopia in\nhumans.\n"
  },
  {
    "year": 2012,
    "title": "A genetic variant near olfactory receptor genes influences cilantro\n  preference",
    "summary": "  The leaves of the Coriandrum sativum plant, known as cilantro or coriander,\nare widely used in many cuisines around the world. However, far from being a\nbenign culinary herb, cilantro can be polarizing---many people love it while\nothers claim that it tastes or smells foul, often like soap or dirt. This soapy\nor pungent aroma is largely attributed to several aldehydes present in\ncilantro. Cilantro preference is suspected to have a genetic component, yet to\ndate nothing is known about specific mechanisms. Here we present the results of\na genome-wide association study among 14,604 participants of European ancestry\nwho reported whether cilantro tasted soapy, with replication in a distinct set\nof 11,851 participants who declared whether they liked cilantro. We find a\nsingle nucleotide polymorphism (SNP) significantly associated with soapy-taste\ndetection that is confirmed in the cilantro preference group. This SNP,\nrs72921001, (p=6.4e-9, odds ratio 0.81 per A allele) lies within a cluster of\nolfactory receptor genes on chromosome 11. Among these olfactory receptor genes\nis OR6A2, which has a high binding specificity for several of the aldehydes\nthat give cilantro its characteristic odor. We also estimate the heritability\nof cilantro soapy-taste detection in our cohort, showing that the heritability\ntagged by common SNPs is low, about 0.087. These results confirm that there is\na genetic component to cilantro taste perception and suggest that cilantro\ndislike may stem from genetic variants in olfactory receptors. We propose that\nOR6A2 may be the olfactory receptor that contributes to the detection of a\nsoapy smell from cilantro in European populations.\n"
  },
  {
    "year": 2012,
    "title": "Horizontal gene transfer drives extreme physiological change in\n  Haloarchaea",
    "summary": "  The haloarchaea are aerobic, heterotrophic, photophosphorylating prokaryotes,\nwhose supposed closest relatives and ancestors, the methanogens, are\nCO2-reducing, anaerobic chemolithotrophs. Using two available haloarchaeal\ngenomes we firstly confirmed the methanogenic ancestry of the group and then\ninvestigated those individual genes in the haloarchaea that differ in their\nphylogenetic signal to this relationship. We found that almost half the genes,\nabout which we can make strong statements, have bacterial ancestry and are\nlikely a result of multiple horizontal transfer events. Futhermore their\nfunctions specifically relate to the phenotypic changes required for a\nchemolithotroph to become a heterotroph. If this phylogenetic relationship is\ncorrect, it implies the development of the haloarchaeal phenotype was among the\nmost extreme changes in cellular physiology fuelled by horizontal gene\ntransfer.\n"
  },
  {
    "year": 2012,
    "title": "Comparative Analysis of Tandem Repeats from Hundreds of Species Reveals\n  Unique Insights into Centromere Evolution",
    "summary": "  Centromeres are essential for chromosome segregation, yet their DNA sequences\nevolve rapidly. In most animals and plants that have been studied, centromeres\ncontain megabase-scale arrays of tandem repeats. Despite their importance, very\nlittle is known about the degree to which centromere tandem repeats share\ncommon properties between different species across different phyla. We used\nbioinformatic methods to identify high-copy tandem repeats from 282 species\nusing publicly available genomic sequence and our own data. The assumption that\nthe most abundant tandem repeat is the centromere DNA was true for most species\nwhose centromeres have been previously characterized, suggesting this is a\ngeneral property of genomes. Our methods are compatible with all current\nsequencing technologies. Long Pacific Biosciences sequence reads allowed us to\nfind tandem repeat monomers up to 1,419 bp. High-copy centromere tandem repeats\nwere found in almost all animal and plant genomes, but repeat monomers were\nhighly variable in sequence composition and in length. Furthermore,\nphylogenetic analysis of sequence homology showed little evidence of sequence\nconservation beyond ~50 million years of divergence. We find that despite an\noverall lack of sequence conservation, centromere tandem repeats from diverse\nspecies showed similar modes of evolution, including the appearance of higher\norder repeat structures in which several polymorphic monomers make up a larger\nrepeating unit. While centromere position in most eukaryotes is epigenetically\ndetermined, our results indicate that tandem repeats are highly prevalent at\ncentromeres of both animals and plants. This suggests a functional role for\nsuch repeats, perhaps in promoting concerted evolution of centromere DNA across\nchromosomes.\n"
  },
  {
    "year": 2012,
    "title": "Protein function influences frequency of encoded regions containing\n  VNTRs and number of unique interactions",
    "summary": "  Proteins encoded by genes containing regions of variable number tandem\nrepeats (VNTRs) are known to be polymorphic within species but the influence of\ntheir instability in molecular interactions remains unclear. VNTRs are\noverrepresented in encoding sequence of particular functional groups where\ntheir presence could influence protein interactions. Using human consensus\ncoding sequence, this work examines if genomic instability, determined by\nregions of VNTRs, influences the number of protein interactions. Findings\nreveal that, in relation to protein function, the frequency of unique\ninteractions in human proteins increase with the number of repeated regions.\nThis supports experimental evidence that repeat expansion may lead to an\nincrease in molecular interactions. Genetic diversity, estimated by Ka/Ks,\nappeared to decrease as the number of protein-protein interactions increased.\nAdditionally, G+C and CpG content were negatively correlated with increasing\noccurrence of VNTRs. This may indicate that nucleotide composition along with\nselective processes can increase genomic stability and thereby restrict the\nexpansion of repeated regions. Proteins involved in acetylation are associated\nwith a high number of repeated regions and interactions but a low G+C and CpG\ncontent. While in contrast, less interactive membrane proteins contain a lower\nnumber of repeated regions but higher levels of C+G and CpGs. This work\nprovides further evidence that VNTRs may provide the genetic variability to\ngenerate unique interactions between proteins.\n"
  },
  {
    "year": 2012,
    "title": "Antibiotic resistant characteristics from 16S rRNA",
    "summary": "  Background: Microbiota have evolved to acclimate themselves to many\nenvironments. Humanity is become ever increasingly medicated and many of those\nmedications are antibiotics. Sadly, Microbiota are adapting to medication and\nwith each passing generation they become more difficult to subdue. The 16S\nsmall subunit of bacterial ribosomal rRNA provides a wealth of information for\nclassifying the species level taxonomy of bacteria. Methodology/Principal\nFindings: Experiments were collected utilizing broad and narrow spectrum\nantibiotics, which act primarily on DNA. In each experiment a statistically\nsignificant, unique and predictable pattern of sequential and thermodynamic\nstability or instability was found to correlate to antibiotic resistance.\nConclusions/Significance: Classification of antibiotic resistance is possible\nfor some species and antibiotic combinations using the 16S rRNA sequential and\nthermodynamic properties.\n"
  },
  {
    "year": 2012,
    "title": "Gene silencing and large-scale domain structure of the E. coli genome",
    "summary": "  The H-NS chromosome-organizing protein in E. coli can stabilize genomic DNA\nloops, and form oligomeric structures connected to repression of gene\nexpression. Motivated by the link between chromosome organization, protein\nbinding and gene expression, we analyzed publicly available genomic data sets\nof various origins, from genome-wide protein binding profiles to evolutionary\ninformation, exploring the connections between chromosomal organization,\ngenesilencing, pseudo-gene localization and horizontal gene transfer. We report\nthe existence of transcriptionally silent contiguous areas corresponding to\nlarge regions of H-NS protein binding along the genome, their position\nindicates a possible relationship with the known large-scale features of\nchromosome organization.\n"
  },
  {
    "year": 2012,
    "title": "LUMPY: A probabilistic framework for structural variant discovery",
    "summary": "  Comprehensive discovery of structural variation (SV) in human genomes from\nDNA sequencing requires the integration of multiple alignment signals including\nread-pair, split-read and read-depth. However, owing to inherent technical\nchallenges, most existing SV discovery approaches utilize only one signal and\nconsequently suffer from reduced sensitivity, especially at low sequence\ncoverage and for smaller SVs. We present a novel and extremely flexible\nprobabilistic SV discovery framework that is capable of integrating any number\nof SV detection signals including those generated from read alignments or prior\nevidence. We demonstrate improved sensitivity over extant methods by combining\npaired-end and split-read alignments and emphasize the utility of our framework\nfor comprehensive studies of structural variation in heterogeneous tumor\ngenomes. We further discuss the broader utility of this approach for\nprobabilistic integration of diverse genomic interval datasets.\n"
  },
  {
    "year": 2012,
    "title": "Integrative modeling of eQTLs and cis-regulatory elements suggest\n  mechanisms underlying cell type specificity of eQTLs",
    "summary": "  Genetic variants in cis-regulatory elements or trans-acting regulators\ncommonly influence the quantity and spatiotemporal distribution of gene\ntranscription. Recent interest in expression quantitative trait locus (eQTL)\nmapping has paralleled the adoption of genome-wide association studies (GWAS)\nfor the analysis of complex traits and disease in humans. Under the hypothesis\nthat many GWAS associations tag non-coding SNPs with small effects, and that\nthese SNPs exert phenotypic control by modifying gene expression, it has become\ncommon to interpret GWAS associations using eQTL data. To exploit the\nmechanistic interpretability of eQTL-GWAS comparisons, an improved\nunderstanding of the genetic architecture and cell type specificity of eQTLs is\nrequired. We address this need by performing an eQTL analysis in four parts:\nfirst we identified eQTLs from eleven studies on seven cell types; next we\nquantified cell type specific eQTLs across the studies; then we integrated eQTL\ndata with cis-regulatory element (CRE) data sets from the ENCODE project;\nfinally we built a classifier to predict cell type specific eQTLs. Consistent\nwith prior studies, we demonstrate that allelic heterogeneity is pervasive at\ncis-eQTLs and that cis-eQTLs are often cell type specific. Within and between\ncell type eQTL replication is associated with eQTL SNP overlap with hundreds of\ncell type specific CRE element classes, including enhancer, promoter, and\nrepressive chromatin marks, regions of open chromatin, and many classes of DNA\nbinding proteins. Using a random forest classifier including 526 CRE data sets\nas features, we successfully predict the cell type specificity of eQTL SNPs in\nthe absence of gene expression data from the cell type of interest. We\nanticipate that such integrative, predictive modeling will improve our ability\nto understand the mechanistic basis of human complex phenotypic variation.\n"
  },
  {
    "year": 2012,
    "title": "Species Identification and Profiling of Complex Microbial Communities\n  Using Shotgun Illumina Sequencing of 16S rRNA Amplicon Sequences",
    "summary": "  The high throughput and cost-effectiveness afforded by short-read sequencing\ntechnologies, in principle, enable researchers to perform 16S rRNA profiling of\ncomplex microbial communities at unprecedented depth and resolution. Existing\nIllumina sequencing protocols are, however, limited by the fraction of the 16S\nrRNA gene that is interrogated and therefore limit the resolution and quality\nof the profiling. To address this, we present the design of a novel protocol\nfor shotgun Illumina sequencing of the bacterial 16S rRNA gene, optimized to\ncapture more than 90% of sequences in the Greengenes database and with nearly\ntwice the resolution of existing protocols. Using several in silico and\nexperimental datasets, we demonstrate that despite the presence of multiple\nvariable and conserved regions, the resulting shotgun sequences can be used to\naccurately quantify the diversity of complex microbial communities. The\nreconstruction of a significant fraction of the 16S rRNA gene also enabled high\nprecision (>90%) in species-level identification thereby opening up potential\napplication of this approach for clinical microbial characterization.\n"
  },
  {
    "year": 2012,
    "title": "Structural attributes of nucleotide sequences in promoter regions of\n  supercoiling-sensitive genes: how to relate microarray expression data with\n  genomic sequences",
    "summary": "  The level of supercoiling in the chromosome can affect gene expression. To\nclarify the basis of supercoiling sensitivity, we analyzed the structural\nfeatures of nucleotide sequences in the vicinity of promoters for the genes\nwith expression enhanced and decreased in response to loss of chromosomal\nsupercoiling in E. coli. Fourier analysis of promoter sequences for\nsupercoiling-sensitive genes reveals the tendency in selection of sequences\nwith helical periodicities close to 10 nt for relaxation-induced genes and to\n11 nt for relaxation-repressed genes. The helical periodicities in the subsets\nof promoters recognized by RNA polymerase with different sigma factors were\nalso studied. A special procedure was developed for study of correlations\nbetween the intensities of periodicities in promoter sequences and the\nexpression levels of corresponding genes. Significant correlations of\nexpression with the AT content and with AT periodicities about 10, 11, and 50\nnt indicate their role in regulation of supercoiling-sensitive genes.\n"
  },
  {
    "year": 2012,
    "title": "Improved proteomic analysis of nuclear proteins, as exemplified by the\n  comparison of two myelo\u00efd cell lines nuclear proteomes",
    "summary": "  One of the challenges of the proteomic analysis by 2D-gel is to visualize the\nlow abundance proteins, particularly those localized in organelles. An\nadditional problem with nuclear proteins lies in their strong interaction with\nnuclear acids. Several experimental procedures have been tested to increase, in\nthe nuclear extract, the ratio of nuclear proteins compared to contaminant\nproteins, and also to obtain reproducible conditions compatible with 2D-gel\nelectrophoresis. The NaCl procedure has been chosen. To test the interest of\nthis procedure, the nuclear protein expression profiles of macrophages and\ndendritic cells have been compared with a proteomic approach by 2D-gel\nelectrophoresis. Delta 2D software and mass spectrometry analyses have allowed\npointing out some proteins of interest. We have chosen some of them, involved\nin transcriptional regulation and/or chromatin structure for further\nvalidations. The immunoblotting experiments have shown that most of observed\nchanges are due to post-translational modifications, thereby a exemplifying the\ninterest of the 2D gel approach. Finally, this approach allowed us to reach not\nonly high abundance nuclear proteins but also lower abundance proteins, such as\nthe HP1 proteins and reinforces the interest of using 2DE-gel in proteomics\nbecause of its ability to visualize intact proteins with their modifications.\n"
  },
  {
    "year": 2012,
    "title": "De novo genomic analyses for non-model organisms: an evaluation of\n  methods across a multi-species data set",
    "summary": "  High-throughput sequencing (HTS) is revolutionizing biological research by\nenabling scientists to quickly and cheaply query variation at a genomic scale.\nDespite the increasing ease of obtaining such data, using these data\neffectively still poses notable challenges, especially for those working with\norganisms without a high-quality reference genome. For every stage of analysis\n- from assembly to annotation to variant discovery - researchers have to\ndistinguish technical artifacts from the biological realities of their data\nbefore they can make inference. In this work, I explore these challenges by\ngenerating a large de novo comparative transcriptomic dataset data for a clade\nof lizards and constructing a pipeline to analyze these data. Then, using a\ncombination of novel metrics and an externally validated variant data set, I\ntest the efficacy of my approach, identify areas of improvement, and propose\nways to minimize these errors. I find that with careful data curation, HTS can\nbe a powerful tool for generating genomic data for non-model organisms.\n"
  },
  {
    "year": 2012,
    "title": "Genome and transcriptome studies of the protozoan parasites Trypanosoma\n  cruzi and Giardia intestinalis",
    "summary": "  Trypanosoma cruzi and Giardia intestinalis are two human pathogens and\nprotozoan parasites responsible for the diseases Chagas disease and giardiasis,\nrespectively. Both diseases cause suffering and illness in several million\nindividuals. The former disease occurs primarily in South America and Central\nAmerica, and the latter disease occurs worldwide. Current therapeutics are\ntoxic and lack efficacy, and potential vaccines are far from the market.\nIncreased knowledge about the biology of these parasites is essential for drug\nand vaccine development, and new diagnostic tests. In this thesis,\nhigh-throughput sequencing was applied together with extensive bioinformatic\nanalyses to yield insights into the biology and evolution of Trypanosoma cruzi\nand Giardia intestinalis. Bioinformatics analysis of DNA and RNA sequences was\nperformed to identify features that may be of importance for parasite biology\nand functional characterization. This thesis is based on five papers (i-v).\nPaper i and ii describe comparative genome studies of three distinct genotypes\nof Giardia intestinalis (A, B and E). Paper iii describes a genome comparison\nof the human infecting Trypanosoma cruzi with the bat-restricted subspecies\nTrypanosoma cruzi marinkellei. Paper iv describes the repertoire of small\nnon-coding RNAs in Trypanosoma cruzi epimastigotes. Paper v describes\ntranscriptome analysis using paired-end RNA-Seq of three distinct genotypes of\nGiardia intestinalis (A, B and E).\n"
  },
  {
    "year": 2012,
    "title": "Utilizing RNA-Seq Data for Cancer Network Inference",
    "summary": "  An important challenge in cancer systems biology is to uncover the complex\nnetwork of interactions between genes (tumor suppressor genes and oncogenes)\nimplicated in cancer. Next generation sequencing provides unparalleled ability\nto probe the expression levels of the entire set of cancer genes and their\ntranscript isoforms. However, there are onerous statistical and computational\nissues in interpreting high-dimensional sequencing data and inferring the\nunderlying genetic network. In this study, we analyzed RNA-Seq data from\nlymphoblastoid cell lines derived from a population of 69 human individuals and\nimplemented a probabilistic framework to construct biologically-relevant\ngenetic networks. In particular, we employed a graphical lasso analysis,\nmotivated by considerations of the maximum entropy formalism, to estimate the\nsparse inverse covariance matrix of RNA-Seq data. Gene ontology, pathway\nenrichment and protein-protein path length analysis were all carried out to\nvalidate the biological context of the predicted network of interacting cancer\ngene isoforms.\n"
  },
  {
    "year": 2012,
    "title": "Illumina Sequencing Artifacts Revealed by Connectivity Analysis of\n  Metagenomic Datasets",
    "summary": "  Sequencing errors and biases in metagenomic datasets affect coverage-based\nassemblies and are often ignored during analysis. Here, we analyze read\nconnectivity in metagenomes and identify the presence of problematic and likely\na-biological connectivity within metagenome assembly graphs. Specifically, we\nidentify highly connected sequences which join a large proportion of reads\nwithin each real metagenome. These sequences show position-specific bias in\nshotgun reads, suggestive of sequencing artifacts, and are only minimally\nincorporated into contigs by assembly. The removal of these sequences prior to\nassembly results in similar assembly content for most metagenomes and enables\nthe use of graph partitioning to decrease assembly memory and time\nrequirements.\n"
  },
  {
    "year": 2012,
    "title": "Biological Database of Images and Genomes: tools for community\n  annotations linking image and genomic information",
    "summary": "  Genomic data and biomedical imaging data are undergoing exponential growth.\nHowever, our understanding of the phenotype-genotype connection linking the two\ntypes of data is lagging behind. While there are many types of software that\nenable the manipulation and analysis of image data and genomic data as separate\nentities, there is no framework established for linking the two. We present a\ngeneric set of software tools, BioDIG, that allows linking of image data to\ngenomic data. BioDIG tools can be applied to a wide range of research problems\nthat require linking images to genomes. BioDIG features the following: rapid\nconstruction of web-based workbenches, community-based annotation, user\nmanagement, and web-services. By using BioDIG to create websites, researchers\nand curators can rapidly annotate large number of images with genomic\ninformation. Here we present the BioDIG software tools that include an image\nmodule, a genome module and a user management module. We also introduce a\nBioDIG-based website, MyDIG, which is being used to annotate images of\nMycoplasma.\n"
  },
  {
    "year": 2012,
    "title": "System Wide Analyses have Underestimated Protein Abundances and the\n  Importance of Transcription in Mammals",
    "summary": "  Large scale surveys in mammalian tissue culture cells suggest that the\nprotein expressed at the median abundance is present at 8,000 - 16,000\nmolecules per cell and that differences in mRNA expression between genes\nexplain only 10-40% of the differences in protein levels. We find, however,\nthat these surveys have significantly underestimated protein abundances and the\nrelative importance of transcription. Using individual measurements for 61\nhousekeeping proteins to rescale whole proteome data from Schwanhausser et al.,\nwe find that the median protein detected is expressed at 170,000 molecules per\ncell and that our corrected protein abundance estimates show a higher\ncorrelation with mRNA abundances than do the uncorrected protein data. In\naddition, we estimated the impact of further errors in mRNA and protein\nabundances, showing that mRNA levels explain at least 56% of the differences in\nprotein abundance for the genes detected by Schwanhausser et al., though\nbecause one major source of error could not be estimated the true percent\ncontribution could be higher. We also employed a second, independent strategy\nto determine the contribution of mRNA levels to protein expression. We show\nthat the variance in translation rates directly measured by ribosome profiling\nis only 12% of that inferred by Schwanhausser et al. and that the measured and\ninferred translation rates correlate only poorly (R2=0.13). Based on this, our\nsecond strategy suggests that mRNA levels explain ~81% of the variance in\nprotein levels. We also determined the percent contributions of transcription,\nRNA degradation, translation and protein degradation to the variance in protein\nabundances using both of our strategies. While the magnitudes of the two\nestimates vary, they both suggest that transcription plays a more important\nrole than the earlier studies implied and translation a much smaller role.\n"
  },
  {
    "year": 2012,
    "title": "Assembling large, complex environmental metagenomes",
    "summary": "  The large volumes of sequencing data required to sample complex environments\ndeeply pose new challenges to sequence analysis approaches. De novo metagenomic\nassembly effectively reduces the total amount of data to be analyzed but\nrequires significant computational resources. We apply two pre-assembly\nfiltering approaches, digital normalization and partitioning, to make large\nmetagenome assemblies more comput\\ ationaly tractable. Using a human gut mock\ncommunity dataset, we demonstrate that these methods result in assemblies\nnearly identical to assemblies from unprocessed data. We then assemble two\nlarge soil metagenomes from matched Iowa corn and native prairie soils. The\npredicted functional content and phylogenetic origin of the assembled contigs\nindicate significant taxonomic differences despite similar function. The\nassembly strategies presented are generic and can be extended to any\nmetagenome; full source code is freely available under a BSD license.\n"
  },
  {
    "year": 2012,
    "title": "Zygotic combinatorial process in plants",
    "summary": "  Experimental data that prove the existence of the zygotic combinatorial\nprocess occurring in an embryogenesis-entering zygote are presented in the\npaper. The zygotic combinatorial process is found when analyzing F1 hybrid\nplants obtained from crossing homozygous forms different, minimum, in two\nmarker enzymes, and it is found in that hybrid plant which, with one marker\nenzyme heterozygous spectrum, has a homozygous spectrum of the other. The\nzygotic combinatorial process leads to F1 hybrids uniformity aberration. The\nzygotic combinatory process revealed in the study is supposed to be conditioned\nby chromosome polyteny in mother plant cells and diminution of chromatin excess\nfrom the embryogenesis-entering zygote. An obligatory condition for\ncombinatorial process is the presence of free exchange of cromatides among\nhomological chromosomes in an embryogenesis-entering cell, i.e. the presence of\ncrossing-over analogous to the one proceeding at meiosis. The found\ncombinatorial process and the earlier-obtained data confirm the hypothesis on\nmulti-dimensionality of inherited information coding. Differential polyteny of\ncertain chromosome regions can lead to differences among homozygous plants\nhaving the same alleles in genes located in polytenized regions and controlling\nmorpho-physiological traits.\n"
  },
  {
    "year": 2013,
    "title": "A comparative analysis of transcription factor expression during\n  metazoan embryonic development",
    "summary": "  During embryonic development, a complex organism is formed from a single\nstarting cell. These processes of growth and differentiation are driven by\nlarge transcriptional changes, which are following the expression and activity\nof transcription factors (TFs). This study sought to compare TF expression\nduring embryonic development in a diverse group of metazoan animals:\nrepresentatives of vertebrates (Danio rerio, Xenopus tropicalis), a chordate\n(Ciona intestinalis) and invertebrate phyla such as insects (Drosophila\nmelanogaster, Anopheles gambiae) and nematodes (Caenorhabditis elegans) were\nsampled, The different species showed overall very similar TF expression\npatterns, with TF expression increasing during the initial stages of\ndevelopment. C2H2 zinc finger TFs were over-represented and Homeobox TFs were\nunder-represented in the early stages in all species. We further clustered TFs\nfor each species based on their quantitative temporal expression profiles. This\nshowed very similar TF expression trends in development in vertebrate and\ninsect species. However, analysis of the expression of orthologous pairs\nbetween more closely related species showed that expression of most individual\nTFs is not conserved, following the general model of duplication and\ndiversification. The degree of similarity between TF expression between Xenopus\ntropicalis and Danio rerio followed the hourglass model, with the greatest\nsimilarity occuring during the early tailbud stage in Xenopus tropicalis and\nthe late segmentation stage in Danio rerio. However, for Drosophila\nmelanogaster and Anopheles gambiae there were two periods of high TF\ntranscriptome similarity, one during the Arthropod phylotypic stage at 8-10\nhours into Drosophila development and the other later at 16-18 hours into\nDrosophila development.\n"
  },
  {
    "year": 2013,
    "title": "TrAp: a Tree Approach for Fingerprinting Subclonal Tumor Composition",
    "summary": "  Revealing the clonal composition of a single tumor is essential for\nidentifying cell subpopulations with metastatic potential in primary tumors or\nwith resistance to therapies in metastatic tumors. Sequencing technologies\nprovide an overview of an aggregate of numerous cells, rather than\nsubclonal-specific quantification of aberrations such as single nucleotide\nvariants (SNVs). Computational approaches to de-mix a single collective signal\nfrom the mixed cell population of a tumor sample into its individual components\nare currently not available. Herein we propose a framework for deconvolving\ndata from a single genome-wide experiment to infer the composition, abundance\nand evolutionary paths of the underlying cell subpopulations of a tumor. The\nmethod is based on the plausible biological assumption that tumor progression\nis an evolutionary process where each individual aberration event stems from a\nunique subclone and is present in all its descendants subclones. We have\ndeveloped an efficient algorithm (TrAp) for solving this mixture problem. In\nsilico analyses show that TrAp correctly deconvolves mixed subpopulations when\nthe number of subpopulations and the measurement errors are moderate. We\ndemonstrate the applicability of the method using tumor karyotypes and somatic\nhypermutation datasets. We applied TrAp to SNV frequency profile from Exome-Seq\nexperiment of a renal cell carcinoma tumor sample and compared the mutational\nprofile of the inferred subpopulations to the mutational profiles of twenty\nsingle cells of the same tumor. Despite the large experimental noise, specific\nco-occurring mutations found in clones inferred by TrAp are also present in\nsome of these single cells. Finally, we deconvolve Exome-Seq data from three\ndistinct metastases from different body compartments of one melanoma patient\nand exhibit the evolutionary relationships of their subpopulations.\n"
  },
  {
    "year": 2013,
    "title": "Kerfuffle: a web tool for multi-species gene colocalization analysis",
    "summary": "  The evolutionary pressures that underlie the large-scale functional\norganization of the genome are not well understood in eukaryotes. Recent\nevidence suggests that functionally similar genes may colocalize (cluster) in\nthe eukaryotic genome, suggesting the role of chromatin-level gene regulation\nin shaping the physical distribution of coordinated genes. However, few of the\nbioinformatic tools currently available allow for a systematic study of gene\ncolocalization across several, evolutionarily distant species. Kerfuffle is a\nweb tool designed to help discover, visualize, and quantify the physical\norganization of genomes by identifying significant gene colocalization and\nconservation across the assembled genomes of available species (currently up to\n47, from humans to worms). Kerfuffle only requires the user to specify a list\nof human genes and the names of other species of interest. Without further\ninput from the user, the software queries the e!Ensembl BioMart server to\nobtain positional information and discovers homology relations in all genes and\nspecies specified. Using this information, Kerfuffle performs a multi-species\nclustering analysis, presents downloadable lists of clustered genes, performs\nMonte Carlo statistical significance calculations, estimates how conserved gene\nclusters are across species, plots histograms and interactive graphs, allows\nusers to save their queries, and generates a downloadable visualization of the\nclusters using the Circos software. These analyses may be used to further\nexplore the functional roles of gene clusters by interrogating the enriched\nmolecular pathways associated with each cluster.\n"
  },
  {
    "year": 2013,
    "title": "Which is faster: Bowtie2GP > Bowtie > Bowtie2 > BWA",
    "summary": "  We have recently used genetic programming to automatically generate an\nimproved version of Langmead's DNA read alignment tool Bowtie2 Sect.5.3\nRN/12/09. We find it runs more than four times faster than the Bioinformatics\nsequencing tool (BWA) currently used with short next generation paired end DNA\nsequences by the Cancer Institute, takes less memory and yet finds similar\nmatches in the human genome.\n"
  },
  {
    "year": 2013,
    "title": "Assemblathon 2: evaluating de novo methods of genome assembly in three\n  vertebrate species",
    "summary": "  Background - The process of generating raw genome sequence data continues to\nbecome cheaper, faster, and more accurate. However, assembly of such data into\nhigh-quality, finished genome sequences remains challenging. Many genome\nassembly tools are available, but they differ greatly in terms of their\nperformance (speed, scalability, hardware requirements, acceptance of newer\nread technologies) and in their final output (composition of assembled\nsequence). More importantly, it remains largely unclear how to best assess the\nquality of assembled genome sequences. The Assemblathon competitions are\nintended to assess current state-of-the-art methods in genome assembly. Results\n- In Assemblathon 2, we provided a variety of sequence data to be assembled for\nthree vertebrate species (a bird, a fish, and snake). This resulted in a total\nof 43 submitted assemblies from 21 participating teams. We evaluated these\nassemblies using a combination of optical map data, Fosmid sequences, and\nseveral statistical methods. From over 100 different metrics, we chose ten key\nmeasures by which to assess the overall quality of the assemblies. Conclusions\n- Many current genome assemblers produced useful assemblies, containing a\nsignificant representation of their genes, regulatory sequences, and overall\ngenome structure. However, the high degree of variability between the entries\nsuggests that there is still much room for improvement in the field of genome\nassembly and that approaches which work well in assembling the genome of one\nspecies may not necessarily work well for another.\n"
  },
  {
    "year": 2013,
    "title": "CG-content log-ratio distributions of Caenorhabditis elegans and\n  Drosophila melanogaster mirtrons",
    "summary": "  Mirtrons are a special type of pre-miRNA which originate from intronic\nregions and are spliced directly from the transcript instead of being processed\nby Drosha. The splicing mechanism is better understood for the processing of\nmRNA for which was established that there is a characteristic CG content around\nsplice sites. Here we analyse the CG-content ratio of pre-miRNAs and mirtrons\nand compare them with their genomic neighbourhood in an attempt to establish\nkey properties which are easy to evaluate and to understand their biogenesis.\nWe propose a simple log-ratio of the CG-content comparing the precursor\nsequence and is flanking region. We discovered that Caenorhabditis elegans and\nDrosophila melanogaster mirtrons, so far without exception, have smaller\nCG-content than their genomic neighbourhood. This is markedly different from\nusual pre-miRNAs which mostly have larger CG-content when compared to their\ngenomic neighbourhood. We also analysed some mammalian and primate mirtrons\nwhich, in contrast the invertebrate mirtrons, have higher CG-content ratio.\n"
  },
  {
    "year": 2013,
    "title": "Pattern Analysis of Tandem Repeats in Nlrp1",
    "summary": "  Pattern analysis of tandem repeats in gene is an indispensable computational\napproach to the understanding of the gene expression and pathogenesis of\ndiseases. This research applied a computational motif model and database\ntechniques to study the distribution of tandem repeats in Nlrp1 gene, which is\na critical gene to detect the invading pathogens in the immunologic mechanisms.\nThe frequency of tandem repeats in Nlrp1 gene was studied for mono-, di-, tri-,\nand tetranucleotides. Mutations of Nlrp1 gene were analyzed to identify the\ninsertion,deletion, and substitution of nucleotides. The results of this\nresearch provide a basis for future work in computational drug design and\nbiomedical engineering in tackling diseases associated with immune system.\n"
  },
  {
    "year": 2013,
    "title": "In Silico Analysis of Tandem Repeats in GIF of Gastric Parietal Cells",
    "summary": "  Tandem repeats are ubiquitous in the genome of organisms and their mutated\nforms play a vital role in pathogenesis. In this study, tandem repeats in\nGastric Intrinsic Factor (GIF) of gastric parietal cells have been investigated\nusing an in silico approach. Six types of the nucleotide tandem repeat motifs\nhave been investigated, including mono-, di-, tri-, tetra-, penta- and\nhexanucleotide. The distribution of the repeat motifs in the gene was analyzed.\nThe results of this study provide an insight into the biomolecular mechanisms\nand pathogenesis implicated by the GIF of gastric parietal cells. Based on the\nfindings of the tandem repeats in GIF of gastric parietal cells, therapeutic\nstrategies and disease markers may be developed accordingly by the biomedical\nscientists.\n"
  },
  {
    "year": 2013,
    "title": "Slow Evolution of rag1 and pomc Genes in Vertebrates with Large Genomes",
    "summary": "  Growing evidence suggests that many vertebrate lineages are evolving at\nsignificantly different rates. As a first approximation of evolutionary rates,\nwe assessed the amount of neutral (dS) and non-neutral (dN) substitutions that\nhave accumulated within and across sister clades since the time of their\ndivergence. We found that in fish, tetraodontiformes (pufferfish) are evolving\nat faster rates than cypriniformes (fresh water teleosts), while cypriniformes\nare evolving faster than elasmobranchs (sharks, skates and rays). A similar\nrate variation was observed in salamanders: plethodontidae were found to evolve\nat a rate nearly two fold faster than the hydromantes lineage. We discuss\npossible explanations for this striking variation in substitution rates among\ndifferent vertebrate lineages that occupy widely diverse habitats and niches.\n"
  },
  {
    "year": 2013,
    "title": "Count-based differential expression analysis of RNA sequencing data\n  using R and Bioconductor",
    "summary": "  RNA sequencing (RNA-seq) has been rapidly adopted for the profiling of\ntranscriptomes in many areas of biology, including studies into gene\nregulation, development and disease. Of particular interest is the discovery of\ndifferentially expressed genes across different conditions (e.g., tissues,\nperturbations), while optionally adjusting for other systematic factors that\naffect the data collection process. There are a number of subtle yet critical\naspects of these analyses, such as read counting, appropriate treatment of\nbiological variability, quality control checks and appropriate setup of\nstatistical modeling. Several variations have been presented in the literature,\nand there is a need for guidance on current best practices. This protocol\npresents a \"state-of-the-art\" computational and statistical RNA-seq\ndifferential expression analysis workflow largely based on the free open-source\nR language and Bioconductor software and in particular, two widely-used tools\nDESeq and edgeR. Hands-on time for typical small experiments (e.g., 4-10\nsamples) can be <1 hour, with computation time <1 day using a standard desktop\nPC.\n"
  },
  {
    "year": 2013,
    "title": "Virtual in situs: Sequencing mRNA from cryo-sliced Drosophila embryos to\n  determine genome-wide spatial patterns of gene expression",
    "summary": "  Complex spatial and temporal patterns of gene expression underlie embryo\ndifferentiation, yet methods do not yet exist for the efficient genome-wide\ndetermination of spatial expression patterns during development. In situ\nimaging of transcripts and proteins is the gold-standard, but it is difficult\nand time consuming to apply to an entire genome, even when highly automated.\nSequencing, in contrast, is fast and genome-wide, but is generally applied to\nhomogenized tissues, thereby discarding spatial information. It is likely that\nthese methods will ultimately converge, and we will be able to sequence RNAs in\nsitu, simultaneously determining their identity and location. As a step along\nthis path, we developed methods to cryosection individual blastoderm stage\nDrosophila melanogaster embryos along the anterior-posterior axis and sequence\nthe mRNA isolated from each 25 micron slice. The spatial patterns of gene\nexpression we infer closely match patterns previously determined by in situ\nhybridization and microscopy. We applied this method to generate a genome-wide\ntimecourse of spatial gene expression from shortly after fertilization through\ngastrulation. We identify numerous genes with spatial patterns that have not\nyet been described in the several ongoing systematic in situ based projects.\nThis simple experiment demonstrates the potential for combining careful\nanatomical dissection with high-throughput sequencing to obtain spatially\nresolved gene expression on a genome-wide scale.\n"
  },
  {
    "year": 2013,
    "title": "SOAP3-dp: Fast, Accurate and Sensitive GPU-based Short Read Aligner",
    "summary": "  To tackle the exponentially increasing throughput of Next-Generation\nSequencing (NGS), most of the existing short-read aligners can be configured to\nfavor speed in trade of accuracy and sensitivity. SOAP3-dp, through leveraging\nthe computational power of both CPU and GPU with optimized algorithms, delivers\nhigh speed and sensitivity simultaneously. Compared with widely adopted\naligners including BWA, Bowtie2, SeqAlto, GEM and GPU-based aligners including\nBarraCUDA and CUSHAW, SOAP3-dp is two to tens of times faster, while\nmaintaining the highest sensitivity and lowest false discovery rate (FDR) on\nIllumina reads with different lengths. Transcending its predecessor SOAP3,\nwhich does not allow gapped alignment, SOAP3-dp by default tolerates alignment\nsimilarity as low as 60 percent. Real data evaluation using human genome\ndemonstrates SOAP3-dp's power to enable more authentic variants and longer\nIndels to be discovered. Fosmid sequencing shows a 9.1 percent FDR on newly\ndiscovered deletions. SOAP3-dp natively supports BAM file format and provides a\nscoring scheme same as BWA, which enables it to be integrated into existing\nanalysis pipelines. SOAP3-dp has been deployed on Amazon-EC2, NIH-Biowulf and\nTianhe-1A.\n"
  },
  {
    "year": 2013,
    "title": "Correlation Between GC-content and Palindromes in Randomly Generated\n  Sequences and Viral Genomes",
    "summary": "  GC-content, the ratio of guanine and cytosine bases in an entire nucleotide\nsequence, and palindromic sequences are unique for every organism due to\ngenomic evolution. The goals of our research was to establish a correlation\nbetween GC-content and palindromic densities in wild-type viral and\nrandomly-generated genomes. Forty viral genomes were downloaded from GenBank\nand their GC-ratios and palindromic densities were calculated and plotted using\nMathematica. The palindromic densities-by-GC-ratios plot of randomly generated\nsequences (palindromic density curve) exhibited a quadratic relationship and\nwas superimposed over the viral genome plot. It was observed that the viral\nplots followed the curvature of the random sequences' quadratic curve,\nsignifying a directly proportional relationship between GC-content and\npalindrome density in viral genomes. However, because viral genomes require\ncertain non-palindromic sequences to function, the palindromic densities of\nmost wild-type genomes were under the palindromic density curve. The variance\nin palindrome densities of wild-type genomes in respect to the random\nsequences' quadratic curve may be examined to determine evolutionary traits in\ngenomes. A better understanding of viral palindromic densities and GC-ratios\nwould help in understanding conserved secondary RNA structures in viral genomes\nand future drug discovery. In addition, certain viral genomes were found to be\nviable recombinant viruses, which are used in gene therapy.\n"
  },
  {
    "year": 2013,
    "title": "Utilizing Protein Structure to Identify Non-Random Somatic Mutations",
    "summary": "  Motivation: Human cancer is caused by the accumulation of somatic mutations\nin tumor suppressors and oncogenes within the genome. In the case of oncogenes,\nrecent theory suggests that there are only a few key \"driver\" mutations\nresponsible for tumorigenesis. As there have been significant pharmacological\nsuccesses in developing drugs that treat cancers that carry these driver\nmutations, several methods that rely on mutational clustering have been\ndeveloped to identify them. However, these methods consider proteins as a\nsingle strand without taking their spatial structures into account. We propose\na new methodology that incorporates protein tertiary structure in order to\nincrease our power when identifying mutation clustering.\n  Results: We have developed a novel algorithm, iPAC: identification of Protein\nAmino acid Clustering, for the identification of non-random somatic mutations\nin proteins that takes into account the three dimensional protein structure. By\nusing the tertiary information, we are able to detect both novel clusters in\nproteins that are known to exhibit mutation clustering as well as identify\nclusters in proteins without evidence of clustering based on existing methods.\nFor example, by combining the data in the Protein Data Bank (PDB) and the\nCatalogue of Somatic Mutations in Cancer, our algorithm identifies new\nmutational clusters in well known cancer proteins such as KRAS and PI3KCa.\nFurther, by utilizing the tertiary structure, our algorithm also identifies\nclusters in EGFR, EIF2AK2, and other proteins that are not identified by\ncurrent methodology.\n"
  },
  {
    "year": 2013,
    "title": "Periodic correlation structures in bacterial and archaeal complete\n  genomes",
    "summary": "  The periodic transference of nucleotide strings in bacterial and archaeal\ncomplete genomes is investigated by using the metric representation and the\nrecurrence plot method. The generated periodic correlation structures exhibit\nfour kinds of fundamental transferring characteristics: a single increasing\nperiod, several increasing periods, an increasing quasi-period and almost\nnoincreasing period. The mechanism of the periodic transference is further\nanalyzed by determining all long periodic nucleotide strings in the bacterial\nand archaeal complete genomes and is explained as follows: both the repetition\nof basic periodic nucleotide strings and the transference of non-periodic\nnucleotide strings would form the periodic correlation structures with\napproximately the same increasing periods.\n"
  },
  {
    "year": 2013,
    "title": "Extensive divergence of transcription factor binding in Drosophila\n  embryos with highly conserved gene expression",
    "summary": "  Extensive divergence of transcription factor binding in Drosophila embryos\nwith highly conserved gene expression\n"
  },
  {
    "year": 2013,
    "title": "A Unifying Model of Genome Evolution Under Parsimony",
    "summary": "  We present a data structure called a history graph that offers a practical\nbasis for the analysis of genome evolution. It conceptually simplifies the\nstudy of parsimonious evolutionary histories by representing both substitutions\nand double cut and join (DCJ) rearrangements in the presence of duplications.\nThe problem of constructing parsimonious history graphs thus subsumes related\nmaximum parsimony problems in the fields of phylogenetic reconstruction and\ngenome rearrangement. We show that tractable functions can be used to define\nupper and lower bounds on the minimum number of substitutions and DCJ\nrearrangements needed to explain any history graph. These bounds become tight\nfor a special type of unambiguous history graph called an ancestral variation\ngraph (AVG), which constrains in its combinatorial structure the number of\noperations required. We finally demonstrate that for a given history graph $G$,\na finite set of AVGs describe all parsimonious interpretations of $G$, and this\nset can be explored with a few sampling moves.\n"
  },
  {
    "year": 2013,
    "title": "RNA-Seq Mapping Errors When Using Incomplete Reference Transcriptomes of\n  Vertebrates",
    "summary": "  Whole transcriptome sequencing is increasingly being used as a functional\ngenomics tool to study non- model organisms. However, when the reference\ntranscriptome used to calculate differential expression is incomplete,\nsignificant error in the inferred expression levels can result. In this study,\nwe use simulated reads generated from real transcriptomes to determine the\naccuracy of read mapping, and measure the error resulting from using an\nincomplete transcriptome. We show that the two primary sources of count- ing\nerror are 1) alternative splice variants that share reads and 2) missing\ntranscripts from the reference. Alternative splice variants increase the false\npositive rate of mapping while incomplete reference tran- scriptomes decrease\nthe true positive rate, leading to inaccurate transcript expression levels.\nGrouping transcripts by gene or read sharing (similar to mapping to a reference\ngenome) significantly decreases false positives, but only by improving the\nreference transcriptome itself can the missing transcript problem be addressed.\nWe also demonstrate that employing different mapping software does not yield\nsubstantial increases in accuracy on simulated data. Finally, we show that read\nlengths or insert sizes must increase past 1kb to resolve mapping ambiguity.\n"
  },
  {
    "year": 2013,
    "title": "CruzDB: software for annotation of genomic intervals with UCSC\n  genome-browser data",
    "summary": "  The biological significance of genomic features is often context-dependent.\nWe present CruzDB, a fast and intuitive programmatic interface to the UCSC\ngenome browser that facilitates integrative analyses of diverse local and\nremotely hosted datasets. We showcase the syntax of CruzDB using miRNA-binding\nsites as examples, and further demonstrate its utility with 3 novel biological\ndiscoveries. First, we find that while exons replicate early, introns tend to\nreplicate late, suggesting a complex replication pattern in gene regions.\nSecond, variants associated with cognitive functions map to lincRNA transcripts\nof relevant function. Third, lamina-associated domains are highly enriched in\nolfaction-related genes. CruzDB is available at\nhttps://github.com/brentp/cruzdb\n"
  },
  {
    "year": 2013,
    "title": "Sensitive Long-Indel-Aware Alignment of Sequencing Reads",
    "summary": "  The tremdendous advances in high-throughput sequencing technologies have made\npopulation-scale sequencing as performed in the 1000 Genomes project and the\nGenome of the Netherlands project possible. Next-generation sequencing has\nallowed genom-wide discovery of variations beyond single-nucleotide\npolymorphisms (SNPs), in particular of structural variations (SVs) like\ndeletions, insertions, duplications, translocations, inversions, and even more\ncomplex rearrangements. Here, we design a read aligner with special emphasis on\nthe following properties: (1) high sensitivity, i.e. find all (reasonable)\nalignments; (2) ability to find (long) indels; (3) statistically sound\nalignment scores; and (4) runtime fast enough to be applied to whole genome\ndata. We compare performance to BWA, bowtie2, stampy and find that our methods\nis especially advantageous on reads containing larger indels.\n"
  },
  {
    "year": 2013,
    "title": "Aligning sequence reads, clone sequences and assembly contigs with\n  BWA-MEM",
    "summary": "  Summary: BWA-MEM is a new alignment algorithm for aligning sequence reads or\nlong query sequences against a large reference genome such as human. It\nautomatically chooses between local and end-to-end alignments, supports\npaired-end reads and performs chimeric alignment. The algorithm is robust to\nsequencing errors and applicable to a wide range of sequence lengths from 70bp\nto a few megabases. For mapping 100bp sequences, BWA-MEM shows better\nperformance than several state-of-art read aligners to date.\n  Availability and implementation: BWA-MEM is implemented as a component of\nBWA, which is available at http://github.com/lh3/bwa.\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2013,
    "title": "Major changes in the core developmental pathways of nematodes:\n  Romanomermis culicivorax reveals the derived status of the Caenorhabditis\n  elegans model",
    "summary": "  Background Despite its status as a model organism, the development of\nCaenorhabditis elegans is not necessarily archetypical for nematodes. The\nphylum Nematoda is divided into the Chromadorea (indcludes C. elegans) and the\nEnoplea. Compared to C. elegans, enoplean nematodes have very different\npatterns of cell division and determination. Embryogenesis of the enoplean\nRomanomermis culicivorax has been studied in great detail, but the genetic\ncircuitry underpinning development in this species is unknown. Results We\ncreated a draft genome of R. culicivorax and compared its developmental gene\ncontent with that of two nematodes, C. elegans and Trichinella spiralis\n(another enoplean), and a representative arthropod Tribolium castaneum. This\ngenome evidence shows that R. culicivorax retains components of the conserved\nmetazoan developmental toolkit lost in C. elegans. T. spiralis has\nindependently lost even more of the toolkit than has C. elegans. However, the\nC. elegans toolkit is not simply depauperate, as many genes essential for\nembryogenesis in C. elegans are unique to this lineage, or have only extremely\ndivergent homologues in R. culicivorax and T. spiralis. These data imply\nfundamental differences in the genetic programmes for early cell specification,\ninductive interactions, vulva formation and sex determination. Conclusions Thus\nnematodes, despite their apparent phylum-wide morphological conservatism, have\nevolved major differences in the molecular logic of their development. R.\nculicivorax serves as a tractable, contrasting model to C. elegans for\nunderstanding how divergent genomic and thus regulatory backgrounds can\ngenerate a conserved phenotype. The availability of the draft genome will\npromote use of R. culicivorax as a research model.\n"
  },
  {
    "year": 2013,
    "title": "Lognormality and oscillations in the coverage of high-throughput\n  transcriptomic data towards gene ends",
    "summary": "  High-throughput transcriptomics experiments have reached the stage where the\ncount of the number of reads alignable to a given position can be treated as an\nalmost-continuous signal. This allows to ask questions of\nbiophysical/biotechnical nature, but which may still have biological\nimplications. Here we show that when sequencing RNA fragments from one end, as\nit is the case on most platforms, an oscillation in the read count is observed\nat the other end. We further show that these oscillations can be well described\nby Kolmogorov's 1941 broken stick model. We investigate how the model can be\nused to improve predictions of gene ends (3' transcript ends) but conclude that\nwith present data the improvement is only marginal. The results highlight\nsubtle effects in high-throughput transcriptomics experiments which do not have\na biological origin, but which may still be used to obtain biological\ninformation.\n"
  },
  {
    "year": 2013,
    "title": "Representing and decomposing genomic structural variants as balanced\n  integer flows on sequence graphs",
    "summary": "  The study of genomic variation has provided key insights into the functional\nrole of mutations. Predominantly, studies have focused on single nucleotide\nvariants (SNV), which are relatively easy to detect and can be described with\nrich mathematical models. However, it has been observed that genomes are highly\nplastic, and that whole regions can be moved, removed or duplicated in bulk.\nThese structural variants (SV) have been shown to have significant impact on\nthe phenotype, but their study has been held back by the combinatorial\ncomplexity of the underlying models. We describe here a general model of\nstructural variation that encompasses both balanced rearrangements and\narbitrary copy-numbers variants (CNV). In this model, we show that the space of\npossible evolutionary histories that explain the structural differences between\nany two genomes can be sampled ergodically.\n"
  },
  {
    "year": 2013,
    "title": "SICLE: A high-throughput tool for extracting evolutionary relationships\n  from phylogenetic trees",
    "summary": "  We present the phylogeny analysis software SICLE (Sister Clade Extractor), an\neasy-to-use, high- throughput tool to describe the nearest neighbors to a node\nof interest in a phylogenetic tree as well as the support value for the\nrelationship. The application is a command line utility that can be embedded\ninto a phylogenetic analysis pipeline or can be used as a subroutine within\nanother C++ program. As a test case, we applied this new tool to the published\nphylome of Salinibacter ruber, a species of halophilic Bacteriodetes,\nidentifying 13 unique sister relationships to S. ruber across the 4589 gene\nphylogenies. S. ruber grouped with bacteria, most often other Bacteriodetes, in\nthe majority of phylogenies, but 91 phylogenies showed a branch-supported\nsister association between S. ruber and Archaea, an evolutionarily intriguing\nrelationship indicative of horizontal gene transfer. This test case\ndemonstrates how SICLE makes it possible to summarize the phylogenetic\ninformation produced by automated phylogenetic pipelines to rapidly identify\nand quantify the possible evolutionary relationships that merit further\ninvestigation. SICLE is available for free for noncommercial use at\nhttp://eebweb.arizona.edu/sicle/.\n"
  },
  {
    "year": 2013,
    "title": "A Graph Theoretic Approach to Utilizing Protein Structure to Identify\n  Non-Random Somatic Mutations",
    "summary": "  Background: It is well known that the development of cancer is caused by the\naccumulation of somatic mutations within the genome. For oncogenes\nspecifically, current research suggests that there is a small set of \"driver\"\nmutations that are primarily responsible for tumorigenesis. Further, due to\nsome recent pharmacological successes in treating these driver mutations and\ntheir resulting tumors, a variety of methods have been developed to identify\npotential driver mutations using methods such as machine learning and\nmutational clustering. We propose a novel methodology that increases our power\nto identify mutational clusters by taking into account protein tertiary\nstructure via a graph theoretical approach.\n  Results: We have designed and implemented GraphPAC (Graph Protein Amino Acid\nClustering) to identify mutational clustering while considering protein spatial\nstructure. Using GraphPAC, we are able to detect novel clusters in proteins\nthat are known to exhibit mutation clustering as well as identify clusters in\nproteins without evidence of prior clustering based on current methods.\nSpecifically, by utilizing the spatial information available in the Protein\nData Bank (PDB) along with the mutational data in the Catalogue of Somatic\nMutations in Cancer (COSMIC), GraphPAC identifies new mutational clusters in\nwell known oncogenes such as EGFR and KRAS. Further, by utilizing graph theory\nto account for the tertiary structure, GraphPAC identifies clusters in DPP4,\nNRP1 and other proteins not identified by existing methods. The R package is\navailable at: http://bioconductor.org/packages/release/bioc/html/GraphPAC.html\n  Conclusion: GraphPAC provides an alternative to iPAC and an extension to\ncurrent methodology when identifying potential activating driver mutations by\nutilizing a graph theoretic approach when considering protein tertiary\nstructure.\n"
  },
  {
    "year": 2013,
    "title": "The Convergence of eQTL Mapping, Heritability Estimation and Polygenic\n  Modeling: Emerging Spectrum of Risk Variation in Bipolar Disorder",
    "summary": "  It is widely held that a substantial genetic component underlies Bipolar\nDisorder (BD) and other neuropsychiatric disease traits. Recent efforts have\nbeen aimed at understanding the genetic basis of disease susceptibility, with\ngenome-wide association studies (GWAS) unveiling some promising associations.\nNevertheless, the genetic etiology of BD remains elusive with a substantial\nproportion of the heritability - which has been estimated to be 80% based on\ntwin and family studies - unaccounted for by the specific genetic variants\nidentified by large-scale GWAS. Furthermore, functional understanding of\nassociated loci generally lags discovery. Studies we report here provide\nconsiderable support to the claim that substantially more remains to be gained\nfrom GWAS on the genetic mechanisms underlying BD susceptibility, and that a\nlarge proportion of the variation in disease risk may be uncovered through\nintegrative functional genomic approaches. We combine recent analytic advances\nin heritability estimation and polygenic modeling and leverage recent\ntechnological advances in the generation of -omics data to evaluate the nature\nand scale of the contribution of functional classes of genetic variation to a\nrelatively intractable disorder. We identified cis eQTLs in cerebellum and\nparietal cortex that capture more than half of the total heritability\nattributable to SNPs interrogated through GWAS and showed that eQTL-based\nheritability estimation is highly tissue-dependent. Our findings show that a\nmuch greater resolution may be attained than has been reported thus far on the\nnumber of common loci that capture a substantial proportion of the heritability\nto disease risk and that the functional nature of contributory loci may be\nclarified en masse.\n"
  },
  {
    "year": 2013,
    "title": "Improving transcriptome assembly through error correction of\n  high-throughput sequence reads",
    "summary": "  The study of functional genomics--particularly in non-model organisms has\nbeen dramatically improved over the last few years by use of transcriptomes and\nRNAseq. While these studies are potentially extremely powerful, a\ncomputationally intensive procedure--the de novo construction of a reference\ntranscriptome must be completed as a prerequisite to further analyses. The\naccurate reference is critically important as all downstream steps, including\nestimating transcript abundance are critically dependent on the construction of\nan accurate reference. Though a substantial amount of research has been done on\nassembly, only recently have the pre-assembly procedures been studied in\ndetail. Specifically, several stand-alone error correction modules have been\nreported on, and while they have shown to be effective in reducing errors at\nthe level of sequencing reads, how error correction impacts assembly accuracy\nis largely unknown. Here, we show via use of a simulated dataset, that applying\nerror correction to sequencing reads has significant positive effects on\nassembly accuracy, by reducing assembly error by nearly 50%, and therefore\nshould be applied to all datasets.\n"
  },
  {
    "year": 2013,
    "title": "Reducing assembly complexity of microbial genomes with single-molecule\n  sequencing",
    "summary": "  Background: The short reads output by first- and second-generation DNA\nsequencing instruments cannot completely reconstruct microbial chromosomes.\nTherefore, most genomes have been left unfinished due to the significant\nresources required to manually close gaps in draft assemblies.\nThird-generation, single-molecule sequencing addresses this problem by greatly\nincreasing sequencing read length, which simplifies the assembly problem.\n  Results: To measure the benefit of single-molecule sequencing on microbial\ngenome assembly, we sequenced and assembled the genomes of six bacteria and\nanalyzed the repeat complexity of 2,267 complete bacteria and archaea. Our\nresults indicate that the majority of known bacterial and archaeal genomes can\nbe assembled without gaps, at finished-grade quality, using a single PacBio RS\nsequencing library. These single-library assemblies are also more accurate than\ntypical short-read assemblies and hybrid assemblies of short and long reads.\n  Conclusions: Automated assembly of long, single-molecule sequencing data\nreduces the cost of microbial finishing to $1,000 for most genomes, and future\nadvances in this technology are expected to drive the cost lower. This is\nexpected to increase the number of completed genomes, improve the quality of\nmicrobial genome databases, and enable high-fidelity, population-scale studies\nof pan-genomes and chromosomal organization.\n"
  },
  {
    "year": 2013,
    "title": "High-speed and accurate color-space short-read alignment with CUSHAW2",
    "summary": "  Summary: We present an extension of CUSHAW2 for fast and accurate alignments\nof SOLiD color-space short-reads. Our extension introduces a double-seeding\napproach to improve mapping sensitivity, by combining maximal exact match seeds\nand variable-length seeds derived from local alignments. We have compared the\nperformance of CUSHAW2 to SHRiMP2 and BFAST by aligning both simulated and real\ncolor-space mate-paired reads to the human genome. The results show that\nCUSHAW2 achieves comparable or better alignment quality compared to SHRiMP2 and\nBFAST at an order-of-magnitude faster speed and significantly smaller peak\nresident memory size. Availability: CUSHAW2 and all simulated datasets are\navailable at http://cushaw2.sourceforge.net. Contact: liuy@uni-mainz.de;\nbertil.schmidt@uni-mainz.de\n"
  },
  {
    "year": 2013,
    "title": "GEMINI: integrative exploration of genetic variation and genome\n  annotations",
    "summary": "  Modern DNA sequencing technologies enable geneticists to rapidly identify\ngenetic variation among many human genomes. However, isolating the minority of\nvariants underlying disease remains an important, yet formidable challenge for\nmedical genetics. We have developed GEMINI (GEnome MINIng), a flexible software\npackage for exploring all forms of human genetic variation. Unlike existing\ntools, GEMINI integrates genetic variation with a diverse and flexible set of\ngenome annotations (e.g., dbSNP, ENCODE, UCSC, ClinVar, KEGG) into a unified\ndatabase to facilitate interpretation and data exploration. Whereas other\nmethods provide an inflexible set of variant filters or variant prioritization\nmethods, GEMINI allows researchers to compose complex queries based on sample\ngenotypes, inheritance patterns, and both pre-installed and custom genome\nannotations. GEMINI also provides methods for ad hoc queries and data\nexploration, a simple programming interface for custom analyses that leverage\nthe underlying database, and both command line and graphical tools for common\nanalyses. We demonstrate the utility of GEMINI for exploring variation in\npersonal genomes and family based genetic studies, and illustrate its ability\nto scale to studies involving thousands of human samples. GEMINI is designed\nfor reproducibility and flexibility and our goal is to will provide researchers\nwith a standard framework for medical genomics.\n"
  },
  {
    "year": 2013,
    "title": "Comparing DNA sequence collections by direct comparison of compressed\n  text indexes",
    "summary": "  Popular sequence alignment tools such as BWA convert a reference genome to an\nindexing data structure based on the Burrows-Wheeler Transform (BWT), from\nwhich matches to individual query sequences can be rapidly determined. However\nthe utility of also indexing the query sequences themselves remains relatively\nunexplored.\n  Here we show that an all-against-all comparison of two sequence collections\ncan be computed from the BWT of each collection with the BWTs held entirely in\nexternal memory, i.e. on disk and not in RAM. As an application of this\ntechnique, we show that BWTs of transcriptomic and genomic reads can be\ncompared to obtain reference-free predictions of splice junctions that have\nhigh overlap with results from more standard reference-based methods.\n  Code to construct and compare the BWT of large genomic data sets is available\nat http://beetl.github.com/BEETL/ as part of the BEETL library.\n"
  },
  {
    "year": 2013,
    "title": "Informed and Automated k-Mer Size Selection for Genome Assembly",
    "summary": "  Genome assembly tools based on the de Bruijn graph framework rely on a\nparameter k, which represents a trade-off between several competing effects\nthat are difficult to quantify. There is currently a lack of tools that would\nautomatically estimate the best k to use and/or quickly generate histograms of\nk-mer abundances that would allow the user to make an informed decision.\n  We develop a fast and accurate sampling method that constructs approximate\nabundance histograms with a several orders of magnitude performance improvement\nover traditional methods. We then present a fast heuristic that uses the\ngenerated abundance histograms for putative k values to estimate the best\npossible value of k. We test the effectiveness of our tool using diverse\nsequencing datasets and find that its choice of k leads to some of the best\nassemblies.\n  Our tool KmerGenie is freely available at: http://kmergenie.bx.psu.edu/\n"
  },
  {
    "year": 2013,
    "title": "Methods to study splicing from high-throughput RNA Sequencing data",
    "summary": "  The development of novel high-throughput sequencing (HTS) methods for RNA\n(RNA-Seq) has provided a very powerful mean to study splicing under multiple\nconditions at unprecedented depth. However, the complexity of the information\nto be analyzed has turned this into a challenging task. In the last few years,\na plethora of tools have been developed, allowing researchers to process\nRNA-Seq data to study the expression of isoforms and splicing events, and their\nrelative changes under different conditions. We provide an overview of the\nmethods available to study splicing from short RNA-Seq data. We group the\nmethods according to the different questions they address: 1) Assignment of the\nsequencing reads to their likely gene of origin. This is addressed by methods\nthat map reads to the genome and/or to the available gene annotations. 2)\nRecovering the sequence of splicing events and isoforms. This is addressed by\ntranscript reconstruction and de novo assembly methods. 3) Quantification of\nevents and isoforms. Either after reconstructing transcripts or using an\nannotation, many methods estimate the expression level or the relative usage of\nisoforms and/or events. 4) Providing an isoform or event view of differential\nsplicing or expression. These include methods that compare relative\nevent/isoform abundance or isoform expression across two or more conditions. 5)\nVisualizing splicing regulation. Various tools facilitate the visualization of\nthe RNA-Seq data in the context of alternative splicing. In this review, we do\nnot describe the specific mathematical models behind each method. Our aim is\nrather to provide an overview that could serve as an entry point for users who\nneed to decide on a suitable tool for a specific analysis. We also attempt to\npropose a classification of the tools according to the operations they do, to\nfacilitate the comparison and choice of methods.\n"
  },
  {
    "year": 2013,
    "title": "KRAS mutation testing in colorectal cancer as an example of the\n  pathologist's role in personalized targeted therapy: a practical approach",
    "summary": "  Identifying targets for personalized targeted therapy is the pathologist's\ndomain and a treasure. For decades, pathologists have had to learn, understand,\nadopt and implement many new laboratory techniques as they arrived on the\nscene. Pathologists successfully integrate the results of those tests into\nfinal pathology reports that were, and still are, the basis of clinical\ntherapeutic decisions. The molecular methods are different but no more\ndifficult to comprehend in the era of \"kit procedures\". Pathologists have the\nknowledge and expertise to identify particular gene mutations using the\nappropriate molecular tests currently available. This review focuses on the\nmost important recent developments in KRAS mutation testing in metastatic\ncolorectal cancer (CRC), and shows that a pathologist is involved in 10 stages\nof this procedure. Recent studies have shown that highly sensitive, simple,\nreliable and rapid assays may significantly improve the identification of CRC\npatients resistant to anti-EGFR therapy. Thus, direct sequencing does not seem\nto be an optimal procedure of KRAS testing for clinical purposes. Twelve\ncurrently available high-sensitivity diagnostic assays (with the CE-IVD mark)\nfor KRAS mutation testing are briefly described and compared. The suggested\npathology report content for somatic mutation tests is described. In\nconclusion, evidence is presented that sending away paraffin blocks with tumor\ntissue for KRAS mutation testing may not be in the best interest of patients.\nInstead, an evidence-based approach indicates that KRAS mutation testing should\nbe performed in pathology departments, only with the use of CE-IVD/FDA-approved\nKRAS tests, and with the obligatory, periodic participation in the KRAS EQA\nscheme organized by the European Society of Pathology as an independent\ninternational body.\n"
  },
  {
    "year": 2013,
    "title": "Role of HIV RNA structure in recombination and speciation: romping in\n  purine A, keeps HTLV away",
    "summary": "  Extreme enrichment of the human immunodeficiency virus (HIV-1) RNA genome for\nthe purine A parallels the mild purine-loading of the RNAs of most organisms.\nThis should militate against loop-loop \"kissing\" interactions between the\nstructured viral genome and structured host RNAs, which can generate segments\nof double-stranded RNA sufficient to trigger intracellular alarms. However,\nhuman T cell leukaemia virus (HTLV-1), with the potential to invade the same\nhost cell, shows extreme enrichment for the pyrimidine C. Assuming the low GC%\nHIV and the high GC% HTLV-1 to share a common ancestor, it was postulated that\ndifferences in GC% arose to prevent homologous recombination between these\nemerging lentiviral species. Sympatrically isolated by this intracellular\nreproductive barrier, prototypic HIV-1 seized the AU-rich (low GC%) high ground\n(thus committing to purine A rather than purine G). Prototypic HTLV-1 forwent\nthis advantage and evolved an independent evolutionary strategy. Evidence\nsupporting this hypothesis since its elaboration in the 1990s is growing. The\nconflict between the needs to encode accurately both a protein, and nucleic\nacid structure, is often resolved in favour of the nucleic acid because, apart\nfrom regulatory roles, structure is critical for recombination. However, above\na sequence difference threshold, structure (and hence recombination) is\nimpaired. New species can then arise.\n"
  },
  {
    "year": 2013,
    "title": "Bayesian Test for Colocalisation Between Pairs of Genetic Association\n  Studies Using Summary Statistics",
    "summary": "  Genetic association studies, in particular the genome-wide association study\ndesign, have provided a wealth of novel insights into the aetiology of a wide\nrange of human diseases and traits. The next challenge consists of\nunderstanding the molecular basis of these associations. The integration of\nmultiple association datasets, including gene expression datasets, can\ncontribute to this goal. We have developed a novel statistical methodology to\nassess whether two association signals are consistent with a shared causal\nvariant. An application is the integration of disease scans with expression\nquantitative trait locus (eQTL) studies, but any pair of GWAS datasets can be\nintegrated in this framework. We demonstrate the value of the approach by\nreanalysing a gene expression dataset in 966 liver samples with a published\nmeta-analysis of lipid traits including >100, 000 individuals of European\nancestry. Combining all lipid biomarkers, our reanalysis supported 29 out of 38\nreported colocalisation results with eQTLs and identified 14 new colocalisation\nresults, highlighting the value of a formal statistical test. In two cases of\nreported eQTL-lipid pairs (IFT172, TBKBP1) for which our analysis suggests that\nthe eQTL pattern is not consistent with the lipid association, we identify\nalternative colocalisation results with GCKR and KPNB1, indicating that these\ngenes are more likely to be causal in these genomic intervals. A key feature of\nthe method is the ability to derive the output statistics from single SNP\nsummary statistics, hence making it possible to perform systematic\nmeta-analysis type comparisons across multiple GWAS datasets\n(http://coloc.cs.ucl.ac.uk/coloc/). Our methodology provides information about\ncandidate causal genes in associated intervals and has direct implications for\nthe understanding of complex diseases and the design of drugs to target disease\npathways.\n"
  },
  {
    "year": 2013,
    "title": "The rise and fall of the Phytophthora infestans lineage that triggered\n  the Irish potato famine",
    "summary": "  Phytophthora infestans, the cause of potato late blight, is infamous for\nhaving triggered the Irish Great Famine in the 1840s. Until the late 1970s, P.\ninfestans diversity outside of its Mexican center of origin was low, and one\nscenario held that a single strain, US-1, had dominated the global population\nfor 150 years; this was later challenged based on DNA analysis of historical\nherbarium specimens. We have compared the genomes of 11 herbarium and 15 modern\nstrains. We conclude that the nineteenth century epidemic was caused by a\nunique genotype, HERB-1, that persisted for over 50 years. HERB-1 is distinct\nfrom all examined modern strains, but it is a close relative of US-1, which\nreplaced it outside of Mexico in the twentieth century. We propose that HERB-1\nand US-1 emerged from a metapopulation that was established in the early 1800s\noutside of the species' center of diversity.\n"
  },
  {
    "year": 2013,
    "title": "Augmenting transcriptome assembly combinatorially",
    "summary": "  RNA-seq allows detection and precise quantification of transcripts, provides\ncomprehensive understanding of exon/intron boundaries, aids discovery of\nalternatively spliced isoforms and fusion transcripts along with measurement of\nallele-specific expression. Researchers interested in studying and constructing\ntranscriptomes, especially for non-model species, often face the conundrum of\nchoosing from a number of available de novo and genome-guided assemblers. A\ncomprehensive comparative study is required to assess and evaluate their\nefficiency and sensitivity for transcript assembly, reconstruction and\nrecovery. None of the popular assembly tools in use today achieves requisite\nsensitivity, specificity or recovery of full-length transcripts on its own.\nHence, it is imperative that methods be developed in order to augment\nassemblies generated from multiple tools, with minimal compounding of error.\nHere, we present an approach to combinatorially augment transciptome assembly\nbased on a rigorous comparative study of popular de novo and genome-guided\ntranscriptome assembly tools.\n"
  },
  {
    "year": 2013,
    "title": "Using R and Bioconductor for proteomics data analysis",
    "summary": "  This review presents how R, the popular statistical environment and\nprogramming language, can be used in the frame of proteomics data analysis. A\nshort introduction to R is given, with special emphasis on some of the features\nthat make R and its add-on packages a premium software for sound and\nreproducible data analysis. The reader is also advised on how to find relevant\nR software for proteomics. Several use cases are then presented, illustrating\ndata input/output, quality control, quantitative proteomics and data analysis.\nDetailed code and additional links to extensive documentation are available in\nthe freely available companion package RforProteomics.\n"
  },
  {
    "year": 2013,
    "title": "SOAPdenovo-Trans: De novo transcriptome assembly with short RNA-Seq\n  reads",
    "summary": "  Motivation: Transcriptome sequencing has long been the favored method for\nquickly and inexpensively obtaining the sequences for a large number of genes\nfrom an organism with no reference genome. With the rapidly increasing\nthroughputs and decreasing costs of next generation sequencing, RNA-Seq has\ngained in popularity; but given the typically short reads (e.g. 2 x 90 bp\npaired ends) of this technol- ogy, de novo assembly to recover complete or\nfull-length transcript sequences remains an algorithmic challenge. Results: We\npresent SOAPdenovo-Trans, a de novo transcriptome assembler designed\nspecifically for RNA-Seq. Its performance was evaluated on transcriptome\ndatasets from rice and mouse. Using the known transcripts from these\nwell-annotated genomes (sequenced a decade ago) as our benchmark, we assessed\nhow SOAPdenovo- Trans and two other popular software handle the practical\nissues of alternative splicing and variable expression levels. Our conclusion\nis that SOAPdenovo-Trans provides higher contiguity, lower redundancy, and\nfaster execution. Availability and Implementation: Source code and user manual\nare at http://sourceforge.net/projects/soapdenovotrans/ Contact:\nxieyl@genomics.cn or bgi-soap@googlegroups.com\n"
  },
  {
    "year": 2013,
    "title": "Genome Sequencing Highlights Genes Under Selection and the Dynamic Early\n  History of Dogs",
    "summary": "  To identify genetic changes underlying dog domestication and reconstruct\ntheir early evolutionary history, we analyzed novel high-quality genome\nsequences of three gray wolves, one from each of three putative centers of dog\ndomestication, two ancient dog lineages (Basenji and Dingo) and a golden jackal\nas an outgroup. We find dogs and wolves diverged through a dynamic process\ninvolving population bottlenecks in both lineages and post-divergence gene\nflow, which confounds previous inferences of dog origins. In dogs, the\ndomestication bottleneck was severe involving a 17 to 49-fold reduction in\npopulation size, a much stronger bottleneck than estimated previously from less\nintensive sequencing efforts. A sharp bottleneck in wolves occurred soon after\ntheir divergence from dogs, implying that the pool of diversity from which dogs\narose was far larger than represented by modern wolf populations. Conditional\non mutation rate, we narrow the plausible range for the date of initial dog\ndomestication to an interval from 11 to 16 thousand years ago. This period\npredates the rise of agriculture, implying that the earliest dogs arose\nalongside hunter-gathers rather than agriculturists. Regarding the geographic\norigin of dogs, we find that surprisingly, none of the extant wolf lineages\nfrom putative domestication centers are more closely related to dogs, and the\nsampled wolves instead form a sister monophyletic clade. This result, in\ncombination with our finding of dog-wolf admixture during the process of\ndomestication, suggests a re-evaluation of past hypotheses of dog origin is\nnecessary. Finally, we also detect signatures of selection, including evidence\nfor selection on genes implicated in morphology, metabolism, and neural\ndevelopment. Uniquely, we find support for selective sweeps at regulatory sites\nsuggesting gene regulatory changes played a critical role in dog domestication.\n"
  },
  {
    "year": 2013,
    "title": "biobambam: tools for read pair collation based algorithms on BAM files",
    "summary": "  Sequence alignment data is often ordered by coordinate (id of the reference\nsequence plus position on the sequence where the fragment was mapped) when\nstored in BAM files, as this simplifies the extraction of variants between the\nmapped data and the reference or of variants within the mapped data. In this\norder paired reads are usually separated in the file, which complicates some\nother applications like duplicate marking or conversion to the FastQ format\nwhich require to access the full information of the pairs. In this paper we\nintroduce biobambam, an API for efficient BAM file reading supporting the\nefficient collation of alignments by read name without performing a complete\nresorting of the input file and some tools based on this API performing tasks\nlike marking duplicate reads and conversion to the FastQ format. In comparison\nwith previous approaches to problems involving the collation of alignments by\nread name like the BAM to FastQ or duplication marking utilities in the Picard\nsuite the approach of biobambam can often perform an equivalent task more\nefficiently in terms of the required main memory and run-time.\n"
  },
  {
    "year": 2013,
    "title": "Low-bandwidth and non-compute intensive remote identification of\n  microbes from raw sequencing reads",
    "summary": "  Cheap high-throughput DNA sequencing may soon become routine not only for\nhuman genomes but also for practically anything requiring the identification of\nliving organisms from their DNA: tracking of infectious agents, control of food\nproducts, bioreactors, or environmental samples.\n  We propose a novel general approach to the analysis of sequencing data in\nwhich the reference genome does not have to be specified. Using a distributed\narchitecture we are able to query a remote server for hints about what the\nreference might be, transferring a relatively small amount of data, and the\nhints can be used for more computationally-demanding work.\n  Our system consists of a server with known reference DNA indexed, and a\nclient with raw sequencing reads. The client sends a sample of unidentified\nreads, and in return receives a list of matching references known to the\nserver. Sequences for the references can be retrieved and used for exhaustive\ncomputation on the reads, such as alignment.\n  To demonstrate this approach we have implemented a web server, indexing tens\nof thousands of publicly available genomes and genomic regions from various\norganisms and returning lists of matching hits from query sequencing reads. We\nhave also implemented two clients, one of them running in a web browser, in\norder to demonstrate that gigabytes of raw sequencing reads of unknown origin\ncould be identified without the need to transfer a very large volume of data,\nand on modestly powered computing devices.\n  A web access is available at http://tapir.cbs.dtu.dk. The source code for a\npython command-line client, a server, and supplementary data is available at\nhttp://bit.ly/1aURxkc.\n"
  },
  {
    "year": 2013,
    "title": "Modeling the dynamics of bivalent histone modifications",
    "summary": "  Epigenetic modifications to histones may promote either activation or\nrepression of the transcription of nearby genes. Recent experimental studies\nshow that the promoters of many lineage-control genes in stem cells have\n\"bivalent domains\" in which the nucleosomes contain both active (H3K4me3) and\nrepressive (H3K27me3) marks. It is generally agreed that bivalent domains play\nan important role in stem cell differentiation, but the underlying mechanisms\nremain unclear. Here we formulate a mathematical model to investigate the\ndynamic properties of histone modification patterns. We then illustrate that\nour modeling framework can be used to capture key features of experimentally\nobserved combinatorial chromatin states.\n"
  },
  {
    "year": 2013,
    "title": "mendelFix: a Perl script for checking Mendelian errors in high density\n  SNP data of trio designs",
    "summary": "  Here we present mendelFix, a Perl script for checking Mendelian errors in\ngenome-wide SNP data of trio designs. The program takes 12-recoded PLINK PED\nand MAP files as input to calculate a series of summary statistics for\nMendelian errors, sets missing offspring genotypes that present Mendelian\ninconsistencies, and implements a simplistic procedure to infer missing\ngenotypes using parent information. The program can be easily incorporated in\nany pipeline for family-based SNP data analysis, and is distributed as free\nsoftware under the GNU General Public License.\n"
  },
  {
    "year": 2013,
    "title": "Sashimi plots: Quantitative visualization of RNA sequencing read\n  alignments",
    "summary": "  We introduce Sashimi plots, a quantitative multi-sample visualization of mRNA\nsequencing reads aligned to gene annotations. Sashimi plots are made using\nalignments (stored in the SAM/BAM format) and gene model annotations (in GFF\nformat), which can be custom-made by the user or obtained from databases such\nas Ensembl or UCSC. We describe two implementations of Sashimi plots: (1) a\nstand-alone command line implementation aimed at making customizable\npublication quality figures, and (2) an implementation built into the\nIntegrated Genome Viewer (IGV) browser, which enables rapid and dynamic\ncreation of Sashimi plots for any genomic region of interest, suitable for\nexploratory analysis of alternatively spliced regions of the transcriptome.\nIsoform expression estimates outputted by the MISO program can be optionally\nplotted along with Sashimi plots. Sashimi plots can be used to quickly screen\ndifferentially spliced exons along genomic regions of interest and can be used\nin publication quality figures. The Sashimi plot software and documentation is\navailable from: http://genes.mit.edu/burgelab/miso/docs/sashimi.html\n"
  },
  {
    "year": 2013,
    "title": "Bound to succeed: Transcription factor binding site prediction and its\n  contribution to understanding virulence and environmental adaptation in\n  bacterial plant pathogens",
    "summary": "  Bacterial plant pathogens rely on a battalion of transcription factors to\nfine-tune their response to changing environmental conditions and marshal the\ngenetic resources required for successful pathogenesis. Prediction of\ntranscription factor binding sites represents an important tool for elucidating\nregulatory networks, and has been conducted in multiple genera of plant\npathogenic bacteria for the purpose of better understanding mechanisms of\nsurvival and pathogenesis. The major categories of transcription factor binding\nsites that have been characterized are reviewed here with emphasis on in silico\nmethods used for site identification and challenges therein, their\napplicability to different types of sequence datasets, and insights into\nmechanisms of virulence and survival that have been gained through binding site\nmapping. An improved strategy for establishing E value cutoffs when using\nexisting models to screen uncharacterized genomes is also discussed.\n"
  },
  {
    "year": 2013,
    "title": "Centromere reference models for human chromosomes X and Y satellite\n  arrays",
    "summary": "  The human genome remains incomplete, with multi-megabase sized gaps\nrepresenting the endogenous centromeres and other heterochromatic regions.\nThese regions are commonly enriched with long arrays of near-identical tandem\nrepeats, known as satellite DNAs, that offer a limited number of variant sites\nto differentiate individual repeat copies across millions of bases. This\nsubstantial sequence homogeneity challenges available assembly strategies, and\nas a result, centromeric regions are omitted from ongoing genomic studies. To\naddress this problem, we present a locally ordered assembly across two haploid\nhuman satellite arrays on chromosomes X and Y, resulting in an initial linear\nrepresentation of 3.83 Mb of centromeric DNA within an individual genome. To\nfurther expand the utility of each centromeric reference sequence, we evaluate\nsites within the arrays for short-read mappability and chromosome specificity.\nAs satellite DNAs evolve in a concerted manner, we use these centromeric\nassemblies to assess the extent of sequence variation among 372 individuals\nfrom distinct human populations. In doing so, we identify two ancient satellite\narray variants in both X and Y centromeres as determined by array length and\nsequence composition. This study provides an initial linear representation and\ncomprehensive sequence characterization of a regional centromere and\nestablishes a foundation to extend genomic characterization to these sites as\nwell as to other repeat-rich regions within complex genomes.\n"
  },
  {
    "year": 2013,
    "title": "A hierarchical network heuristic for solving the orientation problem in\n  genome assembly",
    "summary": "  In the past several years, the problem of genome assembly has received\nconsiderable attention from both biologists and computer scientists. An\nimportant component of current assembly methods is the scaffolding process.\nThis process involves building ordered and oriented linear collections of\ncontigs (continuous overlapping sequence reads) called scaffolds and relies on\nthe use of mate pair data. A mate pair is a set of two reads that are sequenced\nfrom the ends of a single fragment of DNA, and therefore have opposite mutual\norientations. When two reads of a mate-pair are placed into two different\ncontigs, one can infer the mutual orientation of these contigs. While several\norientation algorithms exist as part of assembly programs, all encounter\nchallenges while solving the orientation problem due to errors from\nmis-assemblies in contigs or errors in read placements. In this paper we\npresent an algorithm based on hierarchical clustering that independently solves\nthe orientation problem and is robust to errors. We show that our algorithm can\ncorrectly solve the orientation problem for both faux (generated) assembly data\nand real assembly data for {\\em R. sphaeroides bacteria}. We demonstrate that\nour algorithm is stable to both changes in the initial orientations as well as\nnoise in the data, making it advantageous compared to traditional approaches.\n"
  },
  {
    "year": 2013,
    "title": "Systematic identification of gene families for use as markers for\n  phylogenetic and phylogeny- driven ecological studies of bacteria and archaea\n  and their major subgroups",
    "summary": "  With the astonishing rate that the genomic and metagenomic sequence data sets\nare accumulating, there are many reasons to constrain the data analyses. One\napproach to such constrained analyses is to focus on select subsets of gene\nfamilies that are particularly well suited for the tasks at hand. Such gene\nfamilies have generally been referred to as marker genes. We are particularly\ninterested in identifying and using such marker genes for phylogenetic and\nphylogeny-driven ecological studies of microbes and their communities. We\ntherefore refer to these as PhyEco (for phylogenetic and phylogenetic ecology)\nmarkers. The dual use of these PhyEco markers means that we needed to develop\nand apply a set of somewhat novel criteria for identification of the best\ncandidates for such markers. The criteria we focused on included universality\nacross the taxa of interest, ability to be used to produce robust phylogenetic\ntrees that reflect as much as possible the evolution of the species from which\nthe genes come, and low variation in copy number across taxa. We describe here\nan automated protocol for identifying potential PhyEco markers from a set of\ncomplete genome sequences. The protocol combines rapid searching, clustering\nand phylogenetic tree building algorithms to generate protein families that\nmeet the criteria listed above. We report here the identification of PhyEco\nmarkers for different taxonomic levels including 40 for all bacteria and\narchaea, 114 for all bacteria, and much more for some of the individual phyla\nof bacteria. This new list of PhyEco markers should allow much more detailed\nautomated phylogenetic and phylogenetic ecology analyses of these groups than\npossible previously.\n"
  },
  {
    "year": 2013,
    "title": "Cloudbreak: Accurate and Scalable Genomic Structural Variation Detection\n  in the Cloud with MapReduce",
    "summary": "  The detection of genomic structural variations (SV) remains a difficult\nchallenge in analyzing sequencing data, and the growing size and number of\nsequenced genomes have rendered SV detection a bona fide big data problem.\nMapReduce is a proven, scalable solution for distributed computing on huge data\nsets. We describe a conceptual framework for SV detection algorithms in\nMapReduce based on computing local genomic features, and use it to develop a\ndeletion and insertion detection algorithm, Cloudbreak. On simulated and real\ndata sets, Cloudbreak achieves accuracy improvements over popular SV detection\nalgorithms, and genotypes variants from diploid samples. It provides\ndramatically shorter runtimes and the ability to scale to big data volumes on\nlarge compute clusters. Cloudbreak includes tools to set up and configure\nMapReduce (Hadoop) clusters on cloud services, enabling on-demand cluster\ncomputing. Our implementation and source code are available at\nhttp://github.com/cwhelan/cloudbreak .\n"
  },
  {
    "year": 2013,
    "title": "QuorUM: an error corrector for Illumina reads",
    "summary": "  Motivation: Illumina Sequencing data can provide high coverage of a genome by\nrelatively short (100 bp150 bp) reads at a low cost. Our goal is to produce\ntrimmed and error-corrected reads to improve genome assemblies. Our error\ncorrection procedure aims at producing a set of error-corrected reads (1)\nminimizing the number of distinct false k-mers, i.e. that are not present in\nthe genome, in the set of reads and (2) maximizing the number that are true,\ni.e. that are present in the genome. Because coverage of a genome by Illumina\nreads varies greatly from point to point, we cannot simply eliminate k-mers\nthat occur rarely.\n  Results: Our software, called QuorUM, provides reasonably accurate correction\nand is suitable for large data sets (1 billion bases checked and corrected per\nday per core).\n  Availability: QuorUM is distributed as an independent software package and as\na module of the MaSuRCA assembly software. Both are available under the GPL\nopen source license at http://www.genome.umd.edu.\n  Contact: gmarcais@umd.edu\n"
  },
  {
    "year": 2013,
    "title": "kruX: Matrix-based non-parametric eQTL discovery",
    "summary": "  The Kruskal-Wallis test is a popular non-parametric statistical test for\nidentifying expression quantitative trait loci (eQTLs) from genome-wide data\ndue to its robustness against variations in the underlying genetic model and\nexpression trait distribution, but testing billions of marker-trait\ncombinations one-by-one can become computationally prohibitive. We developed\nkruX, an algorithm implemented in Matlab, Python and R that uses matrix\nmultiplications to simultaneously calculate the Kruskal-Wallis test statistic\nfor several millions of marker-trait combinations at once. KruX is more than\nten thousand times faster than computing associations one-by-one on a typical\nhuman dataset. We used kruX and a dataset of more than 500k SNPs and 20k\nexpression traits measured in 102 human blood samples to compare eQTLs detected\nby the Kruskal-Wallis test to eQTLs detected by the parametric ANOVA and linear\nmodel methods. We found that the Kruskal-Wallis test is more robust against\ndata outliers and heterogeneous genotype group sizes and detects a higher\nproportion of non-linear associations, but is more conservative for calling\nadditive linear associations. In summary, kruX enables the use of robust\nnon-parametric methods for massive eQTL mapping without the need for a\nhigh-performance computing infrastructure.\n"
  },
  {
    "year": 2013,
    "title": "Synteny in Bacterial Genomes: Inference, Organization and Evolution",
    "summary": "  Genes are not located randomly along genomes. Synteny, the conservation of\ntheir relative positions in genomes of different species, reflects fundamental\nconstraints on natural evolution. We present approaches to infer pairs of\nco-localized genes from multiple genomes, describe their organization, and\nstudy their evolutionary history. In bacterial genomes, we thus identify\nsynteny units, or \"syntons\", which are clusters of proximal genes that\nencompass and extend operons. The size distribution of these syntons divide\nthem into large syntons, which correspond to fundamental macro-molecular\ncomplexes of bacteria, and smaller ones, which display a remarkable exponential\ndistribution of sizes. This distribution is \"universal\" in two respects: it\nholds for vastly different genomes, and for functionally distinct genes.\nSimilar statistical laws have been reported previously in studies of bacterial\ngenomes, and generally attributed to purifying selection or neutral processes.\nHere, we perform a new analysis based on the concept of parsimony, and find\nthat the prevailing evolutionary mechanism behind the formation of small\nsyntons is a selective process of gene aggregation. Altogether, our results\nimply a common evolutionary process that selectively shapes the organization\nand diversity of bacterial genomes.\n"
  },
  {
    "year": 2013,
    "title": "Integrating sequencing datasets to form highly confident SNP and indel\n  genotype calls for a whole human genome",
    "summary": "  Clinical adoption of human genome sequencing requires methods with known\naccuracy of genotype calls at millions or billions of positions across a\ngenome. Previous work showing discordance amongst sequencing methods and\nalgorithms has made clear the need for a highly accurate set of genotypes\nacross a whole genome that could be used as a benchmark. We present methods to\nmake highly confident SNP, indel, and homozygous reference genotype calls for\nNA12878, the pilot genome for the Genome in a Bottle Consortium. We minimize\nbias towards any method by integrating and arbitrating between 14 datasets from\n5 sequencing technologies, 7 mappers, and 3 variant callers. Regions for which\nno confident genotype call could be made are identified as uncertain, and\nclassified into different reasons for uncertainty. Our highly confident\ngenotype calls are publicly available on the Genome Comparison and Analytic\nTesting (GCAT) website to enable real-time benchmarking of any method.\n"
  },
  {
    "year": 2013,
    "title": "Agalma: an automated phylogenomics workflow",
    "summary": "  In the past decade, transcriptome data have become an important component of\nmany phylogenetic studies. Phylogenetic studies now regularly include genes\nfrom newly sequenced transcriptomes, as well as publicly available\ntranscriptomes and genomes. Implementing such a phylogenomic study, however, is\ncomputationally intensive, requires the coordinated use of many complex\nsoftware tools, and includes multiple steps for which no published tools exist.\nPhylogenomic studies have therefore been manual or semiautomated. In addition\nto taking considerable user time, this makes phylogenomic analyses difficult to\nreproduce, compare, and extend. In addition, methodological improvements made\nin the context of one study often cannot be easily applied and evaluated in the\ncontext of other studies. We present Agalma, an automated tool that conducts\nphylogenomic analyses. The user provides raw Illumina transcriptome data, and\nAgalma produces annotated assemblies, aligned gene sequence matrices, a\npreliminary phylogeny, and detailed diagnostics that allow the investigator to\nmake extensive assessments of intermediate analysis steps and the final\nresults. Sequences from other sources, such as externally assembled genomes and\ntranscriptomes, can also be incorporated in the analyses. Agalma tracks\nprovenance, profiles processor and memory use, records diagnostics, manages\nmetadata, and enables rich HTML reports for all stages of the analysis. Agalma\nincludes a test data set and a built-in test analysis of these data. In\naddition to describing Agalma, we here present a sample analysis of a larger\nseven-taxon data set. Agalma is available for download at\nhttps://bitbucket.org/caseywdunn/agalma. Agalma allows complex phylogenomic\nanalyses to be implemented and described unambiguously as a series of\nhigh-level commands. This will enable phylogenomic studies to be readily\nreproduced, modified, and extended.\n"
  },
  {
    "year": 2013,
    "title": "Genetics of single-cell protein abundance variation in large yeast\n  populations",
    "summary": "  Many DNA sequence variants influence phenotypes by altering gene expression.\nOur understanding of these variants is limited by sample sizes of current\nstudies and by measurements of mRNA rather than protein abundance. We developed\na powerful method for identifying genetic loci that influence protein\nexpression in very large populations of the yeast Saccharomyes cerevisiae. The\nmethod measures single-cell protein abundance through the use of\ngreen-fluorescent-protein tags. We applied this method to 160 genes and\ndetected many more loci per gene than previous studies. We also observed closer\ncorrespondence between loci that influence protein abundance and loci that\ninfluence mRNA abundance of a given gene. Most loci cluster at hotspot\nlocations that influence multiple proteins - in some cases, more than half of\nthose examined. The variants that underlie these hotspots have profound effects\non the gene regulatory network and provide insights into genetic variation in\ncell physiology between yeast strains.\n"
  },
  {
    "year": 2013,
    "title": "The genome of the medieval Black Death agent (extended abstract)",
    "summary": "  The genome of a 650 year old Yersinia pestis bacteria, responsible for the\nmedieval Black Death, was recently sequenced and assembled into 2,105 contigs\nfrom the main chromosome. According to the point mutation record, the medieval\nbacteria could be an ancestor of most Yersinia pestis extant species, which\nopens the way to reconstructing the organization of these contigs using a\ncomparative approach. We show that recent computational paleogenomics methods,\naiming at reconstructing the organization of ancestral genomes from the\ncomparison of extant genomes, can be used to correct, order and complete the\ncontig set of the Black Death agent genome, providing a full chromosome\nsequence, at the nucleotide scale, of this ancient bacteria. This sequence\nsuggests that a burst of mobile elements insertions predated the Black Death,\nleading to an exceptional genome plasticity and increase in rearrangement rate.\n"
  },
  {
    "year": 2013,
    "title": "Exploring Genome Characteristics and Sequence Quality Without a\n  Reference",
    "summary": "  The de novo assembly of large, complex genomes is a significant challenge\nwith currently available DNA sequencing technology. While many de novo assembly\nsoftware packages are available, comparatively little attention has been paid\nto assisting the user with the assembly. This paper addresses the practical\naspects of de novo assembly by introducing new ways to perform quality\nassessment on a collection of DNA sequence reads. The software implementation\ncalculates per-base error rates, paired-end fragment size histograms and\ncoverage metrics in the absence of a reference genome. Additionally, the\nsoftware will estimate characteristics of the sequenced genome, such as repeat\ncontent and heterozygosity, that are key determinants of assembly difficulty.\nThe software described is freely available and open source under the GNU Public\nLicense.\n"
  },
  {
    "year": 2013,
    "title": "Late-replicating CNVs as a source of new genes",
    "summary": "  Asynchronous replication of the genome has been associated with different\nrates of point mutation and copy number variation (CNV) in human populations.\nHere, we explored if the bias in the generation of CNV that is associated to\nDNA replication timing might have conditioned the birth of new protein-coding\ngenes during evolution. We show that genes that were duplicated during primate\nevolution are more commonly found among the human genes located in\nlate-replicating CNV regions. We traced the relationship between replication\ntiming and the evolutionary age of duplicated genes. Strikingly, we found that\nthere is a significant enrichment of evolutionary younger duplicates in late\nreplicating regions of the human and mouse genome. Indeed, the presence of\nduplicates in late replicating regions gradually decreases as the evolutionary\ntime since duplication extends. Our results suggest that the accumulation of\nrecent duplications in late replicating CNV regions is an active process\ninfluencing genome evolution.\n"
  },
  {
    "year": 2013,
    "title": "SlopMap: a software application tool for quick and flexible\n  identification of similar sequences using exact k-mer matching",
    "summary": "  With the advent of Next-Generation (NG) sequencing, it has become possible to\nsequence an entire genome quickly and inexpensively. However, in some\nexperiments one only needs to extract and assembly a portion of the sequence\nreads, for example when performing transcriptome studies, sequencing\nmitochondrial genomes, or characterizing exomes. With the raw DNA-library of a\ncomplete genome it would appear to be a trivial problem to identify reads of\ninterest. But it is not always easy to incorporate well-known tools such as\nBLAST, BLAT, Bowtie, and SOAP directly into a bioinformatics pipelines before\nthe assembly stage, either due to in- compatibility with the assembler's file\ninputs, or because it is desirable to incorporate information that must be\nextracted separately. For example, in order to incorporate flowgrams from a\nRoche 454 sequencer into the Newbler assembler it is necessary to first extract\nthem from the original SFF files. We present SlopMap, a bioinformatics software\nutility which allows rapid identification similar to provided target sequences\nfrom either Roche 454 or Illumnia DNA library. With a simple and intuitive\ncommand- line interface along with file output formats compatible with assembly\nprograms, SlopMap can be directly embedded in biological data processing\npipeline without any additional programming work. In addition, SlopMap\npreserves flowgram information needed for Roche 454 assembler.\n"
  },
  {
    "year": 2013,
    "title": "Bayesian genome assembly and assessment by Markov Chain Monte Carlo\n  sampling",
    "summary": "  Most genome assemblers construct point estimates, choosing a genome sequence\nfrom among many alternative hypotheses that are supported by the data. We\npresent a Markov Chain Monte Carlo approach to sequence assembly that instead\ngenerates distributions of assembly hypotheses with posterior probabilities,\nproviding an explicit statistical framework for evaluating alternative\nhypotheses and assessing assembly uncertainty. We implement this approach in a\nprototype assembler and illustrate its application to the bacteriophage\nPhiX174.\n"
  },
  {
    "year": 2013,
    "title": "MicroRNAs Implicated in Dysregulation of Gene Expression Following Human\n  Lung Transplantation",
    "summary": "  Lung transplantation remains the only viable treatment option for the\nmajority of patients with advanced lung diseases. However, 5-year\npost-transplant survival rates remain low primarily secondary to chronic\nrejection. Novel insights from global gene expression profiles may provide\nmolecular phenotypes and therapeutic targets to improve outcomes after lung\ntransplantation. We showed the presence of a significant number of dysregulated\ngenes, particularly those genes involved in pathways and biological processes\nsuch as immune response and defense, in the PBMCs derived from a cohort of\npatients after lung transplantation. The contribution of miRNAs in regulating\nthese differential genes was also demonstrated.\n"
  },
  {
    "year": 2013,
    "title": "Estimation of genomic characteristics by analyzing k-mer frequency in de\n  novo genome projects",
    "summary": "  Background: With the fast development of next generation sequencing\ntechnologies, increasing numbers of genomes are being de novo sequenced and\nassembled. However, most are in fragmental and incomplete draft status, and\nthus it is often difficult to know the accurate genome size and repeat content.\nFurthermore, many genomes are highly repetitive or heterozygous, posing\nproblems to current assemblers utilizing short reads. Therefore, it is\nnecessary to develop efficient assembly-independent methods for accurate\nestimation of these genomic characteristics. Results: Here we present a\nframework for modeling the distribution of k-mer frequency from sequencing data\nand estimating the genomic characteristics such as genome size, repeat\nstructure and heterozygous rate. By introducing novel techniques of k-mer\nindividuals, float precision estimation, and proper treatment of sequencing\nerror and coverage bias, the estimation accuracy of our method is significantly\nimproved over existing methods. We also studied how the various genomic and\nsequencing characteristics affect the estimation accuracy using simulated\nsequencing data, and discussed the limitations on applying our method to real\nsequencing data. Conclusion: Based on this research, we show that the k-mer\nfrequency analysis can be used as a general and assembly-independent method for\nestimating genomic characteristics, which can improve our understanding of a\nspecies genome, help design the sequencing strategy of genome projects, and\nguide the development of assembly algorithms. The programs developed in this\nresearch are written using C/C++, and freely accessible at Github URL\n(https://github.com/fanagislab/GCE) or BGI ftp (\nftp://ftp.genomics.org.cn/pub/gce).\n"
  },
  {
    "year": 2013,
    "title": "Predicting genome-wide DNA methylation using methylation marks, genomic\n  position, and DNA regulatory elements",
    "summary": "  Background: Recent assays for individual-specific genome-wide DNA methylation\nprofiles have enabled epigenome-wide association studies to identify specific\nCpG sites associated with a phenotype. Computational prediction of CpG\nsite-specific methylation levels is important, but current approaches tackle\naverage methylation within a genomic locus and are often limited to specific\ngenomic regions. Results: We characterize genome-wide DNA methylation patterns,\nand show that correlation among CpG sites decays rapidly, making predictions\nsolely based on neighboring sites challenging. We built a random forest\nclassifier to predict CpG site methylation levels using as features neighboring\nCpG site methylation levels and genomic distance, and co-localization with\ncoding regions, CGIs, and regulatory elements from the ENCODE project, among\nothers. Our approach achieves 91% -- 94% prediction accuracy of genome-wide\nmethylation levels at single CpG site precision. The accuracy increases to 98%\nwhen restricted to CpG sites within CGIs. Our classifier outperforms\nstate-of-the-art methylation classifiers and identifies features that\ncontribute to prediction accuracy: neighboring CpG site methylation status, CpG\nisland status, co-localized DNase I hypersensitive sites, and specific\ntranscription factor binding sites were found to be most predictive of\nmethylation levels. Conclusions: Our observations of DNA methylation patterns\nled us to develop a classifier to predict site-specific methylation levels that\nachieves the best DNA methylation predictive accuracy to date. Furthermore, our\nmethod identified genomic features that interact with DNA methylation,\nelucidating mechanisms involved in DNA methylation modification and regulation,\nand linking different epigenetic processes.\n"
  },
  {
    "year": 2013,
    "title": "Realistic simulations reveal extensive sample-specificity of RNA-seq\n  biases",
    "summary": "  In line with the importance of RNA-seq, the bioinformatics community has\nproduced numerous data analysis tools incorporating methods to correct\nsample-specific biases. However, few advanced simulation tools exist to enable\nbenchmarking of competing correction methods. We introduce the first framework\nto reproduce the properties of individual RNA-seq runs and, by applying it on\nseveral datasets, we demonstrate the importance of accounting for\nsample-specificity in realistic simulations.\n"
  },
  {
    "year": 2013,
    "title": "Friendship and Natural Selection",
    "summary": "  More than any other species, humans form social ties to individuals who are\nneither kin nor mates, and these ties tend to be with similar people. Here, we\nshow that this similarity extends to genotypes. Across the whole genome,\nfriends' genotypes at the SNP level tend to be positively correlated\n(homophilic); however, certain genotypes are negatively correlated\n(heterophilic). A focused gene set analysis suggests that some of the overall\ncorrelation can be explained by specific systems; for example, an olfactory\ngene set is homophilic and an immune system gene set is heterophilic. Finally,\nhomophilic genotypes exhibit significantly higher measures of positive\nselection, suggesting that, on average, they may yield a synergistic fitness\nadvantage that has been helping to drive recent human evolution.\n"
  },
  {
    "year": 2013,
    "title": "Diminishing Return for Increased Mappability with Longer Sequencing\n  Reads: Implications of the k-mer Distributions in the Human Genome",
    "summary": "  The amount of non-unique sequence (non-singletons) in a genome directly\naffects the difficulty of read alignment to a reference assembly for high\nthroughput-sequencing data. Although a greater length increases the chance for\nreads being uniquely mapped to the reference genome, a quantitative analysis of\nthe influence of read lengths on mappability has been lacking. To address this\nquestion, we evaluate the k-mer distribution of the human reference genome. The\nk-mer frequency is determined for k ranging from 20 to 1000 basepairs. We use\nthe proportion of non-singleton k-mers to evaluate the mappability of reads for\na corresponding read length. We observe that the proportion of non-singletons\ndecreases slowly with increasing k, and can be fitted by piecewise power-law\nfunctions with different exponents at different k ranges. A faster decay at\nsmaller values for k indicates more limited gains for read lengths > 200\nbasepairs. The frequency distributions of k-mers exhibit long tails in a\npower-law-like trend, and rank frequency plots exhibit a concave Zipf's curve.\nThe location of the most frequent 1000-mers comprises 172 kilobase-ranged\nregions, including four large stretches on chromosomes 1 and X, containing\ngenes with biomedical implications. Even the read length 1000 would be\ninsufficient to reliably sequence these specific regions.\n"
  },
  {
    "year": 2013,
    "title": "TreeOTU: Operational Taxonomic Unit Classification Based on Phylogenetic\n  Trees",
    "summary": "  Our current understanding of the taxonomic and phylogenetic diversity of\ncellular organisms, especially the bacteria and archaea, is mostly based upon\nstudies of sequences of the small- subunit rRNAs (ssu-rRNAs). To address the\nlimitation of ssu-rRNA as a phylogenetic marker, such as copy number variation\namong organisms and complications introduced by horizontal gene transfer,\nconvergent evolution, or evolution rate variations, we have identified protein-\ncoding gene families as alternative Phylogenetic and Phylogenetic Ecology\nmarkers (PhyEco). Current nucleotide sequence similarity based Operational\nTaxonomic Unit (OTU) classification methods are not readily applicable to amino\nacid sequences of PhyEco markers. We report here the development of TreeOTU, a\nphylogenetic tree structure based OTU classification method that takes into\naccount of differences in rates of evolution between taxa and between genes.\nOTU sets built by TreeOTU are more faithful to phylogenetic tree structures\nthan sequence clustering (non phylogenetic) methods for ssu-rRNAs. OTUs built\nfrom phylogenetic trees of protein coding PhyEco markers are comparable to our\ncurrent taxonomic classification at different levels. With the included OTU\ncomparing tools, the TreeOTU is robust in phylogenetic referencing with\ndifferent phylogenetic markers and trees.\n"
  },
  {
    "year": 2013,
    "title": "Baa.pl: A tool to evaluate de novo genome assemblies with RNA\n  transcripts",
    "summary": "  Assessing the correctness of genome assemblies is an important step in any\ngenome project. Several methods exist, but most are computationally intensive\nand, in some cases, inappropriate. Here I present baa.pl, a fast and\neasy-to-use program that uses transcript data to evaluate genomic assemblies.\nThrough simulations using human chromosome 22, I show that baa.pl excels at\ndetecting levels of missing sequence and contiguity. The program is freely\navailable at: https://github.com/josephryan/baa.pl\n"
  },
  {
    "year": 2013,
    "title": "Two-dimensional SDS-PAGE fractionation of biological samples for\n  biomarker discovery",
    "summary": "  Two-dimensional electrophoresis is still a very valuable tool in proteomics,\ndue to its reproducibility and its ability to analyze complete proteins.\nHowever, due to its sensitivity to dynamic range issues, its most suitable use\nin the frame of biomarker discovery is not on very complex fluids such as\nplasma, but rather on more proximal, simpler fluids such as CSF, urine, or\nsecretome samples. Here, we describe the complete workflow for the analysis of\nsuch dilute samples by two-dimensional electrophoresis, starting from sample\nconcentration, then the two-dimensional electrophoresis step per se, ending\nwith the protein detection by fluorescence.\n"
  },
  {
    "year": 2013,
    "title": "These are not the k-mers you are looking for: efficient online k-mer\n  counting using a probabilistic data structure",
    "summary": "  K-mer abundance analysis is widely used for many purposes in nucleotide\nsequence analysis, including data preprocessing for de novo assembly, repeat\ndetection, and sequencing coverage estimation. We present the khmer software\npackage for fast and memory efficient online counting of k-mers in sequencing\ndata sets. Unlike previous methods based on data structures such as hash\ntables, suffix arrays, and trie structures, khmer relies entirely on a simple\nprobabilistic data structure, a Count-Min Sketch. The Count-Min Sketch permits\nonline updating and retrieval of k-mer counts in memory which is necessary to\nsupport online k-mer analysis algorithms. On sparse data sets this data\nstructure is considerably more memory efficient than any exact data structure.\nIn exchange, the use of a Count-Min Sketch introduces a systematic overcount\nfor k-mers; moreover, only the counts, and not the k-mers, are stored. Here we\nanalyze the speed, the memory usage, and the miscount rate of khmer for\ngenerating k-mer frequency distributions and retrieving k-mer counts for\nindividual k-mers. We also compare the performance of khmer to several other\nk-mer counting packages, including Tallymer, Jellyfish, BFCounter, DSK, KMC,\nTurtle and KAnalyze. Finally, we examine the effectiveness of profiling\nsequencing error, k-mer abundance trimming, and digital normalization of reads\nin the context of high khmer false positive rates. khmer is implemented in C++\nwrapped in a Python interface, offers a tested and robust API, and is freely\navailable under the BSD license at github.com/ged-lab/khmer.\n"
  },
  {
    "year": 2013,
    "title": "Characterizing the infection-induced transcriptome of Nasonia\n  vitripennis reveals a preponderance of taxonomically-restricted immune genes",
    "summary": "  The innate immune system in insects consists of a conserved core signaling\nnetwork and rapidly diversifying effector and recognition components, often\ncontaining a high proportion of taxonomically-restricted genes. In the absence\nof functional annotation, genes encoding immune system proteins can thus be\ndifficult to identify, as homology-based approaches generally cannot detect\nlineage-specific genes. Here, we use RNA-seq to compare the uninfected and\ninfection-induced transcriptome in the parasitoid wasp Nasonia vitripennis to\nidentify genes regulated by infection. We identify 183 genes significantly\nup-regulated by infection and 61 genes significantly down-regulated by\ninfection. We also produce a new homology-based immune catalog in N.\nvitripennis, and show that most infection-induced genes are not assigned an\nimmune function from homology alone, suggesting the potential for substantial\nnovel immune components in less-well-studied systems. Finally, we show that a\nhigh proportion of these novel induced genes are taxonomically-restricted,\nhighlighting the rapid evolution of immune gene content. The combination of\nfunctional annotation using RNA-seq and homology-based annotation provides a\nrobust method to characterize the innate immune response across a wide variety\nof insects, and reveals significant novel features of the Nasonia immune\nresponse.\n"
  },
  {
    "year": 2013,
    "title": "Integrating diverse datasets improves developmental enhancer prediction",
    "summary": "  Gene-regulatory enhancers have been identified by many lines of evidence,\nincluding evolutionary conservation, regulatory protein binding, chromatin\nmodifications, and DNA sequence motifs. To integrate these different\napproaches, we developed EnhancerFinder, a novel method for predicting\ndevelopmental enhancers and their tissue specificity. EnhancerFinder uses a\ntwo-step multiple-kernel learning approach to integrate DNA sequence motifs,\nevolutionary patterns, and thousands of diverse functional genomics datasets\nfrom a variety of cell types and developmental stages. We trained\nEnhancerFinder on hundreds of experimentally verified human developmental\nenhancers from the VISTA Enhancer Browser, in contrast to histone mark or\nsequence-based enhancer definitions commonly used. We comprehensively evaluated\nEnhancerFinder, and found that our integrative approach improves enhancer\nprediction accuracy over previous approaches that consider a single type of\ndata. Our evaluation highlights the importance of considering information from\nmany tissues when predicting specific types of enhancers. We find that VISTA\nenhancers active in embryonic heart are easier to predict than enhancers active\nin several other tissues due to their uniquely high GC content. We applied\nEnhancerFinder to the entire human genome and predicted 84,301 developmental\nenhancers and their tissue specificity. These predictions provide specific\nfunctional annotations for large amounts of human non-coding DNA, and are\nsignificantly enriched near genes with annotated roles in their predicted\ntissues and hits from genome-wide association studies. We demonstrate the\nutility of our enhancer predictions by identifying and validating a novel\ncranial nerve enhancer in the ZEB2 locus. Our genome-wide developmental\nenhancer predictions will be freely available as a UCSC Genome Browser track.\n"
  },
  {
    "year": 2013,
    "title": "Joint assembly and genetic mapping of the Atlantic horseshoe crab genome\n  reveals ancient whole genome duplication",
    "summary": "  Horseshoe crabs are marine arthropods with a fossil record extending back\napproximately 450 million years. They exhibit remarkable morphological\nstability over their long evolutionary history, retaining a number of ancestral\narthropod traits, and are often cited as examples of \"living fossils.\" As\narthropods, they belong to the Ecdysozoa}, an ancient super-phylum whose\nsequenced genomes (including insects and nematodes) have thus far shown more\ndivergence from the ancestral pattern of eumetazoan genome organization than\ncnidarians, deuterostomes, and lophotrochozoans. However, much of ecdysozoan\ndiversity remains unrepresented in comparative genomic analyses. Here we use a\nnew strategy of combined de novo assembly and genetic mapping to examine the\nchromosome-scale genome organization of the Atlantic horseshoe crab Limulus\npolyphemus. We constructed a genetic linkage map of this 2.7 Gbp genome by\nsequencing the nuclear DNA of 34 wild-collected, full-sibling embryos and their\nparents at a mean redundancy of 1.1x per sample. The map includes 84,307\nsequence markers and 5,775 candidate conserved protein coding genes. Comparison\nto other metazoan genomes shows that the L. polyphemus genome preserves\nancestral bilaterian linkage groups, and that a common ancestor of modern\nhorseshoe crabs underwent one or more ancient whole genome duplications (WGDs)\n~ 300 MYA, followed by extensive chromosome fusion.\n"
  },
  {
    "year": 2013,
    "title": "Theoretical Bounds on Mate-Pair Information for Accurate Genome Assembly",
    "summary": "  Over the past two decades, a series of works have aimed at studying the\nproblem of genome assembly: the process of reconstructing a genome from\nsequence reads. An early formulation of the genome assembly problem showed that\ngenome reconstruction is NP-hard when framed as finding the shortest sequence\nthat contains all observed reads. Although this original formulation is very\nsimplistic and does not allow for mate-pair information, subsequent\nformulations have also proven to be NP-hard, and/or may not be guaranteed to\nreturn a correct assembly.\n  In this paper, we provide an alternate perspective on the genome assembly\nproblem by showing genome assembly is easy when provided with sufficient\nmate-pair information. Moreover, we quantify the number of mate-pair libraries\nnecessary and sufficient for accurate genome assembly, in terms of the length\nof the longest repetitive region within a genome. In our analysis, we consider\nan idealized sequencing model where each mate-pair library generates pairs of\nerror free reads with a fixed and known insert size at each position in the\ngenome.\n  Even in this idealized model, we show that accurate genome reconstruction\ncannot be guaranteed in the worst case unless at least roughly R/2L mate-pair\nlibraries are produced, where R is the length of the longest repetitive region\nin the genome and L is the length of each read. On the other hand, if (R/L)+1\nmate-pair libraries are provided, then a simple algorithm can be used to find a\ncorrect genome assembly easily in polynomial time. Although (R/L)+1 mate-pair\nlibraries can be too much to produce in practice, the previous bounds only hold\nin the worst case. In our last result, we show that if additional conditions\nhold on a genome, a correct assembly can be guaranteed with only O(log (R/L))\nmate-pair libraries.\n"
  },
  {
    "year": 2013,
    "title": "IQRray, a new method for Affymetrix microarray quality control, and the\n  homologous organ conservation score, a new benchmark method for quality\n  control metrics",
    "summary": "  Motivation: Microarray results accumulated in public repositories are widely\nre-used in meta-analytical studies and secondary databases. The quality of the\ndata obtained with this technology varies from experiment to experiment and\nefficient method for quality assessment is neces-sary to ensure their\nreliability. Results: The lack of a good benchmark has hampered evaluation of\nexisting methods for quality control. In this study we propose a new\ninde-pendent quality metric that is based on evolutionary conservation of\nexpression profiles. We show, using 11 large organ-specific datasets, that\nIQRray, a new quality metrics developed by us, exhibits the highest correlation\nwith this reference metric, among 14 metrics tested. IQRray outperforms other\nmethods in identification of poor quality arrays in dataset composed of arrays\nfrom many independent experiments. In con-trast, the performance of methods\ndesigned for detecting outliers in a single experiment like NUSE and RLE was\nlow because of the inability of these method to detect datasets containing only\nlow quality arrays, and the fact that the scores cannot be directly compared\nbetween ex-periments. Availability: The R implementation of IQRray is available\nat: ftp://lausanne.isb-sib.ch/pub/databases/Bgee/general/IQRray.R\n"
  },
  {
    "year": 2013,
    "title": "Identification of cromosomal translocation hotspots via scan statistics",
    "summary": "  The detection of genomic regions unusually rich in a given pattern is an\nimportant undertaking in the analysis of next generation sequencing data.\nRecent studies of chromosomal translocations in activated B lymphocytes have\nidentified regions that are frequently translocated to c-myc oncogene. A\nquantitative method for the identification of translocation hotspots was\ncrucial to this study. Here we improve this analysis by using a simple\nprobabilistic model and the framework provided by scan statistics to define the\nnumber and location of translocation breakpoint hotspots. A key feature of our\nmethod is that it provides a global chromosome-wide significance level to\nclustering, as opposed to previous methods based on local criteria. Whilst\nbeing motivated by a specific application, the detection of unusual clusters is\na widespread problem in bioinformatics. We expect our method to be useful in\nthe analysis of data from other experimental approaches such as of ChIP-seq and\n4C-seq.\n"
  },
  {
    "year": 2013,
    "title": "The Functional Consequences of Variation in Transcription Factor Binding",
    "summary": "  One goal of human genetics is to understand how the information for precise\nand dynamic gene expression programs is encoded in the genome. The interactions\nof transcription factors (TFs) with DNA regulatory elements clearly play an\nimportant role in determining gene expression outputs, yet the regulatory logic\nunderlying functional transcription factor binding is poorly understood. Many\nstudies have focused on characterizing the genomic locations of TF binding, yet\nit is unclear to what extent TF binding at any specific locus has functional\nconsequences with respect to gene expression output. To evaluate the context of\nfunctional TF binding we knocked down 59 TFs and chromatin modifiers in one\nHapMap lymphoblastoid cell line. We then identified genes whose expression was\naffected by the knockdowns. We intersected the gene expression data with\ntranscription factor binding data (based on ChIP-seq and DNase-seq) within 10\nkb of the transcription start sites of expressed genes. This combination of\ndata allowed us to infer functional TF binding. On average, 14.7% of genes\nbound by a factor were differentially expressed following the knockdown of that\nfactor, suggesting that most interactions between TF and chromatin do not\nresult in measurable changes in gene expression levels of putative target\ngenes. We found that functional TF binding is enriched in regulatory elements\nthat harbor a large number of TF binding sites, at sites with predicted higher\nbinding affinity, and at sites that are enriched in genomic regions annotated\nas active enhancers.\n"
  },
  {
    "year": 2013,
    "title": "Ensemble Analysis of Adaptive Compressed Genome Sequencing Strategies",
    "summary": "  Acquiring genomes at single-cell resolution has many applications such as in\nthe study of microbiota. However, deep sequencing and assembly of all of\nmillions of cells in a sample is prohibitively costly. A property that can come\nto rescue is that deep sequencing of every cell should not be necessary to\ncapture all distinct genomes, as the majority of cells are biological\nreplicates. Biologically important samples are often sparse in that sense. In\nthis paper, we propose an adaptive compressed method, also known as distilled\nsensing, to capture all distinct genomes in a sparse microbial community with\nreduced sequencing effort. As opposed to group testing in which the number of\ndistinct events is often constant and sparsity is equivalent to rarity of an\nevent, sparsity in our case means scarcity of distinct events in comparison to\nthe data size. Previously, we introduced the problem and proposed a distilled\nsensing solution based on the breadth first search strategy. We simulated the\nwhole process which constrained our ability to study the behavior of the\nalgorithm for the entire ensemble due to its computational intensity. In this\npaper, we modify our previous breadth first search strategy and introduce the\ndepth first search strategy. Instead of simulating the entire process, which is\nintractable for a large number of experiments, we provide a dynamic programming\nalgorithm to analyze the behavior of the method for the entire ensemble. The\nensemble analysis algorithm recursively calculates the probability of capturing\nevery distinct genome and also the expected total sequenced nucleotides for a\ngiven population profile. Our results suggest that the expected total sequenced\nnucleotides grows proportional to $\\log$ of the number of cells and\nproportional linearly with the number of distinct genomes.\n"
  },
  {
    "year": 2013,
    "title": "A Spatial Simulation Approach to Account for Protein Structure When\n  Identifying Non-Random Somatic Mutations",
    "summary": "  Background: Current research suggests that a small set of \"driver\" mutations\nare responsible for tumorigenesis while a larger body of \"passenger\" mutations\noccurs in the tumor but does not progress the disease. Due to recent\npharmacological successes in treating cancers caused by driver mutations, a\nvariety of of methodologies that attempt to identify such mutations have been\ndeveloped. Based on the hypothesis that driver mutations tend to cluster in key\nregions of the protein, the development of cluster identification algorithms\nhas become critical.\n  Results: We have developed a novel methodology, SpacePAC (Spatial Protein\nAmino acid Clustering), that identifies mutational clustering by considering\nthe protein tertiary structure directly in 3D space. By combining the\nmutational data in the Catalogue of Somatic Mutations in Cancer (COSMIC) and\nthe spatial information in the Protein Data Bank (PDB), SpacePAC is able to\nidentify novel mutation clusters in many proteins such as FGFR3 and CHRM2. In\naddition, SpacePAC is better able to localize the most significant mutational\nhotspots as demonstrated in the cases of BRAF and ALK. The R package is\navailable on Bioconductor at:\nhttp://www.bioconductor.org/packages/release/bioc/html/SpacePAC.html\n  Conclusion: SpacePAC adds a valuable tool to the identification of mutational\nclusters while considering protein tertiary structure\n"
  },
  {
    "year": 2013,
    "title": "A feasible roadmap to identifying significant intercellular genomic\n  heterogeneity in deep sequencing data",
    "summary": "  Intercellular heterogeneity serves as both a confounding factor in studying\nindividual clones and an information source in characterizing any heterogeneous\ntissues, such as blood, tumor systems. Due to inevitable sequencing errors and\nother sample preparation artifacts such as PCR errors, systematic efforts to\ncharacterize intercellular genomic heterogeneity must effectively distinguish\ngenuine clonal sequences from fake derivatives. We developed a novel approach\n(SIGH) for identifying significant genuine clonal sequences directly from mixed\nsequencing reads that can improve genomic analyses in many biological contexts.\nThis method offers several attractive features: (1) it automatically estimates\nthe error rate from raw sequence reads and identifies genuine clonal sequences;\n(2) it is robust to the large variety of error rate due to the various\nexperimental conditions; (3) it is supported by a well grounded statistical\nframework that exploits probabilistic characteristics of sequencing errors; (4)\nits unbiased strategy allows detecting rare clone(s) despite that clone\nrelative abundance; and (5) it estimates constituent proportions in each\nsample. Extensive realistic simulation studies show that our method can\nreliably estimate the error rates and faithfully distinguish the genuine clones\nfrom fake derivatives, paving the way for follow up analysis that is otherwise\nruined by the often dominant fake clones.\n"
  },
  {
    "year": 2013,
    "title": "Can we predict the mutation rate at the single nucleotide scale in the\n  human genome?",
    "summary": "  It has been recently claimed that it is possible to predict the rate of de\nnovo mutation of each site in the human genome with almost perfect accuracy\n(Michaelson et al. (2012) Cell, 151, 1431-1442). We show that this claim is\nunwarranted. By considering the correlation between the rate of de novo\nmutation and the predictions from the model of Michaelson et al., we show that\nthere could be substantial unexplained variance in the mutation rate. We also\ndemonstrate that the model of Michaelson et al. fails to capture a major\ncomponent of the variation in the mutation rate, that which is local but not\nassociated with simple context.\n"
  },
  {
    "year": 2013,
    "title": "BACOM 2.0 facilitates absolute normalization and quantification of\n  somatic copy number alterations in heterogeneous tumor",
    "summary": "  BACOM is a statistically principled and unsupervised method that detects copy\nnumber deletion types (homozygous versus heterozygous), estimates normal cell\nfraction, and recovers cancer specific copy number profiles, using allele\nspecific copy number signals. In a subsequent analysis of TCGA ovarian cancer\ndataset, the average normal cell fraction estimated by BACOM was found higher\nthan expected. In this letter, we first discuss the advantages of the BACOM in\nrelation to alternative approaches. Then, we show that this elevated estimate\nof normal cell fraction is the combined result of inaccurate signal modeling\nand normalization. Lastly, we describe an allele specific signal modeling and\nnormalization scheme that can enhance BACOM applications in many biological\ncontexts. An open source MATLAB program was developed to implement our extended\nmethod and it is publically available.\n"
  },
  {
    "year": 2013,
    "title": "Molecular responses of mouse macrophages to copper and copper oxide\n  nanoparticles inferred from proteomic analyses",
    "summary": "  The molecular responses of macrophages to copper-based nanoparticles have\nbeen investigated via a combination of proteomic and biochemical approaches,\nusing the RAW264.7 cell line as a model. Both metallic copper and copper oxide\nnanoparticles have been tested, with copper ion and zirconium oxide\nnanoparticles used as controls. Proteomic analysis highlighted changes in\nproteins implicated in oxidative stress responses (superoxide dismutases and\nperoxiredoxins), glutathione biosynthesis, the actomyosin cytoskeleton, and\nmitochondrial proteins (especially oxidative phosphorylation complex subunits).\nValidation studies employing functional analyses showed that the increases in\nglutathione biosynthesis and in mitochondrial complexes observed in the\nproteomic screen were critical to cell survival upon stress with copper-based\nnanoparticles; pharmacological inhibition of these two pathways enhanced cell\nvulnerability to copper-based nanoparticles, but not to copper ions.\nFurthermore, functional analyses using primary macrophages derived from bone\nmarrow showed a decrease in reduced glutathione levels, a decrease in the\nmitochondrial transmembrane potential, and inhibition of phagocytosis and of\nlipopolysaccharide-induced nitric oxide production. However, only a fraction of\nthese effects could be obtained with copper ions. In conclusion, this study\nshowed that macrophage functions are significantly altered by copper-based\nnanoparticles. Also highlighted are the cellular pathways modulated by cells\nfor survival and the exemplified cross-toxicities that can occur between\ncopper-based nanoparticles and pharmacological agents.\n"
  },
  {
    "year": 2013,
    "title": "Comparative Assembly Hubs: Web Accessible Browsers for Comparative\n  Genomics",
    "summary": "  We introduce a pipeline to easily generate collections of web accessible UCSC\ngenome browsers interrelated by an alignment. Using the alignment, all\nannotations and the alignment itself can be efficiently viewed with reference\nto any genome in the collection, symmetrically. A new, intelligently scaled\nalignment display makes it simple to view all changes between the genomes at\nall levels of resolution, from substitutions to complex structural\nrearrangements, including duplications.\n"
  },
  {
    "year": 2013,
    "title": "Sequencing and characterisation of rearrangements in three S.\n  pastorianus strains reveals the presence of chimeric genes and gives evidence\n  of breakpoint reuse",
    "summary": "  Gross chromosomal rearrangements have the potential to be evolutionarily\nadvantageous to an adapting organism. The generation of a hybrid species\nincreases opportunity for recombination by bringing together two homologous\ngenomes. We sought to define the location of genomic rearrangements in three\nstrains of Saccharomyces pastorianus, a natural lager-brewing yeast hybrid of\nSaccharomyces cerevisiae and Saccharomyces eubayanus, using whole genome\nshotgun sequencing. Each strain of S. pastorianus has lost species-specific\nportions of its genome and has undergone extensive recombination, producing\nchimeric chromosomes. We predicted 30 breakpoints that we confirmed at the\nsingle nucleotide level by designing species-specific primers that flank each\nbreakpoint, and then sequencing the PCR product. These rearrangements are the\nresult of recombination between areas of homology between the two subgenomes,\nrather than repetitive elements such as transposons or tRNAs. Interestingly,\n28/30 S. cerevisiae- S. eubayanus recombination breakpoints are located within\ngenic regions, generating chimeric genes. Furthermore we show evidence for the\nreuse of two breakpoints, located in HSP82 and KEM1, in strains of proposed\nindependent origin.\n"
  },
  {
    "year": 2013,
    "title": "Improved annotation of 3-prime untranslated regions and complex loci by\n  combination of strand-specific Direct RNA Sequencing, RNA-seq and ESTs",
    "summary": "  The reference annotations made for a genome sequence provide the framework\nfor all subsequent analyses of the genome. Correct annotation is particularly\nimportant when interpreting the results of RNA-seq experiments where short\nsequence reads are mapped against the genome and assigned to genes according to\nthe annotation. Inconsistencies in annotations between the reference and the\nexperimental system can lead to incorrect interpretation of the effect on RNA\nexpression of an experimental treatment or mutation in the system under study.\nUntil recently, the genome-wide annotation of 3-prime untranslated regions\nreceived less attention than coding regions and the delineation of intron/exon\nboundaries. In this paper, data produced for samples in Human, Chicken and A.\nthaliana by the novel single-molecule, strand-specific, Direct RNA Sequencing\ntechnology from Helicos Biosciences which locates 3-prime polyadenylation sites\nto within +/- 2 nt, were combined with archival EST and RNA-Seq data. Nine\nexamples are illustrated where this combination of data allowed: (1) gene and\n3-prime UTR re-annotation (including extension of one 3-prime UTR by 5.9 kb);\n(2) disentangling of gene expression in complex regions; (3) clearer\ninterpretation of small RNA expression and (4) identification of novel genes.\nWhile the specific examples displayed here may become obsolete as genome\nsequences and their annotations are refined, the principles laid out in this\npaper will be of general use both to those annotating genomes and those seeking\nto interpret existing publically available annotations in the context of their\nown experimental data\n"
  },
  {
    "year": 2013,
    "title": "Progenetix: 12 years of oncogenomic data curation",
    "summary": "  DNA copy number aberrations (CNAs) can be found in the majority of cancer\ngenomes, and are crucial for understanding the potential mechanisms underlying\ntumor initiation and progression. Since the first release in 2001, the\nProgenetix project (http://www.progenetix.org) has provided a reference\nresource dedicated to provide the most comprehensive collection of genome-wide\nCNA profiles. Reflecting the application of comparative genomic hybridization\n(CGH) techniques to tens of thousands of cancer genomes, over the past 12 years\nour data curation efforts have resulted in a more than 60-fold increase in the\nnumber of cancer samples presented through Progenetix. In addition, new data\nexploration tools and visualization options have been added. In particular, the\ngene specific CNA frequency analysis should facilitate the assignment of cancer\ngenes to related cancer types. Additionally, the new user file processing\ninterface allows users to take advantage of the online tools, including various\ndata representation options for proprietary data pre-publication. In this\nupdate article, we report recent improvements of the database in terms of\ncontent, user interface and online tools.\n"
  },
  {
    "year": 2013,
    "title": "The genomic landscape of meiotic crossovers and gene conversions in\n  Arabidopsis thaliana",
    "summary": "  Knowledge of the exact distribution of meiotic crossovers (COs) and gene\nconversions (GCs) is essential for understanding many aspects of population\ngenetics and evolution, from haplotype structure and long-distance genetic\nlinkage to the generation of new allelic variants of genes. To this end, we\nresequenced the four products of 13 meiotic tetrads along with 10 doubled\nhaploids derived from Arabidopsis thaliana hybrids. GC detection through short\nreads has previously been confounded by genomic rearrangements. Rigid filtering\nfor misaligned reads allowed GC identification at high accuracy and revealed an\n~80-kb transposition, which undergoes copy-number changes mediated by meiotic\nrecombination. Non-crossover associated GCs were extremely rare most likely due\nto their short average length of ~25-50 bp, which is significantly shorter than\nthe length of CO associated GCs. Overall, recombination preferentially targeted\nnon-methylated nucleosome-free regions at gene promoters, which showed\nsignificant enrichment of two sequence motifs.\n"
  },
  {
    "year": 2013,
    "title": "MCUIUC -- A New Framework for Metagenomic Read Compression",
    "summary": "  Metagenomics is an emerging field of molecular biology concerned with\nanalyzing the genomes of environmental samples comprising many different\ndiverse organisms. Given the nature of metagenomic data, one usually has to\nsequence the genomic material of all organisms in a batch, leading to a mix of\nreads coming from different DNA sequences. In deep high-throughput sequencing\nexperiments, the volume of the raw reads is extremely high, frequently\nexceeding 600 Gb. With an ever increasing demand for storing such reads for\nfuture studies, the issue of efficient metagenomic compression becomes of\nparamount importance. We present the first known approach to metagenome read\ncompression, termed MCUIUC (Metagenomic Compression at UIUC). The gist of the\nproposed algorithm is to perform classification of reads based on unique\norganism identifiers, followed by reference-based alignment of reads for\nindividually identified organisms, and metagenomic assembly of unclassified\nreads. Once assembly and classification are completed, lossless reference based\ncompression is performed via positional encoding. We evaluate the performance\nof the algorithm on moderate sized synthetic metagenomic samples involving 15\nrandomly selected organisms and describe future directions for improving the\nproposed compression method.\n"
  },
  {
    "year": 2013,
    "title": "When 2D is not enough, go for an extra dimension",
    "summary": "  The use of an extra SDS separation in a different buffer system provide a\ntechnique for deconvoluting 2D gel spots made of several proteins (Colignon et\nal. Proteomics, 2013, 13, 2077-2082). This technique keeps the quantitative\nanalysis of the protein amounts and combines it with a strongly improved\nidentification process by mass spectrometry, removing identification\nambiguities in most cases. In some favorable cases, posttranslational variants\ncan be separated by this procedure. This versatile and easy to use technique is\nanticipated to be a very valuable addition to the toolbox used in 2D gel-based\nproteomics.\n"
  },
  {
    "year": 2013,
    "title": "Multidimensional separation prior to mass spectrometry: Getting closer\n  to the bottom of the iceberg",
    "summary": "  While prefractionation has previously been shown to improve results in MS\nanalysis, a novel combination provides an additional dimension of separation:\nprotein fractionation by SDS-PAGE followed by IEF of tryptic peptides before\nseparation by RP-LC [Atanassov and Urlaub, Proteomics 2013, 13, 2947-2955].\nThis three-step separation procedure prior to MS/MS substantially increases\nproteome coverage and represents a further step toward a more comprehensive\nanalysis of complex proteomes.\n"
  },
  {
    "year": 2013,
    "title": "Joint analysis of functional genomic data and genome-wide association\n  studies of 18 human traits",
    "summary": "  Annotations of gene structures and regulatory elements can inform genome-wide\nassociation studies (GWAS). However, choosing the relevant annotations for\ninterpreting an association study of a given trait remains challenging. We\ndescribe a statistical model that uses association statistics computed across\nthe genome to identify classes of genomic element that are enriched or depleted\nfor loci that influence a trait. The model naturally incorporates multiple\ntypes of annotations. We applied the model to GWAS of 18 human traits,\nincluding red blood cell traits, platelet traits, glucose levels, lipid levels,\nheight, BMI, and Crohn's disease. For each trait, we evaluated the relevance of\n450 different genomic annotations, including protein-coding genes, enhancers,\nand DNase-I hypersensitive sites in over a hundred tissues and cell lines. We\nshow that the fraction of phenotype-associated SNPs that influence protein\nsequence ranges from around 2% (for platelet volume) up to around 20% (for LDL\ncholesterol); that repressed chromatin is significantly depleted for SNPs\nassociated with several traits; and that cell type-specific DNase-I\nhypersensitive sites are enriched for SNPs associated with several traits (for\nexample, the spleen in platelet volume). Finally, by re-weighting each GWAS\nusing information from functional genomics, we increase the number of loci with\nhigh-confidence associations by around 5%.\n"
  },
  {
    "year": 2013,
    "title": "H3K4 mono- and di-methyltransferase MLL4 is required for enhancer\n  activation during cell differentiation",
    "summary": "  Enhancers play a central role in cell-type-specific gene expression and are\nmarked by H3K4me1/2. Active enhancers are further marked by H3K27ac. However,\nthe methyltransferases responsible for H3K4me1/2 on enhancers remain elusive.\nFurthermore, how these enzymes function on enhancers to regulate\ncell-type-specific gene expression is unclear. Here we identify MLL4 (KMT2D) as\na major mammalian H3K4 mono- and di-methyltransferase with partial functional\nredundancy with MLL3 (KMT2C). Using adipogenesis and myogenesis as model\nsystems, we show that MLL4 exhibits cell-type- and\ndifferentiation-stage-specific genomic binding and is predominantly localized\non enhancers. MLL4 co-localizes with lineage-determining transcription factors\n(TFs) on active enhancers during differentiation. Deletion of MLL4 markedly\ndecreases H3K4me1/2, H3K27ac, Polymerase II and Mediator levels on enhancers\nand leads to severe defects in cell-type-specific gene expression and cell\ndifferentiation. Together, these findings identify MLL4 as a major mammalian\nH3K4 mono- and di-methyltransferase essential for enhancer activation during\ncell differentiation.\n"
  },
  {
    "year": 2013,
    "title": "How to use 2D gel electrophoresis in plant proteomics",
    "summary": "  Two-dimensional electrophoresis has nurtured the birth of proteomics. It is\nhowever no longer the exclusive setup used in proteomics, with the development\nof shotgun proteomics techniques that appear more fancy and fashionable\nnowadays.Nevertheless, 2D gel-based proteomics still has valuable features, and\nsometimes unique ones, which make it often an attractive choice when a\nproteomics strategy must be selected. These features are detailed in this\nchapter, as is the rationale for selecting or not 2D gel-based proteomics as a\nproteomic strategy.\n"
  },
  {
    "year": 2013,
    "title": "Ribosome profiling reveals post-transcriptional buffering of divergent\n  gene expression in yeast",
    "summary": "  Understanding the patterns and causes of phenotypic divergence is a central\ngoal in evolutionary biology. Much work has shown that mRNA abundance is highly\nvariable between closely related species. However, the extent and mechanisms of\npost-transcriptional gene regulatory evolution are largely unknown. Here we\nused ribosome profiling to compare transcript abundance and translation\nefficiency in two closely related yeast species (S. cerevisiae and S.\nparadoxus). By comparing translation regulatory divergence to interspecies\ndifferences in mRNA sequence features, we show that differences in transcript\nleaders and codon bias substantially contribute to divergent translation.\nGlobally, we find that translation regulatory divergence often buffers\nspecies-differences in mRNA abundance, such that ribosome occupancy is more\nconserved than transcript abundance. We used allele-specific ribosome profiling\nin interspecies hybrids to compare the relative contributions of cis- and\ntrans-regulatory divergence to species differences in mRNA abundance and\ntranslation efficiency. The mode of gene regulatory divergence differs for\nthese processes, as trans-regulatory changes play a greater role in divergent\nmRNA abundance than in divergent translation efficiency. Strikingly, most genes\nwith aberrant transcript abundance in F1 hybrids (either over- or\nunder-expressed compared to both parent species) did not exhibit aberrant\nribosome occupancy. Our results show that interspecies differences in\ntranslation contribute substantially to the evolution of gene expression.\nCompensatory differences in transcript abundance and translation efficiency may\nincrease the robustness of gene regulation.\n"
  },
  {
    "year": 2013,
    "title": "Expanding the Grammar of Biology",
    "summary": "  We metaphorically call \"Grammar of Biology\" a small field of genomic\nresearch, whose main objective is to search for intrinsic DNA sequence\nproperties. Erwin Chargaff inaugurated it back in 50s, but since then little\nprogress has been made. It remained almost neglected until early 90s, when\nVinayakumar V. Prabhu made a major contribution determining the Symmetry\nPrinciple. Remarkably, different sciences have contributed for its development,\nfor instance, Chargaff used his Chemistry background to discover the so-called\nChargaff's rules; taking advantage of several publicly available genomic\nsequences, and through Computational and Statistical analyses, Prabhu\nidentified the Symmetry Principle and, recently, using a Mathematical approach,\nwe have discovered four new Generalized Chargaff's rules. Our work has expanded\nthe \"Grammar of Biology\", and created the conceptual and theoretical framework\nnecessary to further developments.\n"
  },
  {
    "year": 2013,
    "title": "FPCB : a simple and swift strategy for mirror repeat identification",
    "summary": "  After the recent advancement of sequencing strategies, mirror repeats have\nbeen found to be present in the gene sequence of many organisms and species.\nThis presence of mirror repeats in most of the sequences indicates towards some\nimportant functional role of these repeats. However, a simple and quick\nstrategy to search these repeats in a given sequence is not available. We in\nthis manuscript have proposed a simple and swift strategy named as FPCB\nstrategy to identify mirror repeats in a give sequence. The strategy includes\nthree simple steps of downloading sequencing in FASTA format (F), making its\nparallel complement (PC) and finally performing a homology search with the\noriginal sequence (B). At least twenty genes were analyzed using the proposed\nstudy. A number and types of mirror repeats were observed. We have also tried\nto give nomenclature to these repeats. We hope that the proposed FPCB strategy\nwill be quite helpful for the identification of mirror repeats in DNA or mRNA\nsequence. Also the strategy may help in unraveling the functional role of\nmirror repeats in various processes including evolution.\n"
  },
  {
    "year": 2013,
    "title": "CONCOCT: Clustering cONtigs on COverage and ComposiTion",
    "summary": "  Metagenomics enables the reconstruction of microbial genomes in complex\nmicrobial communities without the need for culturing. Since assembly typically\nresults in fragmented genomes the grouping of genome fragments (contigs)\nbelonging to the same genome, a process referred to as binning, remains a major\ninformatics challenge. Here we present CONCOCT, a computer program that\ncombines three types of information - sequence composition, coverage across\nmultiple sample, and read-pair linkage - to automatically bin contigs into\ngenomes. We demonstrate high recall and precision rates of the program on\nartificial as well as real human gut metagenome datasets.\n"
  },
  {
    "year": 2013,
    "title": "An Intuitive Graphical Webserver for Multiple-Choice Protein Sequence\n  Search",
    "summary": "  Every day tens of thousands of sequence searches and sequence alignment\nqueries are submitted to webservers. The capitalized word \"BLAST\" become a\nverb, describing the act of performing sequence search and alignment. However,\nif one needs to search for sequences that contain, for example, two hydrophobic\nand three polar residues at five given positions, the query formation on the\nmost frequently used webservers will be difficult. Some servers support the\nformation of queries with regular expressions, but most of the users are\nunfamiliar with their syntax. Here we present an intuitive, easily applicable\nwebserver, the Protein Sequence Analysis server, that allows the formation of\nmultiple choice queries by simply drawing the residues to their positions; if\nmore than one residue are drawn to the same position, then they will be nicely\nstacked on the user interface, indicating the multiple choice at he given\nposition. This computer-game like interface is natural and intuitive, and the\ncoloring of the residues makes possible to form queries requiring not just\ncertain amino acids in the given positions, but also small nonpolar, negatively\ncharged, hydrophobic, positively charged, or polar ones. The webserver is\navailable at http://psa.pitgroup.org.\n"
  },
  {
    "year": 2013,
    "title": "Sequence Capture Versus Restriction Site Associated DNA Sequencing for\n  Phylogeography",
    "summary": "  Genomic datasets generated with massively parallel sequencing methods have\nthe potential to propel systematics in new and exciting directions, but\nselecting appropriate markers and methods is not straightforward. We applied\ntwo approaches with particular promise for systematics, restriction site\nassociated DNA sequencing (RAD-Seq) and sequence capture (Seq-cap) of\nultraconserved elements (UCEs), to the same set of samples from a non-model,\nNeotropical bird. We found that both RAD-Seq and Seq-cap produced genomic\ndatasets containing thousands of loci and SNPs and that the inferred population\nassignments and species trees were concordant between datasets. However,\nmodel-based estimates of demographic parameters differed between datasets,\nparticularly when we estimated the parameters using a method based on allele\nfrequency spectra. The differences we observed may result from differences in\nassembly, alignment, and filtering of sequence data between methods, and our\nfindings suggest that caution is warranted when using allele frequencies to\nestimate parameters from low-coverage sequencing data. We further explored the\ndifferences between methods using simulated Seq-cap- and RAD-Seq-like datasets.\nAnalyses of simulated data suggest that increasing the number of loci from 500\nto 5000 increased phylogenetic concordance factors and the accuracy and\nprecision of demographic parameter estimates, but increasing the number of loci\npast 5000 resulted in minimal gains. Increasing locus length from 64 bp to 500\nbp improved phylogenetic concordance factors and minimal gains were observed\nwith loci longer than 500 bp, but locus length did not influence the accuracy\nand precision of demographic parameter estimates. We discuss our results\nrelative to the diversity of data collection methods available, and we provide\nadvice for harnessing next-generation sequencing for systematics research.\n"
  },
  {
    "year": 2013,
    "title": "Phenotypic class ratios as marker signs of different types of\n  agamospermy",
    "summary": "  This article focuses on the development of the method for the genetic\nclassification of agamospermous reproduction types in plants using sugar beet\nas an example. The classification feasibility is ensured by the use of isozymes\nas genetic markers allowing the identification of all three phenotypic classes\nin the progeny of individual heterozygous diploid plant and is based on\ndifferent phenotypic class ratios in the progenies obtained by meiotic and\nmitotic agamospermy. The data indicate that for sugar beet meiotic agamospermy\nis the more typical since 13 of 15 explored progenies were classified as those\nproduced by meiotic agamospermy and only 2 as produced by mitotic agamospermy.\n"
  },
  {
    "year": 2014,
    "title": "Fast and accurate alignment of long bisulfite-seq reads",
    "summary": "  Summary: Longer sequencing reads, with at least 200 bases per template are\nnow common. While traditional aligners have adopted new strategies to improve\nthe mapping of longer reads, aligners specific to bisulfite-sequencing were\noptimized when much shorter reads were the norm. We sought to perform the first\ncomparison using longer reads to determine which aligners were most accurate\nand efficient and to evaluate a novel software tool, bwa-meth, built on a\ntraditional mapper that supports insertions, deletions and clipped alignments.\nWe gauge accuracy by comparing the number of on and off-target reads from a\ntargeted sequencing project and by simulations. Availability and\nImplementation: The benchmarking scripts and the bwa-meth software are\navailable at https://github/com/brentp/bwa-meth/ under the MIT License.\n"
  },
  {
    "year": 2014,
    "title": "Statistical Mechanics Model for the Dynamics of Collective Epigenetic\n  Histone Modification",
    "summary": "  Epigenetic histone modifications play an important role in the maintenance of\ndifferent cell phenotypes. The exact molecular mechanism for inheritance of the\nmodification patterns over cell generations remains elusive. We construct a\nPotts-type model based on experimentally observed nearest-neighbor enzyme\nlateral interactions and nucleosome covalent modification state biased enzyme\nrecruitment. The model can lead to effective nonlocal interactions among\nnucleosomes suggested in previous theoretical studies, and epigenetic memory is\nrobustly inheritable against stochastic cellular processes.\n"
  },
  {
    "year": 2014,
    "title": "Integrative genomics analysis identifies pericentromeric regions of\n  human chromosomes affecting patterns of inter-chromosomal interactions",
    "summary": "  Genome-wide analysis of distributions of densities of long-range interactions\nof human chromosomes with each other, nucleoli, nuclear lamina, and binding\nsites of chromatin state regulatory proteins, CTCF and STAT1, identifies\nnon-random highly correlated patterns of density distributions along the\nchromosome length for all these features. Marked co-enrichments and clustering\nof all these interactions are detected at discrete genomic regions on selected\nchromosomes, which are located within pericentromeric heterochromatin and\ndesignated Centromeric Regions of Interphase Chromatin Homing (CENTRICH).\nCENTRICH manifest 199-716-fold higher density of inter-chromosomal binding\nsites compared to genome-wide or chromosomal averages (p =\n2.10E-101-1.08E-292). Sequence alignment analysis shows that CENTRICH represent\nunique DNA sequences of 3.9 to 22.4 Kb in size which are: 1) associated with\nnucleolus; 2) exhibit highly diverse set of DNA-bound chromatin state\nregulators, including marked enrichment of CTCF and STAT1 binding sites; 3)\nbind multiple intergenic disease-associated genomic loci (IDAGL) with\ndocumented long-range enhancer activities and established links to increased\nrisk of developing epithelial malignancies and other common human disorders.\nUsing distances of SNP loci homing sites within genomic coordinates of CENTRICH\nas a proxy of likelihood of disease-linked SNP loci binding to CENTRICH, we\ndemonstrate statistically significant correlations between the probability of\nSNP loci binding to CENTRICH and GWAS-defined odds ratios of increased risk of\na disease for cancer, coronary artery disease, and type 2 diabetes. Our\nanalysis suggests that centromeric sequences and pericentromeric\nheterochromatin may play an important role in human cells beyond the critical\nfunctions in chromosome segregation.\n"
  },
  {
    "year": 2014,
    "title": "Palaeosymbiosis revealed by genomic fossils of Wolbachia in a\n  strongyloidean nematode",
    "summary": "  Wolbachia are common endosymbionts of terrestrial arthropods, and are also\nfound in nematodes: the animal-parasitic filaria, and the plant-parasite\nRadopholus similis. Lateral transfer of Wolbachia DNA to the host genome is\ncommon. We generated a draft genome sequence for the strongyloidean nematode\nparasite Dictyocaulus viviparus, the cattle lungworm. In the assembly, we\nidentified nearly 1 Mb of sequence with similarity to Wolbachia. The fragments\nwere unlikely to derive from a live Wolbachia infection: most were short, and\nthe genes were disabled through inactivating mutations. Many fragments were\nco-assembled with definitively nematode-derived sequence. We found limited\nevidence of expression of the Wolbachia-derived genes. The D. viviparus\nWolbachia genes were most similar to filarial strains, and strains from the\nhost-promiscuous clade F. We conclude that D. viviparus was infected by\nWolbachia in the past. Genome sequence based surveys are a powerful tool for\nrevealing the genome archaeology of infection and symbiosis.\n"
  },
  {
    "year": 2014,
    "title": "A possible gut microbiota basis for weight gain side effects of\n  antipsychotic drugs",
    "summary": "  Weight gain is a well-established side effect of both conventional and newer\nanti-psychotic drugs, but the cause is not well understood. Recent studies\ncorrelate obesity with the presence or absence of particular genetic sequences\nin the gut microbiota. We identified strong associations between protein\ntargets of antipsychotics and microbiota sequences directly related to weight\nregulation in human body, leading to a potential metagenomic mechanism of\naction. Further experimental study is recommended.\n"
  },
  {
    "year": 2014,
    "title": "A model capturing novel strand symmetries in bacterial DNA",
    "summary": "  Chargaff's second parity rule for short oligonucleotides states that the\nfrequency of any short nucleotide sequence on a strand is approximately equal\nto the frequency of its reverse complement on the same strand. Recent studies\nhave shown that, with the exception of organellar DNA, this parity rule\ngenerally holds for double stranded DNA genomes and fails to hold for\nsingle-stranded genomes. While Chargaff's first parity rule is fully explained\nby the Watson-Crick pairing in the DNA double helix, a definitive explanation\nfor the second parity rule has not yet been determined. In this work, we\npropose a model based on a hidden Markov process for approximating the\ndistributional structure of primitive DNA sequences. Then, we use the model to\nprovide another possible theoretical explanation for Chargaff's second parity\nrule, and to predict novel distributional aspects of bacterial DNA sequences.\n"
  },
  {
    "year": 2014,
    "title": "Whole Exome Sequencing to Estimate Alloreactivity Potential Between\n  Donors and Recipients in Stem Cell Transplantation",
    "summary": "  Whole exome sequencing was performed on HLA-matched stem cell donors and\ntransplant recipients to measure sequence variation contributing to minor\nhistocompatibility antigen differences between the two. A large number of\nnonsynonymous single nucleotide polymorphisms were identified in each of the\nnine unique donor-recipient pairs tested. This variation was greater in\nmagnitude in unrelated donors as compared with matched related donors.\nKnowledge of the magnitude of exome variation between stem cell transplant\nrecipients and donors may allow more accurate titration of immunosuppressive\ntherapy following stem cell transplantation.\n"
  },
  {
    "year": 2014,
    "title": "The life cycle of Drosophila orphan genes",
    "summary": "  Orphans are genes restricted to a single phylogenetic lineage and emerge at\nhigh rates. While this predicts an accumulation of genes, the gene number has\nremained remarkably constant through evolution. This paradox has not yet been\nresolved. Because orphan genes have been mainly analyzed over long evolutionary\ntime scales, orphan loss has remained unexplored. Here we study the patterns of\norphan turnover among close relatives in the Drosophila obscura group. We show\nthat orphans are not only emerging at a high rate, but that they are also\nrapidly lost. Interestingly, recently emerged orphans are more likely to be\nlost than older ones. Furthermore, highly expressed orphans with a strong\nmale-bias are more likely to be retained. Since both lost and retained orphans\nshow similar evolutionary signatures of functional conservation, we propose\nthat orphan loss is not driven by high rates of sequence evolution, but\nreflects lineage specific functional requirements.\n"
  },
  {
    "year": 2014,
    "title": "Diverse and widespread contamination evident in the unmapped depths of\n  high throughput sequencing data",
    "summary": "  Background: Trace quantities of contaminating DNA are widespread in the\nlaboratory environment, but their presence has received little attention in the\ncontext of high throughput sequencing. This issue is highlighted by recent\nworks that have rested controversial claims upon sequencing data that appear to\nsupport the presence of unexpected exogenous species.\n  Results: I used reads that preferentially aligned to alternate genomes to\ninfer the distribution of potential contaminant species in a set of independent\nsequencing experiments. I confirmed that dilute samples are more exposed to\ncontaminating DNA, and, focusing on four single-cell sequencing experiments,\nfound that these contaminants appear to originate from a wide diversity of\nclades. Although negative control libraries prepared from \"blank\" samples\nrecovered the highest-frequency contaminants, low-frequency contaminants, which\nappeared to make heterogeneous contributions to samples prepared in parallel\nwithin a single experiment, were not well controlled for. I used these results\nto show that, despite heavy replication and plausible controls, contamination\ncan explain all of the observations used to support a recent claim that\ncomplete genes pass from food to human blood.\n  Conclusions: Contamination must be considered a potential source of signals\nof exogenous species in sequencing data, even if these signals are replicated\nin independent experiments, vary across conditions, or indicate a species which\nseems a priori unlikely to contaminate. Negative control libraries processed in\nparallel are essential to control for contaminant DNAs, but their limited\nability to recover low-frequency contaminants must be recognized.\n"
  },
  {
    "year": 2014,
    "title": "motifDiverge: a model for assessing the statistical significance of gene\n  regulatory motif divergence between two DNA sequences",
    "summary": "  Next-generation sequencing technology enables the identification of thousands\nof gene regulatory sequences in many cell types and organisms. We consider the\nproblem of testing if two such sequences differ in their number of binding site\nmotifs for a given transcription factor (TF) protein. Binding site motifs\nimpart regulatory function by providing TFs the opportunity to bind to genomic\nelements and thereby affect the expression of nearby genes. Evolutionary\nchanges to such functional DNA are hypothesized to be major contributors to\nphenotypic diversity within and between species; but despite the importance of\nTF motifs for gene expression, no method exists to test for motif loss or gain.\nAssuming that motif counts are Binomially distributed, and allowing for\ndependencies between motif instances in evolutionarily related sequences, we\nderive the probability mass function of the difference in motif counts between\ntwo nucleotide sequences. We provide a method to numerically estimate this\ndistribution from genomic data and show through simulations that our estimator\nis accurate. Finally, we introduce the R package {\\tt motifDiverge} that\nimplements our methodology and illustrate its application to gene regulatory\nenhancers identified by a mouse developmental time course experiment. While\nthis study was motivated by analysis of regulatory motifs, our results can be\napplied to any problem involving two correlated Bernoulli trials.\n"
  },
  {
    "year": 2014,
    "title": "MUSIC: A Hybrid Computing Environment for Burrows-Wheeler Alignment for\n  Massive Amount of Short Read Sequence Data",
    "summary": "  High-throughput DNA sequencers are becoming indispensable in our\nunderstanding of diseases at molecular level, in marker-assisted selection in\nagriculture and in microbial genetics research. These sequencing instruments\nproduce enormous amount of data (often terabytes of raw data in a month) that\nrequires efficient analysis, management and interpretation. The commonly used\nsequencing instrument today produces billions of short reads (upto 150 bases)\nfrom each run. The first step in the data analysis step is alignment of these\nshort reads to the reference genome of choice. There are different open source\nalgorithms available for sequence alignment to the reference genome. These\ntools normally have a high computational overhead, both in terms of number of\nprocessors and memory. Here, we propose a hybrid-computing environment called\nMUSIC (Mapping USIng hybrid Computing) for one of the most popular open source\nsequence alignment algorithm, BWA, using accelerators that show significant\nimprovement in speed over the serial code.\n"
  },
  {
    "year": 2014,
    "title": "Alignment-free comparison of next-generation sequencing data using\n  compression-based distance measures",
    "summary": "  Enormous volumes of short reads data from next-generation sequencing (NGS)\ntechnologies have posed new challenges to the area of genomic sequence\ncomparison.\n  The multiple sequence alignment approach is hardly applicable to NGS data due\nto the challenging problem of short read assembly.\n  Thus alignment-free methods need to be developed for the comparison of NGS\nsamples of short reads.\n  Recently, new $k$-mer based distance measures such as {\\it CVTree},\n$d_{2}^{S}$, {\\it co-phylog} have been proposed to address this problem.\n  However, those distances depend considerably on the parameter $k$, and how to\nchoose the optimal $k$ is not trivial since it may depend on different aspects\nof the sequence data.\n  Hence, in this paper we consider an alternative parameter-free approach:\ncompression-based distance measures.\n  These measures have shown impressive performance on long genome sequences in\nprevious studies, but they have not been tested on NGS short reads.\n  In this study we perform extensive validation and show that the\ncompression-based distances are highly consistent with those distances obtained\nfrom the $k$-mer based methods, from the alignment-based approach, and from\nexisting benchmarks in the literature.\n  Moreover, as these measures are parameter-free, no optimization is required\nand they still perform consistently well on multiple types of sequence data,\nfor different kinds of species and taxonomy levels.\n  The compression-based distance measures are assembly-free, alignment-free,\nparameter-free, and thus represent useful tools for the comparison of long\ngenome sequences and NGS samples of short reads.\n"
  },
  {
    "year": 2014,
    "title": "RADIA: RNA and DNA Integrated Analysis for Somatic Mutation Detection",
    "summary": "  The detection of somatic single nucleotide variants is a crucial component to\nthe characterization of the cancer genome. Mutation calling algorithms thus far\nhave focused on comparing the normal and tumor genomes from the same\nindividual. In recent years, it has become routine for projects like The Cancer\nGenome Atlas (TCGA) to also sequence the tumor RNA. Here we present RADIA (RNA\nand DNA Integrated Analysis), a method that combines the patient-matched normal\nand tumor DNA with the tumor RNA to detect somatic mutations. The inclusion of\nthe RNA increases the power to detect somatic mutations, especially at low DNA\nallelic frequencies. By integrating the DNA and RNA, we are able to rescue back\ncalls that would be missed by traditional mutation calling algorithms that only\nexamine the DNA.\n  RADIA was developed for the identification of somatic mutations using both\nDNA and RNA from the same individual. We demonstrate high sensitivity (84%) and\nvery high specificity (98% and 99%) in real data from endometrial carcinoma and\nlung adenocarcinoma from TCGA. Mutations with both high DNA and RNA read\nsupport have the highest validation rate of over 99%. We also introduce a\nsimulation package that spikes in artificial mutations to real data, rather\nthan simulating sequencing data from a reference genome. We evaluate\nsensitivity on the simulation data and demonstrate our ability to rescue back\ncalls at low DNA allelic frequencies by including the RNA. Finally, we\nhighlight mutations in important cancer genes that were rescued back due to the\nincorporation of the RNA.\n  Software available at https://github.com/aradenbaugh/radia/\n"
  },
  {
    "year": 2014,
    "title": "Cross-phenotype meta-analysis reveals large-scale trans-eQTLs mediating\n  patterns of transcriptional co-regulation",
    "summary": "  Genetic variation affecting gene regulation is a central driver of phenotypic\ndifferences between individuals and can be used to uncover how biological\nprocesses are organized in a cell. Although detecting cis-eQTLs is now routine,\ntrans-eQTLs have proven more challenging to find due to the modest variance\nexplained and the multiple tests burden of testing millions of SNPs for\nassociation to thousands of transcripts. Here, we successfully map trans-eQTLs\nwith the complementary approach of looking for SNPs associated to the\nexpression of multiple genes simultaneously. We find 732 trans- eQTLs that\nreplicate across two continental populations; each trans-eQTL controls large\ngroups of target transcripts (regulons), which are part of interacting networks\ncontrolled by transcription factors. We are thus able to uncover co-regulated\ngene sets and begin describing the cell circuitry of gene regulation.\n"
  },
  {
    "year": 2014,
    "title": "The mathematics of the genetic code reveal that frequency degeneracy\n  leads to exponential scaling in the DNA codon distribution of Homo sapiens",
    "summary": "  The nature of the quantitative distribution of the 64 DNA codons in the human\ngenome has been an issue of debate for over a decade. Some groups have proposed\nthat the quantitative distribution of the DNA codons ordered as a\nrank-frequency plot follows a well-known power law called Zipf's law. Others\nhave shown that the DNA codon distribution is best fitted to an exponential\nfunction. However, the reason for such scaling behavior has not yet been\naddressed. In the present study, we demonstrate that the nonlinearity of the\nDNA codon distribution is a direct consequence of the frequency recurrence of\nthe codon usage (i.e., the repetitiveness of codon usage frequencies at the\nwhole genome level). We discover that if frequency recurrence is absent from\nthe human genome, the frequency of occurrence of codons scales linearly with\nthe codon rank. We also show that DNA codons of both low and high frequency of\noccurrence in the genome are best fitted by an exponential function and provide\nstrong evidence to suggest that the coding region of the human genome does not\nfollow Zipf's law. Information-theoretic methods and entropy calculations are\napplied to the DNA codon distribution and a new approach, called the lariat\nmethod, is proposed to quantitatively analyze the DNA codon distribution in\nHomo sapiens.\n"
  },
  {
    "year": 2014,
    "title": "Near-optimal Assembly for Shotgun Sequencing with Noisy Reads",
    "summary": "  Recent work identified the fundamental limits on the information requirements\nin terms of read length and coverage depth required for successful de novo\ngenome reconstruction from shotgun sequencing data, based on the idealistic\nassumption of no errors in the reads (noiseless reads). In this work, we show\nthat even when there is noise in the reads, one can successfully reconstruct\nwith information requirements close to the noiseless fundamental limit. A new\nassembly algorithm, X-phased Multibridging, is designed based on a\nprobabilistic model of the genome. It is shown through analysis to perform well\non the model, and through simulations to perform well on real genomes.\n"
  },
  {
    "year": 2014,
    "title": "MaxSSmap: A GPU program for mapping divergent short reads to genomes\n  with the maximum scoring subsequence",
    "summary": "  Programs based on hash tables and Burrows-Wheeler are very fast for mapping\nshort reads to genomes but have low accuracy in the presence of mismatches and\ngaps. Such reads can be aligned accurately with the Smith-Waterman algorithm\nbut it can take hours and days to map millions of reads even for bacteria\ngenomes. We introduce a GPU program called MaxSSmap with the aim of achieving\ncomparable accuracy to Smith-Waterman but with faster runtimes. Similar to most\nprograms MaxSSmap identifies a local region of the genome followed by exact\nalignment. Instead of using hash tables or Burrows-Wheeler in the first part,\nMaxSSmap calculates maximum scoring subsequence score between the read and\ndisjoint fragments of the genome in parallel on a GPU and selects the highest\nscoring fragment for exact alignment. We evaluate MaxSSmap's accuracy and\nruntime when mapping simulated Illumina E.coli and human chromosome one reads\nof different lengths and 10\\% to 30\\% mismatches with gaps to the E.coli genome\nand human chromosome one. We also demonstrate applications on real data by\nmapping ancient horse DNA reads to modern genomes and unmapped paired reads\nfrom NA12878 in 1000 genomes. We show that MaxSSmap attains comparable high\naccuracy and low error to fast Smith-Waterman programs yet has much lower\nruntimes. We show that MaxSSmap can map reads rejected by BWA and NextGenMap\nwith high accuracy and low error much faster than if Smith-Waterman were used.\nOn short read lengths of 36 and 51 both MaxSSmap and Smith-Waterman have lower\naccuracy compared to at higher lengths. On real data MaxSSmap produces many\nalignments with high score and mapping quality that are not given by NextGenMap\nand BWA. The MaxSSmap source code is freely available from\nhttp://www.cs.njit.edu/usman/MaxSSmap.\n"
  },
  {
    "year": 2014,
    "title": "Genetic influences on translation in yeast",
    "summary": "  Heritable differences in gene expression between individuals are an important\nsource of phenotypic variation. The question of how closely the effects of\ngenetic variation on protein levels mirror those on mRNA levels remains open.\nHere, we addressed this question by using ribosome profiling to examine how\ngenetic differences between two strains of the yeast S. cerevisiae affect\ntranslation. Strain differences in translation were observed for hundreds of\ngenes. Allele specific measurements in the diploid hybrid between the two\nstrains revealed roughly half as many cis-acting effects on translation as were\nobserved for mRNA levels. In both the parents and the hybrid, most effects on\ntranslation were of small magnitude, such that the direction of an mRNA\ndifference was typically reflected in a concordant footprint difference. The\nrelative importance of cis and trans acting variation on footprint levels was\nsimilar to that for mRNA levels. There was a tendency for translation to cause\nlarger footprint differences than expected given the respective mRNA\ndifferences. This is in contrast to translational differences between yeast\nspecies that have been reported to more often oppose than reinforce mRNA\ndifferences. Finally, we catalogued instances of premature translation\ntermination in the two yeast strains and also found several instances where\nerroneous reference gene annotations lead to apparent nonsense mutations that\nin fact reside outside of the translated gene body. Overall, genetic influences\non translation subtly modulate gene expression differences, and translation\ndoes not create strong discrepancies between genetic influences on mRNA and\nprotein levels.\n"
  },
  {
    "year": 2014,
    "title": "The proteomic to biology inference, a frequently overlooked concern in\n  the interpretation of proteomic data: A plea for functional validation",
    "summary": "  Proteomics will celebrate its 20th year in 2014. In this relatively short\nperiod of time, it has invaded most areas of biology and its use will probably\ncontinue to spread in the future. These two decades have seen a considerable\nincrease in the speed and sensitivity of protein identification and\ncharacterization, even from complex samples. Indeed, what was a challenge\ntwenty years ago is now little more than a daily routine. Although not\ncompletely over, the technological challenge now makes room to another\nchallenge, which is the best possible appraisal and exploitation of proteomic\ndata to draw the best possible conclusions from a biological point of view. The\npoint developed in this paper is that proteomic data are almost always\nfragmentary. This means in turn that although better than an mRNA level, a\nprotein level is often insufficient to draw a valid conclusion from a\nbiological point of view, especially in a world where PTMs play such an\nimportant role. This means in turn that transformation of proteomic data into\nbiological data requires an important intermediate layer of functional\nvalidation, i.e. not merely the confirmation of protein abundance changes by\nother methods, but a functional appraisal of the biological consequences of the\nprotein level changes highlighted by the proteomic screens.\n"
  },
  {
    "year": 2014,
    "title": "SAMBLASTER: fast duplicate marking and structural variant read\n  extraction",
    "summary": "  Motivation: Illumina DNA sequencing is now the predominant source of raw\ngenomic data, and data volumes are growing rapidly. Bioinformatic analysis\npipelines are having trouble keeping pace. A common bottleneck in such\npipelines is the requirement to read, write, sort and compress large BAM files\nmultiple times.\n  Results: We present SAMBLASTER, a tool that reduces the number of times such\ncostly operations are performed. SAMBLASTER is designed to mark duplicates in\nread-sorted SAM files as a piped post-pass on DNA aligner output before it is\ncompressed to BAM. In addition, it can simultaneously output into separate\nfiles the discordant read-pairs and/or split-read mappings used for structural\nvariant calling. As an alignment post-pass, its own runtime overhead is\nnegligible, while dramatically reducing overall pipeline complexity and\nruntime. As a stand-alone duplicate marking tool, it performs significantly\nbetter than PICARD or SAMBAMBA in terms of both speed and memory usage, while\nachieving nearly identical results.\n  Availability: SAMBLASTER is open source C++ code and freely available from\nhttps://github.com/GregoryFaust/samblaster\n"
  },
  {
    "year": 2014,
    "title": "Towards Better Understanding of Artifacts in Variant Calling from\n  High-Coverage Samples",
    "summary": "  Motivation: Whole-genome high-coverage sequencing has been widely used for\npersonal and cancer genomics as well as in various research areas. However, in\nthe lack of an unbiased whole-genome truth set, the global error rate of\nvariant calls and the leading causal artifacts still remain unclear even given\nthe great efforts in the evaluation of variant calling methods.\n  Results: We made ten SNP and INDEL call sets with two read mappers and five\nvariant callers, both on a haploid human genome and a diploid genome at a\nsimilar coverage. By investigating false heterozygous calls in the haploid\ngenome, we identified the erroneous realignment in low-complexity regions and\nthe incomplete reference genome with respect to the sample as the two major\nsources of errors, which press for continued improvements in these two areas.\nWe estimated that the error rate of raw genotype calls is as high as 1 in\n10-15kb, but the error rate of post-filtered calls is reduced to 1 in 100-200kb\nwithout significant compromise on the sensitivity.\n  Availability: BWA-MEM alignment: http://bit.ly/1g8XqRt; Scripts:\nhttps://github.com/lh3/varcmp; Additional data:\nhttps://figshare.com/articles/Towards_better_understanding_of_artifacts_in_variating_calling_from_high_coverage_samples/981073\n"
  },
  {
    "year": 2014,
    "title": "Taxator-tk: Fast and Precise Taxonomic Assignment of Metagenomes by\n  Approximating Evolutionary Neighborhoods",
    "summary": "  Metagenomics characterizes microbial communities by random shotgun sequencing\nof DNA isolated directly from an environment of interest. An essential step in\ncomputational metagenome analysis is taxonomic sequence assignment, which\nallows us to identify the sequenced community members and to reconstruct\ntaxonomic bins with sequence data for the individual taxa. We describe an\nalgorithm and the accompanying software, taxator-tk, which performs taxonomic\nsequence assignments by fast approximate determination of evolutionary\nneighbors from sequence similarities. Taxator-tk was precise in its taxonomic\nassignment across all ranks and taxa for a range of evolutionary distances and\nfor short sequences. In addition to the taxonomic binning of metagenomes, it is\nwell suited for profiling microbial communities from metagenome samples\nbecauseit identifies bacterial, archaeal and eukaryotic community members\nwithout being affected by varying primer binding strengths, as in marker gene\namplification, or copy number variations of marker genes across different taxa.\nTaxator-tk has an efficient, parallelized implementation that allows the\nassignment of 6 Gb of sequence data per day on a standard multiprocessor system\nwith ten CPU cores and microbial RefSeq as the genomic reference data.\n"
  },
  {
    "year": 2014,
    "title": "Identification and characterization of unique to human regulatory\n  sequences in embryonic stem cells reveal associations with transposable\n  elements, distal enhancers, non-coding RNA, and DNA methylation-driven\n  mechanisms of genome editing",
    "summary": "  Despite significant progress in structural and functional characterization of\nhuman genome, understanding of mechanisms underlying the genetic basis of human\nphenotypic uniqueness remains limited. We report that non-randomly distributed\ntransposable element-derived sequences, most notably HERV-H/LTR7 and L1HS, are\nassociated with creation of 99.8% unique to human transcription factor binding\nsites in genome of embryonic stem cells (ESC). 4,094 unique to human regulatory\nloci display selective and site-specific binding of critical regulators (NANOG,\nPOU5F1, CTCF, Lamin B1) and are preferentially placed within the matrix of\ntranscriptionally active DNA segments hyper-methylated in ESC. Unique to human\nNANOG-binding sites are enriched near the rapidly evolving in primates\nprotein-coding genes regulating brain size, pluripotency lncRNAs, hESC\nenhancers, and 5-hydroxymethylcytosine-harboring regions immediately adjacent\nto binding sites. We propose a proximity placement model explaining how 33-47%\nexcess of NANOG and POU5F1 proteins immobilized on a DNA scaffold may play a\nfunctional role at distal regulatory elements.\n"
  },
  {
    "year": 2014,
    "title": "Human Y-chromosome gene classification using Fractal Dimension & Shannon\n  Entropy",
    "summary": "  All genes on the human Y-chromosome were studied using fractal dimension and\nShannon entropy. Clear outlier clusters were identified. Among these were 6\nsequences that have since been withdrawn as CDSs and 1 additional sequence that\nis not in the current assembly. A methodology for ranking the sequences based\non deviation from average values of FD and SE was developed. The group of\nsequences scored among the 10% largest deviations had abnormally high\nlikelihood to be from centromeric or pseudoautosomal regions and low likelihood\nto be from X-chromosome transposed regions. lncRNA sequences were also enriched\namong the outliers. In addition, the number of expressed genes previously\nidentified for evolutionary study tended to not have large deviations from the\naverage.\n  Keywords: Y-chromosome; Shannon di-nucleotide entropy; fractal dimension;\ncentromeric genes; gene degredation; lncRNA\n"
  },
  {
    "year": 2014,
    "title": "Modeling DNA methylation dynamics with approaches from phylogenetics",
    "summary": "  Methylation of CpG dinucleotides is a prevalent epigenetic modification that\nis required for proper development in vertebrates, and changes in CpG\nmethylation are essential to cellular differentiation. Genome-wide DNA\nmethylation assays have become increasingly common, and recently distinct\nstages across differentiating cellular lineages have been assayed. How- ever,\ncurrent methods for modeling methylation dynamics do not account for the\ndependency structure between precursor and dependent cell types. We developed a\ncontinuous-time Markov chain approach, based on the observation that changes in\nmethylation state over tissue differentiation can be modeled similarly to DNA\nnucleotide changes over evolutionary time. This model explicitly takes\nprecursor to descendant relationships into account and enables inference of CpG\nmethylation dynamics. To illustrate our method, we analyzed a high-resolution\nmethylation map of the differentiation of mouse stem cells into several blood\ncell types. Our model can successfully infer unobserved CpG methylation states\nfrom observations at the same sites in related cell types (90% correct), and\nthis approach more accurately reconstructs missing data than imputation based\non neighboring CpGs (84% correct). Additionally, the single CpG resolution of\nour methylation dynamics estimates enabled us to show that DNA sequence context\nof CpG sites is informative about methylation dynamics across tissue\ndifferentiation. Finally, we identified genomic regions with clusters of highly\ndynamic CpGs and present a likely functional example. Our work establishes a\nframework for inference and modeling that is well-suited to DNA methylation\ndata, and our success suggests that other methods for analyzing DNA nucleotide\nsubstitutions will also translate to the modeling of epigenetic phenomena.\n"
  },
  {
    "year": 2014,
    "title": "Genetic Influences on Brain Gene Expression in Rats Selected for\n  Tameness and Aggression",
    "summary": "  Inter-individual differences in many behaviors are partly due to genetic\ndifferences, but the identification of the genes and variants that influence\nbehavior remains challenging. Here, we studied an F2 intercross of two outbred\nlines of rats selected for tame and aggressive behavior towards humans for more\nthan 64 generations. By using a mapping approach that is able to identify\ngenetic loci segregating within the lines, we identified four times more loci\ninfluencing tameness and aggression than by an approach that assumes fixation\nof causative alleles, suggesting that many causative loci were not driven to\nfixation by the selection. We used RNA sequencing in 150 F2 animals to identify\nhundreds of loci that influence brain gene expression. Several of these loci\ncolocalize with tameness loci and may reflect the same genetic variants.\nThrough analyses of correlations between allele effects on behavior and gene\nexpression, differential expression between the tame and aggressive rat\nselection lines, and correlations between gene expression and tameness in F2\nanimals, we identify the genes Gltscr2, Lgi4, Zfp40 and Slc17a7 as candidate\ncontributors to the strikingly different behavior of the tame and aggressive\nanimals.\n"
  },
  {
    "year": 2014,
    "title": "Mapping to a Reference Genome Structure",
    "summary": "  To support comparative genomics, population genetics, and medical genetics,\nwe propose that a reference genome should come with a scheme for mapping each\nbase in any DNA string to a position in that reference genome. We refer to a\ncollection of one or more reference genomes and a scheme for mapping to their\npositions as a reference structure. Here we describe the desirable properties\nof reference structures and give examples. To account for natural genetic\nvariation, we consider the more general case in which a reference genome is\nrepresented by a graph rather than a set of phased chromosomes; the latter is\ntreated as a special case.\n"
  },
  {
    "year": 2014,
    "title": "Graph-based data integration predicts long-range regulatory interactions\n  across the human genome",
    "summary": "  Transcriptional regulation of gene expression is one of the main processes\nthat affect cell diversification from a single set of genes. Regulatory\nproteins often interact with DNA regions located distally from the\ntranscription start sites (TSS) of the genes. We developed a computational\nmethod that combines open chromatin and gene expression information for a large\nnumber of cell types to identify these distal regulatory elements. Our method\nbuilds correlation graphs for publicly available DNase-seq and exon array\ndatasets with matching samples and uses graph-based methods to filter findings\nsupported by multiple datasets and remove indirect interactions. The resulting\nset of interactions was validated with both anecdotal information of known\nlong-range interactions and unbiased experimental data deduced from Hi-C and\nCAGE experiments. Our results provide a novel set of high-confidence candidate\nopen chromatin regions involved in gene regulation, often located several Mb\naway from the TSS of their target gene.\n"
  },
  {
    "year": 2014,
    "title": "Hierarchical clustering of DNA k-mer counts in RNA-seq fastq files\n  reveals batch effects",
    "summary": "  Batch effects, artificial sources of variation due to experimental design,\nare a widespread phenomenon in high throughput data. Therefore, mechanisms for\ndetection of batch effects are needed requiring comparison of multiple samples.\nWe apply hierarchical clustering (HC) on DNA k-mer counts of multiple RNA-seq\nderived Fastq files. Ideally, HC generated trees reflect experimental treatment\ngroups and thus may indicate experimental effects, but clustering of\npreparation groups indicates the presence of batch effects. In order to provide\na simple applicable tool we implemented sequential analysis of Fastq reads with\nlow memory usage in an R package (seqTools) available on Bioconductor. DNA\nk-mer counts were analysed on 61 Fastq files containing RNA-seq data from two\ncell types (dermal fibroblasts and Jurkat cells) sequenced on 8 different\nIllumina Flowcells. Results: Pairwise comparison of all Flowcells with\nhierarchical clustering revealed strong Flowcell based tree separation in 6 (21\n%) and detectable Flowcell based clustering in 17 (60.7 %) of 28 Flowcell\ncomparisons. In our samples, batch effects were also present in reads mapped to\nthe human genome. Filtering reads for high quality (Phred >30) did not remove\nthe batch effects. Conclusions: Hierarchical clustering of DNA k-mer counts\nprovides a quality criterion and an unspecific diagnostic tool for RNA-seq\nexperiments.\n"
  },
  {
    "year": 2014,
    "title": "Long non-coding RNAs as a source of new peptides",
    "summary": "  Deep transcriptome sequencing has revealed the existence of many transcripts\nthat lack long or conserved open reading frames and which have been termed long\nnon-coding RNAs (lncRNAs). Despite the existence of several well-characterized\nlncRNAs that play roles in the regulation of gene expression, the vast majority\nof them do not yet have a known function. Motivated by the existence of\nribosome profiling data for several species, we have tested the hypothesis that\nthey may act as a repository for the synthesis of new peptides using data from\nhuman, mouse, zebrafish, fruit fly, Arabidopsis and yeast. The ribosome\nprotection patterns are consistent with the presence of translated open reading\nframes (ORFs) in a very large number of lncRNAs. Most of the ribosome-protected\nORFs are shorter than 100 amino acids and usually cover less than half the\ntranscript. Ribosome density in these ORFs is high and contrasts sharply with\nthe 3UTR region, in which very often there is no detectable ribosome binding,\nsimilar to bona fide protein-coding genes. The coding potential of\nribosome-protected ORFs, measured using hexamer frequencies, is significantly\nhigher than that of randomly selected intronic ORFs and similar to that of\nevolutionary young coding sequences. Selective constraints in\nribosome-protected ORFs from lncRNAs are lower than in typical protein-coding\ngenes but again similar to young proteins. These results strongly suggest that\nlncRNAs play an important role in de novo protein evolution.\n"
  },
  {
    "year": 2014,
    "title": "Evidence for strong co-evolution of mitochondrial and somatic genomes",
    "summary": "  We studied a relations between the triplet frequency composition of\nmitochondria genomes, and the phylogeny of their bearers. First, the clusters\nin 63dimensional space were developed due to $K$-means. Second, the clade\ncomposition of those clusters has been studied. It was found that genomes are\ndistributed among the clusters very regularly, with strong correlation to\ntaxonomy. Strong co-evolution manifests through this correlation: the proximity\nin frequency space was determined over the mitochondrion genomes, while the\nproximity in taxonomy was determined morphologically.\n"
  },
  {
    "year": 2014,
    "title": "High-resolution transcriptome analysis with long-read RNA sequencing",
    "summary": "  RNA sequencing (RNA-seq) enables characterization and quantification of\nindividual transcriptomes as well as detection of patterns of allelic\nexpression and alternative splicing. Current RNA-seq protocols depend on\nhigh-throughput short-read sequencing of cDNA. However, as ongoing advances are\nrapidly yielding increasing read lengths, a technical hurdle remains in\nidentifying the degree to which differences in read length influence various\ntranscriptome analyses. In this study, we generated two paired-end RNA-seq\ndatasets of differing read lengths (2x75 bp and 2x262 bp) for lymphoblastoid\ncell line GM12878 and compared the effect of read length on transcriptome\nanalyses, including read-mapping performance, gene and transcript\nquantification, and detection of allele-specific expression (ASE) and\nallele-specific alternative splicing (ASAS) patterns. Our results indicate\nthat, while the current long-read protocol is considerably more expensive than\nshort-read sequencing, there are important benefits that can only be achieved\nwith longer read length, including lower mapping bias and reduced ambiguity in\nassigning reads to genomic elements, such as mRNA transcript. We show that\nthese benefits ultimately lead to improved detection of cis-acting regulatory\nand splicing variation effects within individuals.\n"
  },
  {
    "year": 2014,
    "title": "Identifying the Genetic Basis of Functional Protein Evolution Using\n  Reconstructed Ancestors",
    "summary": "  A central challenge in the study of protein evolution is the identification\nof historic amino acid sequence changes responsible for creating novel\nfunctions observed in present-day proteins. To address this problem, we\ndeveloped a new method to identify and rank amino acid mutations in ancestral\nprotein sequences according to their function-shifting potential. Our approach\nscans the changes between two reconstructed ancestral sequences in order to\nfind (1) sites with sequence changes that significantly deviate from our\nmodel-based probabilistic expectations, (2) sites that demonstrate extreme\nchanges in mutual information, and (3) sites with extreme gains or losses of\ninformation content. By taking the overlaps of these statistical signals, the\nmethod accurately identifies cryptic evolutionary patterns that are often not\nobvious when examining only the conservation of modern-day protein sequences.\nWe validated this method with a training set of previously-discovered\nfunction-shifting mutations in three essential protein families in animals and\nfungi, whose evolutionary histories were the prior subject of systematic\nmolecular biological investigation. Our method identified the known\nfunction-shifting mutations in the training set with a very low rate of false\npositive discovery. Further, our approach significantly outperformed other\nmethods that use variability in evolutionary rates to detect functional loci.\nThe accuracy of our approach indicates it could be a useful tool for generating\nspecific testable hypotheses regarding the acquisition of new functions across\na wide range of protein families.\n"
  },
  {
    "year": 2014,
    "title": "Genome disorder and breast cancer susceptibility",
    "summary": "  Many common diseases have a complex genetic basis in which large numbers of\ngenetic variations combine with environmental and lifestyle factors to\ndetermine risk. However, quantifying such polygenic effects and their\nrelationship to disease risk has been challenging. In order to address these\ndifficulties we developed a global measure of the information content of an\nindividual's genome relative to a reference population, which may be used to\nassess differences in global genome structure between cases and appropriate\ncontrols. Informally this measure, which we call relative genome information\n(RGI), quantifies the relative \"disorder\" of an individual's genome. In order\nto test its ability to predict disease risk we used RGI to compare single\nnucleotide polymorphism genotypes from two independent samples of women with\nearly-onset breast cancer with three independent sets of controls. We found\nthat RGI was significantly elevated in both sets of breast cancer cases in\ncomparison with all three sets of controls, with disease risk rising sharply\nwith RGI (odds ratio greater than 12 for the highest percentile RGI).\nFurthermore, we found that these differences are not due to associations with\ncommon variants at a small number of disease-associated loci, but rather are\ndue to the combined associations of thousands of markers distributed throughout\nthe genome. Our results indicate that the information content of an\nindividual's genome may be used to measure the risk of a complex disease, and\nsuggest that early-onset breast cancer has a strongly polygenic basis.\n"
  },
  {
    "year": 2014,
    "title": "Mapping the Space of Genomic Signatures",
    "summary": "  We propose a computational method to measure and visualize interrelationships\namong any number of DNA sequences allowing, for example, the examination of\nhundreds or thousands of complete mitochondrial genomes. An \"image distance\" is\ncomputed for each pair of graphical representations of DNA sequences, and the\ndistances are visualized as a Molecular Distance Map: Each point on the map\nrepresents a DNA sequence, and the spatial proximity between any two points\nreflects the degree of structural similarity between the corresponding\nsequences. The graphical representation of DNA sequences utilized, Chaos Game\nRepresentation (CGR), is genome- and species-specific and can thus act as a\ngenomic signature. Consequently, Molecular Distance Maps could inform species\nidentification, taxonomic classifications and, to a certain extent,\nevolutionary history. The image distance employed, Structural Dissimilarity\nIndex (DSSIM), implicitly compares the occurrences of oligomers of length up to\n$k$ (herein $k=9$) in DNA sequences. We computed DSSIM distances for more than\n5 million pairs of complete mitochondrial genomes, and used Multi-Dimensional\nScaling (MDS) to obtain Molecular Distance Maps that visually display the\nsequence relatedness in various subsets, at different taxonomic levels. This\ngeneral-purpose method does not require DNA sequence homology and can thus be\nused to compare similar or vastly different DNA sequences, genomic or\ncomputer-generated, of the same or different lengths. We illustrate potential\nuses of this approach by applying it to several taxonomic subsets: phylum\nVertebrata, (super)kingdom Protista, classes Amphibia-Insecta-Mammalia, class\nAmphibia, and order Primates. This analysis of an extensive dataset confirms\nthat the oligomer composition of full mtDNA sequences can be a source of\ntaxonomic information.\n"
  },
  {
    "year": 2014,
    "title": "BEETL-fastq: a searchable compressed archive for DNA reads",
    "summary": "  Motivation:\n  FASTQ is a standard file format for DNA sequencing data which stores both\nnucleotides and quality scores. A typical sequencing study can easily generate\nhundreds of gigabytes of FASTQ files, while public archives such as ENA and\nNCBI and large international collaborations such as the Cancer Genome Atlas can\naccumulate many terabytes of data in this format. Text compression tools such\nas gzip are often employed to reduce the storage burden, but have the\ndisadvantage that the data must be decompressed before it can be used.\n  Here we present BEETL-fastq, a tool that not only compresses FASTQ-formatted\nDNA reads more compactly than gzip, but also permits rapid search for $k$-mer\nqueries within the archived sequences. Importantly, the full FASTQ record of\neach matching read or read pair is returned, allowing the search results to be\npiped directly to any of the many standard tools that accept FASTQ data as\ninput.\n  Results:\n  We show that 6.6 terabytes of human reads in FASTQ format can be transformed\ninto 1.7 terabytes of indexed files, from where we can search for 1, 10, 100,\n1000, a million of 30-mers in respectively 3, 8, 14, 45 and 567 seconds plus 20\nms per output read. Useful applications of the search capability are\nhighlighted, including the genotyping of structural variant breakpoints and \"in\nsilico pull-down\" experiments in which only the reads that cover a region of\ninterest are selectively extracted for the purposes of variant calling or\nvisualization.\n  Availability:\n  BEETL-fastq is part of the BEETL library, available as a github repository at\ngit@github.com:BEETL/BEETL.git.\n"
  },
  {
    "year": 2014,
    "title": "Assessing Technical Performance in Differential Gene Expression\n  Experiments with External Spike-in RNA Control Ratio Mixtures",
    "summary": "  There is a critical need for standard approaches to assess, report, and\ncompare the technical performance of genome-scale differential gene expression\nexperiments. We assess technical performance with a proposed \"standard\"\ndashboard of metrics derived from analysis of external spike-in RNA control\nratio mixtures. These control ratio mixtures with defined abundance ratios\nenable assessment of diagnostic performance of differentially expressed\ntranscript lists, limit of detection of ratio (LODR) estimates, and expression\nratio variability and measurement bias. The performance metrics suite is\napplicable to analysis of a typical experiment, and here we also apply these\nmetrics to evaluate technical performance among laboratories. An\ninterlaboratory study using identical samples shared amongst 12 laboratories\nwith three different measurement processes demonstrated generally consistent\ndiagnostic power across 11 laboratories. Ratio measurement variability and bias\nwere also comparable amongst laboratories for the same measurement process.\nDifferent biases were observed for measurement processes using different mRNA\nenrichment protocols.\n"
  },
  {
    "year": 2014,
    "title": "Robust identification of noncoding RNA from transcriptomes requires\n  phylogenetically-informed sampling",
    "summary": "  Noncoding RNAs are integral to a wide range of biological processes,\nincluding translation, gene regulation, host-pathogen interactions and\nenvironmental sensing. While genomics is now a mature field, our capacity to\nidentify noncoding RNA elements in bacterial and archaeal genomes is hampered\nby the difficulty of de novo identification. The emergence of new technologies\nfor characterizing transcriptome outputs, notably RNA-seq, are improving\nnoncoding RNA identification and expression quantification. However, a major\nchallenge is to robustly distinguish functional outputs from transcriptional\nnoise. To establish whether annotation of existing transcriptome data has\neffectively captured all functional outputs, we analysed over 400 publicly\navailable RNA-seq datasets spanning 37 different Archaea and Bacteria. Using\ncomparative tools, we identify close to a thousand highly-expressed candidate\nnoncoding RNAs. However, our analyses reveal that capacity to identify\nnoncoding RNA outputs is strongly dependent on phylogenetic sampling.\nSurprisingly, and in stark contrast to protein-coding genes, the phylogenetic\nwindow for effective use of comparative methods is perversely narrow:\naggregating public datasets only produced one phylogenetic cluster where these\ntools could be used to robustly separate unannotated noncoding RNAs from a null\nhypothesis of transcriptional noise. Our results show that for the full\npotential of transcriptomics data to be realized, a change in experimental\ndesign is paramount: effective transcriptomics requires phylogeny-aware\nsampling.\n"
  },
  {
    "year": 2014,
    "title": "Conservation and losses of avian non-coding RNA loci",
    "summary": "  Here we present the results of a large-scale bioinformatic annotation of\nnon-coding RNA loci in 48 avian genomes. Our approach uses probabilistic models\nof hand-curated families from the Rfam database to infer conserved RNA families\nwithin each avian genome. We supplement these annotations with predictions from\nthe tRNA annotation tool, tRNAscan-SE and microRNAs from miRBase. We show that\na number of lncRNA-associated loci are conserved between birds and mammals,\nincluding several intriguing cases where the reported mammalian lncRNA function\nis not conserved in birds. We also demonstrate extensive conservation of\nclassical ncRNAs (e.g., tRNAs) and more recently discovered ncRNAs (e.g.,\nsnoRNAs and miRNAs) in birds. Furthermore, we describe numerous \"losses\" of\nseveral RNA families, and attribute these to genuine loss, divergence or\nmissing data. In particular, we show that many of these losses are due to the\nchallenges associated with assembling Avian microchromosomes. These combined\nresults illustrate the utility of applying homology-based methods for\nannotating novel vertebrate genomes.\n"
  },
  {
    "year": 2014,
    "title": "SEK: Sparsity exploiting $k$-mer-based estimation of bacterial community\n  composition",
    "summary": "  Motivation: Estimation of bacterial community composition from a\nhigh-throughput sequenced sample is an important task in metagenomics\napplications. Since the sample sequence data typically harbors reads of\nvariable lengths and different levels of biological and technical noise,\naccurate statistical analysis of such data is challenging. Currently popular\nestimation methods are typically very time consuming in a desktop computing\nenvironment.\n  Results: Using sparsity enforcing methods from the general sparse signal\nprocessing field (such as compressed sensing), we derive a solution to the\ncommunity composition estimation problem by a simultaneous assignment of all\nsample reads to a pre-processed reference database. A general statistical model\nbased on kernel density estimation techniques is introduced for the assignment\ntask and the model solution is obtained using convex optimization tools.\nFurther, we design a greedy algorithm solution for a fast solution. Our\napproach offers a reasonably fast community composition estimation method which\nis shown to be more robust to input data variation than a recently introduced\nrelated method.\n  Availability: A platform-independent Matlab implementation of the method is\nfreely available at http://www.ee.kth.se/ctsoftware; source code that does not\nrequire access to Matlab is currently being tested and will be made available\nlater through the above website.\n"
  },
  {
    "year": 2014,
    "title": "Maximizing Protein Translation Rate in the Ribosome Flow Model: the\n  Homogeneous Case",
    "summary": "  Gene translation is the process in which intracellular macro-molecules,\ncalled ribosomes, decode genetic information in the mRNA chain into the\ncorresponding proteins. Gene translation includes several steps. During the\nelongation step, ribosomes move along the mRNA in a sequential manner and link\namino-acids together in the corresponding order to produce the proteins.\n  The homogeneous ribosome flow model(HRFM) is a deterministic computational\nmodel for translation-elongation under the assumption of constant elongation\nrates along the mRNA chain. The HRFM is described by a set of n first-order\nnonlinear ordinary differential equations, where n represents the number of\nsites along the mRNA chain. The HRFM also includes two positive parameters:\nribosomal initiation rate and the (constant) elongation rate. In this paper, we\nshow that the steady-state translation rate in the HRFM is a concave function\nof its parameters. This means that the problem of determining the parameter\nvalues that maximize the translation rate is relatively simple. Our results may\ncontribute to a better understanding of the mechanisms and evolution of\ntranslation-elongation. We demonstrate this by using the theoretical results to\nestimate the initiation rate in M. musculus embryonic stem cell. The underlying\nassumption is that evolution optimized the translation mechanism.\n  For the infinite-dimensional HRFM, we derive a closed-form solution to the\nproblem of determining the initiation and transition rates that maximize the\nprotein translation rate. We show that these expressions provide good\napproximations for the optimal values in the n-dimensional HRFM already for\nrelatively small values of n. These results may have applications for synthetic\nbiology where an important problem is to re-engineer genomic systems in order\nto maximize the protein production rate.\n"
  },
  {
    "year": 2014,
    "title": "Analysis of cellular responses of macrophages to zinc ions and zinc\n  oxide nanoparticles: a combined targeted and proteomic approach",
    "summary": "  Two different zinc oxide nanoparticles, as well as zinc ions, are used to\nstudy the cellular responses of the RAW 264 macrophage cell line. A proteomic\nscreen is used to provide a wide view of the molecular effects of zinc, and the\nmost prominent results are cross-validated by targeted studies. Furthermore,\nthe alteration of important macrophage functions (e.g. phagocytosis) by zinc is\nalso investigated. The intracellular dissolution/uptake of zinc is also studied\nto further characterize zinc toxicity. Zinc oxide nanoparticles dissolve\nreadily in the cells, leading to high intracellular zinc concentrations, mostly\nas protein-bound zinc. The proteomic screen reveals a rather weak response in\nthe oxidative stress response pathway, but a strong response both in the\ncentral metabolism and in the proteasomal protein degradation pathway. Targeted\nexperiments confirm that carbohydrate catabolism and proteasome are critical\ndeterminants of sensitivity to zinc, which also induces DNA damage. Conversely,\nglutathione levels and phagocytosis appear unaffected at moderately toxic zinc\nconcentrations.\n"
  },
  {
    "year": 2014,
    "title": "Human-chimpanzee alignment: Ortholog Exponentials and Paralog Power Laws",
    "summary": "  Genomic subsequences conserved between closely related species such as human\nand chimpanzee exhibit an exponential length distribution, in contrast to the\nalgebraic length distribution observed for sequences shared between distantly\nrelated genomes. We find that the former exponential can be further decomposed\ninto an exponential component primarily composed of orthologous sequences, and\na truncated algebraic component primarily composed of paralogous sequences.\n"
  },
  {
    "year": 2014,
    "title": "Maximizing Protein Translation Rate in the Nonhomogeneous Ribosome Flow\n  Model: A Convex Optimization Approach",
    "summary": "  Translation is an important stage in gene expression. During this stage,\nmacro-molecules called ribosomes travel along the mRNA strand linking\namino-acids together in a specific order to create a functioning protein. An\nimportant question is how to maximize protein production. Indeed, translation\nis known to consume most of the cell's energy and it is natural to assume that\nevolution shaped this process so that it maximizes the protein production rate.\nIf this is indeed so then one can estimate various parameters of the\ntranslation machinery by solving an appropriate mathematical optimization\nproblem. The same problem also arises in the context of synthetic biology,\nnamely, re-engineer heterologous genes in order to maximize their translation\nrate in a host organism. We consider the problem of maximizing the protein\nproduction rate using a computational model for translation-elongation called\nthe ribosome flow model (RFM). This model describes the flow of the ribosomes\nalong an mRNA chain of length n using a set of n first-order nonlinear ODEs. It\nalso includes n+1 positive parameters: the ribosomal initiation rate into the\nmRNA chain, and n elongation rates along the chain sites. We show that the\nsteady-state translation rate in the RFM is a strictly concave function of its\nparameters. This means that the problem of maximizing the translation rate\nunder a suitable constraint always admits a unique solution, and that this\nsolution can be determined using highly-efficient algorithms for solving convex\noptimization problems even for large values of n. Furthermore, our analysis\nshows that the optimal translation rate can be computed based only on the\noptimal initiation rate and the elongation rate of the codons near the\nbeginning of the ORF. We discuss some applications of the theoretical results\nto synthetic biology, molecular evolution, and functional genomics.\n"
  },
  {
    "year": 2014,
    "title": "Multi-species network inference improves gene regulatory network\n  reconstruction for early embryonic development in Drosophila",
    "summary": "  Gene regulatory network inference uses genome-wide transcriptome measurements\nin response to genetic, environmental or dynamic perturbations to predict\ncausal regulatory influences between genes. We hypothesized that evolution also\nacts as a suitable network perturbation and that integration of data from\nmultiple closely related species can lead to improved reconstruction of gene\nregulatory networks. To test this hypothesis, we predicted networks from\ntemporal gene expression data for 3,610 genes measured during early embryonic\ndevelopment in six Drosophila species and compared predicted networks to gold\nstandard networks of ChIP-chip and ChIP-seq interactions for developmental\ntranscription factors in five species. We found that (i) the performance of\nsingle-species networks was independent of the species where the gold standard\nwas measured; (ii) differences between predicted networks reflected the known\nphylogeny and differences in biology between the species; (iii) an integrative\nconsensus network which minimized the total number of edge gains and losses\nwith respect to all single-species networks performed better than any\nindividual network. Our results show that in an evolutionarily conserved\nsystem, integration of data from comparable experiments in multiple species\nimproves the inference of gene regulatory networks. They provide a basis for\nfuture studies on the numerous multi-species gene expression datasets for other\nbiological processes available in the literature.\n"
  },
  {
    "year": 2014,
    "title": "Generalized Integrated Functional Test for Regional Methylation Rates",
    "summary": "  Motivation: Methods are needed to test pre-defined genomic regions such as\npromoters for differential methylation in genome-wide association studies,\nwhere the number of samples is limited and the data have large amounts of\nmeasurement error. Results: We developed a new statistical test, the\ngeneralized integrated functional test (GIFT), which tests for regional\ndifferences in methylation based on differences in the functional relationship\nbetween methylation percent and location of the CpG sites within a region. In\nthis method, subject-specific functional profiles are first estimated, and the\naverage profile within groups is compared between groups using an ANOVA-like\ntest. Simulations and analyses of data obtained from patients with chronic\nlymphocytic leukemia indicate that GIFT has good statistical properties and is\nable to identify promising genomic regions. Further, GIFT is likely to work\nwith multiple different types of experiments since different smoothing\nfunctions can be used to estimate the functional relationship between\nmethylation percent and CpG site location. Availability and Implementation:\nMatlab code for GIFT and sample data are available at\nhttp://biostat.gru.edu/~dryu/research.html. Contact: rpodolsk@med.wayne.edu or\ndryu@gru.edu\n"
  },
  {
    "year": 2014,
    "title": "Revised Annotations, Sex-Biased Expression, and Lineage-Specific Genes\n  in the Drosophila melanogaster group",
    "summary": "  Here, we provide revised gene models for D. ananassae, D. yakuba, and D.\nsimulans, which include UTRs and empirically verified intron-exon boundaries,\nas well as ortholog groups identified using a fuzzy reciprocal-best-hit blast\ncomparison. Using these revised annotations, we perform differential expression\ntesting using the cufflinks suite to provide a broad overview of differential\nexpression between reproductive tissues and the carcass. We identify thousands\nof genes that are differentially expressed across tissues in D. yakuba and D.\nsimulans, with roughly 60% agreement in expression patterns of orthologs in D.\nyakuba and D. simulans. We identify several cases of putative polycistronic\ntranscripts, pointing to a combination of transcriptional read-through in the\ngenome as well as putative gene fusion and fission events across taxa. We\nfurthermore identify hundreds of lineage specific genes in each species with no\nblast hits among transcripts of any other Drosophila species, which are\ncandidates for neofunctionalized proteins and a potential source of genetic\nnovelty.\n"
  },
  {
    "year": 2014,
    "title": "Integrative multi-omics module network inference with Lemon-Tree",
    "summary": "  Module network inference is an established statistical method to reconstruct\nco-expression modules and their upstream regulatory programs from integrated\nmulti-omics datasets measuring the activity levels of various cellular\ncomponents across different individuals, experimental conditions or time points\nof a dynamic process. We have developed Lemon-Tree, an open-source,\nplatform-independent, modular, extensible software package implementing\nstate-of-the-art ensemble methods for module network inference. We benchmarked\nLemon-Tree using large-scale tumor datasets and showed that Lemon-Tree\nalgorithms compare favorably with state-of-the-art module network inference\nsoftware. We also analyzed a large dataset of somatic copy-number alterations\nand gene expression levels measured in glioblastoma samples from The Cancer\nGenome Atlas and found that Lemon-Tree correctly identifies known glioblastoma\noncogenes and tumor suppressors as master regulators in the inferred module\nnetwork. Novel candidate driver genes predicted by Lemon-Tree were validated\nusing tumor pathway and survival analyses. Lemon-Tree is available from\nhttp://lemon-tree.googlecode.com under the GNU General Public License version\n2.0.\n"
  },
  {
    "year": 2014,
    "title": "Paleoproteomics explained to youngsters: how did the wedding of\n  two-dimensional electrophoresis and protein sequencing spark proteomics on:\n  Let there be light",
    "summary": "  Taking the opportunity of the 20th anniversary of the word \"proteomics\", this\nyoung adult age is a good time to remember how proteomics came from enormous\nprogress in protein separation and protein microanalysis techniques, and from\nthe conjugation of these advances into a high performance and streamlined\nworking setup. However, in the history of the almost three decades that\nencompass the first attempts to perform large scale analysis of proteins to the\ncurrent high throughput proteomics that we can enjoy now, it is also\ninteresting to underline and to recall how difficult the first decade was.\nIndeed when the word was cast, the battle was already won. This recollection is\nmostly devoted to the almost forgotten period where proteomics was being\nconceived and put to birth, as this collective scientific work will never\nappear when searched through the keyword \"proteomics\". BIOLOGICAL SIGNIFICANCE:\nThe significance of this manuscript is to recall and review the two decades\nthat separated the first attempts of performing large scale analysis of\nproteins from the solid technical corpus that existed when the word\n\"proteomics\" was coined twenty years ago. This recollection is made within the\nscientific historical context of this decade, which also saw the blossoming of\nDNA cloning and sequencing. This article is part of a Special Issue entitled:\n20 years of Proteomics in memory of Viatliano Pallini. Guest Editors: Luca Bini\n, Juan J. Calvete, Natacha Turck, Denis Hochstrasser and Jean-Charles Sanchez.\n"
  },
  {
    "year": 2014,
    "title": "The degenerative evolution from multicellularity to unicellularity\n  during cancer",
    "summary": "  Theoretical reasoning suggests that human cancer may result from knocking\ndown the genetic constraints evolved for maintenance of the metazoan\nmulticellularity, which, however, requires a critical test. Using\nxenograft-based experimental evolution we characterized for the first time the\nfull life history from initiation to metastasis of a tumor at the genomic and\ntranscriptomic levels, and observed metastasis-driving positive selection for\ngenerally loss-of-function mutations on a set of multicellularity-related\ngenes, which is further supported by large-scale exome data of clinical tumor\nsamples. Subsequent expression analysis revealed mainly expression\ndown-regulation of multicellularity-related genes, which form an evolving\nexpression profile approaching that of embryonic stem cells, the cell type with\nthe most characteristics of unicellular life. The theoretical conjecture\npredicts that genes born at the emergence of metazoan multicellularity tend to\nbe cancer drivers, which we validated using a rigorous phylostratigraphy\nanalysis on the birth rate of genes annotated by Cancer Gene Census. Also, the\nnumber of loss-of-function tumor suppressors often predominates over activated\noncogenes in a typical tumor of human patients. These data collectively suggest\nthat, different from typical organismal evolution in which gain of new genes is\nthe mainstream, cancer represents a loss-of-function-driven degenerative\nevolution back to the unicellular ground state. This cancer evolution model may\nexplain the enormous tumoral genetic heterogeneity in the clinic, underlie how\ndistant-organ metastases originate in primary tumors despite distinct\nenvironmental requirements, and hold implications for designing effective\ncancer therapy.\n"
  },
  {
    "year": 2014,
    "title": "On the genetic architecture of intelligence and other quantitative\n  traits",
    "summary": "  How do genes affect cognitive ability or other human quantitative traits such\nas height or disease risk? Progress on this challenging question is likely to\nbe significant in the near future. I begin with a brief review of psychometric\nmeasurements of intelligence, introducing the idea of a \"general factor\" or g\nscore. The main results concern the stability, validity (predictive power), and\nheritability of adult g. The largest component of genetic variance for both\nheight and intelligence is additive (linear), leading to important\nsimplifications in predictive modeling and statistical estimation. Due mainly\nto the rapidly decreasing cost of genotyping, it is possible that within the\ncoming decade researchers will identify loci which account for a significant\nfraction of total g variation. In the case of height analogous efforts are well\nunder way. I describe some unpublished results concerning the genetic\narchitecture of height and cognitive ability, which suggest that roughly 10k\nmoderately rare causal variants of mostly negative effect are responsible for\nnormal population variation. Using results from Compressed Sensing\n(L1-penalized regression), I estimate the statistical power required to\ncharacterize both linear and nonlinear models for quantitative traits. The main\nunknown parameter s (sparsity) is the number of loci which account for the bulk\nof the genetic variation. The required sample size is of order 100s, or roughly\na million in the case of cognitive ability.\n"
  },
  {
    "year": 2014,
    "title": "Convex Clustering: An Attractive Alternative to Hierarchical Clustering",
    "summary": "  The primary goal in cluster analysis is to discover natural groupings of\nobjects. The field of cluster analysis is crowded with diverse methods that\nmake special assumptions about data and address different scientific aims.\nDespite its shortcomings in accuracy, hierarchical clustering is the dominant\nclustering method in bioinformatics. Biologists find the trees constructed by\nhierarchical clustering visually appealing and in tune with their evolutionary\nperspective. Hierarchical clustering operates on multiple scales\nsimultaneously. This is essential, for instance, in transcriptome data where\none may be interested in making qualitative inferences about how lower-order\nrelationships like gene modules lead to higher-order relationships like\npathways or biological processes. The recently developed method of convex\nclustering preserves the visual appeal of hierarchical clustering while\nameliorating its propensity to make false inferences in the presence of\noutliers and noise. The current paper exploits the proximal distance principle\nto construct a novel algorithm for solving the convex clustering problem. The\nsolution paths generated by convex clustering reveal relationships between\nclusters that are hidden by static methods such as k-means clustering. Our\nconvex clustering software separates parameters, accommodates missing data, and\nsupports prior information on relationships. The software is implemented on ATI\nand nVidia graphics processing units (GPUs) for maximal speed. Several\nbiological examples illustrate the strengths of convex clustering and the\nability of the proximal distance algorithm to handle high-dimensional problems.\n"
  },
  {
    "year": 2014,
    "title": "Behavioral individuality reveals genetic control of phenotypic\n  variability",
    "summary": "  Variability is ubiquitous in nature and a fundamental feature of complex\nsystems. Few studies, however, have investigated variance itself as a trait\nunder genetic control. By focusing primarily on trait means and ignoring the\neffect of alternative alleles on trait variability, we may be missing an\nimportant axis of genetic variation contributing to phenotypic differences\namong individuals. To study genetic effects on individual-to-individual\nphenotypic variability (or intragenotypic variability), we used a panel of\nDrosophila inbred lines and focused on locomotor handedness, in an assay\noptimized to measure variability. We discovered that some lines had\nconsistently high levels of intragenotypic variability among individuals while\nothers had low levels. We demonstrate that the degree of variability is itself\nheritable. Using a genome-wide association study (GWAS) for the degree of\nintragenotypic variability as the phenotype across lines, we identified several\ngenes expressed in the brain that affect variability in handedness without\naffecting the mean. One of these genes, Ten-a, implicated a neuropil in the\ncentral complex of the fly brain as influencing the magnitude of behavioral\nvariability, a brain region involved in sensory integration and locomotor\ncoordination. We have validated these results using genetic deficiencies, null\nalleles, and inducible RNAi transgenes. This study reveals the constellation of\nphenotypes that can arise from a single genotype and it shows that different\ngenetic backgrounds differ dramatically in their propensity for phenotypic\nvariability. Because traditional mean-focused GWASs ignore the contribution of\nvariability to overall phenotypic variation, current methods may miss important\nlinks between genotype and phenotype.\n"
  },
  {
    "year": 2014,
    "title": "Methods for Joint Imaging and RNA-seq Data Analysis",
    "summary": "  Emerging integrative analysis of genomic and anatomical imaging data which\nhas not been well developed, provides invaluable information for the holistic\ndiscovery of the genomic structure of disease and has the potential to open a\nnew avenue for discovering novel disease susceptibility genes which cannot be\nidentified if they are analyzed separately. A key issue to the success of\nimaging and genomic data analysis is how to reduce their dimensions. Most\nprevious methods for imaging information extraction and RNA-seq data reduction\ndo not explore imaging spatial information and often ignore gene expression\nvariation at genomic positional level. To overcome these limitations, we extend\nfunctional principle component analysis from one dimension to two dimension\n(2DFPCA) for representing imaging data and develop a multiple functional linear\nmodel (MFLM) in which functional principal scores of images are taken as\nmultiple quantitative traits and RNA-seq profile across a gene is taken as a\nfunction predictor for assessing the association of gene expression with\nimages. The developed method has been applied to image and RNA-seq data of\novarian cancer and KIRC studies. We identified 24 and 84 genes whose\nexpressions were associated with imaging variations in ovarian cancer and KIRC\nstudies, respectively. Our results showed that many significantly associated\ngenes with images were not differentially expressed, but revealed their\nmorphological and metabolic functions. The results also demonstrated that the\npeaks of the estimated regression coefficient function in the MFLM often\nallowed the discovery of splicing sites and multiple isoform of gene\nexpressions.\n"
  },
  {
    "year": 2014,
    "title": "Visual annotations and a supervised learning approach for evaluating and\n  calibrating ChIP-seq peak detectors",
    "summary": "  Many peak detection algorithms have been proposed for ChIP-seq data analysis,\nbut it is not obvious which method and what parameters are optimal for any\ngiven data set. In contrast, peaks can easily be located by visual inspection\nof profile data on a genome browser. We thus propose a supervised machine\nlearning approach to ChIP-seq data analysis, using annotated regions that\nencode an expert's qualitative judgments about which regions contain or do not\ncontain peaks. The main idea is to manually annotate a small subset of the\ngenome, and then learn a model that makes consistent predictions on the rest of\nthe genome. We show how our method can be used to quantitatively calibrate and\nbenchmark the performance of peak detection algorithms on specific data sets.\nWe compare several peak detectors on 7 annotated region data sets, consisting\nof 2 histone marks, 4 expert annotators, and several different cell types. In\nthese data the macs algorithm was best for a narrow peak histone profile\n(H3K4me3) while the hmcan.broad algorithm was best for a broad histone profile\n(H3K36me3). Our benchmark annotated region data sets can be downloaded from a\npublic website, and there is an R package for computing the annotation error on\nGitHub.\n"
  },
  {
    "year": 2014,
    "title": "Beyond the E-value: stratified statistics for protein domain prediction",
    "summary": "  E-values have been the dominant statistic for protein sequence analysis for\nthe past two decades: from identifying statistically significant local sequence\nalignments to evaluating matches to hidden Markov models describing protein\ndomain families. Here we formally show that for \"stratified\" multiple\nhypothesis testing problems, controlling the local False Discovery Rate (lFDR)\nper stratum, or partition, yields the most predictions across the data at any\ngiven threshold on the FDR or E-value over all strata combined. For the\nimportant problem of protein domain prediction, a key step in characterizing\nprotein structure, function and evolution, we show that stratifying statistical\ntests by domain family yields excellent results. We develop the first\nFDR-estimating algorithms for domain prediction, and evaluate how well\nthresholds based on q-values, E-values and lFDRs perform in domain prediction\nusing five complementary approaches for estimating empirical FDRs in this\ncontext. We show that stratified q-value thresholds substantially outperform\nE-values. Contradicting our theoretical results, q-values also outperform\nlFDRs; however, our tests reveal a small but coherent subset of domain\nfamilies, biased towards models for specific repetitive patterns, for which\nFDRs are greatly underestimated due to weaknesses in random sequence models.\nUsage of lFDR thresholds outperform q-values for the remaining families, which\nhave as-expected noise, suggesting that further improvements in domain\npredictions can be achieved with improved modeling of random sequences.\nOverall, our theoretical and empirical findings suggest that the use of\nstratified q-values and lFDRs could result in improvements in a host of\nstructured multiple hypothesis testing problems arising in bioinformatics,\nincluding genome-wide association studies, orthology prediction, motif\nscanning, and multi-microarray analyses.\n"
  },
  {
    "year": 2014,
    "title": "Deciphering regulation in eukaryotic cell: from sequence to function",
    "summary": "  A transversal topic of my research has been the development and application\nof computational methods for DNA sequence analysis. The methods I have been\ndeveloping aim at improving our understanding of the regulation processes\nhappening in normal and cancer cells. This topic connects together the projects\npresented in this thesis. Two chapters of the thesis represent major areas of\nmy research interests: (1) methods for deciphering transcriptional regulation\nand their application to answer specific biological questions, and (2) methods\nto study the genome structure and their application in cancer studies. The\nfirst chapter predominantly focuses on transcriptional regulation. Here I\ndescribe my contribution to the development of methodology for the discovery of\ntranscription factor binding sites and the positioning of histone proteins. I\nalso explain how sequence analysis, in combination with gene expression data,\ncan allow the identification of direct target genes of a transcription factor\nunder study, as well as the physical mechanisms of its action. As two examples,\nI provide the results of my study of transcriptional regulation by (i)\noncogenic protein EWS-FLI1 in Ewing sarcoma and (ii) oncogenic transcription\nfactor Spi-1/PU.1 in erythroleukemia. In the second chapter, I describe the\nsequence analysis methods aimed at the identification of the genomic\nrearrangements in species with existing reference genome. I explain how the\ndeveloped methodology can be applied to detect the structure of cancer genomes.\nI provide an example of how such an analysis of tumor genomes can result in a\ndiscovery of a new phenomenon: chromothripsis, when hundreds of rearrangements\noccur in a single cellular catastrophe. The thesis is concluded by listing the\nmajor challenges in high-throughput sequencing analysis. I also discuss the\ncurrent top questions demanding the integration of sequencing data.\n"
  },
  {
    "year": 2014,
    "title": "MEGAHIT: An ultra-fast single-node solution for large and complex\n  metagenomics assembly via succinct de Bruijn graph",
    "summary": "  MEGAHIT is a NGS de novo assembler for assembling large and complex\nmetagenomics data in a time- and cost-efficient manner. It finished assembling\na soil metagenomics dataset with 252Gbps in 44.1 hours and 99.6 hours on a\nsingle computing node with and without a GPU, respectively. MEGAHIT assembles\nthe data as a whole, i.e., it avoids pre-processing like partitioning and\nnormalization, which might compromise on result integrity. MEGAHIT generates 3\ntimes larger assembly, with longer contig N50 and average contig length than\nthe previous assembly. 55.8% of the reads were aligned to the assembly, which\nis 4 times higher than the previous. The source code of MEGAHIT is freely\navailable at https://github.com/voutcn/megahit under GPLv3 license.\n"
  },
  {
    "year": 2014,
    "title": "Clustering pipeline for determining consensus sequences in targeted\n  next-generation sequencing",
    "summary": "  Analyses of targeted genomic sequencing data from next-generation-sequencing\n(NGS) technologies typically involves mapping reads to a reference sequence or\nclustering reads. For a number of species a reference genome is not available\nso the analyses of targeted sequencing data, for example polymorphic structural\nvariation caused by mobile elements is difficult; clustering methods are\npreferred for such data analysis. Clustering of reads requires a clustering\nthreshold parameter, which is used to compare and group reads. However,\ndetermining the optimal clustering threshold for a read dataset is challenging\nbecause of different sequence composition, the number of sequences present, and\nalso the amount of sequencing errors in the dataset. High values of the\nclustering threshold parameter can falsely inflate the number of recovered\ngenomic regions, while low values of clustering threshold can merge reads from\ndistinct regions into a single cluster. Thus, an algorithm that can empirically\ndetermine clustering threshold is needed. We propose a pipeline for clustering\ngenomic sequences wherein the clustering threshold is empirically determined\nfrom the NGS data. The optimal threshold is decided based on two internal\nclustering measures which assess clusters for small intra-cluster diameters and\nlarge inter-cluster distances. We evaluate the pipeline on two simulated\ndatasets derived from human genome sequence simulating different genomic\nregions and sequencing depth. The total number of clusters obtained from our\npipeline is closer to the actual number of reference sequences when compared to\nsingle round of clustering. Also, the number of clusters whose consensus\nsequence matches a corresponding reference sequence is higher in our pipeline.\nWe observe that the presence of repeat regions affects clustering accuracy.\n"
  },
  {
    "year": 2014,
    "title": "DBG2OLC: Efficient Assembly of Large Genomes Using Long Erroneous Reads\n  of the Third Generation Sequencing Technologies",
    "summary": "  (An updated version of this manuscript has been accepted to Scientific\nReports in 2016, please refer to http://www.nature.com/articles/srep31900)\n  The highly anticipated transition from next generation sequencing (NGS) to\nthird generation sequencing (3GS) has been difficult primarily due to high\nerror rates and excessive sequencing cost. The high error rates make the\nassembly of long erroneous reads of large genomes challenging because existing\nsoftware solutions are often overwhelmed by error correction tasks. Here we\nreport a hybrid assembly approach that simultaneously utilizes NGS and 3GS data\nto address both issues. We gain advantages from three general and basic design\nprinciples: (i) Compact representation of the long reads lead to efficient\nalignments. (ii) Base-level errors can be skipped; structural errors need to be\ndetected and corrected. (iii) Structurally correct 3GS reads are assembled and\npolished. In our implementation, preassembled NGS contigs are used to derive\nthe compact representation of the long reads, which established an algorithmic\nconversion from a de Bruijn graph to an overlap graph, the two major assembly\nparadigms. Moreover, since NGS and 3GS data can compensate each other, our\nhybrid assembly approach reduces both of their sequencing requirements.\nExperiments show that our software is able to assemble mammalian-sized genomes\norders of magnitude more efficiently in time than existing methods, while\nsaving about half of the sequencing cost.\n"
  },
  {
    "year": 2014,
    "title": "On the Structure of the Initiation and Elongation Rates that Maximize\n  Protein Production in the Ribosome Flow Model",
    "summary": "  Translation is a crucial step in gene expression. During translation,\nmacromolecules called ribosomes \"read\" the mRNA strand in a sequential manner\nand produce a corresponding protein. Translation is known to consume most of\nthe cell's energy. Maximizing the protein production rate in mRNA translation,\nsubject to the bounded biomolecullar budget, is thus an important problem in\nboth biology and biotechnology. We consider this problem using a mathematical\nmodel for mRNA translation called the ribosome flow model (RFM). For an mRNA\nstrand with $n$ sites the RFM includes $n$ state-variables that encode the\nnormalized ribosomal density at each site, and $n+1$ positive parameters: the\ninitiation rate and elongation rates along the chain. An affine constraint on\nthese rates is used to model the bounded cellular budget. We show that for a\nhomogeneous constraint the rates that maximize the steady-state protein\nproduction rate have a special structure. They are symmetric with respect to\nthe middle of the chain, and monotonically increase as we move towards the\ncenter of the chain. The ribosomal densities corresponding to the optimal rates\nmonotonically decrease along the chain. We discuss some of the biological\nimplications of these results.\n"
  },
  {
    "year": 2014,
    "title": "DNA methylation variation in Arabidopsis has a genetic basis and shows\n  evidence of local adaptation",
    "summary": "  Epigenome modulation in response to the environment potentially provides a\nmechanism for organisms to adapt, both within and between generations. However,\nneither the extent to which this occurs, nor the molecular mechanisms involved\nare known. Here we investigate DNA methylation variation in Swedish Arabidopsis\nthaliana accessions grown at two different temperatures. Environmental effects\non DNA methylation were limited to transposons, where CHH methylation was found\nto increase with temperature. Genome-wide association mapping revealed that the\nextensive CHH methylation variation was strongly associated with genetic\nvariants in both cis and trans, including a major trans-association close to\nthe DNA methyltransferase CMT2. Unlike CHH methylation, CpG gene body\nmethylation (GBM) on the coding region of genes was not affected by growth\ntemperature, but was instead strongly correlated with the latitude of origin.\nAccessions from colder regions had higher levels of GBM for a significant\nfraction of the genome, and this was correlated with elevated transcription\nlevels for the genes affected. Genome-wide association mapping revealed that\nthis effect was largely due to trans-acting loci, a significant fraction of\nwhich showed evidence of local adaptation. These findings constitute the first\ndirect link between DNA methylation and adaptation to the environment, and\nprovide a basis for further dissecting how environmentally driven and\ngenetically determined epigenetic variation interact and influence organismal\nfitness.\n"
  },
  {
    "year": 2014,
    "title": "Assembly of repetitive regions using next-generation sequencing data",
    "summary": "  High read depth can be used to assemble short sequence repeats. The existing\ngenome assemblers fail in repetitive regions of longer than average read.\n  I propose a new algorithm for a DNA assembly which uses the relative\nfrequency of reads to properly reconstruct repetitive sequences. The\nmathematical model shows the upper limits of accuracy of the results as a\nfunction of read coverage. For high coverage, the estimation error depends\nlinearly on repetitive sequence length and inversely proportional to the\nsequencing coverage. The algorithm requires high read depth, provided by the\nnext-generation sequencers and could use the existing data. The tests on\nerrorless reads, generated in silico from several model genomes, pointed the\nproperly reconstructed repetitive sequences, where existing assemblers fail.\n"
  },
  {
    "year": 2014,
    "title": "Analysis of correlation structures in the Synechocystis PCC6803 genome",
    "summary": "  Transfer of nucleotide strings in the Synechocystis sp. PCC6803 genome is\ninvestigated to exhibit periodic and non-periodic correlation structures by\nusing the recurrence plot method and the phase space reconstruction technique.\nThe periodic correlation structures are generated by periodic transfer of\nseveral substrings in long periodic or non-periodic nucleotide strings embedded\nin the coding regions of genes. The non-periodic correlation structures are\ngenerated by non-periodic transfer of several substrings covering or\noverlapping with the coding regions of genes. In the periodic and non-periodic\ntransfer, some gaps divide the long nucleotide strings into the substrings and\nprevent their global transfer. Most of the gaps are either the replacement of\none base or the insertion/reduction of one base. In the reconstructed phase\nspace, the points generated from two or three steps for the continuous\niterative transfer via the second maximal distance can be fitted by two lines.\nIt partly reveals an intrinsic dynamics in the transfer of nucleotide strings.\nDue to the comparison of the relative positions and lengths, the substrings\nconcerned with the non-periodic correlation structures are almost identical to\nthe mobile elements annotated in the genome. The mobile elements are thus\nendowed with the basic results on the correlation structures.\n"
  },
  {
    "year": 2014,
    "title": "Estimation of the methylation pattern distribution from deep sequencing\n  data",
    "summary": "  Motivation: Bisulphite sequencing enables the detection of cytosine\nmethylation. The sequence of the methylation states of cytosines on any given\nread forms a methylation pattern that carries substantially more information\nthan merely studying the average methylation level at individual positions. In\norder to understand better the complexity of DNA methylation landscapes in\nbiological samples, it is important to study the diversity of these methylation\npatterns. However, the accurate quantification of methylation patterns is\nsubject to sequencing errors and spurious signals due to incomplete bisulphite\nconversion of cytosines. Results: A statistical model is developed which\naccounts for the distribution of DNA methylation patterns at any given locus.\nThe model incorporates the effects of sequencing errors and spurious reads, and\nenables estimation of the true underlying distribution of methylation patterns.\nConclusions: Calculation of the estimated distribution over methylation\npatterns is implemented in the R Bioconductor package MPFE. Source code and\ndocumentation of the package are also available for download at\nhttp://bioconductor.org/packages/3.0/bioc/html/MPFE.html.\n"
  },
  {
    "year": 2015,
    "title": "An Event-Driven Approach for Studying Gene Block Evolution in Bacteria",
    "summary": "  Motivation: Gene blocks are genes co-located on the chromosome. In many\ncases, genes blocks are conserved between bacterial species, sometimes as\noperons, when genes are co-transcribed. The conservation is rarely absolute:\ngene loss, gain, duplication, block splitting, and block fusion are frequently\nobserved. An open question in bacterial molecular evolution is that of the\nformation and breakup of gene blocks, for which several models have been\nproposed. These models, however, are not generally applicable to all types of\ngene blocks, and consequently cannot be used to broadly compare and study gene\nblock evolution. To address this problem we introduce an event-based method for\ntracking gene block evolution in bacteria. Results: We show here that the\nevolution of gene blocks in proteobacteria can be described by a small set of\nevents. Those include the insertion of genes into, or the splitting of genes\nout of a gene block, gene loss, and gene duplication. We show how the\nevent-based method of gene block evolution allows us to determine the\nevolutionary rate, and to trace the ancestral states of their formation. We\nconclude that the event-based method can be used to help us understand the\nformation of these important bacterial genomic structures. Availability: The\nsoftware is available under GPLv3 license on\nhttp://github.com/reamdc1/gene_block_evolution.git Supplementary online\nmaterial: http://iddo-friedberg.net/operon-evolution Contact: Iddo Friedberg\ni.friedberg@miamioh.edu\n"
  },
  {
    "year": 2015,
    "title": "On The Organization Of Human T Cell Receptor Loci",
    "summary": "  The human T cell repertoire is generated by the rearrangement of variable\n(V), diversity (D) and joining (J) segments on the T cell receptor (TCR) loci.\nTo determine whether the structural ordering of these gene segments on the TCR\nloci contributes to the observed clonal frequencies, the TCR loci were examined\nfor self-similarity and periodicity in terms of gene segment organization.\nLogarithmic transformation of numeric sequence order demonstrated that the V\nand J gene segments for both T cell receptor alpha (TRA) and beta (TRB) loci\nwere arranged in a self-similar manner when the spacing between adjacent\nsegments was considered as a function of the size of the neighboring gene\nsegment. The ratio of genomic distance between either the J (in TRA) or D (in\nTRB) segments and successive V segments on these loci declined logarithmically.\nAccounting for the gene segments occurring on helical DNA molecules, in a\nlogarithmic distribution, sine and cosine functions of the log transformed\nangular coordinates of the start and stop nucleotides of successive TCR gene\nsegments showed an ordered progression across the locus, supporting a\nlog-periodic organization. T cell clonal frequencies, based on V and J segment\nusage, from three normal stem cell donors plotted against the respective\nsegment locations on TRB locus demonstrated a periodic variation. We\nhypothesize that this quasi-periodic variation in T cell clonal repertoire may\nbe influenced by the location of the gene segments on the logarithmically\nscaled TCR loci. Interactions between the two strands of DNA in the double\nhelix may influence the probability of gene segment usage by means of either\nconstructive or destructive interference resulting from the superposition of\nthe two helices, impacting probability of DNA recombination.\n"
  },
  {
    "year": 2015,
    "title": "Canonical, Stable, General Mapping using Context Schemes",
    "summary": "  Motivation: Sequence mapping is the cornerstone of modern genomics. However,\nmost existing sequence mapping algorithms are insufficiently general.\n  Results: We introduce context schemes: a method that allows the unambiguous\nrecognition of a reference base in a query sequence by testing the query for\nsubstrings from an algorithmically defined set. Context schemes only map when\nthere is a unique best mapping, and define this criterion uniformly for all\nreference bases. Mappings under context schemes can also be made stable, so\nthat extension of the query string (e.g. by increasing read length) will not\nalter the mapping of previously mapped positions. Context schemes are general\nin several senses. They natively support the detection of arbitrary complex,\nnovel rearrangements relative to the reference. They can scale over orders of\nmagnitude in query sequence length. Finally, they are trivially extensible to\nmore complex reference structures, such as graphs, that incorporate additional\nvariation. We demonstrate empirically the existence of high performance context\nschemes, and present efficient context scheme mapping algorithms.\n  Availability and Implementation: The software test framework created for this\nwork is available from\nhttps://registry.hub.docker.com/u/adamnovak/sequence-graphs/.\n  Contact: benedict@soe.ucsc.edu\n  Supplementary Information: Six supplementary figures and one supplementary\nsection are available with the online version of this article.\n"
  },
  {
    "year": 2015,
    "title": "Retrotransposon mobilization in cancer genomes",
    "summary": "  The Cancer Genome Atlas project was initiated by the National Cancer\nInstitute in order to characterize the genomes of hundreds of tumors of various\ncancer types. While much effort has been put into detecting somatic genomic\nvariation in these data, somatic structural variation induced by the activity\nof transposable element insertions has not been reported. Transposable elements\n(TEs) are particularly relevant in cancer in part because of several known\ncases in which a TE insertion is directly linked to cancer formation and\nstudies linking the epigenetic status of retrotransposons to carcinogenesis and\npatient outcome. Additionally, evidence for somatic retrotransposition in\neukaryotic genomes suggests that some tissues and therefore some cancer types\nmay be disposed to increased retrotransposition. We built upon previous work to\ndevelop a highly efficient computational pipeline for the detection of\nnon-reference mobile ele- ment insertions from high-throughput paired-end whole\ngenome sequencing data that is capable of detecting breakpoints through a local\nassembly strategy. Using this, we analyzed 33 whole genome tumor datasets with\npaired normal samples from TCGA across 3 different cancer types: glioblastoma\nmultiforme (GBM), ovarian serous cystoadenocarcinoma (OV) and colorectal ade-\nnocarcinoma (COAD). We detected 72 insertions in colon samples, almost all of\nthem LINE-1 elements, and none in GBM or OV. The amount of somatic\nretrotransposition varies widely between samples with 61 insertions present in\none case. The lack of somatic retrotransposon insertions in GBM and OV samples\nsuggests that TE activity in cancer is restricted to certain cancer types.\n"
  },
  {
    "year": 2015,
    "title": "TREEOME: A framework for epigenetic and transcriptomic data integration\n  to explore regulatory interactions controlling transcription",
    "summary": "  Motivation: Predictive modelling of gene expression is a powerful framework\nfor the in silico exploration of transcriptional regulatory interactions\nthrough the integration of high-throughput -omics data. A major limitation of\nprevious approaches is their inability to handle conditional and synergistic\ninteractions that emerge when collectively analysing genes subject to different\nregulatory mechanisms. This limitation reduces overall predictive power and\nthus the reliability of downstream biological inference.\n  Results: We introduce an analytical modelling framework (TREEOME: tree of\nmodels of expression) that integrates epigenetic and transcriptomic data by\nseparating genes into putative regulatory classes. Current predictive modelling\napproaches have found both DNA methylation and histone modification epigenetic\ndata to provide little or no improvement in accuracy of prediction of\ntranscript abundance despite, for example, distinct anti-correlation between\nmRNA levels and promoter-localised DNA methylation. To improve on this, in\nTREEOME we evaluate four possible methods of formulating gene-level DNA\nmethylation metrics, which provide a foundation for identifying gene-level\nmethylation events and subsequent differential analysis, whereas most previous\ntechniques operate at the level of individual CpG dinucleotides. We demonstrate\nTREEOME by integrating gene-level DNA methylation (bisulfite-seq) and histone\nmodification (ChIP-seq) data to accurately predict genome-wide mRNA transcript\nabundance (RNA-seq) for H1-hESC and GM12878 cell lines.\n  Availability: TREEOME is implemented using open-source software and made\navailable as a pre-configured bootable reference environment. All scripts and\ndata presented in this study are available online at\nhttp://sourceforge.net/projects/budden2015treeome/.\n"
  },
  {
    "year": 2015,
    "title": "Correcting Illumina sequencing errors for human data",
    "summary": "  Summary: We present a new tool to correct sequencing errors in Illumina data\nproduced from high-coverage whole-genome shotgun resequencing. It uses a\nnon-greedy algorithm and shows comparable performance and higher accuracy in an\nevaluation on real human data. This evaluation has the most complete collection\nof high-performance error correctors so far.\n  Availability and implementation: https://github.com/lh3/bfc\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2015,
    "title": "Approximating the Minimum Breakpoint Linearization Problem for Genetic\n  Maps without Gene Strandedness",
    "summary": "  The study of genetic map linearization leads to a combinatorial hard problem,\ncalled the {\\em minimum breakpoint linearization} (MBL) problem. It is aimed at\nfinding a linearization of a partial order which attains the minimum breakpoint\ndistance to a reference total order. The approximation algorithms previously\ndeveloped for the MBL problem are only applicable to genetic maps in which\ngenes or markers are represented as signed integers. However, current genetic\nmapping techniques generally do not specify gene strandedness so that genes can\nonly be represented as unsigned integers. In this paper, we study the MBL\nproblem in the latter more realistic case. An approximation algorithm is thus\ndeveloped, which achieves a ratio of $(m^2+2m-1)$ and runs in $O(n^7)$ time,\nwhere $m$ is the number of genetic maps used to construct the input partial\norder and $n$ the total number of distinct genes in these maps.\n"
  },
  {
    "year": 2015,
    "title": "Phen-Gen: combining phenotype and genotype to analyze rare disorders",
    "summary": "  We introduce Phen-Gen, a method which combines patient disease symptoms and\nsequencing data with prior domain knowledge to identify the causative gene(s)\nfor rare disorders.\n"
  },
  {
    "year": 2015,
    "title": "An investigation into inter- and intragenomic variations of graphic\n  genomic signatures",
    "summary": "  We provide, on an extensive dataset and using several different distances,\nconfirmation of the hypothesis that CGR patterns are preserved along a genomic\nDNA sequence, and are different for DNA sequences originating from genomes of\ndifferent species. This finding lends support to the theory that CGRs of\ngenomic sequences can act as graphic genomic signatures. In particular, we\ncompare the CGR patterns of over five hundred different 150,000 bp genomic\nsequences originating from the genomes of six organisms, each belonging to one\nof the kingdoms of life: H. sapiens, S. cerevisiae, A. thaliana, P. falciparum,\nE. coli, and P. furiosus. We also provide preliminary evidence of this method's\napplicability to closely related species by comparing H. sapiens (chromosome\n21) sequences and over one hundred and fifty genomic sequences, also 150,000 bp\nlong, from P. troglodytes (Animalia; chromosome Y), for a total length of more\nthan 101 million basepairs analyzed. We compute pairwise distances between CGRs\nof these genomic sequences using six different distances, and construct\nMolecular Distance Maps that visualize all sequences as points in a\ntwo-dimensional or three-dimensional space, to simultaneously display their\ninterrelationships. Our analysis confirms that CGR patterns of DNA sequences\nfrom the same genome are in general quantitatively similar, while being\ndifferent for DNA sequences from genomes of different species. Our analysis of\nthe performance of the assessed distances uses three different quality measures\nand suggests that several distances outperform the Euclidean distance, which\nhas so far been almost exclusively used for such studies. In particular we show\nthat, for this dataset, DSSIM (Structural Dissimilarity Index) and the\ndescriptor distance (introduced here) are best able to classify genomic\nsequences.\n"
  },
  {
    "year": 2015,
    "title": "HetFHMM: A novel approach to infer tumor heterogeneity using factorial\n  Hidden Markov model",
    "summary": "  Cancer arises from successive rounds of mutations which generate tumor cells\nwith different genomic variation i.e. clones. For drug responsiveness and\ntherapeutics, it is necessary to identify the clones in tumor sample\naccurately. Many methods are developed to infer tumor heterogeneity by either\ncomputing cellular prevalence and tumor phylogeny or predicting genotype of\nmutations. All methods suffer some problems e.g. inaccurate computation of\nclonal frequencies, discarding clone specific genotypes etc. In the paper, we\npropose a method, called- HetFHMM to infer tumor heterogeneity by predicting\nclone specific genotypes and cellular prevalence. To infer clone specific\ngenotype, we consider the presence of multiple mutations at any genomic\nlocation. We also tested our model on different simulated data. The results\nshows that HetFHMM outperforms recent methods which infer tumor heterogeneity.\nTherefore, HetFHMM is a novel approach in tumor heterogeneity research area.\n"
  },
  {
    "year": 2015,
    "title": "Reliable scaling of Position Weight Matrices for binding strength\n  comparisons between transcription factors",
    "summary": "  Scoring DNA sequences against Position Weight Matrices (PWMs) is a widely\nadopted method to identify putative transcription factor binding sites. While\ncommon bioinformatics tools produce scores that can reflect the binding\nstrength between a specific transcription factor and the DNA, these scores are\nnot directly comparable between different transcription factors. Here, we\nprovide two different ways to find the scaling parameter $\\lambda$ that allows\nus to infer binding energy from a PWM score. The first approach uses a PWM and\nbackground genomic sequence as input to estimate $\\lambda$ for a specific\ntranscription factor, which we applied to show that $\\lambda$ distributions for\ndifferent transcription factor families correspond with their DNA binding\nproperties. Our second method can reliably convert $\\lambda$ between different\nPWMs of the same transcription factor, which allows us to directly compare PWMs\nthat were generated by different approaches. These two approaches provide\nconsistent and computationally efficient ways to scale PWMs scores and estimate\ntranscription factor binding sites strength.\n"
  },
  {
    "year": 2015,
    "title": "The \"Giant Virus Finder\" Discovers an Abundance of Giant Viruses in the\n  Antarctic Dry Valleys",
    "summary": "  The first giant virus was identified in 2003 from a biofilm of an industrial\nwater-cooling tower in England. Later, numerous new giant viruses were found in\noceans and freshwater habitats, some of them having even 2,500 genes. We have\ndemonstrated their very likely presence in four soil samples taken from the\nKutch Desert (Gujarat, India). Here we describe a bioinformatics work-flow,\ncalled the \"Giant Virus Finder\" that is capable to discover the very likely\npresence of the genomes of giant viruses in metagenomic shotgun-sequenced\ndatasets. The new tool is applied to numerous hot and cold desert soil samples\nas well as some tundra- and forest soils. We show that most of these samples\ncontain giant viruses, and especially many were found in the Antarctic dry\nvalleys. The results imply that giant viruses could be frequent not only in\naqueous habitats, but in a wide spectrum of soils on our planet.\n"
  },
  {
    "year": 2015,
    "title": "Long and short range multi-locus QTL interactions in a complex trait of\n  yeast",
    "summary": "  We analyse interactions of Quantitative Trait Loci (QTL) in heat selected\nyeast by comparing them to an unselected pool of random individuals. Here we\nre-examine data on individual F12 progeny selected for heat tolerance, which\nhave been genotyped at 25 locations identified by sequencing a selected pool\n[Parts, L., Cubillos, F. A., Warringer, J., Jain, K., Salinas, F., Bumpstead,\nS. J., Molin, M., Zia, A., Simpson, J. T., Quail, M. A., Moses, A., Louis, E.\nJ., Durbin, R., and Liti, G. (2011). Genome research, 21(7), 1131-1138]. 960\nindividuals were genotyped at these locations and multi-locus genotype\nfrequencies were compared to 172 sequenced individuals from the original\nunselected pool (a control group). Various non-random associations were found\nacross the genome, both within chromosomes and between chromosomes. Some of the\nnon-random associations are likely due to retention of linkage disequilibrium\nin the F12 population, however many, including the inter-chromosomal\ninteractions, must be due to genetic interactions in heat tolerance. One region\nof particular interest involves 3 linked loci on chromosome IV where the\ncentral variant responsible for heat tolerance is antagonistic, coming from the\nheat sensitive parent and the flanking ones are from the more heat tolerant\nparent. The 3-locus haplotypes in the selected individuals represent a highly\nbiased sample of the population haplotypes with rare double recombinants in\nhigh frequency. These were missed in the original analysis and would never be\nseen without the multigenerational approach. We show that a statistical\nanalysis of entropy and information gain in genotypes of a selected population\ncan reveal further interactions than previously seen. Importantly this must be\ndone in comparison to the unselected population's genotypes to account for\ninherent biases in the original population.\n"
  },
  {
    "year": 2015,
    "title": "Hox genes underlie metazoan development, but what controls them?",
    "summary": "  Although metazoan development is conceived as resulting from gene regulatory\nnetworks (GRNs) controlled by Hox genes, a better analogy is computer\narchitecture: i.e., a task accomplished in sequential steps linked to an\nexternal referent that \"counts\" each step. A developmental \"step\" equals the\nexpression of genes in specific cells at specific times and telomeres represent\nexternal \"counters\" wherein \"counting\" is a function of telomere shortening at\neach cell division that permits the sequential expression of Hox genes and,\nultimately, complex form. Metazoan development thus best resembles a Turing\nmachine, which could be used to model the development of any metazoan.\n"
  },
  {
    "year": 2015,
    "title": "RNF: a general framework to evaluate NGS read mappers",
    "summary": "  Aligning reads to a reference sequence is a fundamental step in numerous\nbioinformatics pipelines. As a consequence, the sensitivity and precision of\nthe mapping tool, applied with certain parameters to certain data, can\ncritically affect the accuracy of produced results (e.g., in variant calling\napplications). Therefore, there has been an increasing demand of methods for\ncomparing mappers and for measuring effects of their parameters.\n  Read simulators combined with alignment evaluation tools provide the most\nstraightforward way to evaluate and compare mappers. Simulation of reads is\naccompanied by information about their positions in the source genome. This\ninformation is then used to evaluate alignments produced by the mapper.\nFinally, reports containing statistics of successful read alignments are\ncreated.\n  In default of standards for encoding read origins, every evaluation tool has\nto be made explicitly compatible with the simulator used to generate reads. In\norder to solve this obstacle, we have created a generic format RNF (Read Naming\nFormat) for assigning read names with encoded information about original\npositions.\n  Futhermore, we have developed an associated software package RNF containing\ntwo principal components. MIShmash applies one of popular read simulating tools\n(among DwgSim, Art, Mason, CuReSim etc.) and transforms the generated reads\ninto RNF format. LAVEnder evaluates then a given read mapper using simulated\nreads in RNF format. A special attention is payed to mapping qualities that\nserve for parametrization of ROC curves, and to evaluation of the effect of\nread sample contamination.\n"
  },
  {
    "year": 2015,
    "title": "Inference of Markovian Properties of Molecular Sequences from NGS Data\n  and Applications to Comparative Genomics",
    "summary": "  Next Generation Sequencing (NGS) technologies generate large amounts of short\nread data for many different organisms. The fact that NGS reads are generally\nshort makes it challenging to assemble the reads and reconstruct the original\ngenome sequence. For clustering genomes using such NGS data, word-count based\nalignment-free sequence comparison is a promising approach, but for this\napproach, the underlying expected word counts are essential.\n  A plausible model for this underlying distribution of word counts is given\nthrough modelling the DNA sequence as a Markov chain (MC). For single long\nsequences, efficient statistics are available to estimate the order of MCs and\nthe transition probability matrix for the sequences. As NGS data do not provide\na single long sequence, inference methods on Markovian properties of sequences\nbased on single long sequences cannot be directly used for NGS short read data.\n  Here we derive a normal approximation for such word counts. We also show that\nthe traditional Chi-square statistic has an approximate gamma distribution,\nusing the Lander-Waterman model for physical mapping. We propose several\nmethods to estimate the order of the MC based on NGS reads and evaluate them\nusing simulations. We illustrate the applications of our results by clustering\ngenomic sequences of several vertebrate and tree species based on NGS reads\nusing alignment-free sequence dissimilarity measures. We find that the\nestimated order of the MC has a considerable effect on the clustering results,\nand that the clustering results that use a MC of the estimated order give a\nplausible clustering of the species.\n"
  },
  {
    "year": 2015,
    "title": "PopIns: population-scale detection of novel sequence insertions",
    "summary": "  The detection of genomic structural variation (SV) has advanced tremendously\nin recent years due to progress in high-throughput sequencing technologies.\nNovel sequence insertions, insertions without similarity to a human reference\ngenome, have received less attention than other types of SVs due to the\ncomputational challenges in their detection from short read sequencing data,\nwhich inherently involves de novo assembly. De novo assembly is not only\ncomputationally challenging, but also requires high-quality data. While the\nreads from a single individual may not always meet this requirement, using\nreads from multiple individuals can increase power to detect novel insertions.\nWe have developed the program PopIns, which can discover and characterize\nnon-reference insertions of 100 bp or longer on a population scale. In this\npaper, we describe the approach we implemented in PopIns. It takes as input a\nreads-to-reference alignment, assembles unaligned reads using a standard\nassembly tool, merges the contigs of different individuals into high-confidence\nsequences, anchors the merged sequences into the reference genome, and finally\ngenotypes all individuals for the discovered insertions. Our tests on simulated\ndata indicate that the merging step greatly improves the quality and\nreliability of predicted insertions and that PopIns shows significantly better\nrecall and precision than the recent tool MindTheGap. Preliminary results on a\ndata set of 305 Icelanders demonstrate the practicality of the new approach.\nThe source code of PopIns is available from http://github.com/bkehr/popins.\n"
  },
  {
    "year": 2015,
    "title": "M\u00e1s all\u00e1 del GWAS: alternativas para localizar QTLs",
    "summary": "  Beyond GWAS: alternatives to localize QTLs in farm animals. Two methods that\ncould be used for QTL mapping as alternatives to standard GWAS are presented.\nThe first relies on the differential frequency of runs of homozygosity (ROH) in\ngroups of animals (e.g. cases and controls), while the second stems from\nresampling techniques used for the prediction of carriers of a mutation, and is\nbased on the frequency of inclusion of polymorphisms (SNP) in the predictive\nmodel. ROH were applied to the detection of reproductive diseases in\nHolstein-Friesian cattle, while resampling was applied to the detection of\ncarriers of the BH2 haplotype in Brown Swiss cattle. These alternative\napproaches may complement GWAS analyses in localizing more accurately QTLs for\ntraits of interest in livestock.\n"
  },
  {
    "year": 2015,
    "title": "Computational reconstruction of mitochondria-encoded mammal ancestral\n  proteins",
    "summary": "  A method based on mapping a symbolic sequence into a set of patterns (strings\nresulting from the sequence parsing) is proposed as a tool for the\nreconstruction of ancestral sequences. The set union of patterns comprises all\nthe patterns present in the family of related proteins sequences of an extant\nspecies. The set of most frequent patterns among protein sequences is selected\nand concatenated. The resulting sequence of amino acids is supposed to be the\nancestral protein of the family. No sequences alignment and phylogenetic tree\nof the species family are necessary. The method is used for inferring the\nancestral amino acid sequences of thirteen mitochondria-encoded protein\nfamilies of mammal species. Statistical distribution of the similarity between\nextant and ancestral sequences exhibits some structures related to\nenvironmental changes in the past.\n"
  },
  {
    "year": 2015,
    "title": "Triander: A new program for visual analysis of nucleotide sequences",
    "summary": "  The Triander program is an interactive software package for nucleotide\nsequence visualization. The program was developed using the freeware Pascal RAD\nIDE Lazarus, and its source code and binaries compiled for Windows are freely\naccessible at http://www.icbge.org.ua/eng/Triander . Triander can produce four\ntypes of plots. It is possible to build three DNA walks independently for each\nnucleotide position in triplets. The use of nucleotide vectors with unequal\nmodulus leads to a significant reduction in the visual information lost in DNA\nwalks. The program can be used in the investigation of the fine structure of\nsequences, to find standard patterns in them and to locate nontrivial regions\nfor further detailed analysis.\n"
  },
  {
    "year": 2015,
    "title": "Exploring genetic variation in the tomato (Solanum section Lycopersicon)\n  clade by whole-genome sequencing",
    "summary": "  Genetic variation in the tomato clade was explored by sequencing a selection\nof 84 tomato accessions and related wild species representative for the\nLycopersicon, Arcanum, Eriopersicon, and Neolycopersicon groups. We present a\nreconstruction of three new reference genomes in support of our comparative\ngenome analyses. Sequence diversity in commercial breeding lines appears\nextremely low, indicating the dramatic genetic erosion of crop tomatoes. This\nis reflected by the SNP count in wild species which can exceed 10 million i.e.\n20 fold higher than in crop accessions. Comparative sequence alignment reveals\ngroup, species, and accession specific polymorphisms, which explain\ncharacteristic fruit traits and growth habits in tomato accessions. Using gene\nmodels from the annotated Heinz reference genome, we observe a bias in dN/dS\nratio in fruit and growth diversification genes compared to a random set of\ngenes, which probably is the result of a positive selection. We detected highly\ndivergent segments in wild S. lycopersicum species, and footprints of\nintrogressions in crop accessions originating from a common donor accession.\nPhylogenetic relationships of fruit diversification and growth specific genes\nfrom crop accessions show incomplete resolution and are dependent on the\nintrogression donor. In contrast, whole genome SNP information has sufficient\npower to resolve the phylogenetic placement of each accession in the four main\ngroups in the Lycopersicon clade using Maximum Likelihood analyses.\nPhylogenetic relationships appear correlated with habitat and mating type and\npoint to the occurrence of geographical races within these groups and thus are\nof practical importance for introgressive hybridization breeding. Our study\nillustrates the need for multiple reference genomes in support of tomato\ncomparative genomics and Solanum genome evolution studies.\n"
  },
  {
    "year": 2015,
    "title": "Introgression Browser: High throughput whole-genome SNP visualization",
    "summary": "  Breeding by introgressive hybridization is a pivotal strategy to broaden the\ngenetic basis of crops. Usually, the desired traits are monitored in\nconsecutive crossing generations by marker-assisted selection, but their\nanalyses fail in chromosome regions where crossover recombinants are rare or\nnot viable. Here, we present the Introgression Browser (IBROWSER), a novel\nbioinformatics tool aimed at visualizing introgressions at nucleotide or SNP\naccuracy. The software selects homozygous SNPs from Variant Call Format (VCF)\ninformation and filters out heterozygous SNPs, Multi-Nucleotide Polymorphisms\n(MNPs) and insertion-deletions (InDels). For data analysis IBROWSER makes use\nof sliding windows, but if needed it can generate any desired fragmentation\npattern through General Feature Format (GFF) information. In an example of\ntomato (Solanum lycopersicum) accessions we visualize SNP patterns and\nelucidate both position and boundaries of the introgressions. We also show that\nour tool is capable of identifying alien DNA in a panel of the closely related\nS. pimpinellifolium by examining phylogenetic relationships of the introgressed\nsegments in tomato. In a third example, we demonstrate the power of the\nIBROWSER in a panel of 600 Arabidopsis accessions, detecting the boundaries of\na SNP-free region around a polymorphic 1.17 Mbp inverted segment on the short\narm of chromosome 4. The architecture and functionality of IBROWSER makes the\nsoftware appropriate for a broad set of analyses including SNP mining, genome\nstructure analysis, and pedigree analysis. Its functionality, together with the\ncapability to process large data sets and efficient visualization of sequence\nvariation, makes IBROWSER a valuable breeding tool.\n"
  },
  {
    "year": 2015,
    "title": "FermiKit: assembly-based variant calling for Illumina resequencing data",
    "summary": "  Summary: FermiKit is a variant calling pipeline for Illumina data. It de novo\nassembles short reads and then maps the assembly against a reference genome to\ncall SNPs, short insertions/deletions (INDELs) and structural variations (SVs).\nFermiKit takes about one day to assemble 30-fold human whole-genome data on a\nmodern 16-core server with 85GB RAM at the peak, and calls variants in half an\nhour to an accuracy comparable to the current practice. FermiKit assembly is a\nreduced representation of raw data while retaining most of the original\ninformation.\n  Availability and implementation: https://github.com/lh3/fermikit\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2015,
    "title": "benchNGS : An approach to benchmark short reads alignment tools",
    "summary": "  In the last decade a number of algorithms and associated software have been\ndeveloped to align next generation sequencing (NGS) reads with relevant\nreference genomes. The accuracy of these programs may vary significantly,\nespecially when the NGS reads are quite different from the available reference\ngenome. We propose a benchmark to assess accuracy of short reads mapping based\non the pre-computed global alignment of related genome sequences.\n  In this paper we propose a benchmark to assess accuracy of the short reads\nmapping based on the pre-computed global alignment of closely related genome\nsequences. We outline the method and also present a short report of an\nexperiment performed on five popular alignment tools based on the pairwise\nalignments of Escherichia coli O6 CFT073 genome with genomes of seven other\nbacteria.\n"
  },
  {
    "year": 2015,
    "title": "Nucleotide 9-mers Characterize the Type II Diabetic Gut Metagenome",
    "summary": "  Discoveries of new biomarkers for frequently occurring diseases are of\nspecial importance in today's medicine. While fully developed type II diabetes\n(T2D) can be detected easily, the early identification of high risk individuals\nis an area of interest in T2D, too. Metagenomic analysis of the human bacterial\nflora has shown subtle changes in diabetic patients, but no specific microbes\nare known to cause or promote the disease. Moderate changes were also detected\nin the microbial gene composition of the metagenomes of diabetic patients, but\nagain, no specific gene was found that is present in disease-related and\nmissing in healthy metagenome. However, these fine differences in microbial\ntaxon- and gene composition are difficult to apply as quantitative biomarkers\nfor diagnosing or predicting type II diabetes. In the present work we report\nsome nucleotide 9-mers with significantly differing frequencies in diabetic and\nhealthy intestinal flora. To our knowledge, it is the first time such short DNA\nfragments have been associated with T2D. The automated, quantitative analysis\nof the frequencies of short nucleotide sequences seems to be more feasible than\naccurate phylogenetic and functional analysis, and thus it might be a promising\ndirection of diagnostic research.\n"
  },
  {
    "year": 2015,
    "title": "Statistical models for RNA-seq data derived from a two-condition\n  48-replicate experiment",
    "summary": "  High-throughput RNA sequencing (RNA-seq) is now the standard method to\ndetermine differential gene expression. Identifying differentially expressed\ngenes crucially depends on estimates of read count variability. These estimates\nare typically based on statistical models such as the negative binomial\ndistribution, which is employed by the tools edgeR, DESeq and cuffdiff. Until\nnow, the validity of these models has usually been tested on either\nlow-replicate RNA-seq data or simulations. A 48-replicate RNA-seq experiment in\nyeast was performed and data tested against theoretical models. The observed\ngene read counts were consistent with both log-normal and negative binomial\ndistributions, while the mean-variance relation followed the line of constant\ndispersion parameter of ~0.01. The high-replicate data also allowed for strict\nquality control and screening of bad replicates, which can drastically affect\nthe gene read-count distribution. RNA-seq data have been submitted to ENA\narchive with project ID PRJEB5348.\n"
  },
  {
    "year": 2015,
    "title": "Evaluation of tools for differential gene expression analysis by RNA-seq\n  on a 48 biological replicate experiment",
    "summary": "  An RNA-seq experiment with 48 biological replicates in each of 2 conditions\nwas performed to determine the number of biological replicates ($n_r$)\nrequired, and to identify the most effective statistical analysis tools for\nidentifying differential gene expression (DGE). When $n_r=3$, seven of the nine\ntools evaluated give true positive rates (TPR) of only 20 to 40 percent. For\nhigh fold-change genes ($|log_{2}(FC)|\\gt2$) the TPR is $\\gt85$ percent. Two\ntools performed poorly; over- or under-predicting the number of differentially\nexpressed genes. Increasing replication gives a large increase in TPR when\nconsidering all DE genes but only a small increase for high fold-change genes.\nAchieving a TPR $\\gt85$% across all fold-changes requires $n_r\\gt20$. For\nfuture RNA-seq experiments these results suggest $n_r\\gt6$, rising to\n$n_r\\gt12$ when identifying DGE irrespective of fold-change is important. For\n$6 \\lt n_r \\lt 12$, superior TPR makes edgeR the leading tool tested. For $n_r\n\\ge12$, minimizing false positives is more important and DESeq outperforms the\nother tools.\n"
  },
  {
    "year": 2015,
    "title": "BACOM2: a Java tool for detecting normal cell contamination of copy\n  number in heterogeneous tumor",
    "summary": "  We develop a cross-platform open-source Java application (BACOM2) with\ngraphic user interface (GUI), and users also can use a XML file to set the\nparameters of algorithm model, file paths and the dataset of paired samples.\nBACOM2 implements the new entire pipeline of copy number change analysis for\nheterogeneous cancer tissues, including extraction of raw copy number signals\nfrom CEL files of paired samples, attenuation correction, identification of\nbalanced AB-genotype loci, copy number detection and segmentation, global\nbaseline calculation and absolute normalization, differentiation of deletion\ntypes, estimation of the normal tissue fraction and correction of normal tissue\ncontamination. BACOM2 focuses on the common tools for data preparation and\nabsolute normalization for copy number analysis of heterogeneous cancer\ntissues. The software provides an additional choice for scientists who require\na user-friendly, high-speed processing, cross-platform computing environment\nfor large copy number data analysis.\n"
  },
  {
    "year": 2015,
    "title": "Dynamics of Wolbachia pipientis gene expression across the Drosophila\n  melanogaster life cycle",
    "summary": "  Symbiotic interactions between microbes and their multicellular hosts have\nmanifold impacts on molecular, cellular and organismal biology. To identify\ncandidate bacterial genes involved in maintaining endosymbiotic associations\nwith insect hosts, we analyzed genome-wide patterns of gene expression in the\nalpha-proteobacteria Wolbachia pipientis across the life cycle of Drosophila\nmelanogaster using public data from the modENCODE project that was generated in\na Wolbachia-infected version of the ISO1 reference strain. We find that the\nmajority of Wolbachia genes are expressed at detectable levels in D.\nmelanogaster across the entire life cycle, but that only 7.8% of 1195 Wolbachia\ngenes exhibit robust stage- or sex-specific expression differences when studied\nin the \"holo-organism\" context. Wolbachia genes that are differentially\nexpressed during development are typically up-regulated after D. melanogaster\nembryogenesis, and include many bacterial membrane, secretion system and\nankyrin-repeat containing proteins. Sex-biased genes are often organised as\nsmall operons of uncharacterised genes and are mainly up-regulated in adult\nmales D. melanogaster in an age-dependent manner suggesting a potential role in\ncytoplasmic incompatibility. Our results indicate that large changes in\nWolbachia gene expression across the Drosophila life-cycle are relatively rare\nwhen assayed across all host tissues, but that candidate genes to understand\nhost-microbe interaction in facultative endosymbionts can be successfully\nidentified using holo-organism expression profiling. Our work also shows that\nmining public gene expression data in D. melanogaster provides a rich set of\nresources to probe the functional basis of the Wolbachia-Drosophila symbiosis\nand annotate the transcriptional outputs of the Wolbachia genome.\n"
  },
  {
    "year": 2015,
    "title": "Chromosomal rearrangements as barriers to genetic homogenization between\n  archaic and modern humans",
    "summary": "  Chromosomal rearrangements, which shuffle DNA throughout the genome, are an\nimportant source of divergence across taxa. Using a paired-end read approach\nwith Illumina sequence data for archaic humans, I identify changes in genome\nstructure that occurred recently in human evolution. Hundreds of rearrangements\nindicate genomic trafficking between the sex chromosomes and autosomes, raising\nthe possibility of sex-specific changes. Additionally, genes adjacent to genome\nstructure changes in Neanderthals are associated with testis-specific\nexpression, consistent with evolutionary theory that new genes commonly form\nwith expression in the testes. I identify one case of new-gene creation through\ntransposition from the Y chromosome to chromosome 10 that combines the 5' end\nof the testis-specific gene Fank1 with previously untranscribed sequence. This\nnew transcript experienced copy number expansion in archaic genomes, indicating\nrapid genomic change. Among rearrangements identified in Neanderthals, 13% are\ntransposition of selfish genetic elements, while 32% appear to be ectopic\nexchange between repeats. In Denisovan, the pattern is similar but numbers are\nsignificantly higher with 18% of rearrangements reflecting transposition and\n40% ectopic exchange between distantly related repeats. There is an excess of\ndivergent rearrangements relative to polymorphism in Denisovan, which might\nresult from non-uniform rates of mutation, possibly reflecting a burst of TE\nactivity in the lineage that led to Denisovan. Finally, loci containing genome\nstructure changes show diminished rates of introgression from Neanderthals into\nmodern humans, consistent with the hypothesis that rearrangements serve as\nbarriers to gene flow during hybridization. Together, these results suggest\nthat this previously unidentified source of genomic variation has important\nbiological consequences in human evolution.\n"
  },
  {
    "year": 2015,
    "title": "Genetically Improved BarraCUDA",
    "summary": "  BarraCUDA is a C program which uses the BWA algorithm in parallel with nVidia\nCUDA to align short next generation DNA sequences against a reference genome.\nThe genetically improved (GI) code is up to three times faster on short paired\nend reads from The 1000 Genomes Project and 60percent more accurate on a short\nBioPlanet.com GCAT alignment benchmark. GPGPU Barracuda running on a single K80\nTesla GPU can align short paired end nextgen sequences up to ten times faster\nthan bwa on a 12 core CPU.\n"
  },
  {
    "year": 2015,
    "title": "Parenclitic network analysis of methylation data for cancer\n  identification",
    "summary": "  We make use of ideas from the theory of complex networks to implement a\nmachine learning classification of human DNA methylation data, that carry\nsignatures of cancer development. The data were obtained from patients with\nvarious kinds of cancers and represented as parenclictic networks, wherein\nnodes correspond to genes, and edges are weighted according to pairwise\nvariation from control group subjects. We demonstrate that for the $10$ types\nof cancer under study, it is possible to obtain a high performance of binary\nclassification between cancer-positive and negative samples based on network\nmeasures. Remarkably, an accuracy as high as $93-99\\%$ is achieved with only\n$12$ network topology indices, in a dramatic reduction of complexity from the\noriginal $15295$ gene methylation levels. Moreover, it was found that the\nparenclictic networks are scale-free in cancer-negative subjects, and deviate\nfrom the power-law node degree distribution in cancer. The node centrality\nranking and arising modular structure could provide insights into the systems\nbiology of cancer.\n"
  },
  {
    "year": 2015,
    "title": "Bermuda: Bidirectional de novo assembly of transcripts with new insights\n  for handling uneven coverage",
    "summary": "  Motivation: RNA-seq has made feasible the analysis of a whole set of\nexpressed mRNAs. Mapping-based assembly of RNA-seq reads sometimes is\ninfeasible due to lack of high-quality references. However, de novo assembly is\nvery challenging due to uneven expression levels among transcripts and also the\nread coverage variation within a single transcript. Existing methods either\napply de Bruijn graphs of single-sized k-mers to assemble the full set of\ntranscripts, or conduct multiple runs of assembly, but still apply graphs of\nsingle-sized k-mers at each run. However, a single k-mer size is not suitable\nfor all the regions of the transcripts with varied coverage. Contribution: This\npaper presents a de novo assembler Bermuda with new insights for handling\nuneven coverage. Opposed to existing methods that use a single k-mer size for\nall the transcripts in each run of assembly, Bermuda self-adaptively uses a few\nk-mer sizes to assemble different regions of a single transcript according to\ntheir local coverage. As such, Bermuda can deal with uneven expression levels\nand coverage not only among transcripts, but also within a single transcript.\nExtensive tests show that Bermuda outperforms popular de novo assemblers in\nreconstructing unevenly-expressed transcripts with longer length, better\ncontiguity and lower redundancy. Further, Bermuda is computationally efficient\nwith moderate memory consumption.\n"
  },
  {
    "year": 2015,
    "title": "BGT: efficient and flexible genotype query across many samples",
    "summary": "  Summary: BGT is a compact format, a fast command line tool and a simple web\napplication for efficient and convenient query of whole-genome genotypes and\nfrequencies across tens to hundreds of thousands of samples. On real data, it\nencodes the haplotypes of 32,488 samples across 39.2 million SNPs into a 7.4GB\ndatabase and decodes a couple of hundred million genotypes per CPU second. The\nhigh performance enables real-time responses to complex queries.\n  Availability and implementation: https://github.com/lh3/bgt\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2015,
    "title": "MicroRNAs -- targeting and target prediction",
    "summary": "  MicroRNAs (miRNAs) are a class of small noncoding RNAs that can regulate many\ngenes by base pairing to sites in mRNAs. The functionality of miRNAs overlaps\nthat of short interfering RNAs (siRNAs), and many features of miRNA targeting\nhave been revealed experimentally by studying miRNA-mimicking siRNAs. This\nreview outlines the features associated with animal miRNA targeting and\ndescribes currently available prediction tools.\n"
  },
  {
    "year": 2015,
    "title": "On mechanisms of neutral evolution of DNA resulting in scale-free\n  behaviour",
    "summary": "  We introduce a family of models incorporating random segmental substitutions\nand point mutations and demonstrate that such models reproduce algebraic length\ndistributions of exact matches with the slope $-4$ observed earlier in pairwise\ncomparisons of DNA of distantly related species. It is demonstrated that\npower-law distributions of exact matches emerge when shorter sequences transfer\ntheir DNA content to longer sequences, indicating potential mechanisms of the\nincreased genome complexity.\n"
  },
  {
    "year": 2015,
    "title": "Rapidly evolving in humans topologically associating domains",
    "summary": "  Genome-wide proximity placement analysis of 10,598 HSGRL within the context\nof the principal regulatory structures of the interphase chromatin, namely\ntopologically-associating domains (TADs) and specific sub-TAD structures termed\nsuper-enhancer domains (SEDs) revealed that 0.8%-10.3% of TADs contain more\nthan half of HSGRL. Of the 3,127 TADs in the hESC genome, 24 (0.8%); 53 (1.7%);\n259 (8.3%); and 322 (10.3%) harbor 1,110 (52.4%); 1,936 (50.9%); 1,151 (59.6%);\nand 1,601 (58.3%) HSGRL sequences from four distinct families, respectively.\nTADs that are enriched for HSGRL and termed rapidly-evolving in humans TADs\n(revTADs) manifest distinct correlation patterns between HSGRL placements and\nrecombination rates. There are significant enrichment within revTAD boundaries\nof hESC-enhancers, primate-specific CTCF-binding sites, human-specific\nRNAPII-binding sites, hCONDELs, and H3K4me3 peaks with human-specific\nenrichment at TSS in prefrontal cortex neurons (p < 0.0001 in all instances).\nIn hESC genome, 331 of 504 (66%) of SE-harboring TADs contain HSGRL and 68% of\nSEs co-localize with HSGRL, suggesting that HSGRL rewired SE-driven GRNs within\nrevTADs by inserting novel and/or erasing existing regulatory sequences.\nConsequently, markedly distinct features of chromatin structures evolved in\nhESC compared to mouse: the SE quantity is 3-fold higher and the median SE size\nis significantly larger; concomitantly, the TAD number is increased by 42%\nwhile the median TAD size is decreased (p=9.11E-37). Present analyses revealed\na global role for HSGRL in increasing both quantity and size of SEs and\nincreasing the number and size reduction of TADs, which may facilitate a\nconvergence of TAD and SED architectures of interphase chromatin and define a\ntrend of increasing regulatory complexity during evolution of GRNs.\n"
  },
  {
    "year": 2015,
    "title": "GMOL: An Interactive Tool for 3D Genome Structure Visualization",
    "summary": "  It has been shown that genome spatial structures largely affect both genome\nactivity and DNA function. Knowing this, many researchers are currently\nattempting to accurately model genome structures. Despite these increased\nefforts there still exists a shortage of tools dedicated to visualizing the\ngenome. Creating a tool that can accurately visualize the genome can aid\nresearchers by highlighting structural relationships that may not be obvious\nwhen examining the sequence information alone. Here we present a desktop\napplication, known as GMOL, designed to effectively visualize genome tertiary\nstructures at multiple scales so that researchers may better analyze their\ngenomic data. GMOL was developed based upon our multi-scale approach that\nallows a user to zoom in and out between six separate levels within the genome.\nThese six scales are full genome, chromosome, loci, fiber, nucleosome, and\nnucleotide. In order to store the data of the different scales, a new file\nformat, known as GSS, was created. With GMOL, a user can choose any unit at any\nscale and scale it up or down to visualize its structure and retrieve\ncorresponding genome sequences from either Ensembl or a local database. Users\ncan also interactively manipulate and measure the whole genome structure and\nextract static images and machine-readable data files in PDB format from the\nmulti-scale structure. By using GMOL researchers will be able to better\nunderstand and analyze genome structure models and the impact their structural\nrelations have on genome activity and DNA function through GMOLs unique\nfeatures and functions, which includes the multi-scale method that can satisfy\nthe users requirement to not only visualize genome tertiary structure, but also\nmeasure it.\n"
  },
  {
    "year": 2015,
    "title": "Codon Bias Patterns of $E.coli$'s Interacting Proteins",
    "summary": "  Synonymous codons, i.e., DNA nucleotide triplets coding for the same amino\nacid, are used differently across the variety of living organisms. The\nbiological meaning of this phenomenon, known as codon usage bias, is still\ncontroversial. In order to shed light on this point, we propose a new codon\nbias index, $CompAI$, that is based on the competition between cognate and\nnear-cognate tRNAs during translation, without being tuned to the usage bias of\nhighly expressed genes. We perform a genome-wide evaluation of codon bias for\n$E.coli$, comparing $CompAI$ with other widely used indices: $tAI$, $CAI$, and\n$Nc$. We show that $CompAI$ and $tAI$ capture similar information by being\npositively correlated with gene conservation, measured by ERI, and\nessentiality, whereas, $CAI$ and $Nc$ appear to be less sensitive to\nevolutionary-functional parameters. Notably, the rate of variation of $tAI$ and\n$CompAI$ with ERI allows to obtain sets of genes that consistently belong to\nspecific clusters of orthologous genes (COGs). We also investigate the\ncorrelation of codon bias at the genomic level with the network features of\nprotein-protein interactions in $E.coli$. We find that the most densely\nconnected communities of the network share a similar level of codon bias (as\nmeasured by $CompAI$ and $tAI$). Conversely, a small difference in codon bias\nbetween two genes is, statistically, a prerequisite for the corresponding\nproteins to interact. Importantly, among all codon bias indices, $CompAI$ turns\nout to have the most coherent distribution over the communities of the\ninteractome, pointing to the significance of competition among cognate and\nnear-cognate tRNAs for explaining codon usage adaptation.\n"
  },
  {
    "year": 2015,
    "title": "Origins of de novo genes in human and chimpanzee",
    "summary": "  The birth of new genes is an important motor of evolutionary innovation.\nWhereas many new genes arise by gene duplication, others originate at genomic\nregions that do not contain any gene or gene copy. Some of these newly\nexpressed genes may acquire coding or non-coding functions and be preserved by\nnatural selection. However, it is yet unclear which is the prevalence and\nunderlying mechanisms of de novo gene emergence. In order to obtain a\ncomprehensive view of this process we have performed in-depth sequencing of the\ntranscriptomes of four mammalian species, human, chimpanzee, macaque and mouse,\nand subsequently compared the assembled transcripts and the corresponding\nsyntenic genomic regions. This has resulted in the identification of over five\nthousand new transcriptional multiexonic events in human and/or chimpanzee that\nare not observed in the rest of species. By comparative genomics we show that\nthe expression of these transcripts is associated with the gain of regulatory\nmotifs upstream of the transcription start site (TSS) and of U1 snRNP sites\ndownstream of the TSS. We also find that the coding potential of the new genes\nis higher than expected by chance, consistent with the presence of\nprotein-coding genes in the dataset. Using available human tissue proteomics\nand ribosome profiling data we identify several de novo genes with translation\nevidence. These genes show significant purifying selection signatures,\nindicating that they are probably functional. Taken together, the data supports\na model in which frequently-occurring new transcriptional events in the genome\nprovide the raw material for the evolution of new proteins.\n"
  },
  {
    "year": 2015,
    "title": "Automatic learning of pre-miRNAs from different species",
    "summary": "  Discovery of microRNAs (miRNAs) relies on predictive models for\ncharacteristic features from miRNA precursors (pre-miRNAs). The short length of\nmiRNA genes and the lack of pronounced sequence features complicate this task.\nTo accommodate the peculiarities of plant and animal miRNAs systems, tools for\nboth systems have evolved differently. However, these tools are biased towards\nthe species for which they were primarily developed and, consequently, their\npredictive performance on data sets from other species of the same kingdom\nmight be lower. While these biases are intrinsic to the species, the\ncharacterization of their occurrence can lead to computational approaches able\nto diminish their negative effect on the accuracy of pre-miRNAs predictive\nmodels. Here, we investigate in this study how 45 predictive models induced for\ndata sets from 45 species, distributed in eight subphyla, perform when applied\nto a species different from the species used in its induction. Our\ncomputational experiments show that the separability of pre-miRNAs and pseudo\npre-miRNAs instances is species-dependent and no feature set performs well for\nall species, even within the same subphylum. Mitigating this species\ndependency, we show that an ensemble of classifiers reduced the classification\nerrors for all 45 species. As the ensemble members were obtained using\nmeaningful, and yet computationally viable feature sets, the ensembles also\nhave a lower computational cost than individual classifiers that rely on energy\nstability parameters, which are of prohibitive computational cost in large\nscale applications. In this study, the combination of multiple pre-miRNAs\nfeature sets and multiple learning biases enhanced the predictive accuracy of\npre-miRNAs classifiers of 45 species. This is certainly a promising approach to\nbe incorporated in miRNA discovery tools towards more accurate and less\nspecies-dependent tools.\n"
  },
  {
    "year": 2015,
    "title": "Pathway-based feature selection algorithms identify genes discriminating\n  patients with multiple sclerosis apart from controls",
    "summary": "  Introduction The focus of analyzing data from microarray experiments and\nextracting biological insight from such data has experienced a shift from\nidentification of individual genes in association with a phenotype to that of\nbiological pathways or gene sets. Meanwhile, feature selection algorithm\nbecomes imperative to cope with the high dimensional nature of many modeling\ntasks in bioinformatics. Many feature selection algorithms use information\ncontained within a gene set as a biological priori, and select relevant\nfeatures by incorporating such information. Thus, an integration of gene set\nanalysis with feature selection is highly desired. Significance analysis of\nmicroarray to gene-set reduction analysis (SAM-GSR) algorithm is a novel\ndirection of gene set analysis, aiming at further reduction of gene set into a\ncore subset. Here, we explore the feature selection trait possessed by SAM-GSR\nand then modify SAM-GSR specifically to better fulfill this role. Results and\nConclusions Training on a multiple sclerosis (MS) microarray data using both\nSAM-GSR and our modification of SAM-GSR, excellent discriminative performance\non an independent test set was achieved. To conclude, absorbing biological\ninformation from a gene set may be helpful for classification and feature\nselection. Discussion Given the fact the complete pathway information is far\nfrom completeness, a statistical method capable of constructing biologically\nmeaningful gene networks is in demand. The basic requirement is that interplay\namong genes must be taken into account.\n"
  },
  {
    "year": 2015,
    "title": "SCARs: endogenous human stem cell-associated retroviruses and\n  therapy-resistant malignant tumors",
    "summary": "  Discoveries of endogenous human stem cell-associated retroviruses (SCARs)\nrevealed consistent activation of specific endogenous retroviral elements in\nhuman preimplantation embryos and documented the essential role of the\nsustained retroviral activities in the maintenance of pluripotency, functional\nidentity and integrity of naive-state embryonic stem cells, and anti-viral\nresistance of the early-stage human embryos. Activation of specific SCARs,\nnamely LTR7.HERVH and LTR5Hs.HERVK, has been demonstrated in patients diagnosed\nwith multiple types of cancer, autoimmune diseases, neurodegenerative disorders\nand it is likely associated with the emergence of clinically lethal therapy\nresistant death-from-cancer phenotypes in a sub-set of cancer patients\ndiagnosed with different types of malignant tumors.\n"
  },
  {
    "year": 2015,
    "title": "A Model for Competition for Ribosomes in the Cell",
    "summary": "  Large-scale simultaneous mRNA translation and the resulting competition for\nthe available ribosomes has important implications to the cell's functioning\nand evolution. Developing a better understanding of the intricate correlations\nbetween these simultaneous processes, rather than focusing on the translation\nof a single isolated transcript, should help in gaining a better understanding\nof mRNA translation regulation and the way elongation rates affect organismal\nfitness. A model of simultaneous translation is specifically important when\ndealing with highly expressed genes, as these consume more resources. In\naddition, such a model can lead to more accurate predictions that are needed in\nthe interconnection of translational modules in synthetic biology. We develop\nand analyze a general model for large-scale simultaneous mRNA translation and\ncompetition for ribosomes. This is based on combining several ribosome flow\nmodels (RFMs) interconnected via a pool of free ribosomes. We prove that the\ncompound system always converges to a steady-state and that it always entrains\nor phase locks to periodically time-varying transition rates in any of the mRNA\nmolecules. We use this model to explore the interactions between the various\nmRNA molecules and ribosomes at steady-state. We show that increasing the\nlength of an mRNA molecule decreases the production rate of all the mRNAs.\nIncreasing any of the codon translation rates in a specific mRNA molecule\nyields a local effect: an increase in the translation rate of this mRNA, and\nalso a global effect: the translation rates in the other mRNA molecules all\nincrease or all decrease. These results suggest that the effect of codon\ndecoding rates of endogenous and heterologous mRNAs on protein production is\nmore complicated than previously thought.\n"
  },
  {
    "year": 2015,
    "title": "Biases in differential expression analysis of RNA-seq data: A matter of\n  replicate type",
    "summary": "  In differential expression (DE) analysis of RNA-seq count data, it is known\nthat genes with a larger read number are more likely to be differentially\nexpressed. This bias has a profound effect on the subsequent Gene Ontology (GO)\nanalysis by perturbing the ranks of gene-sets. Another known bias is that the\ncommonly used parametric DE analysis methods (e.g., edgeR, DESeq and baySeq)\ntend to yield more DE genes as the sequencing depth is increased. We\nnevertheless show that these biases are in fact confined to data of the\ntechnical replicate type. We also show the GO or gene-set enrichment analysis\nmethods applied to technical replicate data result in considerable number of\nfalse positives. In conclusion, the current DE and enrichment analysis methods\ncan be confidently used for biological replicate count data, while caution\nshould be exercised when analysing technical replicate data.\n"
  },
  {
    "year": 2015,
    "title": "The mysterious orphans of Mycoplasmataceae",
    "summary": "  Background: The length of a protein sequence is largely determined by its\nfunction, i.e. each functional group is associated with an optimal size.\nHowever, comparative genomics revealed that proteins length may be affected by\nadditional factors. In 2002 it was shown that in bacterium Escherichia coli and\nthe archaeon Archaeoglobus fulgidus, protein sequences with no homologs are, on\naverage, shorter than those with homologs. Most experts now agree that the\nlength distributions are distinctly different between protein sequences with\nand without homologs in bacterial and archaeal genomes. In this study, we\nexamine this postulate by a comprehensive analysis of all annotated prokaryotic\ngenomes and focusing on certain exceptions.\n  Results: We compared lengths distributions of having homologs proteins (HHPs)\nand non-having homologs proteins (orphans or ORFans) in all currently annotated\ncompletely sequenced prokaryotic genomes. As expected, the HHPs and ORFans have\nstrikingly different length distributions in almost all genomes. As previously\nestablished, the HHPs, indeed, are, on average, longer than the ORFans, and the\nlength distributions for the ORFans have a relatively narrow peak, in contrast\nto the HHPs, whose lengths spread over a wider range of values. However, about\nthirty genomes do not obey these rules. Practically all genomes of Mycoplasma\nand Ureaplasma have atypical ORFans distributions, with the mean lengths of\nORFan larger than the mean lengths of HHPs. These genera constitute over 80% of\natypical genomes.\n  Conclusions: We confirmed on a ubiquitous set of genomes the previous\nobservation that HHPs and ORFans have different gene length distributions. We\nalso showed that Mycoplasmataceae genomes have distinctive distributions of\nORFans lengths. We offer several possible biological explanations of this\nphenomenon.\n"
  },
  {
    "year": 2015,
    "title": "Phenotypic divergence of Homo sapiens is driven by the evolution of\n  human-specific genomic regulatory networks via two mechanistically distinct\n  pathways of creation of divergent regulatory DNA sequences",
    "summary": "  Thousands of candidate human-specific regulatory sequences (HSRS) have been\nidentified, supporting the hypothesis that unique to human phenotypes result\nfrom human-specific alterations of genomic regulatory networks. Here,\nconservation patterns analysis of 18,364 candidate HSRS was carried out based\non definition of the sequence conservation threshold as the minimum ratio of\nbases that must remap of 1.00. A total of 5,535 candidate HSRS were identified\nthat are: i) highly conserved in Great Apes; ii) evolved by the exaptation of\nhighly conserved ancestral DNA; iii) defined by either the acceleration of\nmutation rates on the human lineage or the functional divergence from nonhuman\nprimates. The exaptation of highly conserved ancestral DNA pathway seems\nmechanistically distinct from the evolution of regulatory DNA segments driven\nby the species-specific expansion of transposable elements. Present analysis\nsupports the idea that phenotypic divergence of Homo sapiens is driven by the\nevolution of human-specific genomic regulatory networks via two mechanistically\ndistinct pathways of creation of divergent sequences of regulatory DNA: i)\nexaptation of the highly conserved ancestral regulatory DNA segments; ii)\nhuman-specific insertions of transposable elements.\n"
  },
  {
    "year": 2015,
    "title": "Bipartite Community Structure of eQTLs",
    "summary": "  Genome Wide Association Studies (GWAS) and eQTL analyses have produced a\nlarge and growing number of genetic associations linked to a wide range of\nhuman phenotypes. As of 2013, there were more than 11,000 SNPs associated with\na trait as reported in the NHGRI GWAS Catalog. However, interpreting the\nfunctional roles played by these SNPs remains a challenge. Here we describe an\napproach that uses the inherent bipartite structure of eQTL networks to place\nSNPs into a functional context.\n  Using genotyping and gene expression data from 163 lung tissue samples in a\nstudy of Chronic Obstructive Pulmonary Disease (COPD) we calculated eQTL\nassociations between SNPs and genes and cast significant associations (FDR $<\n0.1$) as links in a bipartite network. To our surprise, we discovered that the\nhighly-connected \"hub\" SNPs within the network were devoid of\ndisease-associations. However, within the network we identified 35 highly\nmodular communities, which comprise groups of SNPs associated with groups of\ngenes; 13 of these communities were significantly enriched for distinct\nbiological functions (P $ < 5 \\times 10^{-4}$) including COPD-related\nfunctions. Further, we found that GWAS-significant SNPs were enriched at the\ncores of these communities, including previously identified GWAS associations\nfor COPD, asthma, and pulmonary function, among others. These results speak to\nour intuition: rather than single SNPs influencing single genes, we see groups\nof SNPs associated with the expression of families of functionally related\ngenes and that disease SNPs are associated with the perturbation of those\nfunctions. These methods are not limited in their application to COPD and can\nbe used in the analysis of a wide variety of disease processes and other\nphenotypic traits.\n"
  },
  {
    "year": 2015,
    "title": "Life without dUTPase",
    "summary": "  Fine-tuned regulation of the cellular nucleotide pools is indispensable for\nfaithful replication of DNA. The genetic information is also safeguarded by DNA\ndamage recognition and repair processes. Uracil is one of the most frequently\noccurring erroneous base in DNA; it can arise from cytosine deamination or\nthymine-replacing incorporation. Two enzyme families are primarily involved in\nkeeping DNA uracil-free: dUTPases that prevent thymine-replacing incorporation\nand uracil-DNA glycosylases that excise uracil from DNA and initiate\nuracil-excision repair. Both dUTPase and the most efficient uracil-DNA\nglycosylase UNG is thought to be ubiquitous in free-living organisms. In the\npresent work, we have systematically investigated the genotype of deposited\nfully sequenced bacterial and Archaeal genomes. Surprisingly, we have found\nthat in contrast to the generally held opinion, a wide number of bacterial and\nArchaeal species lack the dUTPase gene(s). The dut- genotype is present in\ndiverse bacterial phyla indicating that loss of this (or these) gene(s) has\noccurred multiple times during evolution. We have identified several survival\nstrategies in lack of dUTPases: i) simultaneous lack or inhibition of UNG, ii)\nacquisition of a less dUTP-specific sanitizing nucleotide pyrophosphatase, and\niii) supply of dUTPase from bacteriophages. Our data indicate that several\nunicellular microorganisms may efficiently cope with a dut- genotype\npotentially leading to an unusual uracil-enrichment in their genomic DNA.\n"
  },
  {
    "year": 2015,
    "title": "Algorithmic Methods to Infer the Evolutionary Trajectories in Cancer\n  Progression",
    "summary": "  The genomic evolution inherent to cancer relates directly to a renewed focus\non the voluminous next generation sequencing (NGS) data, and machine learning\nfor the inference of explanatory models of how the (epi)genomic events are\nchoreographed in cancer initiation and development. However, despite the\nincreasing availability of multiple additional -omics data, this quest has been\nfrustrated by various theoretical and technical hurdles, mostly stemming from\nthe dramatic heterogeneity of the disease. In this paper, we build on our\nrecent works on \"selective advantage\" relation among driver mutations in cancer\nprogression and investigate its applicability to the modeling problem at the\npopulation level. Here, we introduce PiCnIc (Pipeline for Cancer Inference), a\nversatile, modular and customizable pipeline to extract ensemble-level\nprogression models from cross-sectional sequenced cancer genomes. The pipeline\nhas many translational implications as it combines state-of-the-art techniques\nfor sample stratification, driver selection, identification of\nfitness-equivalent exclusive alterations and progression model inference. We\ndemonstrate PiCnIc's ability to reproduce much of the current knowledge on\ncolorectal cancer progression, as well as to suggest novel experimentally\nverifiable hypotheses.\n"
  },
  {
    "year": 2015,
    "title": "Unified theory of human genome reveals a constrained spatial chromosomal\n  arrangement in interphase nuclei",
    "summary": "  We investigate a densely packed, non-random arrangement of forty-six\nchromosomes (46,XY) in human nuclei. Here, we model systems-level chromosomal\ncrosstalk by unifying intrinsic parameters (chromosomal length and number of\ngenes) across all pairs of chromosomes in the genome to derive an extrinsic\nparameter called effective gene density. The hierarchical clustering and\nunderlying degeneracy in the effective gene density space reveal systems-level\nconstraints for spatial arrangement of clusters of chromosomes that were\npreviously unknown. Our findings corroborate experimental data on spatial\nchromosomal arrangement in human nuclei, from fibroblast and lymphocyte cell\nlines, thereby establishing that human genome constrains chromosomal\narrangement. We propose that this unified theory, which requires no additional\nexperimental input, may be extended to other eukaryotic species with annotated\ngenomes to infer their constrained self-organized spatial arrangement of\nchromosomes.\n"
  },
  {
    "year": 2015,
    "title": "Keep Me Around: Intron Retention Detection and Analysis",
    "summary": "  We present a tool, keep me around (kma), a suite of python scripts and an R\npackage that finds retained introns in RNA-Seq experiments and incorporates\nbiological replicates to reduce the number of false positives when detecting\nretention events. kma uses the results of existing quantification tools that\nprobabilistically assign multi-mapping reads, thus interfacing easily with\ntranscript quantification pipelines. The data is represented in a convenient,\ndatabase style format that allows for easy aggregation across introns, genes,\nsamples, and conditions to allow for further exploratory analysis.\n"
  },
  {
    "year": 2015,
    "title": "The Transcription Factor E4F1 Coordinates CHK1-Dependent Checkpoint and\n  Mitochondrial Functions",
    "summary": "  Recent data support the notion that a group of key transcriptional regulators\ninvolved in tumorigenesis, including MYC, p53, E2F1, and BMI1, share an\nintriguing capacity to simultaneously regulate metabolism and cell cycle. Here,\nwe show that another factor, the multifunctional protein E4F1, directly\ncontrols genes involved in mitochondria functions and cell-cycle checkpoints,\nincluding Chek1, a major component of the DNA damage response. Coordination of\nthese cellular functions by E4F1 appears essential for the survival of\np53-deficient transformed cells. Acute inactivation of E4F1 in these cells\nresults in CHK1-dependent checkpoint deficiency and multiple mitochondrial\ndysfunctions that lead to increased ROS production, energy stress, and\ninhibition of de novo pyrimidine synthesis. This deadly cocktail leads to the\naccumulation of uncompensated oxidative damage to proteins and extensive DNA\ndamage, ending in cell death. This supports the rationale of therapeutic\nstrategies simultaneously targeting mitochondria and CHK1 for selective killing\nof p53-deficient cancer cells.\n"
  },
  {
    "year": 2015,
    "title": "A large scale prediction of bacteriocin gene blocks suggests a wide\n  functional spectrum for bacteriocins",
    "summary": "  Bacteriocins are peptide-derived molecules produced by bacteria, whose\nrecently-discovered functions include virulence factors and signalling\nmolecules as well as their better known roles as antibiotics. To date, close to\nfive hundred bacteriocins have been identified and classified. Recent\ndiscoveries have shown that bacteriocins are highly diverse and widely\ndistributed among bacterial species. Given the heterogeneity of bacteriocin\ncompounds, many tools struggle with identifying novel bacteriocins due to their\nvast sequence and structural diversity. Many bacteriocins undergo\npost-translational processing or modifications necessary for the biosynthesis\nof the final mature form. Enzymatic modification of bacteriocins as well as\ntheir export is achieved by proteins whose genes are often located in a\ndiscrete gene cluster proximal to the bacteriocin precursor gene, referred to\nas \\textit{context genes} in this study. Although bacteriocins themselves are\nstructurally diverse, context genes have been shown to be largely conserved\nacross unrelated species. Using this knowledge, we set out to identify new\ncandidates for context genes which may clarify how bacteriocins are\nsynthesized, and identify new candidates for bacteriocins that bear no sequence\nsimilarity to known toxins. To achieve these goals, we have developed a\nsoftware tool, Bacteriocin Operon and gene block Associator (BOA) that can\nidentify homologous bacteriocin associated gene clusters and predict novel\nones. We discover that several phyla have a strong preference for bactericon\ngenes, suggesting distinct functions for this group of molecules. Availability:\nhttps://github.com/idoerg/BOA\n"
  },
  {
    "year": 2015,
    "title": "Machine learning for metagenomics: methods and tools",
    "summary": "  Owing to the complexity and variability of metagenomic studies, modern\nmachine learning approaches have seen increased usage to answer a variety of\nquestion encompassing the full range of metagenomic NGS data analysis. We\nreview here the contribution of machine learning techniques for the field of\nmetagenomics, by presenting known successful approaches in a unified framework.\nThis review focuses on five important metagenomic problems: OTU-clustering,\nbinning, taxonomic profling and assignment, comparative metagenomics and gene\nprediction. For each of these problems, we identify the most prominent methods,\nsummarize the machine learning approaches used and put them into perspective of\nsimilar methods. We conclude our review looking further ahead at the challenge\nposed by the analysis of interactions within microbial communities and\ndifferent environments, in a field one could call \"integrative metagenomics\".\n"
  },
  {
    "year": 2015,
    "title": "Identifying lineage effects when controlling for population structure\n  improves power in bacterial association studies",
    "summary": "  Bacteria pose unique challenges for genome-wide association studies (GWAS)\nbecause of strong structuring into distinct strains and substantial linkage\ndisequilibrium across the genome. While methods developed for human studies can\ncorrect for strain structure, this risks considerable loss- of-power because\ngenetic differences between strains often contribute substantial phenotypic\nvariability. Here we propose a new method that captures lineage-level\nassociations even when locus-specific associations cannot be fine-mapped. We\ndemonstrate its ability to detect genes and genetic variants underlying\nresistance to 17 antimicrobials in 3144 isolates from four taxonomically\ndiverse clonal and recombining bacteria: Mycobacterium tuberculosis,\nStaphylococcus aureus, Escherichia coli and Klebsiella pneumoniae. Strong\nselection, recombination and penetrance confer high power to recover known\nantimicrobial resistance mechanisms, and reveal a candidate association between\nthe outer membrane porin nmpC and cefazolin resistance in E. coli. Hence our\nmethod pinpoints locus-specific effects where possible, and boosts power by\ndetecting lineage-level differences when fine-mapping is intractable.\n"
  },
  {
    "year": 2015,
    "title": "Rare recombination events generate sequence diversity among balancer\n  chromosomes in Drosophila melanogaster",
    "summary": "  Multiply inverted balancer chromosomes that suppress exchange with their\nhomologs are an essential part of the genetic toolkit in Drosophila\nmelanogaster. Despite their widespread use, the organization of balancer\nchromosomes has not been characterized at the molecular level, and the degree\nof sequence variation among copies of any given balancer chromosome is unknown.\nTo map inversion breakpoints and study potential sequence diversity in the\ndescendants of a structurally identical balancer chromosome, we sequenced a\npanel of laboratory stocks containing the most widely used X-chromosome\nbalancer, First Multiple 7 (FM7). We mapped the locations of FM7 breakpoints to\nprecise euchromatic coordinates and identified the flanking sequence of\nbreakpoints in heterochromatic regions. Analysis of SNP variation revealed\nmegabase-scale blocks of sequence divergence among currently used FM7 stocks.\nWe present evidence that this divergence arose by rare double crossover events\nthat replaced a female-sterile allele of the singed gene (sn[X2]) on FM7c with\nwild type sequence from balanced chromosomes, and propose that many FM7c\nchromosomes in the Bloomington Drosophila Stock Center have lost sn[X2] by this\nmechanism. Finally, we characterize the original allele of the Bar gene (B[1])\nthat is carried on FM7 and validate the hypothesis that the origin and\nsubsequent reversion of the B1 duplication is mediated by unequal exchange. Our\nresults reject a simple non-recombining, clonal mode for the laboratory\nevolution of balancer chromosomes and have implications for how balancer\nchromosomes should be used in the design and interpretation of genetic\nexperiments in Drosophila.\n"
  },
  {
    "year": 2015,
    "title": "Cnidaria: fast, reference-free clustering of raw and assembled genome\n  and transcriptome NGS data",
    "summary": "  Background: Identification of biological specimens is a major requirement for\na range of applications. Reference-free methods analyse unprocessed sequencing\ndata without relying on prior knowledge, but generally do not scale to\narbitrarily large genomes and arbitrarily large phylogenetic distances.\nResults: We present Cnidaria, a practical tool for clustering genomic and\ntranscriptomic data with no limitation on genome size or phylogenetic\ndistances. We successfully simultaneously clustered 169 genomic and\ntranscriptomic datasets from 4 kingdoms, achieving 100% identification accuracy\nat supra-species level and 78% accuracy for species level. Discussion: CNIDARIA\nallows for fast, resource-efficient comparison and identification of both raw\nand assembled genome and transcriptome data. This can help answer both\nfundamental (e.g. in phylogeny, ecological diversity analysis) and practical\nquestions (e.g. sequencing quality control, primer design).\n"
  },
  {
    "year": 2015,
    "title": "Multivariate Functional Regression Models for Epistasis Analysis",
    "summary": "  To date, most genetic analyses of phenotypes have focused on analyzing single\ntraits or, analyzing each phenotype independently. However, joint epistasis\nanalysis of multiple complementary traits will increase statistical power, and\nhold the key to understanding the complicated genetic structure of the complex\ndiseases. Despite their importance in uncovering the genetic structure of\ncomplex traits, the statistical methods for identifying epistasis in multiple\nphenotypes remains fundamentally unexplored. To fill this gap, we formulate a\ntest for interaction between two gens in multiple quantitative trait analysis\nas a multiple functional regression (MFRG) in which the genotype functions\n(genetic variant profiles) are defined as a function of the genomic position of\nthe genetic variants. We use large scale simulations to calculate its type I\nerror rates for testing interaction between two genes with multiple phenotypes\nand to compare its power with multivariate pair-wise interaction analysis and\nsingle trait interaction analysis by a single variate functional regression\nmodel. To further evaluate its performance, the MFRG for epistasis analysis is\napplied to five phenotypes and exome sequence data from the NHLBI Exome\nSequencing Project (ESP) to detect pleiotropic epistasis. A total of 136 pairs\nof genes that formed a genetic interaction network showed significant evidence\nof epistasis influencing five traits. The results demonstrate that the joint\ninteraction analysis of multiple phenotypes has much higher power to detect\ninteraction than the interaction analysis of single trait and may open a new\ndirection to fully uncovering the genetic structure of multiple phenotypes.\n"
  },
  {
    "year": 2015,
    "title": "Minimap and miniasm: fast mapping and de novo assembly for noisy long\n  sequences",
    "summary": "  Motivation: Single Molecule Real-Time (SMRT) sequencing technology and Oxford\nNanopore technologies (ONT) produce reads over 10kbp in length, which have\nenabled high-quality genome assembly at an affordable cost. However, at\npresent, long reads have an error rate as high as 10-15%. Complex and\ncomputationally intensive pipelines are required to assemble such reads.\n  Results: We present a new mapper, minimap, and a de novo assembler, miniasm,\nfor efficiently mapping and assembling SMRT and ONT reads without an error\ncorrection stage. They can often assemble a sequencing run of bacterial data\ninto a single contig in a few minutes, and assemble 45-fold C. elegans data in\n9 minutes, orders of magnitude faster than the existing pipelines. We also\nintroduce a pairwise read mapping format (PAF) and a graphical fragment\nassembly format (GFA), and demonstrate the interoperability between ours and\ncurrent tools.\n  Availability and implementation: https://github.com/lh3/minimap and\nhttps://github.com/lh3/miniasm\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2015,
    "title": "MeFiT: Merging and Filtering Tool for Illumina Paired-End Reads for 16S\n  rRNA Amplicon Sequencing",
    "summary": "  Recent advances in next-generation sequencing have revolutionized genomic\nresearch. 16S rRNA amplicon sequencing using paired-end sequencing on the MiSeq\nplatform from Illumina, Inc., is being used to characterize the composition and\ndynamics of extremely complex/diverse microbial communities. For this analysis\non the Illumina platform, merging and quality filtering of paired-end reads are\nessential first steps in data analysis to ensure the accuracy and reliability\nof downstream analysis. We have developed the Merging and Filtering Tool\n(MeFiT) to combine these pre-processing steps into one simple, intuitive\npipeline. MeFiT provides an open-source solution that permits users to merge\nand filter paired end illumina reads based on user-selected quality parameters.\nThe tool has been implemented in python and the source-code is freely available\nat https://github.com/nisheth/MeFiT.\n"
  },
  {
    "year": 2015,
    "title": "MEEPTOOLS: A maximum expected error based FASTQ read filtering and\n  trimming toolkit",
    "summary": "  Next generation sequencing technology rapidly produces massive volume of data\nand quality control of this sequencing data is essential to any genomic\nanalysis. Here we present MEEPTOOLS, which is a collection of open-source tools\nbased on maximum expected error as a percentage of read length (MEEP score) to\nfilter, trim, truncate and assess next generation DNA sequencing data in FASTQ\nfile format. MEEPTOOLS provides a non-traditional approach towards read\nfiltering/trimming based on maximum error probabilities of the bases in the\nread on a non-logarithmic scale. This method simultaneously retains more\nreliable bases and removes more unreliable bases than the traditional quality\nfiltering strategies.\n"
  },
  {
    "year": 2015,
    "title": "Functional transcription factor target discovery via compendia of\n  binding and expression profiles",
    "summary": "  Genome-wide experiments to map the DNA-binding locations of\ntranscription-associated factors (TFs) have shown that the number of genes\nbound by a TF far exceeds the number of possible direct target genes.\nDistinguishing functional from non-functional binding is therefore a major\nchallenge in the study of transcriptional regulation. We hypothesized that\nfunctional targets can be discovered by correlating binding and expression\nprofiles across multiple experimental conditions. To test this hypothesis, we\nobtained ChIP-seq and RNA-seq data from matching cell types from the human\nENCODE resource, considered promoter-proximal and distal cumulative regulatory\nmodels to map binding sites to genes, and used a combination of linear and\nnon-linear measures to correlate binding and expression data. We found that a\nhigh degree of correlation between a gene's TF-binding and expression profiles\nwas significantly more predictive of the gene being differentially expressed\nupon knockdown of that TF, compared to using binding sites in the cell type of\ninterest only. Remarkably, TF targets predicted from correlation across a\ncompendium of cell types were also predictive of functional targets in other\ncell types. Finally, correlation across a time course of ChIP-seq and RNA-seq\nexperiments was also predictive of functional TF targets in that tissue.\n"
  },
  {
    "year": 2015,
    "title": "Detection and Visualization of Differential Splicing in RNA-Seq Data\n  with JunctionSeq",
    "summary": "  Although RNA-Seq data provide unprecedented isoform-level expression\ninformation, detection of alternative isoform regulation (AIR) remains\ndifficult, particularly when working with an incomplete transcript annotation.\nWe introduce JunctionSeq, a new method that builds on the statistical\ntechniques used by the well-established DEXSeq package to detect differential\nusage of both exonic regions and splice junctions. In particular, JunctionSeq\nis capable of detecting differentials in novel splice junctions without the\nneed for an additional isoform assembly step, greatly improving performance\nwhen the available transcript annotation is flawed or incomplete. JunctionSeq\nalso provides a powerful and streamlined visualization toolset that allows\nbioinformaticians to quickly and intuitively interpret their results. We tested\nour method on publicly available data from several experiments performed on the\nrat pineal gland and Toxoplasma gondii, successfully detecting known and\npreviously validated AIR genes in 19 out of 19 gene-level hypothesis tests. Due\nto its ability to query novel splice sites, JunctionSeq is still able to detect\nthese differentials even when all alternative isoforms for these genes were not\nincluded in the transcript annotation. JunctionSeq thus provides a powerful\nmethod for detecting alternative isoform regulation even with low-quality\nannotations. An implementation of JunctionSeq is available as an R/Bioconductor\npackage.\n"
  },
  {
    "year": 2016,
    "title": "Rubbish DNA: The functionless fraction of the human genome",
    "summary": "  Because genomes are products of natural processes rather than intelligent\ndesign, all genomes contain functional and nonfunctional parts. The fraction of\nthe genome that has no biological function is called rubbish DNA. Rubbish DNA\nconsists of junk DNA, i.e., the fraction of the genome on which selection does\nnot operate, and garbage DNA, i.e., sequences that lower the fitness of the\norganism, but exist in the genome because purifying selection is neither\nomnipotent nor instantaneous. In this chapter, I (1) review the concepts of\ngenomic function and functionlessness from an evolutionary perspective, (2)\npresent a precise nomenclature of genomic function, (3) discuss the evidence\nfor the existence of vast quantities of junk DNA within the human genome, (4)\ndiscuss the mutational mechanisms responsible for generating junk DNA, (5)\nspell out the necessary evolutionary conditions for maintaining junk DNA, (6)\noutline various methodologies for estimating the functional fraction within the\ngenome, and (7) present a recent estimate for the functional fraction of our\ngenome.\n"
  },
  {
    "year": 2016,
    "title": "Separating Putative Pathogens from Background Contamination with\n  Principal Orthogonal Decomposition: Evidence for Leptospira in the Ugandan\n  Neonatal Septisome",
    "summary": "  Neonatal sepsis (NS) is responsible for over a 1 million yearly deaths\nworldwide. In the developing world NS is often treated without an identified\nmicrobial pathogen. Amplicon sequencing of the bacterial 16S rRNA gene can be\nused to identify organisms that are difficult to detect by routine\nmicrobiological methods. However, contaminating bacteria are ubiquitous in both\nhospital settings and research reagents, and must be accounted for to make\neffective use of these data. In the present study, we sequenced the bacterial\n16S rRNA gene obtained from blood and cerebrospinal fluid (CSF) of 80 neonates\npresenting with NS to the Mbarara Regional Hospital in Uganda. Assuming that\npatterns of background contamination would be independent of pathogenic\nmicroorganism DNA, we applied a novel quantitative approach using principal\northogonal decomposition to separate background contamination from potential\npathogens in sequencing data. We designed our quantitative approach contrasting\nblood, CSF, and control specimens, and employed a variety of statistical random\nmatrix bootstrap hypotheses to estimate statistical significance. These\nanalyses demonstrate that Leptospira appears present in some infants presenting\nwithin 48 hr of birth, indicative of infection in utero, and up to 28 days of\nage, suggesting environmental exposure. This organism cannot be cultured in\nroutine bacteriological settings, and is enzootic in the cattle that the rural\npeoples of western Uganda often live in close proximity. Our findings\ndemonstrate that statistical approaches to remove background organisms common\nin 16S sequence data can reveal putative pathogens in small volume biological\nsamples from newborns. This computational analysis thus reveals an important\nmedical finding that has the potential to alter therapy and prevention efforts\nin a critically ill population.\n"
  },
  {
    "year": 2016,
    "title": "Gene Ontology: Pitfalls, Biases, Remedies",
    "summary": "  The Gene Ontology (GO) is a formidable resource but there are several\nconsiderations about it that are essential to understand the data and interpret\nit correctly. The GO is sufficiently simple that it can be used without deep\nunderstanding of its structure or how it is developed, which is both a strength\nand a weakness. In this chapter, we discuss some common misinterpretations of\nthe ontology and the annotations. A better understanding of the pitfalls and\nthe biases in the GO should help users make the most of this very rich\nresource. We also review some of the misconceptions and misleading assumptions\ncommonly made about GO, including the effect of data incompleteness, the\nimportance of annotation qualifiers, and the transitivity or lack thereof\nassociated with different ontology relations. We also discuss several biases\nthat can confound aggregate analyses such as gene enrichment analyses. For each\nof these pitfalls and biases, we suggest remedies and best practices.\n"
  },
  {
    "year": 2016,
    "title": "Primer on the Gene Ontology",
    "summary": "  The Gene Ontology (GO) project is the largest resource for cataloguing gene\nfunction. The combination of solid conceptual underpinnings and a practical set\nof features have made the GO a widely adopted resource in the research\ncommunity and an essential resource for data analysis. In this chapter, we\nprovide a concise primer for all users of the GO. We briefly introduce the\nstructure of the ontology and explain how to interpret annotations associated\nwith the GO.\n"
  },
  {
    "year": 2016,
    "title": "Controllability analysis and control synthesis for the ribosome flow\n  model",
    "summary": "  The ribosomal density along the coding region of the mRNA molecule affect\nvarious fundamental intracellular phenomena including: protein production\nrates, organismal fitness, ribosomal drop off, and co-translational protein\nfolding. Thus, regulating translation in order to obtain a desired ribosomal\nprofile along the mRNA molecule is an important biological problem. We study\nthis problem using a model for mRNA translation, called the ribosome flow model\n(RFM). In the RFM, the mRNA molecule is modeled as chain of n sites. The n\nstate-variables describe the ribosomal density profile along the mRNA molecule,\nwhereas the transition rates from each site to the next are controlled by n+1\npositive constants. To study the problem of controlling the density profile, we\nconsider some or all of the transition rates as time-varying controls. We\nconsider the following problem: given an initial and a desired ribosomal\ndensity profile, determine the time-varying values of the transition rates that\nsteer the RFM to this density profile, if they exist. Specifically, we consider\ntwo control problems. In the first, all transition rates can be regulated and\nthe goal is to steer the ribosomal density profile and the protein production\nrate from a given initial value to a desired value. In the second, a single\ntransition rate is controlled and the goal is to steer the production rate to a\ndesired value. In the first case, we show that the system is controllable, i.e.\nthe control is powerful enough to steer the RFM to any desired value, and we\nprovide closed-form expressions for constant control functions (or transition\nrates) asymptotically steering the RFM to the desired value. For the second\nproblem, we show that the production rate can be steered to any desired value\nin a feasible region determined by the other, constant transition rates. We\ndiscuss some of the biological implications of these results.\n"
  },
  {
    "year": 2016,
    "title": "The combinatorics of overlapping genes",
    "summary": "  Overlapping genes exist in all domains of life and are much more abundant\nthan expected at their first discovery in the late 1970s. Assuming that the\nreference gene is read in frame +0, an overlapping gene can be encoded in two\nreading frames in the sense strand, denoted by +1 and +2, and in three reading\nframes in the opposite strand, denoted by -0, -1 and -2. This motivated\nnumerous researchers to study the constraints induced by the genetic code on\nthe various overlapping frames, mostly based on information theory. Our focus\nin this paper is on the constraints induced on two overlapping genes in terms\nof amino acids, as well as polypeptides. We show that simple linear constraints\nbind the amino acid composition of two proteins encoded by overlapping genes.\nNovel constraints are revealed when polypeptides are considered, and not just\nsingle amino acids. For example, in double-coding sequences with an overlapping\nreading frame -2, each Tyrosine (denoted as Tyr or Y) in the overlapping frame\noverlaps a Tyrosine in the reference frame +0 (and reciprocally), whereas\nspecific words (e.g. YY) never occur. We thus distinguish between null\nconstraints (YY = 0 in frame -2) and non-null constraints (Y in frame +0 <=> Y\nin frame -2). Our equivalence-based constraints are symmetrical and thus enable\nthe characterization of the joint composition of overlapping proteins. We\ndescribe several formal frameworks and a graph algorithm to characterize and\ncompute these constraints. These results yield support for understanding the\nmechanisms and evolution of overlapping genes, and for developing novel\noverlapping gene detection methods.\n"
  },
  {
    "year": 2016,
    "title": "MetaPalette: A $k$-mer painting approach for metagenomic taxonomic\n  profiling and quantification of novel strain variation",
    "summary": "  Metagenomic profiling is challenging in part because of the highly uneven\nsampling of the tree of life by genome sequencing projects and the limitations\nimposed by performing phylogenetic inference at fixed taxonomic ranks. We\npresent the algorithm MetaPalette which uses long $k$-mer sizes ($k=30, 50$) to\nfit a $k$-mer \"palette\" of a given sample to the $k$-mer palette of reference\norganisms. By modeling the $k$-mer palettes of unknown organisms, the method\nalso gives an indication of the presence, abundance, and evolutionary\nrelatedness of novel organisms present in the sample. The method returns a\ntraditional, fixed-rank taxonomic profile which is shown on independently\nsimulated data to be one of the most accurate to date. Tree figures are also\nreturned that quantify the relatedness of novel organisms to reference\nsequences and the accuracy of such figures is demonstrated on simulated\nspike-ins and a metagenomic soil sample. The software implementing MetaPalette\nis available at: https://github.com/dkoslicki/MetaPalette. Pre-trained\ndatabases are included for Archaea, Bacteria, Eukaryota, and viruses.\n"
  },
  {
    "year": 2016,
    "title": "Statistical modeling of isoform splicing dynamics from RNA-seq time\n  series data",
    "summary": "  Isoform quantification is an important goal of RNA-seq experiments, yet it\nremains prob- lematic for genes with low expression or several isoforms. These\ndifficulties may in principle be ameliorated by exploiting correlated\nexperimental designs, such as time series or dosage response experiments. Time\nseries RNA-seq experiments, in particular, are becoming in- creasingly popular,\nyet there are no methods that explicitly leverage the experimental design to\nimprove isoform quantification. Here we present DICEseq, the first isoform\nquantification method tailored to correlated RNA-seq experiments. DICEseq\nexplicitly models the corre- lations between different RNA-seq experiments to\naid the quantification of isoforms across experiments. Numerical experiments on\nsimulated data sets show that DICEseq yields more accurate results than\nstate-of-the-art methods, an advantage that can become considerable at low\ncoverage levels. On real data sets, our results show that DICEseq provides\nsubstan- tially more reproducible and robust quantifications, increasing the\ncorrelation of estimates from replicate data sets by up to 10% on genes with\nlow or moderate expression levels (bot- tom third of all genes). Furthermore,\nDICEseq permits to quantify the trade-off between temporal sampling of RNA and\ndepth of sequencing, frequently an important choice when planning experiments.\nOur results have strong implications for the design of RNA-seq ex- periments,\nand offer a novel tool for improved analysis of such data sets. Python code is\nfreely available at http://diceseq.sf.net.\n"
  },
  {
    "year": 2016,
    "title": "Visualizing Gene Ontology annotations",
    "summary": "  Contemporary techniques in biology produce readouts for large numbers of\ngenes simultaneously, the typical example being differential gene expression\nmeasurements. Moreover, those genes are often richly annotated using GO terms\nthat describe gene function and that can be used to summarize the results of\nthe genome-scale experiments. However, making sense of such GO enrichment\nanalyses may be challenging. For instance, overrepresented GO functions in a\nset of differentially expressed genes are typically output as a flat list, a\nformat not adequate to capture the complexities of the hierarchical structure\nof the GO annotation labels.\n  In this chapter, we survey the various methods to visualize large,\ndifficult-to-interpret lists of GO terms. We catalogue their availability\n(web-based or standalone), the main principles they employ in summarizing large\nlists of GO terms, and the visualization styles they support. These brief\ncommentaries on each software are intended as a helpful inventory, rather than\ncomprehensive descriptions of the underlying algorithms. Instead, we show\nexamples of their use and suggest that the choice of an appropriate\nvisualization tool may be crucial to the utility of GO in biological discovery.\n"
  },
  {
    "year": 2016,
    "title": "Supra-operonic clusters of functionally related genes (SOCs) are a\n  source of horizontal gene co-transfers",
    "summary": "  Adaptation of bacteria occurs predominantly via horizontal gene transfer\n(HGT). While it is widely recognized that horizontal acquisitions frequently\nencompass multiple genes, it is unclear what the size distribution of\nsuccessfully transferred DNA segments looks like and what evolutionary forces\nshape this distribution. Here, we identified 1790 gene family pairs that were\nconsistently co-gained on the same branches across a phylogeny of 53 E. coli\nstrains. We estimated a lower limit of their genomic distances at the time they\nwere transferred to their host genomes; this distribution shows a sharp upper\nbound at 30 kb. The same gene-pairs can have larger distances (up to 70 kb) in\nother genomes. These more distant pairs likely represent recent acquisitions\nvia transduction that involve the co-transfer of excised prophage genes, as\nthey are almost always associated with intervening phage-associated genes. The\nobserved distribution of genomic distances of co-transferred genes is much\nbroader than expected from a model based on the co-transfer of genes within\noperons; instead, this distribution is highly consistent with the size\ndistribution of supra-operonic clusters (SOCs), groups of co-occurring and\nco-functioning genes that extend beyond operons. Thus, we propose that SOCs\nform a basic unit of horizontal gene transfer.\n"
  },
  {
    "year": 2016,
    "title": "Machine Learning for Protein Function",
    "summary": "  Systematic identification of protein function is a key problem in current\nbiology. Most traditional methods fail to identify functionally equivalent\nproteins if they lack similar sequences, structural data or extensive manual\nannotations. In this thesis, I focused on feature engineering and machine\nlearning methods for identifying diverse classes of proteins that share\nfunctional relatedness but little sequence or structural similarity, notably,\nNeuropeptide Precursors (NPPs).\n  I aim to identify functional protein classes solely using unannotated protein\nprimary sequences from any organism. This thesis focuses on feature\nrepresentations of whole protein sequences, sequence derived engineered\nfeatures, their extraction, frameworks for their usage by machine learning (ML)\nmodels, and the application of ML models to biological tasks, focusing on high\nlevel protein functions. I implemented the ideas of feature engineering to\ndevelop a platform (called NeuroPID) that extracts meaningful features for\nclassification of overlooked NPPs. The platform allows mass discovery of new\nNPs and NPPs. It was expanded as a webserver.\n  I expanded our approach towards other challenging protein classes. This is\nimplemented as a novel bioinformatics toolkit called ProFET (Protein Feature\nEngineering Toolkit). ProFET extracts hundreds of biophysical and sequence\nderived attributes, allowing the application of machine learning methods to\nproteins. ProFET was applied on many protein benchmark datasets with state of\nthe art performance. The success of ProFET applies to a wide range of\nhigh-level functions such as metagenomic analysis, subcellular localization,\nstructure and unique functional properties (e.g. thermophiles, nucleic acid\nbinding).\n  These methods and frameworks represent a valuable resource for using ML and\ndata science methods on proteins.\n"
  },
  {
    "year": 2016,
    "title": "SMISS: A protein function prediction server by integrating multiple\n  sources",
    "summary": "  SMISS is a novel web server for protein function prediction. Three different\npredictors can be selected for different usage. It integrates different sources\nto improve the protein function prediction accuracy, including the query\nprotein sequence, protein-protein interaction network, gene-gene interaction\nnetwork, and the rules mined from protein function associations. SMISS\nautomatically switch to ab initio protein function prediction based on the\nquery sequence when there is no homologs in the database. It takes fasta format\nsequences as input, and several sequences can submit together without\ninfluencing the computation speed too much. PHP and Perl are two primary\nprogramming language used in the server. The CodeIgniter MVC PHP web framework\nand Bootstrap front-end framework are used for building the server. It can be\nused in different platforms in standard web browser, such as Windows, Mac OS X,\nLinux, and iOS. No plugins are needed for our website. Availability:\nhttp://tulip.rnet.missouri.edu/profunc/.\n"
  },
  {
    "year": 2016,
    "title": "DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION\n  Nanopore Reads",
    "summary": "  Motivation: The MinION device by Oxford Nanopore is the first portable\nsequencing device. MinION is able to produce very long reads (reads over\n100~kBp were reported), however it suffers from high sequencing error rate. In\nthis paper, we show that the error rate can be reduced by improving the base\ncalling process.\n  Results: We present the first open-source DNA base caller for the MinION\nsequencing platform by Oxford Nanopore. By employing carefully crafted\nrecurrent neural networks, our tool improves the base calling accuracy compared\nto the default base caller supplied by the manufacturer. This advance may\nfurther enhance applicability of MinION for genome sequencing and various\nclinical applications.\n  Availability: DeepNano can be downloaded at\nhttp://compbio.fmph.uniba.sk/deepnano/.\n  Contact: boza@fmph.uniba.sk\n"
  },
  {
    "year": 2016,
    "title": "The Discovery of Mutated Driver Pathways in Cancer: Models and\n  Algorithms",
    "summary": "  The pathogenesis of cancer in human is still poorly understood. With the\nrapid development of high-throughput sequencing technologies, huge volumes of\ncancer genomics data have been generated. Deciphering those data poses great\nopportunities and challenges to computational biologists. One of such key\nchallenges is to distinguish driver mutations, genes as well as pathways from\npassenger ones. Mutual exclusivity of gene mutations (each patient has no more\nthan one mutation in the gene set) has been observed in various cancer types\nand thus has been used as an important property of a driver gene set or\npathway. In this article, we aim to review the recent development of\ncomputational models and algorithms for discovering driver pathways or modules\nin cancer with the focus on mutual exclusivity-based ones.\n"
  },
  {
    "year": 2016,
    "title": "New algorithmic challenges of adaptive immune repertoire construction",
    "summary": "  Motivation: The analysis of antibodies and T-cell receptors (TCRs)\nconcentrations in serum is a fundamental problem in immunoinformatics.\nRepertoire construction is a preliminary step of analysis of clonal lineages,\nunderstanding of immune response dynamics, population analysis of\nimmunoglobulin and TCR loci. Emergence of MiSeq Illumina sequencing machine in\n2013 opened horizons of investigation of adaptive immune repertoires using\nhighly accurate reads. Reads produced by MiSeq are able to cover repertoires of\nmoderate size. At the same time, throughput of sequencing machines increases\nfrom year to year. This will enable ultra deep scanning of adaptive immune\nrepertoires and analysis of their diversity. Such data requires both efficient\nand highly accurate repertoire construction tools. In 2015 Safonova et al.\npresented IgRepertoireConstructor, a tool for accurate construction of antibody\nrepertoire and immunoproteogenomics analysis. Unfortunately, proposed algorithm\nwas very time and memory consuming and could be a bottleneck of processing\nlarge immunosequencing libraries. In this paper we overcome this challenge and\npresent IgReC, a novel algorithm for adaptive repertoire construction problem.\nIgReC reconstructs a repertoire with high precision even if each input read\ncontains sequencing errors and performs well on contemporary datasets. Results\nof computational experiments show that IgReC improves state-of-the-art in the\nfield. Availability: IgReC is an open source and freely available program\nrunning on Linux platforms. The source code is available at GitHub:\nyana-safonova.github.io/ig_repertoire_constructor. Contact:\nsafonova.yana@gmail.com\n"
  },
  {
    "year": 2016,
    "title": "Accurate selfcorrection of errors in long reads using de Bruijn graphs",
    "summary": "  New long read sequencing technologies, like PacBio SMRT and Oxford NanoPore,\ncan produce sequencing reads up to 50,000 bp long but with an error rate of at\nleast 15%. Reducing the error rate is necessary for subsequent utilisation of\nthe reads in, e.g., de novo genome assembly. The error correction problem has\nbeen tackled either by aligning the long reads against each other or by a\nhybrid approach that uses the more accurate short reads produced by second\ngeneration sequencing technologies to correct the long reads. We present an\nerror correction method that uses long reads only. The method consists of two\nphases: first we use an iterative alignment-free correction method based on de\nBruijn graphs with increasing length of k-mers, and second, the corrected reads\nare further polished using long-distance dependencies that are found using\nmultiple alignments. According to our experiments the proposed method is the\nmost accurate one relying on long reads only for read sets with high coverage.\nFurthermore, when the coverage of the read set is at least 75x, the throughput\nof the new method is at least 20% higher. LoRMA is freely available at\nhttp://www.cs.helsinki.fi/u/lmsalmel/LoRMA/.\n"
  },
  {
    "year": 2016,
    "title": "Representing high throughput expression profiles via perturbation\n  barcodes reveals compound targets",
    "summary": "  High throughput mRNA expression profiling can be used to characterize the\nresponse of cell culture models to perturbations such as pharmacologic\nmodulators and genetic perturbations. As profiling campaigns expand in scope,\nit is important to homogenize, summarize, and analyze the resulting data in a\nmanner that captures significant biological signals in spite of various noise\nsources such as batch effects and stochastic variation. We used the L1000\nplatform for large-scale profiling of 978 genes, chosen to be representative of\nthe genome as whole, across thousands of compound treatments. Here, a method is\ndescribed that uses deep learning techniques to convert the expression changes\nof the landmark genes into a perturbation barcode that reveals important\nfeatures of the underlying data, performing better than the raw data in\nrevealing important biological insights. The barcode captures compound\nstructure and target information, in addition to predicting a compound's high\nthroughput screening promiscuity, to a higher degree than the original data\nmeasurements, indicating that the approach uncovers underlying factors of the\nexpression data that are otherwise entangled or masked by noise. Furthermore,\nwe demonstrate that visualizations derived from the perturbation barcode can be\nused to more sensitively assign functions to unknown compounds through a\nguilt-by-association approach, which we use to predict and experimentally\nvalidate the activity of compounds on the MAPK pathway. The demonstrated\napplication of deep metric learning to large-scale chemical genetics projects\nhighlights the utility of this and related approaches to the extraction of\ninsights and testable hypotheses from big, sometimes noisy data.\n"
  },
  {
    "year": 2016,
    "title": "Multiple Comparative Metagenomics using Multiset k-mer Counting",
    "summary": "  Background. Large scale metagenomic projects aim to extract biodiversity\nknowledge between different environmental conditions. Current methods for\ncomparing microbial communities face important limitations. Those based on\ntaxonomical or functional assignation rely on a small subset of the sequences\nthat can be associated to known organisms. On the other hand, de novo methods,\nthat compare the whole sets of sequences, either do not scale up on ambitious\nmetagenomic projects or do not provide precise and exhaustive results.\n  Methods. These limitations motivated the development of a new de novo\nmetagenomic comparative method, called Simka. This method computes a large\ncollection of standard ecological distances by replacing species counts by\nk-mer counts. Simka scales-up today's metagenomic projects thanks to a new\nparallel k-mer counting strategy on multiple datasets.\n  Results. Experiments on public Human Microbiome Project datasets demonstrate\nthat Simka captures the essential underlying biological structure. Simka was\nable to compute in a few hours both qualitative and quantitative ecological\ndistances on hundreds of metagenomic samples (690 samples, 32 billions of\nreads). We also demonstrate that analyzing metagenomes at the k-mer level is\nhighly correlated with extremely precise de novo comparison techniques which\nrely on all-versus-all sequences alignment strategy or which are based on\ntaxonomic profiling.\n"
  },
  {
    "year": 2016,
    "title": "Automated deconvolution of structured mixtures from bulk tumor genomic\n  data",
    "summary": "  Motivation: As cancer researchers have come to appreciate the importance of\nintratumor heterogeneity, much attention has focused on the challenges of\naccurately profiling heterogeneity in individual patients. Experimental\ntechnologies for directly profiling genomes of single cells are rapidly\nimproving, but they are still impractical for large-scale sampling. Bulk\ngenomic assays remain the standard for population-scale studies, but conflate\nthe influences of mixtures of genetically distinct tumor, stromal, and\ninfiltrating immune cells. Many computational approaches have been developed to\ndeconvolute these mixed samples and reconstruct the genomics of genetically\nhomogeneous clonal subpopulations. All such methods, however, are limited to\nreconstructing only coarse approximations to a few major subpopulations. In\nprior work, we showed that one can improve deconvolution of genomic data by\nleveraging substructure in cellular mixtures through a strategy called\nsimplicial complex inference. This strategy, however, is also limited by the\ndifficulty of inferring mixture structure from sparse, noisy assays. Results:\nWe improve on past work by introducing enhancements to automate learning of\nsubstructured genomic mixtures, with specific emphasis on genome-wide copy\nnumber variation (CNV) data. We introduce methods for dimensionality estimation\nto better decompose mixture model substructure; fuzzy clustering to better\nidentify substructure in sparse, noisy data; and automated model inference\nmethods for other key model parameters. We show that these improvements lead to\nmore accurate inference of cell populations and mixture proportions in\nsimulated scenarios. We further demonstrate their effectiveness in identifying\nmixture substructure in real tumor CNV data. Availability: Source code is\navailable at http://www.cs.cmu.edu/~russells/software/WSCUnmix.zip\n"
  },
  {
    "year": 2016,
    "title": "COCACOLA: binning metagenomic contigs using sequence COmposition, read\n  CoverAge, CO-alignment, and paired-end read LinkAge",
    "summary": "  The advent of next-generation sequencing (NGS) technologies enables\nresearchers to sequence complex microbial communities directly from\nenvironment. Since assembly typically produces only genome fragments, also\nknown as contigs, instead of entire genome, it is crucial to group them into\noperational taxonomic units (OTUs) for further taxonomic profiling and\ndown-streaming functional analysis. OTU clustering is also referred to as\nbinning. We present COCACOLA, a general framework automatically bin contigs\ninto OTUs based upon sequence composition and coverage across multiple samples.\n  The effectiveness of COCACOLA is demonstrated in both simulated and real\ndatasets in comparison to state-of-art binning approaches such as CONCOCT,\nGroopM, MaxBin and MetaBAT. The superior performance of COCACOLA relies on two\naspects. One is employing $L_{1}$ distance instead of Euclidean distance for\nbetter taxonomic identification during initialization. More importantly,\nCOCACOLA takes advantage of both hard clustering and soft clustering by\nsparsity regularization.\n  In addition, the COCACOLA framework seamlessly embraces customized knowledge\nto facilitate binning accuracy. In our study, we have investigated two types of\nadditional knowledge, the co-alignment to reference genomes and linkage of\ncontigs provided by paired-end reads, as well as the ensemble of both. We find\nthat both co-alignment and linkage information further improve binning in the\nmajority of cases. COCACOLA is scalable and faster than CONCOCT ,GroopM, MaxBin\nand MetaBAT.\n  The software is available at https://github.com/younglululu/COCACOLA\n"
  },
  {
    "year": 2016,
    "title": "Chloroplast Genome Yields Unusual Seven-Cluster Structure C",
    "summary": "  We studied the structuredness in a chloroplast genome of Siberian larch. The\nclusters in 63-dimensional space were identified with elastic map technique,\nwhere the objects to be clusterized are the different fragments of the genome.\nA seven-cluster structure in the distribution of those fragments reported\npreviously has been found. Unlike the previous results, we have found the\ndrastically other composition of the clusters comprising the fragments\nextracted from coding and non-coding regions of the genome.\n"
  },
  {
    "year": 2016,
    "title": "A frame-based representation of genomic sequences for removing errors\n  and rare variant detection in NGS data",
    "summary": "  We propose a frame-based representation of k-mers for detecting sequencing\nerrors and rare variants in next generation sequencing data obtained from\npopulations of closely related genomes. Frames are sets of non-orthogonal basis\nfunctions, traditionally used in signal processing for noise removal. We define\na frame for genomes and sequenced reads to consist of discrete spatial signals\nof every k-mer of a given size. We show that each k-mer in the sequenced data\ncan be projected onto multiple frames and these projections are maximized for\nspatial signals corresponding to the k-mer's substrings. Our proposed\nclassifier, MultiRes, is trained on the projections of k-mers as features used\nfor marking k-mers as erroneous or true variations in the genome. We evaluate\nMultiRes on simulated and real viral population datasets and compare it to\nother error correction methods known in the literature. MultiRes has 4 to 500\ntimes less false positives k-mer predictions compared to other methods,\nessential for accurate estimation of viral population diversity and their\nde-novo assembly. It has high recall of the true k-mers, comparable to other\nerror correction methods. MultiRes also has greater than 95% recall for\ndetecting single nucleotide polymorphisms (SNPs), fewer false positive SNPs,\nwhile detecting higher number of rare variants compared to other variant\ncalling methods for viral populations. The software is freely available from\nthe GitHub link (https://github.com/raunaq-m/MultiRes).\n"
  },
  {
    "year": 2016,
    "title": "An end-to-end assembly of the Aedes aegypti genome",
    "summary": "  We present an end-to-end genome assembly of a female Aedes aegypti mosquito,\nwhich spreads viral diseases such as yellow fever, dengue, chikungunya, and\nZika to humans. The assembly is based on an earlier genome published in 2007\nand improved in 2013. The new assembly has a scaffold N50 of 419Mb, with 96.9%\nof the ungapped sequence anchored to chromosomes. We used the new assembly to\nexamine the conservation of A. aegypti chromosomes. Our results suggest that\nsynteny is strongly conserved between Ae. aegypti and An. gambiae. Comparison\nto D. melanogaster highlights the extent to which the identity of entire\nchromosome arms is preserved across dipterans.\n"
  },
  {
    "year": 2016,
    "title": "A model for the clustered distribution of SNPs in the human genome",
    "summary": "  Motivated by a non-random but clustered distribution of SNPs, we introduce a\nphenomenological model to account for the clustering properties of SNPs in the\nhuman genome. The phenomenological model is based on a preferential mutation to\nthe closer proximity of existing SNPs. With the Hapmap SNP data, we empirically\ndemonstrate that the preferential model is better for illustrating the\nclustered distribution of SNPs than the random model. Moreover, the model is\napplicable not only to autosomes but also to the X chromosome, although the X\nchromosome has different characteristics from autosomes. The analysis of the\nestimated parameters in the model can explain the pronounced population\nstructure and the low genetic diversity of the X chromosome. In addition,\ncorrelation between the parameters reveals the population-wise difference of\nthe mutation probability. These results support the mutational non-independence\nhypothesis against random mutation.\n"
  },
  {
    "year": 2016,
    "title": "Transcriptional Similarity in Couples Reveals the Impact of Shared\n  Environment and Lifestyle on Gene Regulation through Modified Cytosines",
    "summary": "  Gene expression is a complex and quantitative trait that is influenced by\nboth genetic and non-genetic regulators including environmental factors.\nEvaluating the contribution of environment to gene expression regulation and\nidentifying which genes are more likely to be influenced by environmental\nfactors are important for understanding human complex traits. We hypothesize\nthat by living together as couples, there can be commonly co-regulated genes\nthat may reflect the shared living environment (e.g., diet, indoor air\npollutants, behavioral lifestyle). The lymphoblastoid cell lines (LCLs) derived\nfrom unrelated couples of African ancestry (YRI, Yoruba people from Ibadan,\nNigeria) from the International HapMap Project provided a unique model for us\nto characterize gene expression pattern in couples by comparing gene expression\nlevels between husbands and wives. Strikingly, 778 genes were found to show\nmuch smaller variances in couples than random pairs of individuals at a false\ndiscovery rate (FDR) of 5%. Since genetic variation between unrelated family\nmembers in a general population is expected to be the same assuming a\nrandom-mating society, non-genetic factors (e.g., epigenetic systems) are more\nlikely to be the mediators for the observed transcriptional similarity in\ncouples. We thus evaluated the contribution of modified cytosines to those\ngenes showing transcriptional similarity in couples as well as the\nrelationships these CpG sites with other gene regulatory elements, such as\ntranscription factor binding sites (TFBS). Our findings suggested that\ntranscriptional similarity in couples likely reflected shared common\nenvironment partially mediated through cytosine modifications.\n"
  },
  {
    "year": 2016,
    "title": "Controlling the joint local false discovery rate is more powerful than\n  meta-analysis methods in joint analysis of summary statistics from multiple\n  genome-wide association studies",
    "summary": "  In genome-wide association studies (GWASs) of common diseases/traits, we\noften analyze multiple GWASs with the same phenotype together to discover\nassociated genetic variants with higher power. Since it is difficult to access\ndata with detailed individual measurements, summary-statistics-based\nmeta-analysis methods have become popular to jointly analyze data sets from\nmultiple GWASs. In this paper, we propose a novel summary-statistics-based\njoint analysis method based on controlling the joint local false discovery rate\n(Jlfdr). We prove that our method is the most powerful summary-statistics-based\njoint analysis method when controlling the false discovery rate at a certain\nlevel. In particular, the Jlfdr-based method achieves higher power than\ncommonly used meta-analysis methods when analyzing heterogeneous data sets from\nmultiple GWASs. Simulation experiments demonstrate the superior power of our\nmethod over meta-analysis methods. Also, our method discovers more associations\nthan meta-analysis methods from empirical data sets of four phenotypes. The\nR-package is available at: http://bioinformatics.ust.hk/Jlfdr.html.\n"
  },
  {
    "year": 2016,
    "title": "Dynamic read mapping and online consensus calling for better variant\n  detection",
    "summary": "  Variant detection from high-throughput sequencing data is an essential step\nin identification of alleles involved in complex diseases and cancer. To deal\nwith these massive data, elaborated sequence analysis pipelines are employed. A\ncore component of such pipelines is a read mapping module whose accuracy\nstrongly affects the quality of resulting variant calls.\n  We propose a dynamic read mapping approach that significantly improves read\nalignment accuracy. The general idea of dynamic mapping is to continuously\nupdate the reference sequence on the basis of previously computed read\nalignments. Even though this concept already appeared in the literature, we\nbelieve that our work provides the first comprehensive analysis of this\napproach.\n  To evaluate the benefit of dynamic mapping, we developed a software pipeline\n(http://github.com/karel-brinda/dymas) that mimics different dynamic mapping\nscenarios. The pipeline was applied to compare dynamic mapping with the\nconventional static mapping and, on the other hand, with the so-called\niterative referencing - a computationally expensive procedure computing an\noptimal modification of the reference that maximizes the overall quality of all\nalignments. We conclude that in all alternatives, dynamic mapping results in a\nmuch better accuracy than static mapping, approaching the accuracy of iterative\nreferencing.\n  To correct the reference sequence in the course of dynamic mapping, we\ndeveloped an online consensus caller named OCOCO\n(http://github.com/karel-brinda/ococo). OCOCO is the first consensus caller\ncapable to process input reads in the online fashion.\n  Finally, we provide conclusions about the feasibility of dynamic mapping and\ndiscuss main obstacles that have to be overcome to implement it. We also review\na wide range of possible applications of dynamic mapping with a special\nemphasis on variant detection.\n"
  },
  {
    "year": 2016,
    "title": "Cell lineage tracing using nuclease barcoding",
    "summary": "  Lineage tracing, the determination and mapping of progeny arising from single\ncells, is an important approach enabling the elucidation of mechanisms\nunderlying diverse biological processes ranging from development to disease. We\ndeveloped a dynamic sequence-based barcode for lineage tracing and have\ndemonstrated its performance in C. elegans, a model organism whose lineage tree\nis well established. The strategy we use creates lineage trees based upon the\nintroduction of specific mutations into cells and the propagation of these\nmutations to daughter cells at each cell division. We present an experimental\nproof of concept along with a corresponding simulation and analytical model for\ndeeper understanding of the coding capacity of the system. By introducing\nmutations in a predictable manner using CRISPR/Cas9, our technology will enable\nmore complete investigations of cellular processes.\n"
  },
  {
    "year": 2016,
    "title": "An observation of circular RNAs in bacterial RNA-seq data",
    "summary": "  Circular RNAs (circRNAs) are a class of RNA with an important role in micro\nRNA (miRNA) regulation recently discovered in Human and various other\neukaryotes as well as in archaea. Here, we have analyzed RNA-seq data obtained\nfrom {\\it Enterococcus faecalis} and {\\it Escherichia coli} in a way similar to\nprevious studies performed on eukaryotes. We report observations of circRNAs in\nRNA-seq data that are reproducible across multiple experiments performed with\ndifferent protocols or growth conditions.\n"
  },
  {
    "year": 2016,
    "title": "Recent advancement in Next Generation Sequencing techniques and its\n  computational analysis",
    "summary": "  Next Generation Sequencing (NGS), a recently evolved technology, have served\na lot in the research and development sector of our society. This novel\napproach is a newbie and has critical advantages over the traditional Capillary\nElectrophoresis (CE) based Sanger Sequencing. The advancement of NGS has led to\nnumerous important discoveries, which could have been costlier and time taking\nin case of traditional CE based Sanger sequencing. NGS methods are highly\nparallelized enabling to sequence thousands to millions of molecules\nsimultaneously. This technology results into huge amount of data, which need to\nbe analysed to conclude valuable information. Specific data analysis algorithms\nare written for specific task to be performed. The algorithms in group, act as\na tool in analysing the NGS data. Analysis of NGS data unravels important clues\nin quest for the treatment of various life-threatening diseases; improved crop\nvarieties and other related scientific problems related to human welfare. In\nthis review, an effort was made to address basic background of NGS\ntechnologies, possible applications, computational approaches and tools\ninvolved in NGS data analysis, future opportunities and challenges in the area.\n"
  },
  {
    "year": 2016,
    "title": "H(O)TA: estimation of DNA methylation and hydroxylation levels and\n  efficiencies from time course data",
    "summary": "  Methylation and hydroxylation of cytosines to form 5-methylcytosine (5mC) and\n5-droxymethylcytosine (5hmC) belong to the most important epigenetic\nmodifications and their vital role in the regulation of gene expression has\nbeen widely recognized. Recent experimental techniques allow to infer\nmethylation and hydroxylation levels at CpG dinucleotides but require a\nsophisticated statistical analysis to achieve accurate estimates.\n"
  },
  {
    "year": 2016,
    "title": "Optimal Down Regulation of mRNA Translation",
    "summary": "  Down regulation of mRNA translation is an important problem in various\nbio-medical domains ranging from developing effective medicines for tumors and\nfor viral diseases to developing attenuated virus strains that can be used for\nvaccination. Here, we study the problem of down regulation of mRNA translation\nusing a mathematical model called the ribosome flow model (RFM). In the RFM,\nthe mRNA molecule is modeled as a chain of $n$ sites. The flow of ribosomes\nbetween consecutive sites is regulated by $n+1$ transition rates. Given a set\nof feasible transition rates, that models the outcome of all possible\nmutations, we consider the problem of maximally down regulating the translation\nrate by altering the rates within this set of feasible rates. Under certain\nconditions on the feasible set, we show that an optimal solution can be\ndetermined efficiently. We also rigorously analyze two special cases of the\ndown regulation optimization problem. Our results suggest that one must focus\non the position along the mRNA molecule where the transition rate has the\nstrongest effect on the protein production rate. However, this rate is not\nnecessarily the slowest transition rate along the mRNA molecule. We discuss\nsome of the biological implications of these results.\n"
  },
  {
    "year": 2016,
    "title": "Characterization of Methicillin-resistant Staphylococcus aureus Isolates\n  from Fitness Centers in Memphis Metropolitan Area, USA",
    "summary": "  Indoor skin-contact surfaces of public fitness centers may serve as\nreservoirs of potential human transmission of methicillin-resistant\nStaphylococcus aureus (MRSA). We found a high prevalence of multi-drug\nresistant (MDR)-MRSA of CC59 lineage harboring a variety of extracellular toxin\ngenes from surface swab samples collected from inanimate surfaces of fitness\ncenters in Memphis metropolitan area, USA. Our findings underscore the role of\ninanimate surfaces as potential sources of transmission of MDR-MRSA strains\nwith considerable genetic diversity.\n"
  },
  {
    "year": 2016,
    "title": "Reanalyzing variable directionality of gene expression in\n  transgenerational epigenetic inheritance",
    "summary": "  A previous report claimed no evidence of transgenerational epigenetic\ninheritance in a mouse model of in utero environmental exposure, based on the\nobservation that gene expression changes observed in the germ cells of G1 and\nG2 male fetus were not in the same direction. A subsequent data reanalysis\nhowever showed a statistically significant overlap between G1 and G2 genes\nirrespective of direction, leading to the suggestion that, as phenotypic\nvariability in epigenetic transmission has been observed in several other\nexamples also, the above report provided evidence in favor of, not against,\ntransgenerational inheritance. This criticism has recently been questioned.\nHere, it is shown that the questions raised are based not only on incorrect\nstatistical calculations but also on wrong premise that gene expression changes\ndo not constitute a phenotype.\n"
  },
  {
    "year": 2016,
    "title": "Enhancing power of rare variant association test by Zoom-Focus Algorithm\n  (ZFA) to locate optimal testing region",
    "summary": "  Motivation: Exome or targeted sequencing data exerts analytical challenge to\ntest single nucleotide polymorphisms (SNPs) with extremely small minor allele\nfrequency (MAF). Various rare variant tests were proposed to increase power by\naggregating SNPs within a fixed genomic region, such as a gene or pathway.\nHowever, a gene could contain from several to thousands of markers, and not all\nof them may be related to the phenotype. Combining functional and\nnon-functional SNPs in arbitrary genomic region could impair the testing power.\nResults: We propose a Zoom-Focus algorithm (ZFA) to locate the optimal testing\nregion within a given genomic region, as a wrapper function to be applied in\nconjunction with rare variant association tests. In the first Zooming step, a\ngiven genomic region is partitioned by order of two, and the best partition is\nlocated within all partition levels. In the next Focusing step, boundaries of\nthe zoomed region are refined. Simulation studies showed that ZFA substantially\nenhanced the statistical power of rare variant tests by over 10 folds,\nincluding the WSS, SKAT and W-test. The algorithm is applied on real exome\nsequencing data of hypertensive disorder, and identified biologically relevant\ngenetic markers to metabolic disorder that are undiscoverable by testing using\nfull gene. The proposed algorithm is an efficient and powerful tool to increase\nthe effectiveness of rare variant association tests for exome sequencing\ndatasets of complex disorder.\n"
  },
  {
    "year": 2016,
    "title": "Analysis of Chromosome 20 - A Study",
    "summary": "  Since the arrival of next-generation sequencing technologies the amount of\ngenetic sequencing data has increased dramatically. This has has fueled an\nincrease in human genetics research. At the same time, with the recent advent\nof technologies in processing large data sets, lot of these technologies are\nproving valuable and efficient in analyzing these huge datasets. In this paper\nwe use some of these technologies to analyze genetic sequencing data of 1000\nGenomes Project,produce and evaluate a framework to process the sequencing data\nthereof and look into structural variations with respect to population groups.\n"
  },
  {
    "year": 2016,
    "title": "A draft genome assembly of southern bluefin tuna Thunnus maccoyii",
    "summary": "  Tuna are large pelagic fish whose populations are close to panmixia. In\naddition, they are threatened species, so it is important for the maintenance\nand monitoring of genetic diversity that genetic information at a genome level\nbe obtained. Here we report the draft assembly of the southern bluefin tuna\ngenome and the collection of genome-wide sequence data for five other tuna\nspecies. We sampled five tuna species of the genus Thunnus, the northern and\nsouthern bluefin, yellowfin, albacore, and bigeye, as well as the skipjack\n(Katsuwonis pelamis), a tuna-like species. Genome assembly was facilitated at\nk-mer=25 while k-mer=51 generated assembly artefacts. The estimated size of the\nsouthern bluefin tuna genome was 795 Mb. We assembled two southern bluefin tuna\nindividuals independently using both paired end and mate pair sequence. This\nresulted in scaffolds with N50>174,000 bp and maximum scaffold lengths>1.4 Mb.\nOur estimate of the size of the assembled genome was the scaffolded sequences\nin common to both assemblies, which amounted to 721 Mb of the 795 Mb of the\nsouthern bluefin tuna genome sequence. Using BLAST, there were matches between\n13,039 of 14,341 (91%) refseq mRNA of the zebrafish Danio rerio to the tuna\nassembly indicating that most of a generic fish transcriptome was covered by\nthe assembly.\n"
  },
  {
    "year": 2016,
    "title": "On the Ribosomal Density that Maximizes Protein Translation Rate",
    "summary": "  During mRNA translation, several ribosomes attach to the same mRNA molecule\nsimultaneously translating it into a protein. This pipelining increases the\nprotein production rate. A natural and important question is what ribosomal\ndensity maximizes the protein production rate. Using mathematical models of\nribosome flow along both a linear and a circular mRNA molecule we prove that\ntypically the steady-state production rate is maximized when the ribosomal\ndensity is one half of the maximal possible density. We discuss the\nimplications of our results to endogenous genes under natural cellular\nconditions and also to synthetic biology.\n"
  },
  {
    "year": 2016,
    "title": "Cytomegalovirus Antigenic Mimicry of Human Alloreactive Peptides: A\n  Potential Trigger for Graft versus Host Disease",
    "summary": "  The association between human cytomegalovirus (hCMV) reactivation and the\ndevelopment of graft-versus-host-disease (GVHD) has been observed in stem cell\ntransplantation (SCT). Seventy seven SCT donor-recipient pairs (DRP) (HLA\nmatched unrelated donor (MUD), n=50; matched related donor (MRD), n=27)\nunderwent whole exome sequencing to identify single nucleotide polymorphisms\n(SNPs) generating alloreactive peptide libraries for each DRP (9-mer\npeptide-HLA complexes); Human CMV CROSS (Cross-Reactive Open Source Sequence)\nDatabase was compiled from NCBI; HLA class I binding affinity for each DRPs HLA\nwas calculated by NetMHCpan 2.8 and hCMV- derived 9-mers algorithmically\ncompared to the alloreactive peptide-HLA complex libraries. Short consecutive\n(6 or greater) amino acid (AA) sequence homology matching hCMV to recipient\npeptides was considered for HLA-bound-peptide (IC50<500 nM) cross reactivity.\nOf the 70,686 hCMV 9-mers contained within the hCMV CROSS database, 29,658.8\n+/- 9038.5 were found to match MRD DRP alloreactive peptides and 52,910.2 +/-\n16121.8 matched MUD DRP peptides (Student's T-test, p<0.001). In silico\nanalysis revealed multiple high affinity, immunogenic CMV-Human peptide matches\n(IC50<500 nM) expressed in GVHD-affected tissue-specific manner (proteins\nexpressed at 10 RPKM or greater). hCMV+GVHD was found in 18 patients, 13\ndeveloping hCMV viremia before GVHD onset with a subset analysis of 7 instances\nof hCMV viremia prior to acute GVHD onset (n=3), chronic GVHD (n=2) and acute +\nchronic GVHD (n=2) indicating cross reactive peptide expression within affected\norgans. We propose that based on our analysis and preliminary clinical\ncorrelations that hCMV immune cross-reactivity may cause antigenic mimicry of\nhuman alloreactive peptides triggering GVHD.\n"
  },
  {
    "year": 2016,
    "title": "Sam2bam: High-Performance Framework for NGS Data Preprocessing Tools",
    "summary": "  This paper introduces a high-throughput software tool framework called {\\it\nsam2bam} that enables users to significantly speedup pre-processing for\nnext-generation sequencing data. The sam2bam is especially efficient on\nsingle-node multi-core large-memory systems. It can reduce the runtime of data\npre-processing in marking duplicate reads on a single node system by 156-186x\ncompared with de facto standard tools. The sam2bam consists of parallel\nsoftware components that can fully utilize the multiple processors, available\nmemory, high-bandwidth of storage, and hardware compression accelerators if\navailable.\n  The sam2bam provides file format conversion between well-known genome file\nformats, from SAM to BAM, as a basic feature. Additional features such as\nanalyzing, filtering, and converting the input data are provided by {\\it\nplug-in} tools, e.g., duplicate marking, which can be attached to sam2bam at\nruntime.\n  We demonstrated that sam2bam could significantly reduce the runtime of NGS\ndata pre-processing from about two hours to about one minute for a whole-exome\ndata set on a 16-core single-node system using up to 130 GB of memory. The\nsam2bam could reduce the runtime for whole-genome sequencing data from about 20\nhours to about nine minutes on the same system using up to 711 GB of memory.\n"
  },
  {
    "year": 2016,
    "title": "Selecting between-sample RNA-Seq normalization methods from the\n  perspective of their assumptions",
    "summary": "  RNA-Seq is a widely-used method for studying the behavior of genes under\ndifferent biological conditions. An essential step in an RNA-Seq study is\nnormalization, in which raw data are adjusted to account for factors that\nprevent direct comparison of expression measures. Errors in normalization can\nhave a significant impact on downstream analysis, such as inflated false\npositives in differential expression analysis. An under-emphasized feature of\nnormalization is the assumptions upon which the methods rely and how the\nvalidity of these assumptions can have a substantial impact on the performance\nof the methods. In this paper, we explain how assumptions provide the link\nbetween raw RNA-Seq read counts and meaningful measures of gene expression. We\nexamine normalization methods from the perspective of their assumptions, as an\nunderstanding of methodological assumptions is necessary for choosing methods\nappropriate for the data at hand. Furthermore, we discuss why normalization\nmethods perform poorly when their assumptions are violated and how this causes\nproblems in subsequent analysis. To analyze a biological experiment,\nresearchers must select a normalization method with assumptions that are met\nand that produces a meaningful measure of expression for the given experiment.\n"
  },
  {
    "year": 2016,
    "title": "Learning Directed-Acyclic-Graphs from Large-Scale Genomics Data",
    "summary": "  In this paper we consider the problem of learning the\ngenetic-interaction-map, i.e., the topology of a directed acyclic graph (DAG)\nof genetic interactions from noisy double knockout (DK) data. Based on a set of\nwell established biological interaction models we detect and classify the\ninteractions between genes. We propose a novel linear integer optimization\nprogram called the Genetic-Interactions-Detector (GENIE) to identify the\ncomplex biological dependencies among genes and to compute the DAG topology\nthat matches the DK measurements best. Furthermore, we extend the GENIE-program\nby incorporating genetic-interactions-profile (GI-profile) data to further\nenhance the detection performance. In addition, we propose a sequential\nscalability technique for large sets of genes under study, in order to provide\nstatistically stressable results for real measurement data. Finally, we show\nvia numeric simulations that the GENIE-program as well as the GI-profile data\nextended GENIE (GI-GENIE)-program clearly outperform the conventional\ntechniques and present real data results for our proposed sequential\nscalability technique.\n"
  },
  {
    "year": 2016,
    "title": "BAUM: A DNA Assembler by Adaptive Unique Mapping and Local\n  Overlap-Layout-Consensus",
    "summary": "  Genome assembly from the high-throughput sequencing (HTS) reads is a\nfundamental yet challenging computational problem. An intrinsic challenge is\nthe uncertainty caused by the widespread repetitive elements. Here we get\naround the uncertainty using the notion of uniquely mapped (UM) reads, which\nmotivated the design of a new assembler BAUM. It mainly consists of two types\nof iterations. The first type of iterations constructs initial contigs from a\nreference, say a genome of a species that could be quite distant, by adaptive\nread mapping, filtration by the reference's unique regions, and reference\nupdating. A statistical test is proposed to split the layouts at possible\nstructural variation sites. The second type of iterations includes mapping,\nscaffolding/contig-extension, and contig merging. We extend each contig by\nlocally assembling the reads whose mates are uniquely mapped to an end of the\ncontig. Instead of the de Bruijn graph method, we take the\noverlap-layout-consensus (OLC) paradigm. The OLC is implemented by parallel\ncomputation, and has linear complexity with respect to the number of contigs.\nThe adjacent extended contigs are merged if their alignment is confirmed by the\nadjusted gap distance. Throughout the assembling, the mapping criterion is\nselected by probabilistic calculations. These innovations can be used\ncomplementary to the existing de novo assemblers. Applying this novel method to\nthe assembly of wild rice Oryza longistaminata genome, we achieved much\nimproved contig N50, 18.8k, compared with other assemblers. The assembly was\nfurther validated by contigs constructed from an independent library of long\n454 reads.\n"
  },
  {
    "year": 2016,
    "title": "Relation between Gene Content and Taxonomy in Chloroplasts",
    "summary": "  The aim of this study is to investigate the relation that can be found\nbetween the phylogeny of a large set of complete chloroplast genomes, and the\nevolution of gene content inside these sequences. Core and pan genomes have\nbeen computed on \\textit{de novo} annotation of these 845 genomes, the former\nbeing used for producing well-supported phylogenetic tree while the latter\nprovides information regarding the evolution of gene contents over time. It\ndetails too the specificity of some branches of the tree, when specificity is\nobtained on accessory genes. After having detailed the material and methods, we\nemphasize some remarkable relation between well-known events of the chloroplast\nhistory, like endosymbiosis, and the evolution of gene contents over the\nphylogenetic tree.\n"
  },
  {
    "year": 2016,
    "title": "A spectral algorithm for fast de novo layout of uncorrected long\n  nanopore reads",
    "summary": "  Motivation: New long read sequencers promise to transform sequencing and\ngenome assembly by producing reads tens of kilobases long. However their high\nerror rate significantly complicates assembly and requires expensive correction\nsteps to layout the reads using standard assembly engines.\n  Results: We present an original and efficient spectral algorithm to layout\nthe uncorrected nanopore reads, and its seamless integration into a\nstraightforward overlap/layout/consensus (OLC) assembly scheme. The method is\nshown to assemble Oxford Nanopore reads from several bacterial genomes into\ngood quality (~99% identity to the reference) genome-sized contigs, while\nyielding more fragmented assemblies from a Sacharomyces cerevisiae reference\nstrain.\n  Availability and implementation: http://github.com/antrec/spectrassembler\n  Contact: antoine.recanati@inria.fr\n"
  },
  {
    "year": 2016,
    "title": "Prediction of Prokaryotic and Eukaryotic Promoters Using Convolutional\n  Deep Learning Neural Networks",
    "summary": "  Accurate computational identification of promoters remains a challenge as\nthese key DNA regulatory regions have variable structures composed of\nfunctional motifs that provide gene specific initiation of transcription. In\nthis paper we utilize Convolutional Neural Networks (CNN) to analyze sequence\ncharacteristics of prokaryotic and eukaryotic promoters and build their\npredictive models. We trained the same CNN architecture on promoters of four\nvery distant organisms: human, plant (Arabidopsis), and two bacteria\n(Escherichia coli and Mycoplasma pneumonia). We found that CNN trained on\nsigma70 subclass of Escherichia coli promoter gives an excellent classification\nof promoters and non-promoter sequences (Sn=0.90, Sp=0.96, CC=0.84). The\nBacillus subtilis promoters identification CNN model achieves Sn=0.91, Sp=0.95,\nand CC=0.86. For human and Arabidopsis promoters we employ CNNs for\nidentification of two well-known promoter classes (TATA and non-TATA\npromoters). CNNs models nicely recognize these complex functional regions. For\nhuman Sn/Sp/CC accuracy of prediction reached 0.95/0.98/0,90 on TATA and\n0.90/0.98/0.89 for non-TATA promoter sequences, respectively. For Arabidopsis\nwe observed Sn/Sp/CC 0.95/0.97/0.91 (TATA) and 0.94/0.94/0.86 (non-TATA)\npromoters. Thus, the developed CNN models (implemented in CNNProm program)\ndemonstrated the ability of deep learning with grasping complex promoter\nsequence characteristics and achieve significantly higher accuracy compared to\nthe previously developed promoter prediction programs. As the suggested\napproach does not require knowledge of any specific promoter features, it can\nbe easily extended to identify promoters and other complex functional regions\nin sequences of many other and especially newly sequenced genomes. The CNNProm\nprogram is available to run at web server http://www.softberry.com.\n"
  },
  {
    "year": 2016,
    "title": "An Improved Filtering Algorithm for Big Read Datasets",
    "summary": "  For single-cell or metagenomic sequencing projects, it is necessary to\nsequence with a very high mean coverage in order to make sure that all parts of\nthe sample DNA get covered by the reads produced. This leads to huge datasets\nwith lots of redundant data. A filtering of this data prior to assembly is\nadvisable. Titus Brown et al. (2012) presented the algorithm Diginorm for this\npurpose, which filters reads based on the abundance of their $k$-mers. We\npresent Bignorm, a faster and quality-conscious read filtering algorithm. An\nimportant new feature is the use of phred quality scores together with a\ndetailed analysis of the $k$-mer counts to decide which reads to keep. With\nrecommended parameters, in terms of median we remove 97.15% of the reads while\nkeeping the mean phred score of the filtered dataset high. Using the SDAdes\nassembler, we produce assemblies of high quality from these filtered datasets\nin a fraction of the time needed for an assembly from the datasets filtered\nwith Diginorm. We conclude that read filtering is a practical method for\nreducing read data and for speeding up the assembly process. Our Bignorm\nalgorithm allows assemblies of competitive quality in comparison to Diginorm,\nwhile being much faster. Bignorm is available for download at\nhttps://git.informatik.uni-kiel.de/axw/Bignorm.git\n"
  },
  {
    "year": 2016,
    "title": "Pan-genome Analysis of the Genus Serratia",
    "summary": "  Pan-genome analysis is a standard procedure to decipher genome heterogeneity\nand diversification of bacterial species. Specie evolution is traced by\ndefining and comparing the core (conserved), accessory (dispensable) and unique\n(strain-specific) gene pool with other strains of interest. Here, we present\npan-genome analysis of the genus Serratia, comprising of a dataset of 100\ngenomes. The isolates have clinical to environmental origin and consist of ten\ndifferent species from the genus, along with two subspecies of the\nrepresentative strain Serratia marcescens. Out of 19430 non-redundant coding\nDNA sequences (CDS) from the dataset, 972 (5%) belonged to the core genome.\nMajority of these genes were linked to metabolic function, followed by cellular\nprocesses/signalling, information storage/processing while rest of them were\npoorly characterized. 10,135 CDSs (52.16%) were associated with dispensible\ngenome while 8,321 CDSs (42.82%) were singletons or strain specific. The\nPan-genome orthologs indicated a positive correlation to the number of genomes\nwhereas negative correlation was obtained for core genome. Genomes were aligned\nto obtain information about synteny, insertion/inversion, deletion and\nduplications. This study provides insights into variation of Serratia species\nand paves way for pan-genome analysis of other bacterial species at genus\nlevel.\n"
  },
  {
    "year": 2016,
    "title": "Computational genomic algorithms for miRNA-based diagnosis of lung\n  cancer: the potential of machine learning",
    "summary": "  The advent of large scale, high-throughput genomic screening has introduced a\nwide range of tests for diagnostic purposes. Prominent among them are tests\nusing miRNA expression levels. Genomics and proteomics now provide expression\nlevels of hundreds of miRNAs at a time. However, for actual diagnostic tools to\nbecome reality requires the simultaneous development of methods to interpret\nthe large amounts of miRNA expression data that can be generated from a single\npatient sample. Because these data are in numeric form, quantitative methods\nmust be developed. Statistics such as p-values and log fold change give some\ninsight, but the diagnostic effectiveness of each miRNA test must first be\nevaluated. Here, the author has developed a traditional, sensitivity- and\nspecificity-based algorithm, as well as a modern machine learning algorithm,\nand evaluated their diagnostic potential for lung cancer against a publicly\navailable database. The findings suggest that the machine learning algorithm\nachieves higher accuracy (97% for cancerous and 73% for normal samples), in\naddition to providing confidence intervals that could provide valuable\ndiagnostic support. The machine learning algorithm also has significant\npotential for expansion to more complex diagnoses of lung cancer sub-types, to\nother cancers as well diseases beyond cancer. Both algorithms are available on\nthe Github repo: https://github.com/neerja-g/machine-learning-miRNA.\n"
  },
  {
    "year": 2016,
    "title": "Exploring shared genetic bases and causal relationships of schizophrenia\n  and bipolar disorder with 28 cardiovascular and metabolic traits",
    "summary": "  Cardiovascular diseases (CVD) represent a major health issue in patients with\nschizophrneia (SCZ) and bipolar disorder (BD), but the exact nature of\ncardiometabolic (CM) abnormalities involved and the underlying mechanisms\nremain unclear. Using polygenic risk scores (PRS) and LD score regression, we\ninvestigated the shared genetic bases of SCZ and BD with a panel of 28\ncardiometabolic traits. We performed Mendelian randomization (MR) to elucidate\ncasual relationships between the two groups of disorders. The analysis was\nbased on large-scale meta-analyses of genome-wide association studies (GWAS).\nWe also identified the potential shared genetic variants by a statistical\napproach based on local true discovery rates, and inferred the pathways\ninvolved. We found polygenic associations of SCZ with glucose metabolism\nabnormalities, adverse adipokine profiles, increased wait-hip ratio and raised\nvisceral adiposity. However, BMI showed inverse genetic correlation and\npolygenic link with SCZ. On the other hand, we observed polygenic associations\nwith an overall favorable CM profile in BD. MR analysis showed that SCZ may be\ncausally linked to raised triglyceride and that lower fasting glucose may be\nlinked to BD; otherwise MR did not reveal other significant causal\nrelationships in general. We also identified numerous SNPs and pathways shared\nbetween SCZ/BD with cardiometabolic traits, some of which are related to\ninflammation or the immune system. In conclusion, SCZ patients may be\ngenetically associated with several CM abnormalities independent of medication\nside-effects, and proper surveillance and management of CV risk factors may be\nrequired from the onset of the disease. On the other hand, CM abnormalities in\nBD are more likely to be secondary.\n"
  },
  {
    "year": 2016,
    "title": "Discovery of cancer common and specific driver gene sets",
    "summary": "  Cancer is known as a disease mainly caused by gene alterations. Discovery of\nmutated driver pathways or gene sets is becoming an important step to\nunderstand molecular mechanisms of carcinogenesis. However, systematically\ninvestigating commonalities and specificities of driver gene sets among\nmultiple cancer types is still a great challenge, but this investigation will\nundoubtedly benefit deciphering cancers and will be helpful for personalized\ntherapy and precision medicine in cancer treatment. In this study, we propose\ntwo optimization models to \\emph{de novo} discover common driver gene sets\namong multiple cancer types (ComMDP) and specific driver gene sets of one\ncertain or multiple cancer types to other cancers (SpeMDP), respectively. We\nfirst apply ComMDP and SpeMDP to simulated data to validate their efficiency.\nThen, we further apply these methods to 12 cancer types from The Cancer Genome\nAtlas (TCGA) and obtain several biologically meaningful driver pathways. As\nexamples, we construct a common cancer pathway model for BRCA and OV, infer a\ncomplex driver pathway model for BRCA carcinogenesis based on common driver\ngene sets of BRCA with eight cancer types, and investigate specific driver\npathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)\nversus other solid cancer types. In these processes more candidate cancer genes\nare also found.\n"
  },
  {
    "year": 2017,
    "title": "HSEARCH: fast and accurate protein sequence motif search and clustering",
    "summary": "  Protein motifs are conserved fragments occurred frequently in protein\nsequences. They have significant functions, such as active site of an enzyme.\nSearch and clustering protein sequence motifs are computational intensive. Most\nexisting methods are not fast enough to analyze large data sets for motif\nfinding or achieve low accuracy for motif clustering. We present a new protein\nsequence motif finding and clustering algorithm, called HSEARCH. It converts\nfixed length protein sequences to data points in high dimensional space, and\napplies locality-sensitive hashing to fast search homologous protein sequences\nfor a motif. HSEARCH is significantly faster than the brute force algorithm for\nprotein motif finding and achieves high accuracy for protein motif clustering.\n"
  },
  {
    "year": 2017,
    "title": "Predicting the Plant Root-Associated Ecological Niche of 21 Pseudomonas\n  Species Using Machine Learning and Metabolic Modeling",
    "summary": "  Plants rarely occur in isolated systems. Bacteria can inhabit either the\nendosphere, the region inside the plant root, or the rhizosphere, the soil\nregion just outside the plant root. Our goal is to understand if using genomic\ndata and media dependent metabolic model information is better for training\nmachine learning of predicting bacterial ecological niche than media\nindependent models or pure genome based species trees. We considered three\nmachine learning techniques: support vector machine, non-negative matrix\nfactorization, and artificial neural networks. In all three machine-learning\napproaches, the media-based metabolic models and flux balance analyses were\nmore effective at predicting bacterial niche than the genome or PRMT models.\nSupport Vector Machine trained on a minimal media base with Mannose, Proline\nand Valine was most predictive of all models and media types with an f-score of\n0.8 for rhizosphere and 0.97 for endosphere. Thus we can conclude that\nmedia-based metabolic modeling provides a holistic view of the metabolome,\nallowing machine learning algorithms to highlight the differences between and\ncategorize endosphere and rhizosphere bacteria. There was no single media type\nthat best highlighted differences between endosphere and rhizosphere bacteria\nmetabolism and therefore no single enzyme, reaction, or compound that defined\nwhether a bacteria's origin was of the endosphere or rhizosphere.\n"
  },
  {
    "year": 2017,
    "title": "A generalized linear model for decomposing cis-regulatory,\n  parent-of-origin, and maternal effects on allele-specific gene expression",
    "summary": "  Joint quantification of genetic and epigenetic effects on gene expression is\nimportant for understanding the establishment of complex gene regulation\nsystems in living organisms. In particular, genomic imprinting and maternal\neffects play important roles in the developmental process of mammals and\nflowering plants. However, the influence of these effects on gene expression\nare difficult to quantify because they act simultaneously with cis-regulatory\nmutations. Here we propose a simple method to decompose cis-regulatory (i.e.,\nallelic genotype, AG), genomic imprinting (i.e., parent-of-origin, PO), and\nmaternal (i.e., maternal genotype, MG) effects on allele-specific gene\nexpression using RNA-seq data obtained from reciprocal crosses. We evaluated\nthe efficiency of method using a simulated dataset and applied the method to\nwhole-body Drosophila and mouse trophoblast stem cell (TSC) and liver RNA-seq\ndata. Consistent with previous studies, we found little evidence of PO and MG\neffects in adult Drosophila samples. In contrast, we identified dozens and\nhundreds of mouse genes with significant PO and MG effects, respectively.\nInterestingly, a similar number of genes with significant PO effect were detect\nin mouse TSCs and livers, whereas more genes with significant MG effect were\nobserved in livers. Further application of this method will clarify how these\nthree effects influence gene expression levels in different tissues and\ndevelopmental stages, and provide novel insight into the evolution of gene\nexpression regulation.\n"
  },
  {
    "year": 2017,
    "title": "Inferring clonal composition from multiple tumor biopsies",
    "summary": "  Explicit accounting for copy number alterations can dramatically improve\nmutation frequency estimates, leading to more accurate phylogeny\nreconstructions and subclone characterizations.\n"
  },
  {
    "year": 2017,
    "title": "Epigenome-wide association study and integrative analysis with the\n  transcriptome based on GWAS summary statistics",
    "summary": "  The past decade has seen a rapid growth in omics technologies. Genome-wide\nassociation studies (GWAS) have uncovered susceptibility variants for a variety\nof complex traits. However, the functional significance of most discovered\nvariants are still not fully understood. On the other hand, there is increasing\ninterest in exploring the role of epigenetic variations such as DNA methylation\nin disease pathogenesis. In this work, we present a general framework for\nepigenome-wide association study and integrative analysis with the\ntranscriptome based on GWAS summary statistics and data from methylation and\nexpression quantitative trait loci (QTL) studies. The framework is based on\nMendelian randomization, which is much less vulnerable to confounding and\nreverse causation compared to conventional studies. The framework was applied\nto five complex diseases. We first identified loci that are differentially\nmethylated due to genetic variations, and then developed several approaches for\njoint testing with the GWAS-imputed transcriptome. We discovered a number of\nnovel candidate genes that are not implicated in the original GWAS studies. We\nalso observed strong evidence (lowest p = 2.01e-184) for differential\nexpression among the top genes mapped to methylation loci. The framework\nproposed here opens a new way of analyzing GWAS summary data and will be useful\nfor gaining deeper insight into disease mechanisms.\n"
  },
  {
    "year": 2017,
    "title": "Evidence-based gene models for structural and functional annotations of\n  the oil palm genome",
    "summary": "  The advent of rapid and inexpensive DNA sequencing has led to an explosion of\ndata waiting to be transformed into knowledge about genome organization and\nfunction. Gene prediction is customarily the starting point for genome\nanalysis. This paper presents a bioinformatics study of the oil palm genome,\nincluding comparative genomics analysis, database and tools development, and\nmining of biological data for genes of interest. We have annotated 26,059 oil\npalm genes integrated from two independent gene-prediction pipelines, Fgenesh++\nand Seqping. This integrated annotation constitutes a significant improvement\nin comparison to the preliminary annotation published in 2013. We conducted a\ncomprehensive analysis of intronless, resistance and fatty acid biosynthesis\ngenes, and demonstrated that the high quality of the current genome annotation.\n3,658 intronless genes were identified in the oil palm genome, an important\nresource for evolutionary study. Further analysis of the oil palm genes\nrevealed 210 candidate resistance genes involved in pathogen defense. Fatty\nacids have diverse applications ranging from food to industrial feedstocks, and\nwe identified 42 key genes involved in fatty acid biosynthesis in oil palm.\nThese results provide an important resource for studies of plant genomes and a\ntheoretical foundation for marker-assisted breeding of oil palm and related\ncrops.\n"
  },
  {
    "year": 2017,
    "title": "Essential role of long non-coding RNAs in de novo chromatin\n  modifications: The genomic address code hypothesis",
    "summary": "  The epigenome, i.e. the whole of chromatin modifications, is transferred from\nmother to daughter cells during cell differentiation. When de novo chromatin\nmodifications (establishment or erasure of, respectively, new or pre-existing\nDNA methylations and/or histone modifications) are made in a daughter cell,\nhowever, it has a different epigenome than its mother cell. Although de novo\nchromatin modifications are an important event that comprises elementary\nprocesses of cell differentiation, its molecular mechanism remains poorly\nunderstood. We argue in this Letter that a key to solving this problem lies in\nunderstanding the role of long non-coding RNAs (lncRNAs)- a type of RNA that is\nbecoming increasingly prominent in epigenetic studies. Many studies show that\nlncRNAs form ribonucleo-protein complexes in the nucleus and are involved in\nchromatin modifications. However, chromatin-modifying enzymes lack the\ninformation about genomic positions on which they act. It is known, on the\nother hand, that a single-stranded RNA in general can bind to a double-stranded\nDNA to form a triple helix. If each lncRNA forms a ribonucleo-protein complex\nwith chromatin-modifying enzymes on one hand and, at the same time, a triple\nhelix with a genomic region based on its specific nucleotide sequence on the\nother hand, it can induce de novo chromatin modifications at specific sites.\nThus, the great variety of lncRNAs can be explained by the requirement for the\ndiversity of \"genomic address codes\" specific to their cognate genomic regions\nwhere de novo chromatin modifications take place.\n"
  },
  {
    "year": 2017,
    "title": "An Algorithm for Cellular Reprogramming",
    "summary": "  The day we understand the time evolution of subcellular elements at a level\nof detail comparable to physical systems governed by Newton's laws of motion\nseems far away. Even so, quantitative approaches to cellular dynamics add to\nour understanding of cell biology, providing data-guided frameworks that allow\nus to develop better predictions about and methods for control over specific\nbiological processes and system-wide cell behavior. In this paper we describe\nan approach to optimizing the use of transcription factors in the context of\ncellular reprogramming. We construct an approximate model for the natural\nevolution of a synchronized population of fibroblasts, based on data obtained\nby sampling the expression of some 22,083 genes at several times along the cell\ncycle. (These data are based on a colony of cells that have been cell cycle\nsynchronized) In order to arrive at a model of moderate complexity, we cluster\ngene expression based on the division of the genome into topologically\nassociating domains (TADs) and then model the dynamics of the expression levels\nof the TADs. Based on this dynamical model and known bioinformatics, we develop\na methodology for identifying the transcription factors that are the most\nlikely to be effective toward a specific cellular reprogramming task. The\napproach used is based on a device commonly used in optimal control. From this\ndata-guided methodology, we identify a number of validated transcription\nfactors used in reprogramming and/or natural differentiation. Our findings\nhighlight the immense potential of dynamical models models, mathematics, and\ndata guided methodologies for improving methods for control over biological\nprocesses.\n"
  },
  {
    "year": 2017,
    "title": "Development and characterization of Brassica juncea fruticulosa\n  introgression lines exhibiting resistance to mustard aphid",
    "summary": "  Background: Mustard aphid is a major pest of Brassica oilseeds. No source for\naphid resistance is presently available in Brassica juncea . A wild crucifer,\nBrassica fruticulosa is known to be resistant to mustard aphid. An artificially\nsynthesized amphiploid, AD-4 (B. fruticulosa x B. rapa var. brown sarson) was\ndeveloped for use as a bridge species to transfer fruticulosa resistance to B.\njuncea. Using the selfed backcross we could select a large number of lines with\nresistance to mustard aphid. This paper reports cytogenetic stability of\nintrogression lines, molecular evidence for alien introgression and their\nreaction to mustard aphid infestation. Results: Majority of introgression lines\nhad expected euploid chromosome number(2n= 36), showed normal meiosis and high\npollen grain fertility. Well-distributed and transferable simple-sequence\nrepeats (SSR) markers for all the 18 B. juncea chromosomes helped to\ncharacterize introgression events. Average proportions of recipient and donor\ngenome in the substitution lines were 49.72 and 35.06%, respectively. Minimum\nalien parent genome presence (27.29%) was observed in the introgression line,\nAd3K-280 . Introgressed genotypes also varied for their resistance responses to\nmustard aphid infestations under artificial release conditions for two\ncontinuous seasons. Some of the test genotypes showed consistent resistant\nreaction. Conclusions: B.juncea-fruticulosa introgression set may prove to be a\nvery powerful breeding tool for aphid resistance related QTL/gene discovery and\nfine mapping of the desired genes/QTLs to facilitate marker assisted transfer\nof identified gene(s) for mustard aphid resistance in the background of\ncommercial mustard genotypes.\n"
  },
  {
    "year": 2017,
    "title": "Anchor points for genome alignment based on Filtered Spaced Word Matches",
    "summary": "  Alignment of large genomic sequences is a fundamental task in computational\ngenome analysis. Most methods for genomic alignment use high-scoring local\nalignments as {\\em anchor points} to reduce the search space of the alignment\nprocedure. Speed and quality of these methods therefore depend on the\nunderlying anchor points. Herein, we propose to use {\\em Filtered Spaced Word\nMatches} to calculate anchor points for genome alignment. To evaluate this\napproach, we used these anchor points in the the widely used alignment pipeline\n{\\em Mugsy}. For distantly related sequence sets, we could substantially\nimprove the quality of alignments produced by {\\em Mugsy}.\n"
  },
  {
    "year": 2017,
    "title": "Meraculous-2D: Haplotype-sensitive Assembly of Highly Heterozygous\n  genomes",
    "summary": "  While many short read assemblers attempt to simplify the de Brujin graph by\nidentifying and resolving variant-induced bubbles to produce a haploid mosaic\nresult, this approach is only viable when variants are relatively rare and the\nbubbles are well defined in a graph context. We observed that diploid genomes\nwith very high levels of heterozygosity fail to display well-resolved bubble\nstructures in a typical assembly graph and thus result in highly fragmented and\nincomplete assemblies. Here we present an enhancement of Meraculous2 algorithm,\ncalled Meraculous-2D, which preserves haplotypes across variant sites and\ngenerates accurate assembly of highly heterozygous diploid genomes. Preserving\nand taking advantage of the allelic variation throughout the assembly process\nallows reconstructing both haplomes at once, without the need to pick arbitrary\npaths through bubble structures. We also enhanced the original diploidy\nresolution method of Meraculous2 to maintain and report phased haplotype\nvariant information.\n"
  },
  {
    "year": 2017,
    "title": "Comparative genomic analysis of the human gut microbiome reveals a broad\n  distribution of metabolic pathways for the degradation of host-synthetized\n  mucin glycans",
    "summary": "  The colonic mucus layer is a dynamic and complex structure formed by secreted\nand transmembrane mucins, which are high-molecular-weight and heavily\nglycosylated proteins. Colonic mucus consists of a loose outer layer and a\ndense epithelium-attached layer. The outer layer is inhabited by various\nrepresentatives of the human gut microbiota (HGM). Glycans of the colonic mucus\ncan be used by the HGM as a source of carbon and energy when dietary fibers are\nnot sufficiently available. Here, we analyzed 397 individual HGM genomes to\nidentify pathways for the cleavage of host-synthetized mucin glycans to\nmonosaccharides as well as for the catabolism of the derived monosaccharides.\nOur key results are as follows: (i) Genes for the cleavage of mucin glycans\nwere found in 86% of the analyzed genomes, whereas genes for the catabolism of\nderived monosaccharides were found in 89% of the analyzed genomes. (ii)\nComparative genomic analysis identified four alternative forms of the\nmonosaccharide-catabolizing enzymes and four alternative forms of\nmonosaccharide transporters. (iii) Eighty-five percent of the analyzed genomes\nmay be involved in exchange pathways for the monosaccharides derived from\ncleaved mucin glycans. (iv) The analyzed genomes demonstrated different\nabilities to degrade known mucin glycans. Generally, the ability to degrade at\nleast one type of mucin glycan was predicted for 81% of the analyzed genomes.\n(v) Eighty-two percent of the analyzed genomes can form mutualistic pairs that\nare able to degrade mucin glycans and are not degradable by any of the paired\norganisms alone. Taken together, these findings provide further insight into\nthe inter-microbial communications of the HGM as well as into host-HGM\ninteractions.\n"
  },
  {
    "year": 2017,
    "title": "ASB1 differential methylation in ischaemic cardiomyopathy. Relationship\n  with left ventricular performance in end stage heart failure patients",
    "summary": "  Aims: Ischaemic cardiomyopathy (ICM) leads to impaired contraction and\nventricular dysfunction causing high rates of morbidity and mortality.\nEpigenomics allows the identification of epigenetic signatures in human\ndiseases. We analyse the differential epigenetic patterns of ASB gene family in\nICM patients and relate these alterations to their haemodynamic and functional\nstatus. Methods and Results: Epigenomic analysis was carried out using 16 left\nventricular (LV) tissue samples, 8 from ICM patients undergoing heart\ntransplantation and 8 from control (CNT) subjects without cardiac disease. We\nincreased the sample size up to 13 ICM and 10 CNT for RNA-sequencing and to 14\nICM for pyrosequencing analyses. We found a hypermethylated profile\n(cg11189868) in the ASB1 gene that showed a differential methylation of 0.26\nbeta difference, P < 0.05. This result was validated by pyrosequencing\ntechnique (0.23 beta difference, P < 0.05). Notably, the methylation pattern\nwas strongly related to LV ejection fraction (r = -0.849, P = 0.008) stroke\nvolume (r = -0.929, P = 0.001) and end-systolic and diastolic LV diameters (r =\n-0.743, P = 0.035 for both). ASB1 showed a down regulation in mRNA levels (-1.2\nfold, P < 0.05). Conclusion: Our findings link a specific ASB1 methylation\npattern to LV structure and performance in end-stage ICM, opening new\ntherapeutic opportunities and providing new insights regarding which is the\nfunctionally relevant genome in the ischemic failing myocardium. Keywords:\nischaemic cardiomyopathy; epigenomics; heart failure; left ventricular\ndysfunction; stroke volume; ASB1.\n"
  },
  {
    "year": 2017,
    "title": "Exponential scaling of single-cell RNA-seq in the last decade",
    "summary": "  The ability to measure the transcriptomes of single cells has only been\nfeasible for a few years, and is becoming an extremely popular assay. While\nmany types of analysis and questions can be answered using single cell\nRNA-sequencing, a central focus is the ability to survey the diversity of cell\ntypes within a sample. Unbiased and reproducible cataloging of distinct cell\ntypes requires large numbers of cells. Technological developments and protocol\nimprovements have fuelled a consistent exponential increase in the numbers of\ncells studied in single cell RNA-seq analyses. In this perspective, we will\nhighlight the key technological developments which have enabled this growth in\ndata.\n"
  },
  {
    "year": 2017,
    "title": "Reconstructing antibody repertoires from error-prone immunosequencing\n  datasets",
    "summary": "  Transforming error-prone immunosequencing datasets into antibody repertoires\nis a fundamental problem in immunogenomics, and a prerequisite for studies of\nimmune responses. Although various repertoire reconstruction algorithms were\nreleased in the last three years, it remains unclear how to benchmark them and\nhow to assess the accuracy of the reconstructed repertoires. We describe a\nnovel IgReC algorithm for constructing antibody repertoires from\nhigh-throughput immunosequencing datasets and a new framework for assessing the\nquality of reconstructed repertoires. Benchmarking IgReC against the existing\nantibody repertoire reconstruction tools has demonstrated that it results in\nhighly accurate repertoire reconstructions. Surprisingly, antibody repertoires\nconstructed by IgReC from barcoded immunosequencing datasets in blind mode\n(without using unique molecular identifiers information) improved upon the\nrepertoires constructed by the state-of-the-art tools that use barcoding. This\nfinding suggests that IgReC may alleviate the need to generate repertoires\nusing the barcoding technology (the workhorse of current immunogenomics\nefforts) because our computational approach to error correction of\nimmunosequencing data ends up being nearly as powerful as the experimental\napproach based on barcoding.\n"
  },
  {
    "year": 2017,
    "title": "On the ability to reconstruct ancestral genomes from Mycobacterium genus",
    "summary": "  Technical signs of progress during the last decades has led to a situation in\nwhich the accumulation of genome sequence data is increasingly fast and cheap.\nThe huge amount of molecular data available nowadays can help addressing new\nand essential questions in Evolution. However, reconstructing evolution of DNA\nsequences requires models, algorithms, statistical and computational methods of\never increasing complexity. Since most dramatic genomic changes are caused by\ngenome rearrangements (gene duplications, gain/loss events), it becomes crucial\nto understand their mechanisms and reconstruct ancestors of the given genomes.\nThis problem is known to be NP-complete even in the \"simplest\" case of three\ngenomes. Heuristic algorithms are usually executed to provide approximations of\nthe exact solution.\n  We state that, even if the ancestral reconstruction problem is NP-hard in\ntheory, its exact resolution is feasible in various situations, encompassing\norganelles and some bacteria. Such accurate reconstruction, which identifies\ntoo some highly homoplasic mutations whose ancestral status is undecidable,\nwill be initiated in this work-in-progress, to reconstruct ancestral genomes of\ntwo Mycobacterium pathogenetic bacterias. By mixing automatic reconstruction of\nobvious situations with human interventions on signaled problematic cases, we\nwill indicate that it should be possible to achieve a concrete, complete, and\nreally accurate reconstruction of lineages of the Mycobacterium tuberculosis\ncomplex. Thus, it is possible to investigate how these genomes have evolved\nfrom their last common ancestors.\n"
  },
  {
    "year": 2017,
    "title": "Genetic load makes cancer cells more sensitive to common drugs: evidence\n  from Cancer Cell Line Encyclopedia",
    "summary": "  Genetic alterations initiate tumors and enable the evolution of drug\nresistance. The pro-cancer view of mutations is however incomplete, and several\nstudies show that mutational load can reduce tumor fitness. Given its negative\neffect, genetic load should make tumors more sensitive to anticancer drugs.\nHere, we test this hypothesis across all major types of cancer from the Cancer\nCell Line Encyclopedia, which provides genetic and expression data of 496 cell\nlines together with their response to 24 common anticancer drugs. We found that\nthe efficacy of 9 out of 24 drugs showed significant association with genetic\nload in a pan-cancer analysis. The associations for some tissue-drug\ncombinations were remarkably strong, with genetic load explaining up to 83% of\nthe variance in the drug response. Overall, the role of genetic load depended\non both the drug and the tissue type with 10 tissues being particularly\nvulnerable to genetic load. We also identified changes in gene expression\nassociated with increased genetic load, which included cell-cycle checkpoints,\nDNA damage and apoptosis. Our results show that genetic load is an important\ncomponent of tumor fitness and can predict drug sensitivity. Beyond being a\nbiomarker, genetic load might be a new, unexplored vulnerability of cancer.\n"
  },
  {
    "year": 2017,
    "title": "Genetic control of plasticity of oil yield for combined abiotic stresses\n  using a joint approach of crop modeling and genome-wide association",
    "summary": "  Understanding the genetic basis of phenotypic plasticity is crucial for\npredicting and managing climate change effects on wild plants and crops. Here,\nwe combined crop modeling and quantitative genetics to study the genetic\ncontrol of oil yield plasticity for multiple abiotic stresses in sunflower.\n  First we developed stress indicators to characterize 14 environments for\nthree abiotic stresses (cold, drought and nitrogen) using the SUNFLO crop model\nand phenotypic variations of three commercial varieties. The computed plant\nstress indicators better explain yield variation than descriptors at the\nclimatic or crop levels. In those environments, we observed oil yield of 317\nsunflower hybrids and regressed it with three selected stress indicators. The\nslopes of cold stress norm reaction were used as plasticity phenotypes in the\nfollowing genome-wide association study.\n  Among the 65,534 tested SNP, we identified nine QTL controlling oil yield\nplasticity to cold stress. Associated SNP are localized in genes previously\nshown to be involved in cold stress responses: oligopeptide transporters, LTP,\ncystatin, alternative oxidase, or root development. This novel approach opens\nnew perspectives to identify genomic regions involved in\ngenotype-by-environment interaction of a complex traits to multiple stresses in\nrealistic natural or agronomical conditions.\n"
  },
  {
    "year": 2017,
    "title": "Essentiality, conservation, evolutionary pressure and codon bias in\n  bacterial genes",
    "summary": "  Essential genes constitute the core of genes which cannot be mutated too much\nnor lost along the evolutionary history of a species. Natural selection is\nexpected to be stricter on essential genes and on conserved (highly shared)\ngenes, than on genes that are either nonessential or peculiar to a single or a\nfew species. In order to further assess this expectation, we study here how\nessentiality of a gene is connected with its degree of conservation among\nseveral unrelated bacterial species, each one characterised by its own codon\nusage bias. Confirming previous results on E. coli, we show the existence of a\nuniversal exponential relation between gene essentiality and conservation in\nbacteria. Moreover we show that, within each bacterial genome, there are at\nleast two groups of functionally distinct genes, characterised by different\nlevels of conservation and codon bias: i) a core of essential genes, mainly\nrelated to cellular information processing; ii) a set of less conserved\nnonessential genes with prevalent functions related to metabolism. In\nparticular, the genes in the first group are more retained among species, are\nsubject to a stronger purifying conservative selection and display a more\nlimited repertoire of synonymous codons. The core of essential genes is close\nto the minimal bacterial genome, which is in the focus of recent studies in\nsynthetic biology, though we confirm that orthologs of genes that are essential\nin one species are not necessarily essential in other species. We also list a\nset of highly shared genes which, reasonably, could constitute a reservoir of\ntargets for new anti-microbial drugs.\n"
  },
  {
    "year": 2017,
    "title": "Role of distal enhancers in shaping 3D-folding patterns and defining\n  human-specific features of interphase chromatin architecture in embryonic\n  stem cells",
    "summary": "  Molecular and genetic definitions of human-specific changes to genomic\nregulatory networks (GRNs) contributing to development of unique to human\nphenotypes remain a highly significant challenge. Genome-wide proximity\nplacement analysis of diverse families of human-specific genomic regulatory\nloci (HSGRL) identified topologically-associating domains (TADs) that are\nsignificantly enriched for HSGRL and designated rapidly-evolving in humans TADs\n(Genome Biol Evol. 2016 8; 2774-88). Here, the analysis of HSGRL, hESC-enriched\nenhancers, super-enhancers (SEs), and specific sub-TAD structures termed\nsuper-enhancer domains (SEDs) has been performed. Markedly distinct features of\nthe principal regulatory structures of interphase chromatin evolved in the hESC\ngenome compared to mouse: the SED quantity is 3-fold higher and the median SED\nsize is significantly larger. Concomitantly, the overall TAD quantity is\nincreased by 42% while the median TAD size is significantly decreased (p =\n9.11E-37) in the hESC genome. Present analyses illustrate a putative global\nrole for HSGRL in shaping the human-specific features of the interphase\nchromatin organization and functions, which are facilitated by accelerated\ncreation of new enhancers associated with targeted placement of HSGRL at\ndefined genomic coordinates. A trend toward the convergence of TAD and SED\narchitectures of interphase chromatin in the hESC genome may reflect changes of\n3D-folding patterns of linear chromatin fibers designed to enhance both\nregulatory complexity and functional precision of GRNs by creating\npredominantly a single gene per regulatory domain structures.\n"
  },
  {
    "year": 2017,
    "title": "MDA in Capillary for Whole Genome Amplification",
    "summary": "  Whole genome amplification (WGA) plays an important role in sample\npreparation of low-input templates for high-throughput sequencing. Multiple\ndisplacement amplification (MDA), a popular isothermal WGA method, suffers a\nmajor hurdle of highly uneven amplification. Optimizations have been made in\nthe past by separating the reagents into numbers of tiny chambers or droplets\nin microfluidic devices, which significantly improves the amplification\nuniformity of MDA. However, skill barrier still exists for biological\nresearchers to handle chip fabrication and droplet manipulation. Here, we\npresent a novel MDA protocol, in-capillary MDA (icMDA), which significantly\nsimplifies the manipulation and improves the uniformity of amplification by\ndispersing reagents in a long quasi-1D capillary tubing. We demonstrated that\nicMDA is able to accurately detect SNVs with higher efficiency and sensitivity.\nMoreover, this straightforward method employs neither customized instruments\nnor complicated operations, making it a ready-to-use approach for most\nlaboratories.\n"
  },
  {
    "year": 2017,
    "title": "Convolutional Kitchen Sinks for Transcription Factor Binding Site\n  Prediction",
    "summary": "  We present a simple and efficient method for prediction of transcription\nfactor binding sites from DNA sequence. Our method computes a random\napproximation of a convolutional kernel feature map from DNA sequence and then\nlearns a linear model from the approximated feature map. Our method outperforms\nstate-of-the-art deep learning methods on five out of six test datasets from\nthe ENCODE consortium, while training in less than one eighth the time.\n"
  },
  {
    "year": 2017,
    "title": "Relation between Insertion Sequences and Genome Rearrangements in\n  Pseudomonas aeruginosa",
    "summary": "  During evolution of microorganisms genomes underwork have different changes\nin their lengths, gene orders, and gene contents. Investigating these\nstructural rearrangements helps to understand how genomes have been modified\nover time. Some elements that play an important role in genome rearrangements\nare called insertion sequences (ISs), they are the simplest types of\ntransposable elements (TEs) that widely spread within prokaryotic genomes. ISs\ncan be defined as DNA segments that have the ability to move (cut and paste)\nthemselves to another location within the same chromosome or not. Due to their\nability to move around, they are often presented as responsible of some of\nthese genomic recombination. Authors of this research work have regarded this\nclaim, by checking if a relation between insertion sequences (ISs) and genome\nrearrangements can be found. To achieve this goal, a new pipeline that combines\nvarious tools have firstly been designed, for detecting the distribution of\nORFs that belongs to each IS category. Secondly, links between these predicted\nISs and observed rearrangements of two close genomes have been investigated, by\nseeing them with the naked eye, and by using computational approaches. The\nproposal has been tested on 18 complete bacterial genomes of Pseudomonas\naeruginosa, leading to the conclusion that IS3 family of insertion sequences\nare related to genomic inversions.\n"
  },
  {
    "year": 2017,
    "title": "Genetic variation in human drug-related genes",
    "summary": "  Variability in drug efficacy and adverse effects are observed in clinical\npractice. While the extent of genetic variability in classical pharmacokinetic\ngenes is rather well understood, the role of genetic variation in drug targets\nis typically less studied. Based on 60,706 human exomes from the ExAC dataset,\nwe performed an in-depth computational analysis of the prevalence of\nfunctional-variants in in 806 drug-related genes, including 628 known drug\ntargets. We find that most genetic variants in these genes are very rare (f <\n0.1%) and thus likely not observed in clinical trials. Overall, however, four\nin five patients are likely to carry a functional-variant in a target for\ncommonly prescribed drugs and many of these might alter drug efficacy. We\nfurther computed the likelihood of 1,236 FDA approved drugs to be affected by\nfunctional-variants in their targets and show that the patient-risk varies for\nmany drugs with respect to geographic ancestry. A focused analysis of\noncological drug targets indicates that the probability of a patient carrying\ngermline variants in oncological drug targets is with 44% high enough to\nsuggest that not only somatic alterations, but also germline variants carried\nover into the tumor genome should be included in therapeutic decision-making.\n"
  },
  {
    "year": 2017,
    "title": "A Pipeline for Insertion Sequence Detection and Study for Bacterial\n  Genome",
    "summary": "  Insertion Sequences (ISs) are small DNA segments that have the ability of\nmoving themselves into genomes. These types of mobile genetic elements (MGEs)\nseem to play an essential role in genomes rearrangements and evolution of\nprokaryotic genomes, but the tools that deal with discovering ISs in an\nefficient and accurate way are still too few and not totally precise. Two main\nfactors have big effects on IS discovery, namely: genes annotation and\nfunctionality prediction. Indeed, some specific genes called \"transposases\" are\nenzymes that are responsible of the production and catalysis for such\ntransposition, but there is currently no fully accurate method that could\ndecide whether a given predicted gene is either a real transposase or not. This\nis why authors of this article aim at designing a novel pipeline for ISs\ndetection and classification, which embeds the most recently available tools\ndeveloped in this field of research, namely OASIS (Optimized Annotation System\nfor Insertion Sequence) and ISFinder database (an up-to-date and accurate\nrepository of known insertion sequences). As this latter depend on predicted\ncoding sequences, the proposed pipeline will encompass too various kinds of\nbacterial genes annotation tools (that is, Prokka, BASys, and Prodigal). A\ncomplete IS detection and classification pipeline is then proposed and tested\non a set of 23 complete genomes of Pseudomonas aeruginosa. This pipeline can\nalso be used as an investigator of annotation tools performance, which has led\nus to conclude that Prodigal is the best software for IS prediction. A deepen\nstudy regarding IS elements in P.aeruginosa has then been conducted, leading to\nthe conclusion that close genomes inside this species have also a close numbers\nof IS families and groups.\n"
  },
  {
    "year": 2017,
    "title": "A Stochastic Model for the Formation of Spatial Methylation Patterns",
    "summary": "  DNA methylation is an epigenetic mechanism whose important role in\ndevelopment has been widely recognized. This epigenetic modification results in\nheritable changes in gene expression not encoded by the DNA sequence. The\nunderlying mechanisms controlling DNA methylation are only partly understood\nand recently different mechanistic models of enzyme activities responsible for\nDNA methylation have been proposed. Here we extend existing Hidden Markov\nModels (HMMs) for DNA methylation by describing the occurrence of spatial\nmethylation patterns over time and propose several models with different\nneighborhood dependencies. We perform numerical analysis of the HMMs applied to\nbisulfite sequencing measurements and accurately predict wild-type data. In\naddition, we find evidence that the enzymes' activities depend on the left 5'\nneighborhood but not on the right 3' neighborhood.\n"
  },
  {
    "year": 2017,
    "title": "MAGNET: Understanding and Improving the Accuracy of Genome Pre-Alignment\n  Filtering",
    "summary": "  In the era of high throughput DNA sequencing (HTS) technologies, calculating\nthe edit distance (i.e., the minimum number of substitutions, insertions, and\ndeletions between a pair of sequences) for billions of genomic sequences is the\ncomputational bottleneck in todays read mappers. The shifted Hamming distance\n(SHD) algorithm proposes a fast filtering strategy that can rapidly filter out\ninvalid mappings that have more edits than allowed. However, SHD shows high\ninaccuracy in its filtering by admitting invalid mappings to be marked as\ncorrect ones. This wastes the execution time and imposes a large computational\nburden. In this work, we comprehensively investigate four sources that lead to\nthe filtering inaccuracy. We propose MAGNET, a new filtering strategy that\nmaintains high accuracy across different edit distance thresholds and data\nsets. It significantly improves the accuracy of pre-alignment filtering by one\nto two orders of magnitude. The MATLAB implementations of MAGNET and SHD are\nopen source and available at: https://github.com/BilkentCompGen/MAGNET.\n"
  },
  {
    "year": 2017,
    "title": "Discovery of new drug therapeutic indications from gene mutation\n  information for hepatocellular carcinoma",
    "summary": "  Hepatocellular carcinoma (HCC) is the most common primary liver malignancy\nand is a leading cause of cancer-related death worldwide. However, cure is not\npossible with currently used therapies, and there is not so much approved\ntargeted therapy for HCC despite numerous attempts and clinical trials. So, it\nis essential to identify additional therapeutic strategies to block the growth\nof HCC tumors. As a cancer disease, it is associated with aberrant genomic and\ntranscriptional landscapes. We sought to use a systematic drug repositioning\nbioinformatics approach to identify novel candidate drugs to treat HCC, which\nconsiders not only aberrant genomic information, but also the changes of\ntranscriptional landscapes. First, we screen the collection of HCC feature\ngenes that frequently mutated in most samples of HCC based on human mutation\ndata. Then, the gene expression data of HCC in TCGA are combined to classify\nthe kernel genes of HCC. Finally, the therapeutic score (TS) of each drug is\ncalculated based on the kolmogorov-smirnov statistical method. Using this\nstrategy, we identified five drugs that associated with HCC, including three\ndrugs that could treat HCC and two drugs that might have side-effect on HCC. In\naddition, we also make Connectivity Map (CMap) profiles similarity analysis and\nKEGG enrichment analysis on drug targets. All these findings suggest that our\napproach is effective for accurate discovering novel therapeutic options for\nHCC and easily to be extended to other tumors.\n"
  },
  {
    "year": 2017,
    "title": "Minimap2: pairwise alignment for nucleotide sequences",
    "summary": "  Motivation: Recent advances in sequencing technologies promise ultra-long\nreads of $\\sim$100 kilo bases (kb) in average, full-length mRNA or cDNA reads\nin high throughput and genomic contigs over 100 mega bases (Mb) in length.\nExisting alignment programs are unable or inefficient to process such data at\nscale, which presses for the development of new alignment algorithms.\n  Results: Minimap2 is a general-purpose alignment program to map DNA or long\nmRNA sequences against a large reference database. It works with accurate short\nreads of $\\ge$100bp in length, $\\ge$1kb genomic reads at error rate $\\sim$15%,\nfull-length noisy Direct RNA or cDNA reads, and assembly contigs or closely\nrelated full chromosomes of hundreds of megabases in length. Minimap2 does\nsplit-read alignment, employs concave gap cost for long insertions and\ndeletions (INDELs) and introduces new heuristics to reduce spurious alignments.\nIt is 3-4 times faster than mainstream short-read mappers at comparable\naccuracy and $\\ge$30 times faster at higher accuracy for both genomic and mRNA\nreads, surpassing most aligners specialized in one type of alignment.\n  Availability and implementation: https://github.com/lh3/minimap2\n  Contact: hengli@broadinstitute.org\n"
  },
  {
    "year": 2017,
    "title": "Determining whether the non-protein-coding DNA sequences are in a\n  complex interactive relationship by using an artificial intelligence method",
    "summary": "  Non protein coding regions of the human genome contain many complex patterns\nwhich regulate the cellular activity. Studying the human genome is limited by\nthe lack of understanding of its features and their complex interactions.\nHowever, recent advances in AI research have enabled automatically learning\nrepresentations of high dimensional complex data without feature engineering,\nusing deep neural networks. Therefore, in this paper, we demonstrate that a\nconvolutional neural network can learn a representation of DNA sequence without\nspecifying any motifs or patterns, such that it becomes capable of predicting\nwhether a DNA sequence is natural or artificial. The trained model could\ndistinguish scrambled vs real DNA sequences for scrambling lengths of 2 bp, 10\nbp, 50 bp and even 100 bp, with a significantly higher accuracy than linear\nSVMs. With this study, we have discovered that regions of non protein coding\nDNA might have meaningful interactions at even longer than 100 bp distances\neven though they do not code proteins.\n"
  },
  {
    "year": 2017,
    "title": "GRIM-filter: fast seed filtering in read mapping using emerging memory\n  technologies",
    "summary": "  Motivation: Seed filtering is critical in DNA read mapping, a process where\nbillions of DNA fragments (reads) sampled from a donor are mapped onto a\nreference genome to identify genomic variants of the donor. Read mappers 1)\nquickly generate possible mapping locations (i.e., seeds) for each read, 2)\nextract reference sequences at each of the mapping locations, and then 3) check\nsimilarity between each read and its associated reference sequences with a\ncomputationally expensive dynamic programming algorithm (alignment) to\ndetermine the origin of the read. Location filters come into play before\nalignment, discarding seed locations that alignment would have deemed a poor\nmatch. The ideal location filter would discard all poor matching locations\nprior to alignment such that there is no wasted computation on poor alignments.\n  Results: We propose a novel filtering algorithm, GRIM-Filter, optimized to\nexploit emerging 3D-stacked memory systems that integrate computation within a\nstacked logic layer, enabling processing-in-memory (PIM). GRIM-Filter quickly\nfilters locations by 1) introducing a new representation of coarse-grained\nsegments of the reference genome and 2) using massively-parallel in-memory\noperations to identify read presence within each coarse-grained segment. Our\nevaluations show that for 5% error acceptance rates, GRIM-Filter eliminates\n5.59x-6.41x more false negatives and exhibits end-to-end speedups of\n1.81x-3.65x compared to mappers employing the best previous filtering\nalgorithm.\n"
  },
  {
    "year": 2017,
    "title": "Integrative analysis reveals disrupted pathways regulated by microRNAs\n  in cancer",
    "summary": "  MicroRNAs (miRNAs) are small endogenous regulatory molecules that modulate\ngene expression post-transcriptionally. Although differential expression of\nmiRNAs have been implicated in many diseases (including cancers), the\nunderlying mechanisms of action remain unclear. Because each miRNA can target\nmultiple genes, miRNAs may potentially have functional implications for the\noverall behavior of entire pathways. Here we investigate the functional\nconsequences of miRNA dysregulation through an integrative analysis of miRNA\nand mRNA expression data using a novel approach that incorporates pathway\ninformation a priori. By searching for miRNA-pathway associations that differ\nbetween healthy and tumor tissue, we identify specific relationships at the\nsystems-level which are disrupted in cancer. Our approach is motivated by the\nhypothesis that if a miRNA and pathway are associated, then the expression of\nthe miRNA and the collective behavior of the genes in a pathway will be\ncorrelated. As such, we first obtain an expression-based summary of pathway\nactivity using Isomap, a dimension reduction method which can articulate\nnonlinear structure in high-dimensional data. We then search for miRNAs that\nexhibit differential correlations with the pathway summary between phenotypes\nas a means of finding aberrant miRNA-pathway coregulation in tumors. We apply\nour method to cancer data using gene and miRNA expression datasets from The\nCancer Genome Atlas (TCGA) and compare ${\\sim}10^5$ miRNA-pathway relationships\nbetween healthy and tumor samples from four tissues (breast, prostate, lung,\nand liver). Many of the flagged pairs we identify have a biological basis for\ndisruption in cancer.\n"
  },
  {
    "year": 2017,
    "title": "Integrate Multi-omic Data Using Affinity Network Fusion (ANF) for Cancer\n  Patient Clustering",
    "summary": "  Clustering cancer patients into subgroups and identifying cancer subtypes is\nan important task in cancer genomics. Clustering based on comprehensive\nmulti-omic molecular profiling can often achieve better results than those\nusing a single data type, since each omic data type (representing one view of\npatients) may contain complementary information. However, it is challenging to\nintegrate heterogeneous omic data types directly. Based on one popular method\n-- Similarity Network Fusion (SNF), we presented Affinity Network Fusion (ANF)\nin this paper, an \"upgrade\" of SNF with several advantages. Similar to SNF, ANF\ntreats each omic data type as one view of patients and learns a fused affinity\n(transition) matrix for clustering. We applied ANF to a carefully processed\nharmonized cancer dataset downloaded from GDC data portals consisting of 2193\npatients, and generated promising results on clustering patients into correct\ndisease types. Our experimental results also demonstrated the power of feature\nselection and transformation combined with using ANF in patient clustering.\nMoreover, eigengap analysis suggests that the learned affinity matrices of four\ncancer types using our proposed framework may have successfully captured\npatient group structure and can be used for discovering unknown cancer\nsubtypes.\n"
  },
  {
    "year": 2017,
    "title": "Method for identification of condition-associated public antigen\n  receptor sequences",
    "summary": "  Diverse repertoires of hypervariable immunoglobulin receptors (TCR and BCR)\nrecognize antigens in the adaptive immune system. The development of\nimmunoglobulin receptor repertoire sequencing methods makes it possible to\nperform repertoire-wide disease association studies of antigen receptor\nsequences. We developed a statistical framework for associating receptors to\ndisease from only a small cohort of patients, with no need for a control\ncohort. Our method successfully identifies previously validated Cytomegalovirus\nand type 1 diabetes responsive receptors.\n"
  },
  {
    "year": 2017,
    "title": "MetaHMM: A Webserver for Identifying Novel Genes with Specified\n  Functions in Metagenomic Samples",
    "summary": "  The fast and affordable sequencing of large clinical and environmental\nmetagenomic datasets opens up new horizons in medical and biotechnological\napplications. It is believed that today we have described only about 1\\% of the\nmicroorganisms on the Earth, therefore, metagenomic analysis mostly deals with\nunknown species in the samples. Microbial communities in extreme environments\nmay contain genes with high biotechnological potential, and clinical\nmetagenomes, related to diseases, may uncover still unknown pathogens and\npathological mechanisms in known diseases. While the species-level\nidentification and description of the taxa in the samples does not seem to be\npossible today, we can search for novel genes with known functions in these\nsamples, using numerous techniques, including artificial intelligence tools,\nlike the hidden Markov models (HMMs). Here we describe a simple-to-use\nwebserver, the MetaHMM, which is capable of homology-based automatic\nmodel-building for the genes to be searched for, and it also finds the closest\nmatches in the metagenome. The webserver uses already highly successful\nbuilding blocks: it performs multiple alignment by applying Clustal Omega,\nbuilds a hidden Markov model with HMMER components of hmmbuild and uses\nhmmsearch for finding similar sequences to the specified model in the\nmetagenomes. The webserver is publicly available at\n\\url{https://metahmm.pitgroup.org}.\n"
  },
  {
    "year": 2017,
    "title": "Nanopore Sequencing Technology and Tools for Genome Assembly:\n  Computational Analysis of the Current State, Bottlenecks and Future\n  Directions",
    "summary": "  Nanopore sequencing technology has the potential to render other sequencing\ntechnologies obsolete with its ability to generate long reads and provide\nportability. However, high error rates of the technology pose a challenge while\ngenerating accurate genome assemblies. The tools used for nanopore sequence\nanalysis are of critical importance as they should overcome the high error\nrates of the technology. Our goal in this work is to comprehensively analyze\ncurrent publicly available tools for nanopore sequence analysis to understand\ntheir advantages, disadvantages, and performance bottlenecks. It is important\nto understand where the current tools do not perform well to develop better\ntools. To this end, we 1) analyze the multiple steps and the associated tools\nin the genome assembly pipeline using nanopore sequence data, and 2) provide\nguidelines for determining the appropriate tools for each step. We analyze\nvarious combinations of different tools and expose the tradeoffs between\naccuracy, performance, memory usage and scalability. We conclude that our\nobservations can guide researchers and practitioners in making conscious and\neffective choices for each step of the genome assembly pipeline using nanopore\nsequence data. Also, with the help of bottlenecks we have found, developers can\nimprove the current tools or build new ones that are both accurate and fast, in\norder to overcome the high error rates of the nanopore sequencing technology.\n"
  },
  {
    "year": 2017,
    "title": "Ococo: an online variant and consensus caller",
    "summary": "  Motivation: Identifying genomic variants is an essential step for connecting\ngenotype and phenotype. The usual approach consists of statistical inference of\nvariants from alignments of sequencing reads. State-of-the-art variant callers\ncan resolve a wide range of different variant types with high accuracy.\nHowever, they require that all read alignments be available from the beginning\nof variant calling and be sorted by coordinates. Sorting is computationally\nexpensive, both memory- and speed-wise, and the resulting pipelines suffer from\nstoring and retrieving large alignments files from external memory. Therefore,\nthere is interest in developing methods for resource-efficient variant calling.\n  Results: We present Ococo, the first program capable of inferring variants in\na real-time, as read alignments are fed in. Ococo inputs unsorted alignments\nfrom a stream and infers single-nucleotide variants, together with a genomic\nconsensus, using statistics stored in compact several-bit counters. Ococo\nprovides a fast and memory-efficient alternative to the usual variant calling.\nIt is particularly advantageous when reads are sequenced or mapped\nprogressively, or when available computational resources are at a premium.\n"
  },
  {
    "year": 2017,
    "title": "DCJVis: visualization of genome rearrangements using DCJ operations",
    "summary": "  The {\\em double-cut-and-join} (DCJ) operation, introduced by Yancopoulos\n\\emph{et al.}, allows minimum edit distance to be computed by modeling all\npossible classical rearrangement operations, such as inversions, fusions,\nfissions, translocations, and transpositions, in linear-time between two\ngenomes. However, there is lack of visualization tool that can effectively\npresent DCJ operations that will help biologists to use DCJ operation. In this\npaper, a new visualization program is introduced, DCJVis, to create a diagram\nof each DCJ operation necessary to transform between the genomes of two\ndistinct organisms by describing a possible sequence of genome graphs based on\nthe selected gene adjacency on the source genome for the DCJ operation. Our\nprogram is the first visualization tool for DCJ operations using circular\nlayout. Specifically, the genomes of \\textit{Saccharomyces cerevisiae} and\n\\textit{Candida albicans} are used to demonstrate the functionality of this\nprogram and provide an example of the type of problem this program can solve\nfor biologists.\n"
  },
  {
    "year": 2018,
    "title": "Universal and idiosyncratic characteristic lengths in bacterial genomes",
    "summary": "  In condensed matter physics, simplified descriptions are obtained by\ncoarse-graining the features of a system at a certain characteristic length,\ndefined as the typical length beyond which some properties are no longer\ncorrelated. From a physics standpoint, in vitro DNA has thus a characteristic\nlength of 300 base pairs (bp), the Kuhn length of the molecule beyond which\ncorrelations in its orientations are typically lost. From a biology standpoint,\nin vivo DNA has a characteristic length of 1000 bp, the typical length of\ngenes. Since bacteria live in very different physico-chemical conditions and\nsince their genomes lack translational invariance, whether larger, universal\ncharacteristic lengths exist is a non-trivial question. Here, we examine this\nproblem by leveraging the large number of fully sequenced genomes available in\npublic databases. By analyzing GC content correlations and the evolutionary\nconservation of gene contexts (synteny) in hundreds of bacterial chromosomes,\nwe conclude that a fundamental characteristic length around 10-20 kb can be\ndefined. This characteristic length reflects elementary structures involved in\nthe coordination of gene expression, which are present all along the genome of\nnearly all bacteria. Technically, reaching this conclusion required us to\nimplement methods that are insensitive to the presence of large idiosyncratic\ngenomic features, which may co-exist along these fundamental universal\nstructures.\n"
  },
  {
    "year": 2018,
    "title": "Differential proteomics highlights macrophage-specific responses to\n  amorphous silica nanoparticles",
    "summary": "  The technological and economic benefits of engineered nanomaterials may be\noffset by their adverse effects on living organisms. One of the highly produced\nnanomaterials under such scrutiny is amorphous silica nanoparticles, which are\nknown to have an appreciable, although reversible, inflammatory potential. This\nis due to their selective toxicity toward macrophages, and it is thus important\nto study the cellular responses of this cell type to silica nanoparticles to\nbetter understand the direct or indirect adverse effects of nanosilica. We have\nhere studied the responses of the RAW264.7 murine macrophage cells and of the\ncontrol MPC11 plasma cells to subtoxic concentrations of nanosilica, using a\ncombination of pro-teomic and targeted approaches. This allowed us to document\nalterations in the cellular cytoskeleton, in the phagocytic capacity of the\ncells as well as their ability to respond to bacterial stimuli. More\nsurprisingly, silica nanoparticles also induce a greater sensitivity of\nmacrophages to DNA alkylating agents, such as styrene oxide, even at doses\nwhich do not induce any appreciable cell death.\n"
  },
  {
    "year": 2018,
    "title": "Zinc oxide induces the stringent response and major reorientations in\n  the central metabolism of Bacillus subtilis",
    "summary": "  Microorganisms, such as bacteria, are one of the first targets of\nnanoparticles in the environment. In this study, we tested the effect of two\nnanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the\nGram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based\nproteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no\ndetectable effect on the proteomic pattern, while ZnO NPs and ZnSO4\nsignificantly modified B. subtilis metabolism. These results allowed us to\nconclude that the effects of ZnO observed in this work were mainly attributable\nto Zn dissolution in the culture media. Proteomic analysis highlighted twelve\nmodulated proteins related to central metabolism: MetE and MccB (cysteine\nmetabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,\nYqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free\nsulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed\ncentral metabolism reorientation and showed that Zn stress induced oxidative\nstress, probably as a consequence of thiol chelation stress by Zn ions. The\nother patterns affected by ZnO and ZnSO4 were the stringent response and the\ngeneral stress response. Nine proteins involved in or controlled by the\nstringent response showed a modified expression profile in the presence of ZnO\nNPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An\nincrease in the ppGpp concentration confirmed the involvement of the stringent\nresponse during a Zn stress. All these metabolic reorientations in response to\nZn stress were probably the result of complex regulatory mechanisms including\nat least the stringent response via YwaC.\n"
  },
  {
    "year": 2018,
    "title": "NGS Based Haplotype Assembly Using Matrix Completion",
    "summary": "  We apply matrix completion methods for haplotype assembly from NGS reads to\ndevelop the new HapSVT, HapNuc, and HapOPT algorithms. This is performed by\napplying a mathematical model to convert the reads to an incomplete matrix and\nestimating unknown components. This process is followed by quantizing and\ndecoding the completed matrix in order to estimate haplotypes. These algorithms\nare compared to the state-of-the-art algorithms using simulated data as well as\nthe real fosmid data. It is shown that the SNP missing rate and the haplotype\nblock length of the proposed HapOPT are better than those of HapCUT2 with\ncomparable accuracy in terms of reconstruction rate and switch error rate. A\nprogram implementing the proposed algorithms in MATLAB is freely available at\nhttps://github.com/smajidian/HapMC.\n"
  },
  {
    "year": 2018,
    "title": "Eight-cluster structure of chloroplast genomes differs from similar one\n  observed for bacteria",
    "summary": "  Previously, a seven-cluster pattern claiming to be a universal one in\nbacterial genomes has been reported. Keeping in mind the most popular theory of\nchloroplast origin, we checked whether a similar pattern is observed in\nchloroplast genomes. Surprisingly, eight cluster structure has been found, for\nchloroplasts. The pattern observed for chloroplasts differs rather\nsignificantly, from bacterial one, and from that latter observed for\ncyanobacteria. The structure is provided by clustering of the fragments of\nequal length isolated within a genome so that each fragment is converted in\ntriplet frequency dictionary with non-overlapping triplets with no gaps in\nframe tiling. The points in 63-dimensional space were clustered due to elastic\nmap technique. The eight cluster found in chloroplasts comprises the fragments\nof a genome bearing tRNA genes and exhibiting excessively high\n$\\mathsf{GC}$-content, in comparison to the entire genome.\n"
  },
  {
    "year": 2018,
    "title": "Predicting the spectrum of TCR repertoire sharing with a data-driven\n  model of recombination",
    "summary": "  Despite the extreme diversity of T cell repertoires, many identical T-cell\nreceptor (TCR) sequences are found in a large number of individual mice and\nhumans. These widely-shared sequences, often referred to as `public', have been\nsuggested to be over-represented due to their potential immune functionality or\ntheir ease of generation by V(D)J recombination. Here we show that even for\nlarge cohorts the observed degree of sharing of TCR sequences between\nindividuals is well predicted by a model accounting for by the known\nquantitative statistical biases in the generation process, together with a\nsimple model of thymic selection. Whether a sequence is shared by many\nindividuals is predicted to depend on the number of queried individuals and the\nsampling depth, as well as on the sequence itself, in agreement with the data.\nWe introduce the degree of publicness conditional on the queried cohort size\nand the size of the sampled repertoires. Based on these observations we propose\na public/private sequence classifier, `PUBLIC' (Public Universal Binary\nLikelihood Inference Classifier), based on the generation probability, which\nperforms very well even for small cohort sizes.\n"
  },
  {
    "year": 2018,
    "title": "Single nucleotide polymorphisms that modulate microRNA regulation of\n  gene expression in tumors",
    "summary": "  Genome-wide association studies (GWAS) have identified single nucleotide\npolymorphisms (SNPs) associated with trait diversity and disease\nsusceptibility, yet the functional properties of many genetic variants and\ntheir molecular interactions remains unclear. It has been hypothesized that\nSNPs in microRNA binding sites may disrupt gene regulation by microRNAs\n(miRNAs), short non-coding RNAs that bind to mRNA and downregulate the target\ngene. While a number of studies have been conducted to predict the location of\nSNPs in miRNA binding sites, to date there has been no comprehensive analysis\nof how SNP variants may impact miRNA regulation of genes. Here we investigate\nthe functional properties of genetic variants and their effects on miRNA\nregulation of gene expression in cancer. Our analysis is motivated by the\nhypothesis that distinct alleles may cause differential binding (from miRNAs to\nmRNAs or from transcription factors to DNA) and change the expression of genes.\nWe previously identified pathways--systems of genes conferring specific cell\nfunctions--that are dysregulated by miRNAs in cancer, by comparing\nmiRNA-pathway associations between healthy and tumor tissue. We draw on these\nresults as a starting point to assess whether SNPs in genes on dysregulated\npathways are responsible for miRNA dysregulation of individual genes in tumors.\nUsing an integrative analysis that incorporates miRNA expression, mRNA\nexpression, and SNP genotype data, we identify SNPs that appear to influence\nthe association between miRNAs and genes, which we term \"regulatory QTLs\n(regQTLs)\": loci whose alleles impact the regulation of genes by miRNAs. We\ndescribe the method, apply it to analyze four cancer types (breast, liver,\nlung, prostate) using data from The Cancer Genome Atlas (TCGA), and provide a\ntool to explore the findings.\n"
  },
  {
    "year": 2018,
    "title": "Minimum error correction-based haplotype assembly: considerations for\n  long read data",
    "summary": "  The single nucleotide polymorphism (SNP) is the most widely studied type of\ngenetic variation. A haplotype is defined as the sequence of alleles at SNP\nsites on each haploid chromosome. Haplotype information is essential in\nunravelling the genome-phenotype association. Haplotype assembly is a\nwell-known approach for reconstructing haplotypes, exploiting reads generated\nby DNA sequencing devices. The Minimum Error Correction (MEC) metric is often\nused for reconstruction of haplotypes from reads. However, problems with the\nMEC metric have been reported. Here, we investigate the MEC approach to\ndemonstrate that it may result in incorrectly reconstructed haplotypes for\ndevices that produce error-prone long reads. Specifically, we evaluate this\napproach for devices developed by Illumina, Pacific BioSciences and Oxford\nNanopore Technologies. We show that imprecise haplotypes may be reconstructed\nwith a lower MEC than that of the exact haplotype. The performance of MEC is\nexplored for different coverage levels and error rates of data. Our simulation\nresults reveal that in order to avoid incorrect MEC-based haplotypes, a\ncoverage of 25 is needed for reads generated by Pacific BioSciences RS systems.\n"
  },
  {
    "year": 2018,
    "title": "The bromodomain-containing protein Ibd1 links multiple chromatin related\n  protein complexes to highly expressed genes in Tetrahymena thermophila",
    "summary": "  Background: The chromatin remodelers of the SWI/SNF family are critical\ntranscriptional regulators. Recognition of lysine acetylation through a\nbromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,\nthis function is attributed to SNF2/Brg1.\n  Results: Using affinity purification coupled to mass spectrometry (AP-MS) we\nidentified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.\nSWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,\ntwo proteins with potential chromatin interacting domains and four proteins\nwithout orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits\nlocalize exclusively to the transcriptionally active macronucleus (MAC) during\ngrowth and development, consistent with a role in transcription. While\nTetrahymena Brg1 does not contain a BRD, our AP-MS results identified a\nBRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during\ngrowth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it\nto be a subunit of several additional protein complexes, including putative\nSWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl\ntransferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on\nhistones correlated with active transcription. Consistent with our AP-MS and\nhistone array data suggesting a role in regulation of gene expression, ChIP-Seq\nanalysis of Ibd1 indicated that it primarily binds near promoters and within\ngene bodies of highly expressed genes during growth.\n  Conclusions: Our results suggest that through recognizing specific histones\nmarks, Ibd1 targets active chromatin regions of highly expressed genes in\nTetrahymena where it subsequently might coordinate the recruitment of several\nchromatin remodeling complexes to regulate the transcriptional landscape of\nvegetatively growing Tetrahymena cells.\n"
  },
  {
    "year": 2018,
    "title": "Modeling and analysis of RNA-seq data: a review from a statistical\n  perspective",
    "summary": "  Background: Since the invention of next-generation RNA sequencing (RNA-seq)\ntechnologies, they have become a powerful tool to study the presence and\nquantity of RNA molecules in biological samples and have revolutionized\ntranscriptomic studies. The analysis of RNA-seq data at four different levels\n(samples, genes, transcripts, and exons) involve multiple statistical and\ncomputational questions, some of which remain challenging up to date.\n  Results: We review RNA-seq analysis tools at the sample, gene, transcript,\nand exon levels from a statistical perspective. We also highlight the\nbiological and statistical questions of most practical considerations.\n  Conclusion: The development of statistical and computational methods for\nanalyzing RNA- seq data has made significant advances in the past decade.\nHowever, methods developed to answer the same biological question often rely on\ndiverse statical models and exhibit different performance under different\nscenarios. This review discusses and compares multiple commonly used\nstatistical models regarding their assumptions, in the hope of helping users\nselect appropriate methods as needed, as well as assisting developers for\nfuture method development.\n"
  },
  {
    "year": 2018,
    "title": "Identification of a complete YPT1 Rab GTPase sequence from the fungal\n  pathogen Colletotrichum incanum",
    "summary": "  Colletotrichum represent a genus of fungal species primarily known as plant\npathogens with severe economic impacts in temperate, subtropical and tropical\nclimates Consensus taxonomy and classification systems for Colletotrichum\nspecies have been undergoing revision as high resolution genomic data becomes\navailable. Here we propose an alternative annotation that provides a complete\nsequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun\nsequence of Colletotrichum incanum isolated from soybean crops in Illinois,\nUSA.\n"
  },
  {
    "year": 2018,
    "title": "The Qatar Genome: A Population-Specific Tool for Precision Medicine in\n  the Middle East",
    "summary": "  Reaching the full potential of precision medicine depends on the quality of\npersonalized genome interpretation. In order to facilitate precision medicine\nin regions of the Middle East and North Africa (MENA), a population-specific\nreference genome for the indigenous Arab popula-tion of Qatar (QTRG) was\nconstructed by incorporating allele frequency data from sequencing of 1,161\nQataris, representing 0.4% of the population. A total of 20.9 million SNP and\n3.1 million indels were observed in Qatar, including an average of 1.79% novel\nvariants per individual ge-nome. Replacement of the GRCh37 standard reference\nwith QTRG in a best practices genome analysis workflow resulted in an average\nof 7* deeper coverage depth (an improvement of 23%), and 756,671 fewer variants\non average, a reduction of 16% that is attributed to common Qatari alleles\nbeing present in the QTRG reference. The benefit for using QTRG varies across\nances-tries, a factor that should be taken into consideration when selecting an\nappropriate reference for analysis.\n"
  },
  {
    "year": 2018,
    "title": "A quick guide for student-driven community genome annotation",
    "summary": "  High quality gene models are necessary to expand the molecular and genetic\ntools available for a target organism, but these are available for only a\nhandful of model organisms that have undergone extensive curation and\nexperimental validation over the course of many years. The majority of gene\nmodels present in biological databases today have been identified in draft\ngenome assemblies using automated annotation pipelines that are frequently\nbased on orthologs from distantly related model organisms. Manual curation is\ntime consuming and often requires substantial expertise, but is instrumental in\nimproving gene model structure and identification. Manual annotation may seem\nto be a daunting and cost-prohibitive task for small research communities but\ninvolving undergraduates in community genome annotation consortiums can be\nmutually beneficial for both education and improved genomic resources. We\noutline a workflow for efficient manual annotation driven by a team of\nprimarily undergraduate annotators. This model can be scaled to large teams and\nincludes quality control processes through incremental evaluation. Moreover, it\ngives students an opportunity to increase their understanding of genome biology\nand to participate in scientific research in collaboration with peers and\nsenior researchers at multiple institutions.\n"
  },
  {
    "year": 2018,
    "title": "Transcription Factor-DNA Binding Via Machine Learning Ensembles",
    "summary": "  We present ensemble methods in a machine learning (ML) framework combining\npredictions from five known motif/binding site exploration algorithms. For a\ngiven TF the ensemble starts with position weight matrices (PWM's) for the\nmotif, collected from the component algorithms. Using dimension reduction, we\nidentify significant PWM-based subspaces for analysis. Within each subspace a\nmachine classifier is built for identifying the TF's gene (promoter) targets\n(Problem 1). These PWM-based subspaces form an ML-based sequence analysis tool.\nProblem 2 (finding binding motifs) is solved by agglomerating k-mer (string)\nfeature PWM-based subspaces that stand out in identifying gene targets. We\napproach Problem 3 (binding sites) with a novel machine learning approach that\nuses promoter string features and ML importance scores in a classification\nalgorithm locating binding sites across the genome. For target gene\nidentification this method improves performance (measured by the F1 score) by\nabout 10 percentage points over the (a) motif scanning method and (b) the\ncoexpression-based association method. Top motif outperformed 5 component\nalgorithms as well as two other common algorithms (BEST and DEME). For\nidentifying individual binding sites on a benchmark cross species database\n(Tompa et al., 2005) we match the best performer without much human\nintervention. It also improved the performance on mammalian TFs.\n  The ensemble can integrate orthogonal information from different weak\nlearners (potentially using entirely different types of features) into a\nmachine learner that can perform consistently better for more TFs. The TF gene\ntarget identification component (problem 1 above) is useful in constructing a\ntranscriptional regulatory network from known TF-target associations. The\nensemble is easily extendable to include more tools as well as future PWM-based\ninformation.\n"
  },
  {
    "year": 2018,
    "title": "The sequencing and interpretation of the genome obtained from a Serbian\n  individual",
    "summary": "  Recent genetic studies and whole-genome sequencing projects have greatly\nimproved our understanding of human variation and clinically actionable genetic\ninformation. Smaller ethnic populations, however, remain underrepresented in\nboth individual and large-scale sequencing efforts and hence present an\nopportunity to discover new variants of biomedical and demographic\nsignificance. This report describes the sequencing and analysis of a genome\nobtained from an individual of Serbian origin, introducing tens of thousands of\npreviously unknown variants to the currently available pool. Ancestry analysis\nplaces this individual in close proximity of the Central and Eastern European\npopulations; i.e., closest to Croatian, Bulgarian and Hungarian individuals\nand, in terms of other Europeans, furthest from Ashkenazi Jewish, Spanish,\nSicilian, and Baltic individuals. Our analysis confirmed gene flow between\nNeanderthal and ancestral pan-European populations, with similar contributions\nto the Serbian genome as those observed in other European groups. Finally, to\nassess the burden of potentially disease-causing/clinically relevant variation\nin the sequenced genome, we utilized manually curated genotype-phenotype\nassociation databases and variant-effect predictors. We identified several\nvariants that have previously been associated with severe early-onset disease\nthat is not evident in the proband, as well as variants that could yet prove to\nbe clinically relevant to the proband over the next decades. The presence of\nnumerous private and low-frequency variants along with the observed and\npredicted disease-causing mutations in this genome exemplify some of the global\nchallenges of genome interpretation, especially in the context of understudied\nethnic groups.\n"
  },
  {
    "year": 2018,
    "title": "Quantifying Local Randomness in Human DNA and RNA Sequences Using Erdos\n  Motifs",
    "summary": "  In 1932, Paul Erdos asked whether a random walk constructed from a binary\nsequence can achieve the lowest possible deviation (lowest discrepancy), for\nthe sequence itself and for all its subsequences formed by homogeneous\narithmetic progressions. Although avoiding low discrepancy is impossible for\ninfinite sequences, as recently proven by Terence Tao, attempts were made to\nconstruct such sequences with finite lengths. We recognize that such\nconstructed sequences (we call these \"Erdos sequences\") exhibit certain\nhallmarks of randomness at the local level: they show roughly equal frequencies\nof subsequences, and at the same time exclude the trivial periodic patterns.\nFor the human DNA we examine the frequency of a set of Erdos motifs of\nlength-10 using three nucleotides-to-binary mappings. The particular length-10\nErdos sequence is derived by the length-11 Mathias sequence and is identical\nwith the first 10 digits of the Thue-Morse sequence, underscoring the fact that\nboth are deficient in periodicities. Our calculations indicate that: (1) the\npurine (A and G)/pyridimine (C and T) based Erdos motifs are greatly\nunderrepresented in the human genome, (2) the strong(G and C)/weak(A and T)\nbased Erdos motifs are slightly overrepresented, (3) the densities of the two\nare negatively correlated, (4) the Erdos motifs based on all three mappings\nbeing combined are slightly underrepresented, and (5) the strong/weak based\nErdos motifs are greatly overrepresented in the human messenger RNA sequences.\n"
  },
  {
    "year": 2018,
    "title": "Identifying viruses from metagenomic data by deep learning",
    "summary": "  The recent development of metagenomic sequencing makes it possible to\nsequence microbial genomes including viruses in an environmental sample.\nIdentifying viral sequences from metagenomic data is critical for downstream\nvirus analyses. The existing reference-based and gene homology-based methods\nare not efficient in identifying unknown viruses or short viral sequences. Here\nwe have developed a reference-free and alignment-free machine learning method,\nDeepVirFinder, for predicting viral sequences in metagenomic data using deep\nlearning techniques. DeepVirFinder was trained based on a large number of viral\nsequences discovered before May 2015. Evaluated on the sequences after that\ndate, DeepVirFinder outperformed the state-of-the-art method VirFinder at all\ncontig lengths. Enlarging the training data by adding millions of purified\nviral sequences from environmental metavirome samples significantly improves\nthe accuracy for predicting under-represented viruses. Applying DeepVirFinder\nto real human gut metagenomic samples from patients with colorectal carcinoma\n(CRC) identified 51,138 viral sequences belonging to 175 bins. Ten bins were\nassociated with the cancer status, indicating their potential use for\nnon-invasive diagnosis of CRC. In summary, DeepVirFinder greatly improved the\nprecision and recall rates of viral identification, and it will significantly\naccelerate the discovery rate of viruses.\n"
  },
  {
    "year": 2018,
    "title": "Innovative method for reducing uninformative calls in non-invasive\n  prenatal testing",
    "summary": "  Non-invasive prenatal testing or NIPT is currently among the top researched\ntopic in obstetric care. While the performance of the current state-of-the-art\nNIPT solutions achieve high sensitivity and specificity, they still struggle\nwith a considerable number of samples that cannot be concluded with certainty.\nSuch uninformative results are often subject to repeated blood sampling and\nre-analysis, usually after two weeks, and this period may cause a stress to the\nfuture mothers as well as increase the overall cost of the test. We propose a\nsupplementary method to traditional z-scores to reduce the number of such\nuninformative calls. The method is based on a novel analysis of the length\nprofile of circulating cell free DNA which compares the change in such profiles\nwhen random-based and length-based elimination of some fragments is performed.\nThe proposed method is not as accurate as the standard z-score; however, our\nresults suggest that combination of these two independent methods correctly\nresolves a substantial portion of healthy samples with an uninformative result.\nAdditionally, we discuss how the proposed method can be used to identify\nmaternal aberrations, thus reducing the risk of false positive and false\nnegative calls.\n  Keywords: Next-generation sequencing, Cell-free DNA, Uninformative result,\nMethod, Trisomy, Prenatal testing\n"
  },
  {
    "year": 2018,
    "title": "Genesis of the alpha beta T-cell receptor",
    "summary": "  The T-cell (TCR) repertoire relies on the diversity of receptors composed of\ntwo chains, called $\\alpha$ and $\\beta$, to recognize pathogens. Using results\nof high throughput sequencing and computational chain-pairing experiments of\nhuman TCR repertoires, we quantitively characterize the $\\alpha\\beta$\ngeneration process. We estimate the probabilities of a rescue recombination of\nthe $\\beta$ chain on the second chromosome upon failure or success on the first\nchromosome. Unlike $\\beta$ chains, $\\alpha$ chains recombine simultaneously on\nboth chromosomes, resulting in correlated statistics of the two genes which we\npredict using a mechanistic model. We find that $\\sim 28 \\%$ of cells express\nboth $\\alpha$ chains. We report that clones sharing the same $\\beta$ chain but\ndifferent $\\alpha$ chains are overrepresented, suggesting that they respond to\ncommon immune challenges. Altogether, our statistical analysis gives a complete\nquantitative mechanistic picture that results in the observed correlations in\nthe generative process. We learn that the probability to generate any\nTCR$\\alpha\\beta$ is lower than $10^{-12}$ and estimate the generation diversity\nand sharing properties of the $\\alpha\\beta$ TCR repertoire.\n"
  },
  {
    "year": 2018,
    "title": "The exon junction complex undergoes a compositional switch that alters\n  mRNP structure and nonsense-mediated mRNA decay activity",
    "summary": "  The exon junction complex (EJC) deposited upstream of mRNA exon junctions\nshapes structure, composition and fate of spliced mRNA ribonucleoprotein\nparticles (mRNPs). To achieve this, the EJC core nucleates assembly of a\ndynamic shell of peripheral proteins that function in diverse\npost-transcriptional processes. To illuminate consequences of EJC composition\nchange, we purified EJCs from human cells via peripheral proteins RNPS1 and\nCASC3. We show that EJC originates as an SR-rich mega-dalton sized RNP that\ncontains RNPS1 but lacks CASC3. After mRNP export to the cytoplasm and before\ntranslation, the EJC undergoes a remarkable compositional and structural\nremodeling into an SR-devoid monomeric complex that contains CASC3.\nSurprisingly, RNPS1 is important for nonsense-mediated mRNA decay (NMD) in\ngeneral whereas CASC3 is needed for NMD of only select mRNAs. The promotion of\nswitch to CASC3-EJC slows down NMD. Overall, the EJC compositional switch\ndramatically alters mRNP structure and specifies two distinct phases of\nEJC-dependent NMD.\n"
  },
  {
    "year": 2018,
    "title": "OLGA: fast computation of generation probabilities of B- and T-cell\n  receptor amino acid sequences and motifs",
    "summary": "  Motivation: High-throughput sequencing of large immune repertoires has\nenabled the development of methods to predict the probability of generation by\nV(D)J recombination of T- and B-cell receptors of any specific nucleotide\nsequence. These generation probabilities are very non-homogeneous, ranging over\n20 orders of magnitude in real repertoires. Since the function of a receptor\nreally depends on its protein sequence, it is important to be able to predict\nthis probability of generation at the amino acid level. However, brute-force\nsummation over all the nucleotide sequences with the correct amino acid\ntranslation is computationally intractable. The purpose of this paper is to\npresent a solution to this problem.\n  Results: We use dynamic programming to construct an efficient and flexible\nalgorithm, called OLGA (Optimized Likelihood estimate of immunoGlobulin\nAmino-acid sequences), for calculating the probability of generating a given\nCDR3 amino acid sequence or motif, with or without V/J restriction, as a result\nof V(D)J recombination in B or T cells. We apply it to databases of\nepitope-specific T-cell receptors to evaluate the probability that a typical\nhuman subject will possess T cells responsive to specific disease-associated\nepitopes. The model prediction shows an excellent agreement with published\ndata. We suggest that OLGA may be a useful tool to guide vaccine design.\n  Availability: Source code is available at https://github.com/zsethna/OLGA\n"
  },
  {
    "year": 2018,
    "title": "Detecting T-cell receptors involved in immune responses from single\n  repertoire snapshots",
    "summary": "  Hypervariable T-cell receptors (TCR) play a key role in adaptive immunity,\nrecognising a vast diversity of pathogen-derived antigens. High throughput\nsequencing of TCR repertoires (RepSeq) produces huge datasets of T-cell\nreceptor sequences from blood and tissue samples. However, our ability to\nextract clinically relevant information from RepSeq data is limited, mainly\nbecause little is known about TCR-disease associations. Here we present a\nstatistical approach called ALICE (Antigen-specific Lymphocyte Identification\nby Clustering of Expanded sequences) that identifies TCR sequences that are\nactively involved in the current immune response from a single RepSeq sample,\nand apply it to repertoires of patients with a variety of disorders -\nautoimmune disease (ankylosing spondylitis), patients under cancer\nimmunotherapy, or subject to an acute infection (live yellow fever vaccine).\nThe method's robustness is demonstrated by the agreement of its predictions\nwith independent assays, and is supported by its ability to selectively detect\nresponding TCR in the memory but not in the naive subset. ALICE requires no\nlongitudinal data collection nor large cohorts, and is thus directly applicable\nto most RepSeq datasets. Its results facilitate the identification of TCR\nvariants associated with a wide variety of diseases and conditions, which can\nbe used for diagnostics, rational vaccine design and evaluation of the adaptive\nimmune system state.\n"
  },
  {
    "year": 2018,
    "title": "Evolution of biosequence search algorithms: a brief survey",
    "summary": "  The paper surveys the evolution of main algorithmic techniques to compare and\nsearch biological sequences. We highlight key algorithmic ideas emerged in\nresponse to several interconnected factors: shifts of biological analytical\nparadigm, advent of new sequencing technologies, and a substantial increase in\nsize of the available data. We discuss the expansion of alignment-free\ntechniques coming to replace alignment-based algorithms in large-scale\nanalyses. We further emphasize recently emerged and growing applications of\nsketching methods which support comparison of massive datasets, such as\nmetagenomics samples. Finally, we focus on the transition to population\ngenomics and outline associated algorithmic challenges.\n"
  },
  {
    "year": 2018,
    "title": "Quantitative and functional post-translational modification proteomics\n  reveals that TREPH1 plays a role in plant thigmomorphogenesis",
    "summary": "  Plants can sense both intracellular and extracellular mechanical forces and\ncan respond through morphological changes. The signaling components responsible\nfor mechanotransduction of the touch response are largely unknown. Here, we\nperformed a high-throughput SILIA (stable isotope labeling in\nArabidopsis)-based quantitative phosphoproteomics analysis to profile changes\nin protein phosphorylation resulting from 40 seconds of force stimulation in\nArabidopsis thaliana. Of the 24 touch-responsive phosphopeptides identified,\nmany were derived from kinases, phosphatases, cytoskeleton proteins, membrane\nproteins and ion transporters. TOUCH-REGULATED PHOSPHOPROTEIN1 (TREPH1) and MAP\nKINASE KINASE 2 (MKK2) and/or MKK1 became rapidly phosphorylated in\ntouch-stimulated plants. Both TREPH1 and MKK2 are required for touch-induced\ndelayed flowering, a major component of thigmomorphogenesis. The treph1-1 and\nmkk2 mutants also exhibited defects in touch-inducible gene expression. A\nnon-phosphorylatable site-specific isoform of TREPH1 (S625A) failed to restore\ntouch-induced flowering delay of treph1-1, indicating the necessity of S625 for\nTREPH1 function and providing evidence consistent with the possible functional\nrelevance of the touch-regulated TREPH1 phosphorylation. Bioinformatic analysis\nand biochemical subcellular fractionation of TREPH1 protein indicate that it is\na soluble protein. Altogether, these findings identify new protein players in\nArabidopsis thigmomorphogenesis regulation, suggesting that protein\nphosphorylation may play a critical role in plant force responses.\n"
  },
  {
    "year": 2018,
    "title": "SSCU: an R/Bioconductor package for analyzing selective profile in\n  synonymous codon usage",
    "summary": "  Background Synonymous codon choice is mainly affected by mutation and\nselection. For the majority of genes within a genome, mutational pressure is\nthe major driving force, but selective strength can be strong and dominant for\nspecific set of genes or codons. More specifically, the selective strength on\ntranslational efficiency and accuracy increases with the gene's expression\nlevel. Many statistical approaches have been developed to evaluate and quantify\nthe selective profile in codon usage, including S index and Akashi's test, but\nno program or pipeline has been developed that includes these tests and\nautomates the calculation.\n  Results In this study, we release an R package SSCU (selective strength for\ncodon usage, v2.4.0), which includes tools for codon usage analyses. The\npackage identifies optimal codons using two approaches (comparative and\ncorrelative methods), implements well-established statistics for detecting\ncodon selection, such as S index, Akashi's test, and estimates standard genomic\nstatistics, such as genomic GC3, RSCU and Nc.\n  Conclusions The package is useful for researchers working on the codon usage\nanalysis, and thus has general interest to the biological research community.\nThe package is deposited and curated at the Bioconductor site, and has\ncurrently been downloaded for more than 2000 times and ranked as top 50%\npackages.\n"
  },
  {
    "year": 2018,
    "title": "Pharmacogenomics in the Age of GWAS, Omics Atlases, and PheWAS",
    "summary": "  The search for causative pharmacogenomic loci is being transformed by\nintegrative omics pipelines, but their outputs have only begun being applied to\ntest design. We assess the direction of the field in light of Biobanks/PheWAS,\nomics atlases, and AI. We first assess the potential of recent epigenome and\nspatial genome concepts, datasets, and methods to improve the functionality of\nPIP-style pipelines. We then discuss new potential methods of genetic test\ndesign on the basis of the outputs of such pipelines. We conclude with a vision\nfor a pharmacophenomic atlas, in which omics atlas data, PheWAS associations,\nand biobank data would be used with AI to design thousands of genetic tests for\nclinical deployment in an automated parallel process.\n"
  },
  {
    "year": 2018,
    "title": "Whole genome resequencing reveals diagnostic markers for investigating\n  global migration and hybridization between minke whale species",
    "summary": "  Background: In the marine environment, where there are few absolute physical\nbarriers, contemporary contact between previously isolated species can occur\nacross great distances, and in some cases, may be inter-oceanic. [..] in the\nminke whale species complex [...] migrations [..] have been documented and\nfertile hybrids and back-crossed individuals between both species have also\nbeen identified. However, it is not known whether this represents a\ncontemporary event, potentially driven by ecosystem changes in the Antarctic,\nor a sporadic occurrence happening over an evolutionary time-scale. We\nsuccessfully used whole genome resequencing to identify a panel of diagnostic\nSNPs which now enable us address this evolutionary question.\n  Results: A large number of SNPs displaying fixed or nearly fixed allele\nfrequency differences among the minke whale species were identified from the\nsequence data. Five panels of putatively diagnostic markers were established on\na genotyping platform for validation of allele frequencies; two panels (26 and\n24 SNPs) separating the two species of minke whale, and three panels (22, 23,\nand 24 SNPs) differentiating the three subspecies of common minke whale. The\npanels were validated against a set of reference samples, demonstrating the\nability to accurately identify back-crossed whales up to three generations.\n  Conclusions: This work has resulted in the development of a panel of novel\ndiagnostic genetic markers to address inter-oceanic and global contact among\nthe genetically isolated minke whale species and sub-species. These markers,\nincluding a globally relevant genetic reference data set for this species\ncomplex, are now openly available for researchers [..]. The approach used here,\ncombining whole genome resequencing and high-throughput genotyping, represents\na universal approach to develop similar tools for other species and population\ncomplexes.\n"
  },
  {
    "year": 2018,
    "title": "A reproducible effect size is more useful than an irreproducible\n  hypothesis test to analyze high throughput sequencing datasets",
    "summary": "  Motivation: P values derived from the null hypothesis significance testing\nframework are strongly affected by sample size, and are known to be\nirreproducible in underpowered studies, yet no suitable replacement has been\nproposed. Results: Here we present implementations of non-parametric\nstandardized median effect size estimates, dNEF, for high-throughput sequencing\ndatasets. Case studies are shown for transcriptome and tag-sequencing datasets.\nThe dNEF measure is shown to be more reproducible and robust than P values and\nrequires sample sizes as small as 3 to reproducibly identify differentially\nabundant features. Availability: Source code and binaries freely available at:\nhttps://bioconductor.org/packages/ALDEx2.html , omicplotR, and\nhttps://github.com/ggloor/CoDaSeq .\n"
  },
  {
    "year": 2018,
    "title": "Virus genome sequence classification using features based on\n  nucleotides, words and compression",
    "summary": "  The ICTV develops, refines and maintains a universal virus taxonomy; Order is\nthe highest taxon in the branching hierarchy of recognised viral taxa.\nHistorically, ICTV (sub)committees have classified viruses on the basis of\nmorphological characteristics and various other attributes. Today, virtually\nall new viral genomes are assembled from metagenomic datasets and are not\nlinked directly to biological agents. Thus, placing a virus into a taxonomic\nscheme solely from primary genome structure is an increasingly important\nproblem. Various simple descriptive statistics of a viral genome sequence have\nbeen used successfully for virus classification. Here, we use the NCBI's viral\nand viroid reference sequence collection (RefSeq) and a common experimental\nframework to compare the performance of different genome sequence-derived\nfeatures and classifiers in the task of assigning a virus to one of seven ICTV\nOrders. The nucleotide-, word-, and compression-based features we consider\ninclude genome length, the k-mer Natural Vector (k = 1, ..., 6) and its\nderivatives, return time distribution, and general-purpose and DNA-specific\ncompression ratios; the classifiers used are the k-NN and SVM. The combination\nof genome length and k-NN has the worst, yet still respectable, performance\n(mean error rate of 0.137); the best performance is achieved using 4-mer counts\nand SVM (mean error rate of 0.006). We investigate the main causes of\nmisclassification, explore which viruses are more difficult to classify, and\nuse the best performing combination to predict the Orders of 1,834 unclassified\nviruses. A subsequent version of RefSeq assigned Orders to 17 of these\npreviously unlabelled viruses. Since 16 of our predictions match these\nassignments, our approach could aid virologists dealing with viruses that are\nknown only from sequence data.\n"
  },
  {
    "year": 2018,
    "title": "Effect of Blast Exposure on Gene-Gene Interactions",
    "summary": "  Repeated exposure to low-level blast may initiate a range of adverse health\nproblem such as traumatic brain injury (TBI). Although many studies\nsuccessfully identified genes associated with TBI, yet the cellular mechanisms\nunderpinning TBI are not fully elucidated. In this study, we investigated\nunderlying relationship among genes through constructing transcript Bayesian\nnetworks using RNA-seq data. The data for pre- and post-blast transcripts,\nwhich were collected on 33 individuals in Army training program, combined with\nour system approach provide unique opportunity to investigate the effect of\nblast-wave exposure on gene-gene interactions. Digging into the networks, we\nidentified four subnetworks related to immune system and inflammatory process\nthat are disrupted due to the exposure. Among genes with relatively high fold\nchange in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,\nS100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the\ngene-gene interactions. This study reveals how repeated exposures to traumatic\nconditions increase the level of fold change of transcript expression and\nhypothesizes new targets for further experimental studies.\n"
  },
  {
    "year": 2018,
    "title": "Integrated systems approach identifies pathways from the genome to\n  triglycerides through a metabolomic causal network",
    "summary": "  Introduction: To leverage functionality and clinical relevance into\nunderstanding systems biology, one needs to understand the pathway of the\ngenetic effects on risk factors/disease through intermediate molecular levels,\nsuch as metabolomics. Systems approaches integrate multi-omic information to\nfind pathways to disease endpoints and make optimal inference decisions.\nMethod: Here, we introduce a multi-stage approach to integrate causal networks\nin observational studies and GWAS to facilitate mechanistic understanding\nthrough identification of pathways from the genome to risk factors/disease via\nmetabolomics. The pathways in causal networks reveal the underlying\nrelationships behind observations, which do not play a significant role in more\ntraditional correlative analyses, where one variable at a time is considered.\nResults: We identified a causal network over the metabolomic level using the\ngenome directed acyclic graph (G-DAG), to systematically assess whether\nvariations in the genome lead to variations in triglyceride levels as a risk\nfactor of cardiovascular disease. We found LRRC46 and LRRC69 harboring\nloss-of-function mutations have significant effect on two metabolites with\ndirect effects on triglyceride levels. We also found pathways of FAM198B and\nC6orf25 to triglycerides through indirect paths from metabolites. Conclusion:\nIntegrating causal networks with GWAS facilitates mechanistic understanding in\ncomparison to one-variable-at-a-time approaches due to accounting for\nrelationships among components at intermediate molecular levels. This approach\nis complementary to experimental studies to identify efficacious targets in the\nage of big data sets.\n"
  },
  {
    "year": 2018,
    "title": "Transient crosslinking kinetics optimize gene cluster interactions",
    "summary": "  Our understanding of how chromosomes structurally organize and dynamically\ninteract has been revolutionized through the lens of long-chain polymer\nphysics. Major protein contributors to chromosome structure and dynamics are\ncondensin and cohesin that stochastically generate loops within and between\nchains, and entrap proximal strands of sister chromatids. In this paper, we\nexplore the ability of transient, protein-mediated, gene-gene crosslinks to\ninduce clusters of genes, thereby dynamic architecture, within the highly\nrepeated ribosomal DNA that comprises the nucleolus of budding yeast. We\nimplement three approaches: live cell microscopy; computational modeling of the\nfull genome during G1 in budding yeast, exploring four decades of timescales\nfor transient crosslinks between 5k bp domains in the nucleolus on Chromosome\nXII; and, temporal network models with automated community detection algorithms\napplied to the full range of 4D modeling datasets. The data analysis tools\ndetect and track gene clusters, their size, number, persistence time, and their\nplasticity. Of biological significance, our analysis reveals an optimal mean\ncrosslink lifetime that promotes pairwise and cluster gene interactions through\n\"flexible\" clustering. In this state, large gene clusters self-assemble yet\nfrequently interact, marked by gene exchanges between clusters, which in turn\nmaximizes global gene interactions in the nucleolus. This regime stands between\ntwo limiting cases each with far less global gene interactions: with shorter\ncrosslink lifetimes, \"rigid\" clustering emerges with clusters that interact\ninfrequently; with longer crosslink lifetimes, there is a dissolution of\nclusters. These observations are compared with imaging experiments on a normal\nyeast strain and two condensin-modified mutant cell strains, applying the same\nimage analysis pipeline to the experimental and simulated datasets.\n"
  },
  {
    "year": 2018,
    "title": "Integrating splice-isoform expression into genome-scale models\n  characterizes breast cancer metabolism",
    "summary": "  Motivation: Despite being often perceived as the main contributors to cell\nfate and physiology, genes alone cannot predict cellular phenotype. During the\nprocess of gene expression, 95% of human genes can code for multiple proteins\ndue to alternative splicing. While most splice variants of a gene carry the\nsame function, variants within some key genes can have remarkably different\nroles. To bridge the gap between genotype and phenotype, condition- and\ntissue-specific models of metabolism have been constructed. However, current\nmetabolic models only include information at the gene level. Consequently, as\nrecently acknowledged by the scientific community, common situations where\nchanges in splice-isoform expression levels alter the metabolic outcome cannot\nbe modeled. Results: We here propose GEMsplice, the first method for the\nincorporation of splice-isoform expression data into genome-scale metabolic\nmodels. Using GEMsplice, we make full use of RNA-Seq quantitative expression\nprofiles to predict, for the first time, the effects of splice isoform-level\nchanges in the metabolism of 1455 patients with 31 different breast cancer\ntypes. We validate GEMsplice by generating cancer-versus-normal predictions on\nmetabolic pathways, and by comparing with gene-level approaches and available\nliterature on pathways affected by breast cancer. GEMsplice is freely available\nfor academic use at https://github.com/GEMsplice/GEMsplice_code. Compared to\nstate-of-the-art methods, we anticipate that GEMsplice will enable for the\nfirst time computational analyses at transcript level with splice-isoform\nresolution.\n"
  },
  {
    "year": 2018,
    "title": "Computational and molecular dissection of an X-box cis-Regulatory module",
    "summary": "  Ciliopathies are a class of human diseases marked by dysfunction of the\ncellular organelle, cilia. While many of the molecular components that make up\ncilia have been identified and studied, comparatively little is understood\nabout the transcriptional regulation of genes encoding these components. The\nconserved transcription factor Regulatory Factor X (RFX)/DAF-19, which acts\nthrough binding to the cis-regulatory motif known as X-box, has been shown to\nregulate ciliary genes in many animals from Caenorhabditis elegans to humans.\nHowever, accumulating evidence suggests that RFX is unable to initiate\ntranscription on its own. Therefore, other factors and cis-regulatory elements\nare likely required. One such element, a DNA motif called the C-box, has\nrecently been identified in C. elegans. It is still unclear if the X-box and\nC-boxes are the only regulatory elements involved and how they interact. To\nthis end, I analyzed the transcriptional regulation of dyf-5, the C. elegans\northolog of the human ciliopathy gene Male-Associated Kinase (MAK). Using\ncomputational methods, I was able to confirm the presence of the previously\nreported X-box and C-boxes as well as identifying an additional C-box. By\nsequentially mutating each of the identified motifs, I identified the role each\npotential motif plays in transcriptional regulation of dyf-5. My results showed\nthat only the X-box and the three C-boxes are necessary and are sufficient to\ndrive transcription, with the X-box and the centre C-box being the major\ncontributors and the other two C-boxes enhancing expression. This study\nadvances the knowledge of gene regulation in general and will further our\nunderstanding of ciliopathies and the mutations that cause them.\n"
  },
  {
    "year": 2018,
    "title": "Analysis of evolutionary origins of genomic loci harboring 59,732\n  candidate human-specific regulatory sequences identifies genetic divergence\n  patterns during evolution of Great Apes",
    "summary": "  Our view of the universe of genomic regions harboring various types of\ncandidate human-specific regulatory sequences (HSRS) has been markedly expanded\nin recent years. To infer the evolutionary origins of loci harboring HSRS,\nanalyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,\nBonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two\nmajor evolutionary pathways have been identified comprising thousands of\nsequences that were either inherited from extinct common ancestors (ECAs) or\ncreated de novo in humans after human/chimpanzee split. Thousands of HSRS\nappear inherited from ECAs yet bypassed genomes of our closest evolutionary\nrelatives, presumably due to the incomplete lineage sorting and/or\nspecies-specific loss or regulatory DNA. The bypassing pattern is prominent for\nHSRS associated with development and functions of human brain. Common genomic\nloci that may contributed to speciation during evolution of Great Apes comprise\n248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p\n= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for\nHSRS associated with human-specific (HS) changes of gene expression in cerebral\norganoids. Among non-human primates (NHP), most significant fractions of\ncandidate HSRS associated with HS expression changes in both excitatory neurons\n(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla\ngenome. Modern Humans acquired unique combinations of regulatory sequences\nhighly conserved in distinct species of six NHP separated by 30 million years\nof evolution. Concurrently, this unique mosaic of regulatory sequences\ninherited from ECAs was supplemented with 12,486 created de novo HSRS. These\nobservations support the model of complex continuous speciation process during\nevolution of Great Apes that is not likely to occur as an instantaneous event.\n"
  },
  {
    "year": 2018,
    "title": "CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse\n  Embryonic Stem Cells",
    "summary": "  Transcriptional repressor CTCF is an important regulator of chromatin 3D\nstructure, facilitating the formation of topologically associating domains\n(TADs). However, its direct effects on gene regulation is less well understood.\nHere, we utilize previously published ChIP-seq and RNA-seq data to investigate\nthe effects of CTCF on alternative splicing of genes with CTCF sites. We\ncompared the amount of RNA-seq signals in exons upstream and downstream of\nbinding sites following auxin-induced degradation of CTCF in mouse embryonic\nstem cells. We found that changes in gene expression following CTCF depletion\nwere significant, with a general increase in the presence of upstream exons. We\ninfer that a possible mechanism by which CTCF binding contributes to\nalternative splicing is by causing pauses in the transcription mechanism during\nwhich splicing elements are able to concurrently act on upstream exons already\ntranscribed into RNA.\n"
  },
  {
    "year": 2018,
    "title": "A Comparison of Microbial Genome Web Portals",
    "summary": "  Microbial genome web portals have a broad range of capabilities that address\na number of information-finding and analysis needs for scientists. This article\ncompares the capabilities of the major microbial genome web portals to aid\nresearchers in determining which portal(s) are best suited to solving their\ninformation-finding and analytical needs. We assessed both the bioinformatics\ntools and the data content of BioCyc, KEGG, Ensembl Bacteria, KBase, IMG, and\nPATRIC. For each portal, our assessment compared and tallied the available\ncapabilities. The strengths of BioCyc include its genomic and metabolic tools,\nmulti-search capabilities, table-based analysis tools, regulatory network tools\nand data, omics data analysis tools, breadth of data content, and large amount\nof curated data. The strengths of KEGG include its genomic and metabolic tools.\nThe strengths of Ensembl Bacteria include its genomic tools and large number of\ngenomes. The strengths of KBase include its genomic tools and metabolic models.\nThe strengths of IMG include its genomic tools, multi-search capabilities,\nlarge number of genomes, table-based analysis tools, and breadth of data\ncontent. The strengths of PATRIC include its large number of genomes,\ntable-based analysis tools, metabolic models, and breadth of data content.\n"
  },
  {
    "year": 2018,
    "title": "Whole genome single nucleotide polymorphism genotyping of Staphylococcus\n  aureus",
    "summary": "  Next-generation sequencing technology enables routine detection of bacterial\npathogens for clinical diagnostics and genetic research. Whole genome\nsequencing has been of importance in the epidemiologic analysis of bacterial\npathogens. However, few whole genome sequencing-based genotyping pipelines are\navailable for practical applications. Here, we present the whole genome\nsequencing-based single nucleotide polymorphism (SNP) genotyping method and\napply to the evolutionary analysis of methicillin-resistant Staphylococcus\naureus. The SNP genotyping method calls genome variants using next-generation\nsequencing reads of whole genomes and calculates the pair-wise Jaccard\ndistances of the genome variants. The method may reveal the high-resolution\nwhole genome SNP profiles and the structural variants of different isolates of\nmethicillin-resistant S. aureus (MRSA) and methicillin-susceptible S. aureus\n(MSSA) strains. The phylogenetic analysis of whole genomes and particular\nregions may monitor and track the evolution and the transmission dynamic of\nbacterial pathogens. The computer programs of the whole genome sequencing-based\nSNP genotyping method are available to the public at\nhttps://github.com/cyinbox/NGS.\n"
  },
  {
    "year": 2018,
    "title": "Searching by index for similar sequences: the SEQR algorithm",
    "summary": "  This paper describes a method to efficiently retrieve protein database\nsequences similar to a query sequence, while allowing for significant numbers\nof mutations. We call this method SEQR for SEQuence Retrieval. This approach\nincreases the speed of sequence similarity searches by an order of magnitude\ncompared to conventional algorithms at the expense of sensitivity. Furthermore,\nretrieval time increases less than linearly with the number of sequences, a\ndesirable property during an era when next generation sequencing technologies\nhave yielded greater than exponential increases in sequence records. The lower\nsensitivity of the algorithm for distantly related sequences compared to\nbenchmarks is not intrinsic to the method itself, but rather due to the\nprocedure used to construct the indexing terms, and may be improved. The\nindexing terms themselves can be added to standard information retrieval\nengines, enabling complex queries that include sequence similarity and other\ndescriptors such as taxonomy and text descriptions.\n"
  },
  {
    "year": 2018,
    "title": "Linking de novo assembly results with long DNA reads by dnaasm-link\n  application",
    "summary": "  Currently, third-generation sequencing techniques, which allow to obtain much\nlonger DNA reads compared to the next-generation sequencing technologies, are\nbecoming more and more popular. There are many possibilities to combine data\nfrom next-generation and third-generation sequencing.\n  Herein, we present a new application called dnaasm-link for linking contigs,\na result of \\textit{de novo} assembly of second-generation sequencing data,\nwith long DNA reads. Our tool includes an integrated module to fill gaps with a\nsuitable fragment of appropriate long DNA read, which improves the consistency\nof the resulting DNA sequences. This feature is very important, in particular\nfor complex DNA regions, as presented in the paper. Finally, our implementation\noutperforms other state-of-the-art tools in terms of speed and memory\nrequirements, which may enable the usage of the presented application for\norganisms with a large genome, which is not possible in~existing applications.\n  The presented application has many advantages as (i) significant memory\noptimization and reduction of computation time (ii) filling the gaps through\nthe appropriate fragment of a specified long DNA read (iii) reducing number of\nspanned and unspanned gaps in the existing genome drafts.\n  The application is freely available to all users under GNU Library or Lesser\nGeneral Public License version 3.0 (LGPLv3). The demo application, docker image\nand source code are available at http://dnaasm.sourceforge.net.\n"
  },
  {
    "year": 2018,
    "title": "DNA methylation markers to assess biological age",
    "summary": "  Among the different biomarkers of aging based on omics and clinical data, DNA\nmethylation clocks stand apart providing unmatched accuracy in assessing the\nbiological age of both humans and animal models of aging. Here, we discuss\nrobustness of DNA methylation clocks and bounds on their out-of-sample\nperformance and review computational strategies for development of the clocks.\n"
  },
  {
    "year": 2018,
    "title": "JS-MA: A Jensen-Shannon Divergence Based Method for Mapping Genome-wide\n  Associations on Multiple Diseases",
    "summary": "  Taking advantages of high-throughput genotyping technology of single\nnucleotide polymorphism (SNP), large genome-wide association studies (GWASs)\nhave been considered as the promise to unravel the complex relationships\nbetween genotypes and phenotypes, in particularly common diseases. However,\ncurrent multi-locus-based methods are insufficient, in terms of computational\ncost and discrimination power, to detect statistically significant interactions\nand they are lacking in the ability of finding diverse genetic effects on\nmultifarious diseases. Especially, multiple statistic tests for high-order\nepistasis ($ \\geq $ 2 SNPs) will raise huge analytical challenges because the\ncomputational cost increases exponentially as the growth of the cardinality of\nSNPs in an epistatic module. In this paper, we develop a simple, fast and\npowerful method, named JS-MA, using the Jensen-Shannon divergence and a\nhigh-dimensional $ k $-mean clustering algorithm for mapping the genome-wide\nmulti-locus epistatic interactions on multiple diseases. Compared with some\nstate-of-the-art association mapping tools, our method is demonstrated to be\nmore powerful and efficient from the experimental results on the systematical\nsimulations. We also applied JS-MA to the GWAS datasets from WTCCC for two\ncommon diseases, i.e. Rheumatoid Arthritis and Type 1 Diabetes. JS-MA not only\nconfirms some recently reported biologically meaningful associations but also\nidentifies some novel findings. Therefore, we believe that our method is\nsuitable and efficient for the full-scale analysis of multi-disease-related\ninteractions in the large GWASs.\n"
  },
  {
    "year": 2018,
    "title": "A Multi-Trait Approach Identified Genetic Variants Including a Rare\n  Mutation in RGS3 with Impact on Abnormalities of Cardiac Structure/Function",
    "summary": "  Heart failure is a major cause for premature death. Given heterogeneity of\nthe heart failure syndrome, identifying genetic determinants of cardiac\nfunction and structure may provide greater insights into heart failure. Despite\nprogress in understanding the genetic basis of heart failure through genome\nwide association studies, heritability of heart failure is not well understood.\nGaining further insights into mechanisms that contribute to heart failure\nrequires systematic approaches that go beyond single trait analysis. We\nintegrated Bayesian multi-trait approach and Bayesian networks for the analysis\nof 10 correlated traits of cardiac structure and function measured for 3387\nindividuals with whole exome sequence data. While using single-trait based\napproaches did not find any significant genetic variant, applying the\nintegrative Bayesian multi-trait approach, we identified 3 novel variants\nlocated in genes, RGS3, CHD3, and MRPL38 with significant impact on the cardiac\ntraits such as left ventricular volume index, parasternal long axis\ninterventricular septum thickness, and mean left ventricular wall thickness.\nAmong these, the rare variant NC_000009.11:g.116346115C>A (rs144636307) in RGS3\nshowed pleiotropic effect on left ventricular mass index, left ventricular\nvolume index and Maximum left atrial anterior-posterior diameter while RGS3 can\ninhibit TGF-beta signaling associated with left ventricle dilation and systolic\ndysfunction.\n"
  },
  {
    "year": 2018,
    "title": "Private Shotgun DNA Sequencing",
    "summary": "  Current techniques in sequencing a genome allow a service provider (e.g. a\nsequencing company) to have full access to the genome information, and thus the\nprivacy of individuals regarding their lifetime secret is violated. In this\npaper, we introduce the problem of private DNA sequencing, where the goal is to\nkeep the DNA sequence private to the sequencer. We propose an architecture,\nwhere the task of reading fragments of DNA and the task of DNA assembly are\nseparated, the former is done at the sequencer(s), and the later is completed\nat a local trusted data collector. To satisfy the privacy constraint at the\nsequencer and reconstruction condition at the data collector, we create an\ninformation gap between these two relying on two techniques: (i) we use more\nthan one non-colluding sequencer, all reporting the read fragments to the\nsingle data collector, (ii) adding the fragments of some known DNA molecules,\nwhich are still unknown to the sequencers, to the pool. We prove that these two\ntechniques provide enough freedom to satisfy both conditions at the same time.\n"
  },
  {
    "year": 2018,
    "title": "GenHap: A Novel Computational Method Based on Genetic Algorithms for\n  Haplotype Assembly",
    "summary": "  The computational problem of inferring the full haplotype of a cell starting\nfrom read sequencing data is known as haplotype assembly, and consists in\nassigning all heterozygous Single Nucleotide Polymorphisms (SNPs) to exactly\none of the two chromosomes. Indeed, the knowledge of complete haplotypes is\ngenerally more informative than analyzing single SNPs and plays a fundamental\nrole in many medical applications. To reconstruct the two haplotypes, we\naddressed the weighted Minimum Error Correction (wMEC) problem, which is a\nsuccessful approach for haplotype assembly. This NP-hard problem consists in\ncomputing the two haplotypes that partition the sequencing reads into two\ndisjoint sub-sets, with the least number of corrections to the SNP values. To\nthis aim, we propose here GenHap, a novel computational method for haplotype\nassembly based on Genetic Algorithms, yielding optimal solutions by means of a\nglobal search process. In order to evaluate the effectiveness of our approach,\nwe run GenHap on two synthetic (yet realistic) datasets, based on the Roche/454\nand PacBio RS II sequencing technologies. We compared the performance of GenHap\nagainst HapCol, an efficient state-of-the-art algorithm for haplotype phasing.\nOur results show that GenHap always obtains high accuracy solutions (in terms\nof haplotype error rate), and is up to 4x faster than HapCol in the case of\nRoche/454 instances and up to 20x faster when compared on the PacBio RS II\ndataset. Finally, we assessed the performance of GenHap on two different real\ndatasets. Future-generation sequencing technologies, producing longer reads\nwith higher coverage, can highly benefit from GenHap, thanks to its capability\nof efficiently solving large instances of the haplotype assembly problem.\n"
  },
  {
    "year": 2019,
    "title": "De novo inference of diversity genes and analysis of non-canonical\n  V(DD)J recombination in immunoglobulins",
    "summary": "  The V(D)J recombination forms the immunoglobulin genes by joining the\nvariable (V), diversity (D), and joining (J) germline genes. Since variations\nin germline genes have been linked to various diseases, personalized\nimmunogenomics aims at finding alleles of germline genes across various\npatients. Although recent studies described algorithms for de novo inference of\nV and J genes from immunosequencing data, they stopped short of solving a more\ndifficult problem of reconstructing D genes that form the highly divergent CDR3\nregions and provide the most important contribution to the antigen binding. We\npresent the IgScout algorithm for de novo D gene reconstruction and apply it to\nreveal new alleles of human D genes and previously unknown D genes in camel, an\nimportant model organism in immunology. We further analyze non-canonical V(DD)J\nrecombination that results in unusually long tandem CDR3s and thus expands the\ndiversity of the antibody repertoires. We demonstrate that tandem CDR3s\nrepresent a consistent and functional feature of all analyzed immunosequencing\ndatasets, reveal ultra-long tandem CDR3s, and shed light on the mechanism\nresponsible for their formation.\n"
  },
  {
    "year": 2019,
    "title": "A Hybrid HMM Approach for the Dynamics of DNA Methylation",
    "summary": "  The understanding of mechanisms that control epigenetic changes is an\nimportant research area in modern functional biology. Epigenetic modifications\nsuch as DNA methylation are in general very stable over many cell divisions.\nDNA methylation can however be subject to specific and fast changes over a\nshort time scale even in non-dividing (i.e. not-replicating) cells. Such\ndynamic DNA methylation changes are caused by a combination of active\ndemethylation and de novo methylation processes which have not been\ninvestigated in integrated models. Here we present a hybrid (hidden) Markov\nmodel to describe the cycle of methylation and demethylation over (short) time\nscales. Our hybrid model decribes several molecular events either happening at\ndeterministic points (i.e. describing mechanisms that occur only during cell\ndivision) and other events occurring at random time points. We test our model\non mouse embryonic stem cells using time-resolved data. We predict methylation\nchanges and estimate the efficiencies of the different modification steps\nrelated to DNA methylation and demethylation.\n"
  },
  {
    "year": 2019,
    "title": "Identifying centromeric satellites with dna-brnn",
    "summary": "  Summary: Human alpha satellite and satellite 2/3 contribute to several\npercent of the human genome. However, identifying these sequences with\ntraditional algorithms is computationally intensive. Here we develop dna-brnn,\na recurrent neural network to learn the sequences of the two classes of\ncentromeric repeats. It achieves high similarity to RepeatMasker and is times\nfaster. Dna-brnn explores a novel application of deep learning and may\naccelerate the study of the evolution of the two repeat classes.\n  Availability and implementation: https://github.com/lh3/dna-nn\n  Contact: hli@jimmy.harvard.edu\n"
  },
  {
    "year": 2019,
    "title": "Proteomic and metagenomic insights into prehistoric Spanish Levantine\n  Rock Art",
    "summary": "  The Iberian Mediterranean Basin is home to one of the largest groups of\nprehistoric rock art sites in Europe. Despite the cultural relevance of\nprehistoric Spanish Levantine rock art, pigment composition remains partially\nunknown, and the nature of the binders used for painting has yet to be\ndisclosed. In this work, we present the first omic analysis applied to one of\nthe flagship Levantine rock art sites: the Valltorta ravine (Castell{\\'o}n,\nSpain). We used high-throughput sequencing to provide the first description of\nthe bacterial communities colonizing the rock art patina, which proved to be\ndominated by Firmicutes species and might have a protective effect on the\npaintings. Proteomic analysis was also performed on rock art microsamples in\norder to determine the organic binders present in Levantine prehistoric rock\nart pigments. This information could shed light on the controversial dating of\nthis UNESCO Cultural Heritage, and contribute to defining the chrono-cultural\nframework of the societies responsible for these paintings.\n"
  },
  {
    "year": 2019,
    "title": "Predicting Toxicity from Gene Expression with Neural Networks",
    "summary": "  We train a neural network to predict chemical toxicity based on gene\nexpression data. The input to the network is a full expression profile\ncollected either in vitro from cultured cells or in vivo from live animals. The\noutput is a set of fine grained predictions for the presence of a variety of\npathological effects in treated animals. When trained on the Open TG-GATEs\ndatabase it produces good results, outperforming classical models trained on\nthe same data. This is a promising approach for efficiently screening chemicals\nfor toxic effects, and for more accurately evaluating drug candidates based on\npreclinical data.\n"
  },
  {
    "year": 2019,
    "title": "BOAssembler: a Bayesian Optimization Framework to Improve RNA-Seq\n  Assembly Performance",
    "summary": "  High throughput sequencing of RNA (RNA-Seq) can provide us with millions of\nshort fragments of RNA transcripts from a sample. How to better recover the\noriginal RNA transcripts from those fragments (RNA-Seq assembly) is still a\ndifficult task. For example, RNA-Seq assembly tools typically require\nhyper-parameter tuning to achieve good performance for particular datasets.\nThis kind of tuning is usually unintuitive and time-consuming. Consequently,\nusers often resort to default parameters, which do not guarantee consistent\ngood performance for various datasets.\n  Here we propose BOAssembler (https://github.com/olivomao/boassembler), a\nframework that enables end-to-end automatic tuning of RNA-Seq assemblers, based\non Bayesian Optimization principles. Experiments show this data-driven approach\nis effective to improve the overall assembly performance. The approach would be\nhelpful for downstream (e.g. gene, protein, cell) analysis, and more broadly,\nfor future bioinformatics benchmark studies.\n"
  },
  {
    "year": 2019,
    "title": "Using sequencing coverage statistics to identify sex chromosomes in\n  minke whales",
    "summary": "  The ever-increasing number of genome sequencing and resequencing projects is\na central source of insights into the ecology and evolution of non-model\norganisms. An important aspect of genomics is the elucidation of sex\ndetermination systems and identifying genes on sex chromosomes. This not only\nhelps reveal mechanisms behind sex determination in the species under study,\nbut their characteristics make sex chromosomes a unique tool for studying the\nmechanisms and effects of recombination and genomic rearrangements and how they\naffect adaption and selection. Despite this, many sequencing projects omit such\ninvestigations. Here, we apply a simple method using sequencing coverage\nstatistics to identify scaffolds belonging to the sex chromosomes of minke\nwhale, and show how the sex chromosome system can be determined using coverage\nstatistics alone. Using publicly available data, we identify the previously\nunknown sex of an Antarctic minke whale as female. We further investigate\npublic sequence data from the different species and sub-species of minke whale,\nand classify genomic scaffolds from a published minke whale assembly as X or Y\nchromosomal sequences. Our findings are consistent with previous results that\nidentified a handful of scaffolds as sex chromosomal, but we are able to\nidentify a much larger set of scaffolds, likely to represent close to the\ncomplete sex chromosomal sequences for the minke whale. Sequence coverage\nstatistics provides a readily available tool for investigating the sex\ndetermination system and locate genes on sex chromosomes. This analysis is\nstraightforward and can often be performed with existing resources.\n"
  },
  {
    "year": 2019,
    "title": "HIV-1 virus cycle replication: a review of RNA polymerase II\n  transcription, alternative splicing and protein synthesis",
    "summary": "  HIV virus replication is a time-related process that includes several stages.\nFocusing on the core steps, RNA polymerase II transcripts in an early stage\npre-mRNA containing regulator proteins (i.e nef,tat,rev,vif,vpr,vpu), which are\ncompletely spliced by the spliceosome complex (0.9kb and 1.8kb) and exported to\nthe ribosome for protein synthesis. These splicing and export processes are\nregulated by tat protein, which binds on Trans-activation response (TAR)\nelement, and by rev protein, which binds to the Rev-responsive Element (RRE).\nAs long as these regulators are synthesized, splicing is progressively\ninhibited (from 4.0kb to 9.0kb) and mRNAs are translated into structural and\nenzymatic proteins (env, gag-pol). During this RNAPII scanning and splicing,\naround 40 different multi-cystronic mRNA have been produced. Long-read\nsequencing has been applied to the HIV-1 virus genome (type HXB2CG) with the\nHIV.pro software, a fortran 90 code for simulating the virus replication cycle,\nspecially RNAPII transcription, exon/intron splicing and ribosome protein\nsynthesis, including the frameshift at gag/pol gene and the ribosome pause at\nenv gene. All HIV-1 virus proteins have been identified as far as other ORFs.\nAs observed, tat/rev protein regulators have different length depending on the\nsplicing cleavage site: tat protein varies from 224aa to a final state of 72aa,\nwhereas rev protein from 25aa to 27aa, with a maximum of 119aa. Furthermore,\nseveral ORFs coding for small polypeptides sPEP (less than 10 amino acids) and\nfor other unidentified proteins have been localised with unknown functionality.\nThe detailed analysis of the HIV virus replication and the virus proteomics are\nimportant for identifying which antigens are presented by macrophages to CD4\ncells, for localizing reactive epitopes or for creating transfer vectors to\ndevelop new HIV vaccines and effective therapies.\n"
  },
  {
    "year": 2019,
    "title": "pdbmine: A Node.js API for the RCSB Protein Data Bank (PDB)",
    "summary": "  Summary: The advent of Web-based tools that assist in the analysis and\nvisualization of macromolecules require application programming interfaces\n(APIs) designed for modern web frameworks. To this end, we have developed a\nNode.js module pdbmine that allows any user to generate faster data-request\nqueries to the RCSB Protein Data Bank (PDB). This JavaScript API acts as a\nlayer over the XML-based RCSB PDB RESTful API. The relatively simple nature of\nthe function calls within this module allows the user to easily implement and\nintegrate pdbmine into larger Node.js web applications.\n  Availability: This module can be installed via the Node Package Manager (NPM)\nat https://www.npmjs.com/package/pdbmine/, and is hosted on GitHub under the\nopen-source MIT license at https://github.com/nnj1/pdbmine/. Relevant\ndocumentation is detailed at https://nnj1.github.io/pdbmine/\n"
  },
  {
    "year": 2019,
    "title": "Statistical methods for the quantitative genetic analysis of\n  high-throughput phenotyping data",
    "summary": "  The advent of plant phenomics, coupled with the wealth of genotypic data\ngenerated by next-generation sequencing technologies, provides exciting new\nresources for investigations into and improvement of complex traits. However,\nthese new technologies also bring new challenges in quantitative genetics,\nnamely, a need for the development of robust frameworks that can accommodate\nthese high-dimensional data. In this chapter, we describe methods for the\nstatistical analysis of high-throughput phenotyping (HTP) data with the goal of\nenhancing the prediction accuracy of genomic selection (GS). Following the\nIntroduction in Section 1, Section 2 discusses field-based HTP, including the\nuse of unmanned aerial vehicles and light detection and ranging, as well as how\nwe can achieve increased genetic gain by utilizing image data derived from HTP.\nSection 3 considers extending commonly used GS models to integrate HTP data as\ncovariates associated with the principal trait response, such as yield.\nParticular focus is placed on single-trait, multi-trait, and genotype by\nenvironment interaction models. One unique aspect of HTP data is that phenomics\nplatforms often produce large-scale data with high spatial and temporal\nresolution for capturing dynamic growth, development, and stress responses.\nSection 4 discusses the utility of a random regression model for performing\nlongitudinal GS. The chapter concludes with a discussion of some standing\nissues.\n"
  },
  {
    "year": 2019,
    "title": "A bioinformatics pipeline for the identification of CHO cell\n  differential gene expression from RNA-Seq data",
    "summary": "  In recent years the publication of genome sequences for the Chinese hamster\nand Chinese hamster ovary (CHO) cell lines have facilitated study of these\nbiopharmaceutical cell factories with unprecedented resolution. Our\nunderstanding of the CHO cell transcriptome, in particular, has rapidly\nadvanced through the application of next-generation sequencing (NGS) technology\nto characterise RNA expression (RNA-Seq). In this chapter we present a\ncomputational pipeline for the analysis of CHO cell RNA-Seq data from the\nIllumina platform to identify differentially expressed genes. The example data\nand bioinformatics workflow required to run this analysis are freely available\nat www.cgcdb.org/rnaseq_analysis_protocol.html.\n"
  },
  {
    "year": 2019,
    "title": "RACS: Rapid Analysis of ChIP-Seq data for contig based genomes",
    "summary": "  Background: Chromatin immunoprecipitation coupled to next generation\nsequencing (ChIP-Seq) is a widely used technique to investigate the function of\nchromatin-related proteins in a genome-wide manner. ChIP-Seq generates large\nquantities of data which can be difficult to process and analyse, particularly\nfor organisms with contig based genomes. Contig-based genomes often have poor\nannotations for cis-elements, for example enhancers, that are important for\ngene expression. Poorly annotated genomes make a comprehensive analysis of\nChIP-Seq data difficult and as such standardized analysis pipelines are\nlacking. Methods: We report a computational pipeline that utilizes traditional\nHigh-Performance Computing techniques and open source tools for processing and\nanalysing data obtained from ChIP-Seq. We applied our computational pipeline\n\"Rapid Analysis of ChIP-Seq data\" (RACS) to ChIP-Seq data that was generated in\nthe model organism Tetrahymena thermophila, an example of an organism with a\ngenome that is available in contigs. Results: To test the performance and\nefficiency of RACs, we performed control ChIP-Seq experiments allowing us to\nrapidly eliminate false positives when analyzing our previously published data\nset. Our pipeline segregates the found read accumulations between genic and\nintergenic regions and is highly efficient for rapid downstream analyses.\nConclusions: Altogether, the computational pipeline presented in this report is\nan efficient and highly reliable tool to analyze genome-wide ChIP-Seq data\ngenerated in model organisms with contig-based genomes.\n  RACS is an open source computational pipeline available to download from:\nhttps://bitbucket.org/mjponce/racs --or--\nhttps://gitrepos.scinet.utoronto.ca/public/?a=summary&p=RACS\n"
  },
  {
    "year": 2019,
    "title": "RNASeqR: an R package for automated two-group RNA-Seq analysis workflow",
    "summary": "  RNA-Seq analysis has revolutionized researchers' understanding of the\ntranscriptome in biological research. Assessing the differences in\ntranscriptomic profiles between tissue samples or patient groups enables\nresearchers to explore the underlying biological impact of transcription.\nRNA-Seq analysis requires multiple processing steps and huge computational\ncapabilities. There are many well-developed R packages for individual steps;\nhowever, there are few R/Bioconductor packages that integrate existing software\ntools into a comprehensive RNA-Seq analysis and provide fundamental end-to-end\nresults in pure R environment so that researchers can quickly and easily get\nfundamental information in big sequencing data. To address this need, we have\ndeveloped the open source R/Bioconductor package, RNASeqR. It allows users to\nrun an automated RNA-Seq analysis with only six steps, producing essential\ntabular and graphical results for further biological interpretation. The\nfeatures of RNASeqR include: six-step analysis, comprehensive visualization,\nbackground execution version, and the integration of both R and command-line\nsoftware. RNASeqR provides fast, light-weight, and easy-to-run RNA-Seq analysis\npipeline in pure R environment. It allows users to efficiently utilize popular\nsoftware tools, including both R/Bioconductor and command-line tools, without\npredefining the resources or environments. RNASeqR is freely available for\nLinux and macOS operating systems from Bioconductor\n(https://bioconductor.org/packages/release/bioc/html/RNASeqR.html).\n"
  },
  {
    "year": 2019,
    "title": "Tumor Microenvironment-based Gene Signatures Divides Novel Immune and\n  Stromal Subgroup Classification of Lung Adenocarcinoma",
    "summary": "  Tumor microenvironment has complex effects on tumorigenesis and metastasis.\nHowever, there is still a lack of comprehensive understanding of the\nrelationship among molecular and cellular characteristics in tumor\nmicroenvironment, clinical prognosis and immunotherpy response. In this study,\nthe immune and stromal (non-immune) signatures of tumor microenvironment were\nintegrated to identify novel subgroups of lung adenocarcinoma by\neigendecomposition and extraction algorithms of bioinformatics and machine\nlearning, such as non-negative matrix factorization and multitask learning.\nTumors were classified into 4 groups according to the activation of immunity\nand stroma by novel signatures. The 4 groups had different mutation landscape,\nmolecular, cellular characteristics and prognosis, which have been validation\nin 6 independent data sets containing 1551 patients. High-immune and\nlow-stromal activation group links to high immunocyte infiltration, high\nimmunocompetence, low fibroblasts, endothelial cells, collagen, laminin, tumor\nmutation burden, and better overall survival. We developed a novel model based\non tumor microenvironment by integrating immune and stromal activation, namely\nPMBT (prognostic model based on tumor microenvironment). The PMBT showed the\nvalue to predict overall survival and immunotherapy responses.\n"
  },
  {
    "year": 2019,
    "title": "Predicting Gene Expression Between Species with Neural Networks",
    "summary": "  We train a neural network to predict human gene expression levels based on\nexperimental data for rat cells. The network is trained with paired human/rat\nsamples from the Open TG-GATES database, where paired samples were treated with\nthe same compound at the same dose. When evaluated on a test set of held out\ncompounds, the network successfully predicts human expression levels. On the\nmajority of the test compounds, the list of differentially expressed genes\ndetermined from predicted expression levels agrees well with the list of\ndifferentially expressed genes determined from actual human experimental data.\n"
  },
  {
    "year": 2019,
    "title": "SpliceCombo: A Hybrid Technique efficiently use for Principal Component\n  Analysis of Splice Site Prediction",
    "summary": "  The primary step in search of the gene prediction is an identification of the\ncoding region from genomic DNA sequence. Gene structure in the case of a\neukaryotic organism is composed of promoter, intron, start codon, exons, stop\ncodon, etc. Splice site prediction, which separates the junction between exon\nand intron, though the sequence beside. The splice sites have huge\npreservation, however, the precision of the tool exhibits less than 90%. The\nmain objective of this work to exhibits a hybrid technique that efficiently\nimproves the existing gene recognition technique. Therefore to enhance the\nidentification of splice sites, the respective algorithm needs to be improved.\nOver the last decade, the researcher paid more attention to improve the\naccuracy of a predicted model in this domain. Our proposed method, SpliceCombo\ninvolves three stages. At initial stage, which considers the principal\nComponent Analysis, based on the feature extracted. In the intermediate stage,\ni.e.,, the second stage Case- Based Reasoning is done, i.e., feature selection.\nThe third stage uses support vector machine based along with polynomial kernel\nfunction for final classification. In comparison with other methods, the\nproposed SpliceCombo model outperforms other prediction models with respect to\nprediction accuracies. Particularly for donor splice site the methodology\nexhibits sensitivity is 97.25% accurate and specificity is 97.46% accurate. For\nacceptor Splice Site the sensitivity is 96.51% and Specificity is 94.48%\ncorrect.\n"
  },
  {
    "year": 2019,
    "title": "ReadsClean: a new approach to error correction of sequencing reads based\n  on alignments clustering",
    "summary": "  Motivation: Next generation methods of DNA sequencing produce relatively high\nrate of reading errors, which interfere with de novo genome assembly of newly\nsequenced organisms and particularly affect the quality of SNP detection\nimportant for diagnostics of many hereditary diseases. There exists a number of\nprograms developed for correcting errors in NGS reads. Such programs utilize\nvarious approaches and are optimized for different specific tasks, but all of\nthem are far from being able to correct all errors, especially in sequencing\nreads that crossing by repeats and DNA from di/polyploid eukaryotic genomes.\nResults: This paper describes a novel method of error correction based on\nclustering of alignments of similar reads. This method is implemented in\nReadsClean program, which is designed for cleaning Illumina HiSeq sequencing\nreads. We compared ReadsClean to other reads cleaning programs recognized to be\nthe best by several publications. Our sequence assembly tests using actual and\nsimulated sequencing reads show superior results achieved by ReadsClean.\nAvailability and implementation: ReadsClean is implemented as a standalone C\ncode. It is incorporated in an error correction pipeline and is freely\navailable to academic users at Softberry web server www.softberry.com.\n"
  },
  {
    "year": 2019,
    "title": "ReadsMap: a new tool for high precision mapping of DNAseq and RNAseq\n  read sequences",
    "summary": "  There are currently plenty of programs available for mapping short sequences\n(reads) to a genome. Most of them, however, including such popular and actively\ndeveloped programs as Bowtie, BWA, TopHat and many others, are based on\nBurrows-Wheeler Transform (BWT) algorithm. This approach is very effective for\nmapping high-homology reads, but runs into problems when mapping reads with\nhigh level of errors or SNP. Also it has problems with mapping RNASeq spliced\nreads (such as reads that aligning with gaps corresponding intron sequences),\nthe kind that is essential for finding introns and alternative splicing gene\nisoforms. Meanwhile, finding intron positions is the most important task for\ndetermining the gene structure, and especially alternatively spliced variants\nof genes. In this paper, we propose a new algorithm that involves hashing\nreference genome. ReadsMap program, implementing such algorithm, demonstrate\nvery high-accuracy mapping of large number of short reads to one or more\ngenomic contigs. It is achieved mostly by better alignment of very short parts\nof reads separated by long introns with accounting information from mapping\nother reads containing the same intron inserted between bigger blocks.\nAvailability and implementation: ReadsMap is implemented in C. It is\nincorporated in Fgenesh++ gene identification pipeline and is freely available\nto academic users at Softberry web server www.softberry.com.\n"
  },
  {
    "year": 2019,
    "title": "Transcriptomic Causal Networks identified patterns of differential gene\n  regulation in human brain from Schizophrenia cases versus controls",
    "summary": "  Common and complex traits are the consequence of the interaction and\nregulation of multiple genes simultaneously, which work in a coordinated way.\nHowever, the vast majority of studies focus on the differential expression of\none individual gene at a time. Here, we aim to provide insight into the\nunderlying relationships of the genes expressed in the human brain in cases\nwith schizophrenia (SCZ) and controls. We introduced a novel approach to\nidentify differential gene regulatory patterns and identify a set of essential\ngenes in the brain tissue. Our method integrates genetic, transcriptomic, and\nHi-C data and generates a transcriptomic-causal network. Employing this\napproach for analysis of RNA-seq data from CommonMind Consortium, we identified\ndifferential regulatory patterns for SCZ cases and control groups to unveil the\nmechanisms that control the transcription of the genes in the human brain. Our\nanalysis identified modules with a high number of SCZ-associated genes as well\nas assessing the relationship of the hubs with their down-stream genes in both,\ncases and controls. In addition, the results identified essential genes for\nbrain function and suggested new genes putatively related to SCZ.\n"
  },
  {
    "year": 2019,
    "title": "A multi-modal neural network for learning cis and trans regulation of\n  stress response in yeast",
    "summary": "  Deciphering gene regulatory networks is a central problem in computational\nbiology. Here, we explore the use of multi-modal neural networks to learn\npredictive models of gene expression that include cis and trans regulatory\ncomponents. We learn models of stress response in the budding yeast\nSaccharomyces cerevisiae. Our models achieve high performance and substantially\noutperform other state-of-the-art methods such as boosting algorithms that use\npre-defined cis-regulatory features. Our model learns several cis and trans\nregulators including well-known master stress response regulators. We use our\nmodels to perform in-silico TF knock-out experiments and demonstrate that\nin-silico predictions of target gene changes correlate with the results of the\ncorresponding TF knockout microarray experiment.\n"
  },
  {
    "year": 2019,
    "title": "Reading tea leaves? Polygenic scores and differences in traits among\n  groups",
    "summary": "  In the past decade, Genome-Wide Association Studies (GWAS) have delivered an\nincreasingly broad view of the genetic basis of human phenotypic variation. One\nof the major developments from GWAS is polygenic scores, a genetic predictor of\nan individual's genetic predisposition towards a trait constructed from GWAS.\nThe success of GWAS and polygenic scores seems to suggest that we will soon be\nable to settle debates about whether phenotypic differences among groups are\ndriven in part by genetics. However, answering these questions is more\ncomplicated than it seems at first glance and touches on many old issues about\nthe interpretation of human genetic variation. In this perspective piece, I\noutline the ways in which issues of causality, stratification,\ngene-by-environment interactions, and divergence among groups all complicate\nthe interpretation of among-population polygenic score differences.\n"
  },
  {
    "year": 2019,
    "title": "Organizing genome engineering for the gigabase scale",
    "summary": "  Engineering the entire genome of an organism enables large-scale changes in\norganization, function, and external interactions, with significant\nimplications for industry, medicine, and the environment. Improvements to DNA\nsynthesis and organism engineering are already enabling substantial changes to\norganisms with megabase genomes, such as Escherichia coli and Saccharomyces\ncerevisiae. Simultaneously, recent advances in genome-scale modeling are\nincreasingly informing the design of metabolic networks. However, major\nchallenges remain for integrating these and other relevant technologies into\nworkflows that can scale to the engineering of gigabase genomes.\n  In particular, we find that a major under-recognized challenge is\ncoordinating the flow of models, designs, constructs, and measurements across\nthe large teams and complex technological systems that will likely be required\nfor gigabase genome engineering. We recommend that the community address these\nchallenges by 1) adopting and extending existing standards and technologies for\nrepresenting and exchanging information at the gigabase genomic scale, 2)\ndeveloping new technologies to address major open questions around data\ncuration and quality control, 3) conducting fundamental research on the\nintegration of modeling and design at the genomic scale, and 4) developing new\nlegal and contractual infrastructure to better enable collaboration across\nmultiple institutions.\n"
  },
  {
    "year": 2019,
    "title": "Autism spectrum disorder: a neuro-immunometabolic hypothesis of the\n  developmental origins",
    "summary": "  Fetal neuroinflammation and prenatal stress (PS) may contribute to lifelong\nneurological disabilities. Astrocytes and microglia, among the brain's\nnon-neuronal glia cell populations, play a pivotal role in neurodevelopment,\npredisposition to and initiation of disease throughout lifespan. One of the\nmost common neurodevelopmental disorders manifesting between 1-4 years of age\nis autism spectrum disorder (ASD). A pathological glial-neuronal interplay is\nthought to increase the risk for clinical manifestation of ASD in at-risk\nchildren, but the mechanisms remain poorly understood and integrative,\nmulti-scale models are needed. We propose a model that integrates the data\nacross the scales of physiological organization, from genome to phenotype, and\nprovides a foundation to explain the disparate findings on the genomic level.\nWe hypothesize that via gene-environment interactions, fetal neuroinflammation\nand PS may reprogram glial immunometabolic phenotypes that impact\nneurodevelopment and neurobehavior. Drawing on genomic data from the recently\npublished series of ovine and rodent glial transcriptome analyses with fetuses\nexposed to neuroinflammation or PS, we conduct an analysis on the Simons\nFoundation Autism Research Initiative (SFARI) Gene database. We confirm 21 gene\nhits. Using unsupervised statistical network analysis, we then identify six\nclusters of probable protein-protein interactions mapping onto the\nimmunometabolic and stress response networks and epigenetic memory. These\nfindings support our hypothesis. We discuss the implications for ASD etiology,\nearly detection, and novel therapeutic approaches. We conclude with delineation\nof the next steps to verify our model on the individual gene level in an\nassumption-free manner.\n"
  },
  {
    "year": 2019,
    "title": "Whole genome sequencing identifies putative associations between genomic\n  polymorphisms and clinical response to the antiepileptic drug levetiracetam",
    "summary": "  In the context of pharmacogenomics, whole genome sequencing provides a\npowerful approach for identifying correlations between response variability to\nspecific drugs and genomic polymorphisms in a population, in an unbiased\nmanner. In this study, we employed whole genome sequencing of DNA samples from\npatients showing extreme response (n=72) and non-response (n=27) to the\nantiepileptic drug levetiracetam, in order to identify genomic variants that\nunderlie response to the drug. Although no common SNP (MAF>5%) crossed the\nconventional genome-wide significance threshold of 5e-8, we found common\npolymorphisms in genes SPNS3, HDC, MDGA2, NSG1 and RASGEF1C, which collectively\npredict clinical response to levetiracetam in our cohort with ~91% predictive\naccuracy. Among these genes, HDC, NSG1, MDGA2 and RASGEF1C are potentially\nimplicated in synaptic neurotransmission, while SPNS3 is an atypical solute\ncarrier transporter homologous to SV2A, the known molecular target of\nlevetiracetam. Furthermore, we performed gene- and pathway-based statistical\nanalysis on sets of rare and low-frequency variants (MAF<5%) and we identified\nassociations between the following genes or pathways and response to\nlevetiracetam: a) genes PRKCB and DLG2, which are involved in glutamatergic\nneurotransmission, a known target of anticonvulsants, including levetiracetam;\nb) genes FILIP1 and SEMA6D, which are involved in axon guidance and modelling\nof neural connections; and c) pathways with a role in synaptic\nneurotransmission, such as WNT5A-dependent internalization of FZD4 and\ndisinhibition of SNARE formation. In summary, our approach to utilise whole\ngenome sequencing on subjects with extreme response phenotypes is a feasible\nroute to generate plausible hypotheses for investigating the genetic factors\nunderlying drug response variability in cases of pharmaco-resistant epilepsy.\n"
  },
  {
    "year": 2019,
    "title": "AFDP: An Automated Function Description Prediction Approach to Improve\n  Accuracy of Protein Function Predictions",
    "summary": "  With the rapid growth in high-throughput biological sequencing technologies\nand subsequently the amount of produced omics data, it is essential to develop\nautomated methods to annotate the functionality of unknown genes and proteins.\nThere are developed tools such as AHRD applying known proteins characterization\nto annotate unknown ones. Some other algorithms such as eggNOG apply\northologous groups of proteins to detect the most probable function. However,\nwhile the available tools focus on the detection of the most similar\ncharacterization, they are not able to generalize and integrate information\nfrom multiple homologs while maintaining accuracy. Here, we devise AFDP, an\nintegrated approach for protein function prediction which benefits from the\ncombination of two available tools, AHRD and eggNOG, to predict the\nfunctionality of novel proteins and produce more precise human readable\ndescriptions by applying our stCFExt algorithm. StCFExt creates function\ndescriptions applying available manually curated descriptions in swiss-prot.\nUsing a benchmark dataset we show that the annotations predicted by our\napproach are more accurate than eggNOG and AHRD annotations.\n"
  },
  {
    "year": 2019,
    "title": "A Stochastic Automata Network Description for Spatial DNA-Methylation\n  Models",
    "summary": "  DNA methylation is an important biological mechanism to regulate gene\nexpression and control cell development. Mechanistic modeling has become a\npopular approach to enhance our understanding of the dynamics of methylation\npattern formation in living cells. Recent findings suggest that the methylation\nstate of a cytosine base can be influenced by its DNA neighborhood. Therefore,\nit is necessary to generalize existing mathematical models that consider only\none cytosine and its partner on the opposite DNA-strand (CpG), in order to\ninclude such neighborhood dependencies. One approach is to describe the system\nas a stochastic automata network (SAN) with functional transitions. We show\nthat single-CpG models can successfully be generalized to multiple CpGs using\nthe SAN description and verify the results by comparing them to results from\nextensive Monte-Carlo simulations.\n"
  },
  {
    "year": 2019,
    "title": "CD44 alternative splicing is a sensor of intragenic DNA methylation in\n  tumors",
    "summary": "  DNA methylation (meDNA) is a suspected modulator of alternative splicing,\nwhile splicing in turn is involved in tumour formations nearly as frequently as\nDNA mutations. Yet, the impact of meDNA on tumorigenesis via its effect on\nsplicing has not been thoroughly explored. Here, we find that HCT116 colon\ncarcinoma cells inactivated for the DNA methylases DNMT1 and DNMT3b undergo a\npartial epithelial to mesenchymal transition (EMT) associated with alternative\nsplicing of the CD44 transmembrane receptor. The skipping of CD44 variant exons\nis in part explained by altered expression or splicing of splicing and\nchromatin factors. A direct effect of meDNA on alternative splicing was\nsustained by transient depletion of DNMT1 and the methyl-binding genes MBD1,\nMBD2, and MBD3. Yet, local changes in intragenic meDNA also altered recruitment\nof MBD1 protein and of the chromatin factor HP1$\\gamma$ known to alter\ntranscriptional pausing and alternative splicing decisions. We further tested\nif meDNA level has sufficiently strong direct impact on the outcome of\nalternative splicing to have a predictive value in the MCF10A model for breast\ncancer progression and in patients with acute lymphoblastic leukemia (B ALL).\nWe found that a small number of differentially spliced genes mostly involved in\nsplicing and signal transduction is systematically correlated with local meDNA.\nAltogether, our observations suggest that, although DNA methylation has\nmultiple avenues to alternative splicing, its indirect effect may be also\nmediated through alternative splicing isoforms of these sensors of meDNA.\n"
  },
  {
    "year": 2019,
    "title": "Primary and secondary anti-viral response captured by the dynamics and\n  phenotype of individual T cell clones",
    "summary": "  The diverse repertoire of T-cell receptors (TCR) plays a key role in the\nadaptive immune response to infections. Previous studies show that secondary\nresponses to the yellow fever vaccine - the model for acute infection in humans\n- are weaker than primary ones, but only quantitative measurements can describe\nthe concentration changes and lineage fates for distinct T-cell clones in vivo\nover time. Using TCR alpha and beta repertoire sequencing for T-cell subsets,\nas well as single-cell RNAseq and TCRseq, we track the concentrations and\nphenotypes of individual T-cell clones in response to primary and secondary\nyellow fever immunization showing their large diversity. We confirm the\nsecondary response is an order of magnitude weaker, albeit $\\sim10$ days faster\nthan the primary one. Estimating the fraction of the T-cell response directed\nagainst the single immunodominant epitope, we identify the sequence features of\nTCRs that define the high precursor frequency of the two major TCR motifs\nspecific for this particular epitope. We also show the consistency of clonal\nexpansion dynamics between bulk alpha and beta repertoires, using a new\nmethodology to reconstruct alpha-beta pairings from clonal trajectories.\n"
  },
  {
    "year": 2019,
    "title": "Guidelines for reporting single-cell RNA-Seq experiments",
    "summary": "  Single-cell RNA-Sequencing (scRNA-Seq) has undergone major technological\nadvances in recent years, enabling the conception of various organism-level\ncell atlassing projects. With increasing numbers of datasets being deposited in\npublic archives, there is a need to address the challenges of enabling the\nreproducibility of such data sets. Here, we describe guidelines for a minimum\nset of metadata to sufficiently describe scRNA-Seq experiments, ensuring\nreproducibility of data analyses.\n"
  },
  {
    "year": 2019,
    "title": "Generalized Method of Moments Estimation for Stochastic Models of DNA\n  Methylation Patterns",
    "summary": "  With recent advances in sequencing technologies, large amounts of epigenomic\ndata have become available and computational methods are contributing\nsignificantly to the progress of epigenetic research. As an orthogonal approach\nto methods based on machine learning, mechanistic modeling aims at a\ndescription of the mechanisms underlying epigenetic changes. Here, we propose\nan efficient method for parameter estimation for stochastic models that\ndescribe the dynamics of DNA methylation patterns over time. Our method is\nbased on the Generalized Method of Moments (GMM) and gives results with an\naccuracy similar to that of maximum likelihood-based estimation approaches.\nHowever, in contrast to the latter, the GMM still allows an efficient and\naccurate calibration of parameters even if the complexity of the model is\nincreased by considering longer methylation patterns. We show the usefulness of\nour method by applying it to hairpin bisulfite sequencing data from mouse ESCs\nfor varying pattern lengths.\n"
  },
  {
    "year": 2019,
    "title": "Turning genome-wide association study findings into opportunities for\n  drug repositioning",
    "summary": "  Drug development is a very costly and lengthy process, while repositioned or\nrepurposed drugs could be brought into clinical practice within a shorter\ntime-frame and at a much reduced cost. The past decade has observed a massive\ngrowth in the amount of data from genome-wide association studies (GWAS). The\nrich information contained in GWAS data has great potential to guide drug\ndiscovery or repositioning. Here we provide an overview of different\ncomputational approaches which employ GWAS data to guide drug repositioning.\nThese methods include selection of top candidate genes from GWAS as drug\ntargets, deducing drug candidates based on drug-drug and disease-disease\nsimilarity, searching for reversed expression profiles between drugs and\ndiseases, pathway-based methods as well as repositioning based on analysis of\nbiological networks. Each method is illustrated with examples, and their\nrespective strengths and limitations are discussed. Finally we discussed\nseveral areas for future research.\n"
  },
  {
    "year": 2019,
    "title": "Identification of key genes related to the mechanism and prognosis of\n  lung squamous cell carcinoma using bioinformatics analysis",
    "summary": "  Objectives Lung squamous cell carcinoma (LUSC) often diagnosed as advanced\nwith poor prognosis. The mechanisms of its pathogenesis and prognosis require\nurgent elucidation. This study was performed to screen potential biomarkers\nrelated to the occurrence, development and prognosis of LUSC to reveal unknown\nphysiological and pathological processes. Materials and Methods Using\nbioinformatics analysis, the lung squamous cell carcinoma microarray datasets\nfrom the GEO and TCGA databases were analyzed to identify differentially\nexpressed genes(DEGs). Furthermore, PPI and WGCNA network analysis were\nintegrated to identify the key genes closely related to the process of LUSC\ndevelopment. In addition, survival analysis was performed to achieve a\nprognostic model that accomplished a high level of prediction accuracy. Results\nand Conclusion Eighty-five up-regulated and 39 down-regulated genes were\nidentified, on which functional and pathway enrichment analysis was conducted.\nGO analysis demonstrated that up-regulated genes were principally enriched in\nepidermal development and DNA unwinding in DNA replication. Down-regulated\ngenes were mainly involved in cell adhesion, signal transduction and positive\nregulation of inflammatory response. After PPI and WGCNA network analysis,\neight genes, including AURKA, RAD51, TTK, AURKB, CCNA2, TPX2, KPNA2 and KIF23,\nhave been found to play a vital role in LUSC development. The prognostic model\ncontained 20 genes, 18 of which were detrimental to prognosis. The AUC of the\nestablished prognostic model for predicting the survival of patients at 1, 3,\nand 5 years was 0.828, 0.826 and 0.824, respectively. To conclude, this study\nidentified a number of biomarkers of significant interest for additional\ninvestigation of the therapies and methods of prognosis of lung squamous cell\ncarcinoma.\n"
  },
  {
    "year": 2019,
    "title": "Indoor microbiome, environmental characteristics and asthma among junior\n  high school students in Johor Bahru, Malaysia",
    "summary": "  Indoor microbial diversity and composition are suggested to affect the\nprevalence and severity of asthma. In this study, we collected floor dust and\nenvironmental characteristics from 21 classrooms, and health data related to\nasthma symptoms from 309 students, in junior high schools in Johor Bahru,\nMalaysia. Bacterial and fungal composition was characterized by sequencing 16s\nrRNA gene and internal transcribed spacer (ITS) region, and the absolute\nmicrobial concentration was quantified by qPCR. In total, 326 bacterial and 255\nfungal genera were characterized. Five bacterial (Sphingobium, Rhodomicrobium,\nShimwellia, Solirubrobacter, Pleurocapsa) and two fungal (Torulaspora and\nLeptosphaeriaceae) taxa were protective for asthma severity. Two bacterial\ntaxa, Izhakiella and Robinsoniella, were positively associated with asthma\nseverity. Several protective bacterial taxa including Rhodomicrobium,\nShimwellia and Sphingobium has been reported as protective microbes in previous\nstudies, whereas other taxa were first time reported. Environmental\ncharacteristics, such as age of building, size of textile curtain per room\nvolume, occurrence of cockroaches, concentration of house dust mite allergens\ntransferred from homes by the occupants, were involved in shaping the overall\nmicrobial community but not asthma-associated taxa; whereas visible dampness\nand mold, which did not change the overall microbial community for floor dust,\ndecreased the concentration of protective bacteria Rhodomicrobium\n(\\b{eta}=-2.86, p=0.021) of asthma, indicating complex interactions between\nmicrobes, environmental characteristics and asthma symptoms. Overall, this is\nthe first indoor microbiome study to characterize the asthma-associated\nmicrobes and their environmental determinant in tropical area, promoting the\nunderstanding of microbial exposure and respiratory health in this region.\n"
  },
  {
    "year": 2019,
    "title": "A genomic dominion with regulatory dependencies on human-specific\n  single-nucleotide changes in Modern Humans",
    "summary": "  Gene set enrichment analyses of 8,405 genes linked with 35,074 human-specific\n(hs) regulatory single-nucleotide changes (SNCs) revealed the staggering\nbreadth of significant associations with morphological structures,\nphysiological processes, and pathological conditions of Modern Humans.\nSignificant enrichment traits include more than 1,000 anatomically-distinct\nregions of the adult human brain, many different types of human cells and\ntissues, more than 200 common human disorders and more than 1,000 records of\nrare diseases. Thousands of genes connected with regulatory hsSNCs have been\nidentified in this contribution, which represent essential genetic elements of\nthe autosomal inheritance and survival of species phenotypes: a total of 1,494\ngenes linked with either autosomal dominant or recessive inheritance as well as\n2,273 genes associated with premature death, embryonic lethality, as well as\npre-, peri-, neo-, and post-natal lethality of both complete and incomplete\npenetrance. Therefore, thousands of heritable traits and critical genes\nimpacting the offspring survival appear under the human-specific regulatory\ncontrol in genomes of Modern Humans. These observations highlight the\nremarkable translational opportunities afforded by the discovery of genetic\nregulatory loci harboring hsSNCs that are fixed in humans, distinct from other\nprimates, and located in differentially-accessible (DA) chromatin regions\nduring human brain development.\n"
  },
  {
    "year": 2019,
    "title": "DomainScope: A disease network based on protein domain connections",
    "summary": "  Protein domains are highly conserved functional units of proteins. Because\nthey carry functionally significant information, the majority of the coding\ndisease variants are located on domains. Additionally, domains are specific\nunits of the proteins that can be targeted for drug delivery purposes. Here,\nusing information about variants sites associated with diseases, a disease\nnetwork was built, based on their sharing the same domain and domain variation\nsite. The result was 49,990 disease pairs linked by domain variant site and\n533,687 disease pairs that share the same mutated domain. These pairs were\ncompared to disease pairs made using previous methods such as gene identity and\ngene variant site identity, which revealed that over 8,000 of these pairs were\nnot only missing from the gene pairings but also not found commonly together in\nliterature. The disease network was analyzed from their disease subject\ncategories, which when compared to the gene-based disease network revealed that\nthe domain method results in higher number of connections across disease\ncategories versus within a disease category. Further, a study into the drug\nrepurposing possibilities of the disease network created using domain revealed\nthat 16,902 of the disease pairs had a drug reported for one disease but not\nthe other, highlighting the drug repurposing potential of this new methodology.\n"
  },
  {
    "year": 2019,
    "title": "Can artificial neural networks supplant the polygene risk score for risk\n  prediction of complex disorders given very large sample sizes?",
    "summary": "  Genome-wide association studies (GWAS) provide a means of examining the\ncommon genetic variation underlying a range of traits and disorders. In\naddition, it is hoped that GWAS may provide a means of differentiating affected\nfrom unaffected individuals. This has potential applications in the area of\nrisk prediction. Current attempts to address this problem focus on using the\npolygene risk score (PRS) to predict case-control status on the basis of GWAS\ndata. However this approach has so far had limited success for complex traits\nsuch as schizophrenia (SZ). This is essentially a classification problem.\nArtificial neural networks (ANNs) have been shown in recent years to be highly\neffective in such applications. Here we apply an ANN to the problem of\ndistinguishing SZ patients from unaffected controls. We compare the\neffectiveness of the ANN with the PRS in classifying individuals by\ncase-control status based only on genetic data from a GWAS. We use the\nschizophrenia dataset from the Psychiatric Genomics Consortium (PGC) for this\nstudy. Our analysis indicates that the ANN is more sensitive to sample size\nthan the PRS. As larger and larger sample sizes become available, we suggest\nthat ANNs are a promising alternative to the PRS for classification and risk\nprediction for complex genetic disorders.\n"
  },
  {
    "year": 2019,
    "title": "Identification of Biomarkers Driving Blood Cell Development",
    "summary": "  A blood cell lineage consists of several consecutive developmental stages\nfrom the pluripotent or multipotent stem cell to a particular stage of\nterminally differentiated cells. There is considerable interest in identifying\nthe key regulatory genes that govern blood cell development from the gene\nexpression data without considering the underlying network between\ntranscription factors (TFs) and their target genes. In this study, we introduce\na novel expression pattern that key regulators expose along the differentiation\npath. We deploy this pattern to identify the cell-specific key regulators\nresponsible for the development. As proof of concept, we consider this approach\nto data on six developmental stages from mouse embryonic stem cells to\nterminally differentiated macrophages.\n"
  },
  {
    "year": 2019,
    "title": "A Common Gene Expression Signature Analysis Method for Multiple Types of\n  Cancer",
    "summary": "  Mining gene expression profiles has proven valuable for identifying\nsignatures serving as surrogates of cancer phenotypes. However, the\nsimilarities of such signatures across different cancer types have not been\nstrong enough to conclude that they represent a universal biological mechanism\nshared among multiple cancer types. Here we describe a network-based approach\nthat explores gene-to-gene connections in multiple cancer datasets while\nmaximizing the overall association of the subnetwork with clinical outcomes.\nWith the dataset of The Cancer Genome Atlas (TCGA), we studied the\ncharacteristics of common gene expression of three types of cancers: Rectum\nadenocarcinoma (READ), Breast invasive carcinoma (BRCA) and Colon\nadenocarcinoma (COAD). By analyzing several pairs of highly correlated genes\nafter filtering and clustering work, we found that the co-expressed genes\nacross multiple types of cancers point to particular biological mechanisms\nrelated to cancer cell progression , suggesting that they represent important\nattributes of cancer in need of being elucidated for potential applications in\ndiagnostic, prognostic and therapeutic products applicable to multiple cancer\ntypes.\n"
  },
  {
    "year": 2020,
    "title": "De Novo Assembly of Uca minax Transcriptome from Next Generation\n  Sequencing",
    "summary": "  High-throughput cDNA sequencing (RNA-seq) is a very powerful technique to\nquantify gene expression in an unbiased way. The Crustacean family is among the\ngroups of organisms sparsely represented in current genomic databases. Here we\npresent transcriptome data from Uca minax (red-jointed fiddler crab) as an\nopportunity to extend our knowledge. Next generation sequencing was performed\non six tissue samples from Uca minax using the Illumina HiSeq system. Six\nTranscriptome libraries were created using Trinity; a free, open-source\nsoftware tool for de novo transcriptome assembly of high-throughput mRNA\nsequencing (RNA-seq) data with the absence of a reference genome. In addition,\nseveral tools that aid in management of data were used, such as RSEM, Bowtie,\nBlast, and IGV; a tool for visualizing RNA-seq analysis results. Fast quality\ncontrol (FastQC) analysis of the raw sequenced files revealed that both adapter\nand PCR primer sequences were prevalently present, which may require a\npreprocessing step.\n"
  },
  {
    "year": 2020,
    "title": "Deciphering the regulatory genome of $\\textit{Escherichia coli}$, one\n  hundred promoters at a time",
    "summary": "  Advances in DNA sequencing have revolutionized our ability to read genomes.\nHowever, even in the most well-studied of organisms, the bacterium ${\\it\nEscherichia coli}$, for $\\approx$ 65$\\%$ of the promoters we remain completely\nignorant of their regulation. Until we have cracked this regulatory Rosetta\nStone, efforts to read and write genomes will remain haphazard. We introduce a\nnew method (Reg-Seq) linking a massively-parallel reporter assay and mass\nspectrometry to produce a base pair resolution dissection of more than 100\npromoters in ${\\it E. coli}$ in 12 different growth conditions. First, we show\nthat our method recapitulates regulatory information from known sequences.\nThen, we examine the regulatory architectures for more than 80 promoters in the\n${\\it E. coli}$ genome which previously had no known regulation. In many cases,\nwe also identify which transcription factors mediate their regulation. The\nmethod introduced here clears a path for fully characterizing the regulatory\ngenome of model organisms, with the potential of moving on to an array of other\nmicrobes of ecological and medical relevance.\n"
  },
  {
    "year": 2020,
    "title": "DNA methylation heterogeneity induced by collaborations between\n  enhancers",
    "summary": "  During mammalian embryo development, reprogramming of DNA methylation plays\nimportant roles in the erasure of parental epigenetic memory and the\nestablishment of na\\\"{i}ve pluripogent cells. Multiple enzymes that regulate\nthe processes of methylation and demethylation work together to shape the\npattern of genome-scale DNA methylation and guid the process of cell\ndifferentiation. Recent availability of methylome information from single-cell\nwhole genome bisulfite sequencing (scBS-seq) provides an opportunity to study\nDNA methylation dynamics in the whole genome in individual cells, which reveal\nthe heterogeneous methylation distributions of enhancers in embryo stem cells\n(ESCs). In this study, we developed a computational model of enhancer\nmethylation inheritance to study the dynamics of genome-scale DNA methylation\nreprogramming during exit from pluripotency. The model enables us to track\ngenome-scale DNA methylation reprogramming at single-cell level during the\nembryo development process, and reproduce the DNA methylation heterogeneity\nreported by scBS-seq. Model simulations show that DNA methylation heterogeneity\nis an intrinsic property driven by cell division along the development process,\nand the collaboration between neighboring enhancers is required for\nheterogeneous methylation. Our study suggest that the mechanism of genome-scale\noscillation proposed by Rulands et al. (2018) might not necessary to the DNA\nmethylation during exit from pluripotency.\n"
  },
  {
    "year": 2020,
    "title": "Phylogenetic analyses of the severe acute respiratory syndrome\n  coronavirus 2 reflected the several routes of introduction to Taiwan, the\n  United States, and Japan",
    "summary": "  Worldwide Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)\ninfection is disrupting in the economy and anxiety of people. The public\nanxiety has increased the psychological burden on government and healthcare\nprofessionals, resulting in a government worker suicide in Japan. The terrified\npeople are asking the government for border measures. However, are border\nmeasures possible for this virus? By analyzing 48 almost complete virus genome\nsequences, we found out that the viruses that invaded Taiwan, the United\nStates, and Japan were introduced independently. We identified thirteen\nparsimony-informative sites and three groups (CTC, TCC, and TCT). Viruses found\noutside China did not form a monophyletic clade, opposite to previous study.\nThese results suggest the difficulty of implementing effective border measures\nagainst this virus.\n"
  },
  {
    "year": 2020,
    "title": "Phylogenetic Study of 2019-nCoV by Using Alignment Free Method\n  (Evolutionary Bifurcation of Novel Coronavirus Mutants)",
    "summary": "  The phylogenetic tree of SARS-CoV-2 (nCov-19) viruses is reconstructed\naccording to the similarity of genome sequences. The tree topology of\nBetacoronavirus is remarkably consistent with biologist's systematics. Because\nthe tree construction contains enough information about virus mutants, it is\nsuitable to study the evolutionary relationship between novel coronavirus\nmutants transmitted among humans. The emergences of 14 kinds of main mutants\nare studied and these strains can be classified as eight bifurcations of the\nphylogenetic tree. It is found that there exist three types of virus mutations,\nnamely, the mutation among sub-branches of the same branch, the off-root\nmutation and the root-oriented mutation between large branches of the tree.\nFrom the point of the relation between viral mutation and host selection we\nfound that individuals with low immunity provide a special environment for the\npositive natural selection of virus evolution. It gives a mechanism to explain\nwhy large mutations between two distant branches generally occur in the nCov-19\nphylogenetic tree. The finding is helpful to formulate strategies to control\nthe spread of COVID-19.\n"
  },
  {
    "year": 2020,
    "title": "The design and construction of reference pangenome graphs",
    "summary": "  The recent advances in sequencing technologies enables the assembly of\nindividual genomes to the reference quality. How to integrate multiple genomes\nfrom the same species and to make the integrated representation accessible to\nbiologists remain an open challenge. Here we propose a graph-based data model\nand associated formats to represent multiple genomes while preserving the\ncoordinate of the linear reference genome. We implemented our ideas in the\nminigraph toolkit and demonstrate that we can efficiently construct a pangenome\ngraph and compactly encode tens of thousands of structural variants missing\nfrom the current reference genome.\n"
  },
  {
    "year": 2020,
    "title": "Genome Variant Calling with a Deep Averaging Network",
    "summary": "  Variant calling, the problem of estimating whether a position in a DNA\nsequence differs from a reference sequence, given noisy, redundant, overlapping\nshort sequences that cover that position, is fundamental to genomics. We\npropose a deep averaging network designed specifically for variant calling. Our\nmodel takes into account the independence of each short input read sequence by\ntransforming individual reads through a series of convolutional layers,\nlimiting the communication between individual reads to averaging and\nconcatenating operations. Training and testing on the precisionFDA Truth\nChallenge (pFDA), we match state of the art overall 99.89 F1 score. Genome\ndatasets exhibit extreme skew between easy examples and those on the decision\nboundary. We take advantage of this property to converge models at 5x the speed\nof standard epoch-based training by skipping easy examples during training. To\nfacilitate future work, we release our code, trained models and pre-processed\npublic domain datasets.\n"
  },
  {
    "year": 2020,
    "title": "Analysis of genetic differences between psychiatric disorders: Exploring\n  pathways and cell-types/tissues involved and ability to differentiate the\n  disorders by polygenic scores",
    "summary": "  Although displaying genetic correlations, psychiatric disorders are\nclinically defined as categorical entities as they each have distinguishing\nclinical features and may involve different treatments. Identifying\ndifferential genetic variations between these disorders may reveal how the\ndisorders differ biologically and help to guide more personalized treatment.\n  Here we presented a comprehensive analysis to identify genetic markers\ndifferentially associated with various psychiatric disorders/traits based on\nGWAS summary statistics, covering 18 psychiatric traits/disorders and 26\ncomparisons. We also conducted comprehensive analysis to unravel the genes,\npathways and SNP functional categories involved, and the cell types and tissues\nimplicated. We also assessed how well one could distinguish between psychiatric\ndisorders by polygenic risk scores (PRS).\n  SNP-based heritabilities (h2SNP) were significantly larger than zero for most\ncomparisons. Based on current GWAS data, PRS have mostly modest power to\ndistinguish between psychiatric disorders. For example, we estimated that AUC\nfor distinguishing schizophrenia from major depressive disorder (MDD), bipolar\ndisorder (BPD) from MDD and schizophrenia from BPD were 0.694, 0.602 and 0.618\nrespectively, while the maximum AUC (based on h2SNP) were 0.763, 0.749 and\n0.726 respectively. We also uncovered differences in each pair of studied\ntraits in terms of their differences in genetic correlation with comorbid\ntraits. For example, clinically-defined MDD appeared to more strongly\ngenetically correlated with other psychiatric disorders and heart disease, when\ncompared to non-clinically-defined depression in UK Biobank.\n  Our findings highlight genetic differences between psychiatric disorders and\nthe mechanisms involved. PRS may aid differential diagnosis of selected\npsychiatric disorders in the future with larger GWAS samples.\n"
  },
  {
    "year": 2020,
    "title": "A framework to decipher the genetic architecture of combinations of\n  complex diseases: applications in cardiovascular medicine",
    "summary": "  Genome-wide association studies(GWAS) have proven to be highly useful in\nrevealing the genetic basis of complex diseases. At present, most GWAS are\nstudies of a particular single disease diagnosis against controls. However, in\npractice, an individual is often affected by more than one condition/disorder.\nFor example, patients with coronary artery disease(CAD) are often comorbid with\ndiabetes mellitus(DM). Along a similar line, it is often clinically meaningful\nto study patients with one disease but without a comorbidity. For example,\nobese DM may have different pathophysiology from non-obese DM.\n  Here we developed a statistical framework to uncover susceptibility variants\nfor comorbid disorders (or a disorder without comorbidity), using GWAS summary\nstatistics only. In essence, we mimicked a case-control GWAS in which the cases\nare affected with comorbidities or a disease without a relevant comorbid\ncondition (in either case, we may consider the cases as those affected by a\nspecific subtype of disease, as characterized by the presence or absence of\ncomorbid conditions). We extended our methodology to deal with continuous\ntraits with clinically meaningful categories (e.g. lipids). In addition, we\nillustrated how the analytic framework may be extended to more than two traits.\nWe verified the feasibility and validity of our method by applying it to\nsimulated scenarios and four cardiometabolic (CM) traits. We also analyzed the\ngenes, pathways, cell-types/tissues involved in CM disease subtypes. LD-score\nregression analysis revealed some subtypes may indeed be biologically distinct\nwith low genetic correlations. Further Mendelian randomization analysis found\ndifferential causal effects of different subtypes to relevant complications. We\nbelieve the findings are of both scientific and clinical value, and the\nproposed method may open a new avenue to analyzing GWAS data.\n"
  },
  {
    "year": 2020,
    "title": "Estimation of genome size using k-mer frequencies from corrected long\n  reads",
    "summary": "  The third-generation long reads sequencing technologies, such as PacBio and\nNanopore, have great advantages over second-generation Illumina sequencing in\nde novo assembly studies. However, due to the inherent low base accuracy,\nthird-generation sequencing data cannot be used for k-mer counting and\nestimating genomic profile based on k-mer frequencies. Thus, in current genome\nprojects, second-generation data is also necessary for accurately determining\ngenome size and other genomic characteristics. We show that corrected\nthird-generation data can be used to count k-mer frequencies and estimate\ngenome size reliably, in replacement of using second-generation data.\nTherefore, future genome projects can depend on only one sequencing technology\nto finish both assembly and k-mer analysis, which will largely decrease\nsequencing cost in both time and money. Moreover, we present a fast\nlight-weight tool kmerfreq and use it to perform all the k-mer counting tasks\nin this work. We have demonstrated that corrected third-generation sequencing\ndata can be used to estimate genome size and developed a new open-source C/C++\nk-mer counting tool, kmerfreq, which is freely available at\nhttps://github.com/fanagislab/kmerfreq.\n"
  },
  {
    "year": 2020,
    "title": "SOS: Online probability estimation and generation of T and B cell\n  receptors",
    "summary": "  Recent advances in modelling VDJ recombination and subsequent selection of T\nand B cell receptors provide useful tools to analyze and compare immune\nrepertoires across time, individuals, and tissues. A suite of tools--IGoR [1],\nOLGA [2] and SONIA [3]--have been publicly released to the community that allow\nfor the inference of generative and selection models from high-throughput\nsequencing data. However using these tools requires some scripting or\ncommand-line skills and familiarity with complex datasets. As a result the\napplication of the above models has not been available to a broad audience. In\nthis application note we fill this gap by presenting Simple OLGA & SONIA (SOS),\na web-based interface where users with no coding skills can compute the\ngeneration and post-selection probabilities of their sequences, as well as\ngenerate batches of synthetic sequences. The application also functions on\nmobile phones.\n"
  },
  {
    "year": 2020,
    "title": "Coronavirus SARS-CoV-2: Analysis of subgenomic mRNA transcription,\n  3CLpro and PL2pro protease cleavage sites and protein synthesis",
    "summary": "  Coronaviruses have recently caused world-wide severe outbreaks: SARS (Severe\nAcute Respiratory Syndrome) in 2002 and MERS (Middle-East Respiratory Syndrome)\nin 2012. At the end of 2019, a new coronavirus outbreak appeared in Wuhan\n(China) seafood market as first focus of infection, becoming a pandemics in\n2020, spreading mainly into Europe and Asia. Although the virus family is\nwell-known, this specific virus presents considerable differences, as higher\ntransmission rates, being a challenge for diagnostic methods, treatments and\nvaccines. Coronavirus(C++).pro is a C++ application which simulates Coronavirus\nreplication cycle. This software has identified virus type in short times and\nprovided FASTA files of virus proteins, a list of mRNA sequences and secondary\nstructures. Furthermore, the software has identified a list of structural,\nnon-structural and accessory proteins in 2019-nCoV virus genome more similar to\nSARS than to MERS, as several fusion proteins characteristics of this virus\ntype. These results are useful as a first step in order to develop diagnostic\nmethods, new vaccines or antiviral drugs, which could avoid virus replication\nin any stage: fusion inhibitors, RdRp inhibitors and PL2pro/3CLpro protease\ninhibitors.\n"
  },
  {
    "year": 2020,
    "title": "HLA predictions from the bronchoalveolar lavage fluid samples of five\n  patients at the early stage of the Wuhan seafood market COVID-19 outbreak",
    "summary": "  We are in the midst of a global viral pandemic, one with no cure and a high\nmortality rate. The Human Leukocyte Antigen (HLA) gene complex plays a critical\nrole in host immunity. We predicted HLA class I and II alleles from the\ntranscriptome sequencing data prepared from the bronchoalveolar lavage fluid\nsamples of five patients at the early stage of the COVID-19 outbreak. We\nidentified the HLA-I allele A*24:02 in four out of five patients, which is\nhigher than the expected frequency (17.2%) in the South Han Chinese population.\nThe difference is statistically significant with a p-value less than $10^{-4}$.\nOur analysis results may help provide future insights on disease\nsusceptibility.\n"
  },
  {
    "year": 2020,
    "title": "LAI-Net: Local-Ancestry Inference with Neural Networks",
    "summary": "  Local-ancestry inference (LAI), also referred to as ancestry deconvolution,\nprovides high-resolution ancestry estimation along the human genome. In both\nresearch and industry, LAI is emerging as a critical step in DNA sequence\nanalysis with applications extending from polygenic risk scores (used to\npredict traits in embryos and disease risk in adults) to genome-wide\nassociation studies, and from pharmacogenomics to inference of human population\nhistory. While many LAI methods have been developed, advances in computing\nhardware (GPUs) combined with machine learning techniques, such as neural\nnetworks, are enabling the development of new methods that are fast, robust and\neasily shared and stored. In this paper we develop the first neural network\nbased LAI method, named LAI-Net, providing competitive accuracy with\nstate-of-the-art methods and robustness to missing or noisy data, while having\na small number of layers.\n"
  },
  {
    "year": 2020,
    "title": "High fidelity epigenetic inheritance: Information theoretic model\n  predicts $k$-threshold filling of histone modifications post replication",
    "summary": "  Beyond the genetic code, there is another layer of information encoded as\nchemical modifications on histone proteins positioned along the DNA.\nMaintaining these modifications is crucial for survival and identity of cells.\nHow the information encoded in the histone marks gets inherited, given that\nonly half the parental nucleosomes are transferred to each daughter chromatin,\nis a puzzle. We address this problem using ideas from Information theory and\nunderstanding from recent biological experiments. Mapping the replication and\nreconstruction of modifications to equivalent problems in communication, we ask\nhow well an enzyme-machinery can recover information, if they were ideal\ncomputing machines. Studying a parameter regime where realistic enzymes can\nfunction, our analysis predicts that, pragmatically, enzymes may implement a\nthreshold$-k$ filling algorithm which derives from maximum \\`a posteriori\nprobability decoding. Simulations using our method produce modification\npatterns similar to what is observed in recent experiments.\n"
  },
  {
    "year": 2020,
    "title": "Identification of Repurposable Drugs and Adverse Drug Reactions for\n  Various Courses of COVID-19 Based on Single-Cell RNA Sequencing Data",
    "summary": "  Coronavirus disease 2019 (COVID-19) has impacted almost every part of human\nlife worldwide, posing a massive threat to human health. There is no specific\ndrug for COVID-19, highlighting the urgent need for the development of\neffective therapeutics. To identify potentially repurposable drugs, we employed\na systematic approach to mine candidates from U.S. FDA-approved drugs and\npreclinical small-molecule compounds by integrating the gene expression\nperturbation data for chemicals from the Library of Integrated Network-Based\nCellular Signatures project with a publicly available single-cell RNA\nsequencing dataset from mild and severe COVID-19 patients. We identified 281\nFDA-approved drugs that have the potential to be effective against SARS-CoV-2\ninfection, 16 of which are currently undergoing clinical trials to evaluate\ntheir efficacy against COVID-19. We experimentally tested the inhibitory\neffects of tyrphostin-AG-1478 and brefeldin-a on the replication of the\nsingle-stranded ribonucleic acid (ssRNA) virus influenza A virus. In\nconclusion, we have identified a list of repurposable anti-SARS-CoV-2 drugs\nusing a systems biology approach.\n"
  },
  {
    "year": 2020,
    "title": "Longitudinal high-throughput TCR repertoire profiling reveals the\n  dynamics of T cell memory formation after mild COVID-19 infection",
    "summary": "  COVID-19 is a global pandemic caused by the SARS-CoV-2 coronavirus. T cells\nplay a key role in the adaptive antiviral immune response by killing infected\ncells and facilitating the selection of virus-specific antibodies. However\nneither the dynamics and cross-reactivity of the SARS-CoV-2-specific T cell\nresponse nor the diversity of resulting immune memory are well understood. In\nthis study we use longitudinal high-throughput T cell receptor (TCR) sequencing\nto track changes in the T cell repertoire following two mild cases of COVID-19.\nIn both donors we identified CD4+ and CD8+ T cell clones with transient clonal\nexpansion after infection. The antigen specificity of CD8+ TCR sequences to\nSARS-CoV-2 epitopes was confirmed by both MHC tetramer binding and presence in\nlarge database of SARS-CoV-2 epitope-specific TCRs. We describe characteristic\nmotifs in TCR sequences of COVID-19-reactive clones and show preferential\noccurence of these motifs in publicly available large dataset of repertoires\nfrom COVID-19 patients. We show that in both donors the majority of\ninfection-reactive clonotypes acquire memory phenotypes. Certain T cell clones\nwere detected in the memory fraction at the pre-infection timepoint, suggesting\nparticipation of pre-existing cross-reactive memory T cells in the immune\nresponse to SARS-CoV-2.\n"
  },
  {
    "year": 2020,
    "title": "Dinucleotide repeats in coronavirus SARS-CoV-2 genome: evolutionary\n  implications",
    "summary": "  The ongoing global pandemic of infection disease COVID-19 caused by the 2019\nnovel coronavirus (SARS-COV-2, formerly 2019-nCoV) presents critical threats to\npublic health and the economy since it was identified in China, December 2019.\nThe genome of SARS-CoV-2 had been sequenced and structurally annotated, yet\nlittle is known of the intrinsic organization and evolution of the genome. To\nthis end, we present a mathematical method for the genomic spectrum, a kind of\nbarcode, of SARS-CoV-2 and common human coronaviruses. The genomic spectrum is\nconstructed according to the periodic distributions of nucleotides, and\ntherefore reflects the unique characteristics of the genome. The results\ndemonstrate that coronavirus SARS-CoV-2 exhibits dinucleotide TT islands in the\nnon-structural proteins 3, 4, 5, and 6. Further analysis of the dinucleotide\nregions suggests that the dinucleotide repeats are increased during evolution\nand may confer the evolutionary fitness of the virus. The special dinucleotide\nregions in the SARS-CoV-2 genome identified in this study may become diagnostic\nand pharmaceutical targets in monitoring and curing the COVID-19 disease.\n"
  },
  {
    "year": 2020,
    "title": "EDGE COVID-19: A Web Platform to generate submission-ready genomes for\n  SARS-CoV-2 sequencing efforts",
    "summary": "  Genomics has become an essential technology for surveilling emerging\ninfectious disease outbreaks. A wide range of technologies and strategies for\npathogen genome enrichment and sequencing are being used by laboratories\nworldwide, together with different, and sometimes ad hoc, analytical procedures\nfor generating genome sequences. As a result, public repositories now contain\nnon-standard entries of varying quality. A standardized analytical process for\nconsensus genome sequence determination, particularly for outbreaks such as the\nongoing COVID-19 pandemic, is critical to provide a solid genomic basis for\nepidemiological analyses and well-informed decision making. To address this\nneed, we have developed a bioinformatic workflow to standardize the analysis of\nSARS-CoV-2 sequencing data generated with either the Illumina or Oxford\nNanopore platforms. Using an intuitive web-based interface, this workflow\nautomates SARS-CoV-2 reference-based genome assembly, variant calling, lineage\ndetermination, and provides the ability to submit the consensus sequence and\nnecessary metadata to GenBank or GISAID. Given a raw Illumina or Oxford\nNanopore FASTQ read file, this web-based platform enables non-bioinformatics\nexperts to automatically produce a SARS-CoV-2 genome that is ready for\nsubmission to GISAID or GenBank.\n  Availability:https://edge-covid19.edgebioinformatics.org;https://github.com/LANL-Bioinformatics/EDGE/tree/SARS-CoV2\n"
  },
  {
    "year": 2020,
    "title": "An Immune-related lncRNAs Model for Prognostic of SKCM Patients Base on\n  Cox Regression and Coexpression Analysis",
    "summary": "  SKCM is the most dangerous one of skin cancer, its high degree of malignant,\nis the leading cause of skin cancer. And the level of radiation treatment and\nchemical treatment is minimal, so the mortality is high. Because of its complex\nmolecular and cellular heterogeneity, the existing prediction model of skin\ncancer risk is not ideal. In this study, we developed an immune-related lncRNAs\nmodel to predict the prognosis of patients with SKCM. Screening for\nSKCM-related differential expression of lncRNA from TCGA. Identified\nimmune-related lncRNAs and lncRNA-related mRNA based on the co-expression\nmethod. Through univariate and multivariate analysis, an immune-related lncRNA\nmodel is established to analyze the prognosis of SKCM patients. A 4-lncRNA skin\ncancer prediction model was constructed, including MIR155HG, AL137003.2,\nAC011374.2, and AC009495.2. According to the model, SKCM samples were divided\ninto a high-risk group and low-risk group, and predict the survival of the two\ngroups in 30 years. The area under the ROC curve is 0.749, which shows that the\nmodel has excellent performance. We constructed a 4-lncRNA model to predict the\nprognosis of patients with SKCM, indicating that these lncRNAs may play a\nunique role in the carcinogenesis of SKCM.\n"
  },
  {
    "year": 2020,
    "title": "Immune Fingerprinting through Repertoire Similarity",
    "summary": "  Immune repertoires provide a unique fingerprint reflecting the immune history\nof individuals, with potential applications in precision medicine. However, the\nquestion of how personal that information is and how it can be used to identify\nindividuals has not been explored. Here, we show that individuals can be\nuniquely identified from repertoires of just a few thousands lymphocytes. We\npresent \"Immprint,\" a classifier using an information-theoretic measure of\nrepertoire similarity to distinguish pairs of repertoire samples coming from\nthe same versus different individuals. Using published T-cell receptor\nrepertoires and statistical modeling, we tested its ability to identify\nindividuals with great accuracy, including identical twins, by computing false\npositive and false negative rates $< 10^{-6}$ from samples composed of 10,000\nT-cells. We verified through longitudinal datasets and simulations that the\nmethod is robust to acute infections and the passage of time. These results\nemphasize the private and personal nature of repertoire data.\n"
  },
  {
    "year": 2020,
    "title": "Using micro- and macro-level network metrics unveils top communicative\n  gene modules in psoriasis",
    "summary": "  Background: Psoriasis is a multifactorial chronic inflammatory disorder of\nthe skin with significant morbidity, characterized by hyper proliferation of\nthe epidermis. Even though psoriasis etiology is not fully understood, it is\nbelieved to be multifactorial with numerous key components. Methods: In order\nto cast light on the complex molecular interactions in psoriasis vulgaris at\nboth protein-protein interactions and transcriptomics levels, we analyzed a set\nof microarray gene expression analysis consisting of 170 paired lesional and\nnon-lesional samples. Afterwards, a network analysis was conducted on\nprotein-protein interaction network of differentially expressed genes based on\nmicro- and macro-level network metrics at a systemic level standpoint. Results:\nWe found 17 top communicative genes, all of which experimentally proven to be\npivotal in psoriasis were identified in two modules, namely, cell cycle and\nimmune system. Intra- and inter-gene interaction subnetworks from the top\ncommunicative genes might provide further insight into the corresponding\ncharacteristic mechanisms. Conclusions: Potential gene combinations for\ntherapeutic/diagnostics purposes were identified. Moreover, our proposed\npipeline could be of interest to a broader range of biological network analysis\nstudies.\n"
  },
  {
    "year": 2020,
    "title": "Comprehensive assessment of error correction methods for high-throughput\n  sequencing data",
    "summary": "  The advent of DNA and RNA sequencing has revolutionized the study of genomics\nand molecular biology. Next generation sequencing (NGS) technologies like\nIllumina, Ion Torrent, SOLiD sequencing etc. have brought about a quick and\ncheap way to sequence genomes. Recently, third generation sequencing (TGS)\ntechnologies like PacBio and Oxford Nanopore Technology (ONT) have also been\ndeveloped. Different technologies use different underlying methods for\nsequencing and are prone to different error rates. Though many tools exist for\nerror correction of sequencing data from NGS and TGS methods, no standard\nmethod is available yet to evaluate the accuracy and effectiveness of these\nerror-correction tools. In this study, we present a Software Package for Error\nCorrection Tool Assessment on nuCLEic acid sequences (SPECTACLE) providing\ncomprehensive algorithms to evaluate error-correction methods for DNA and RNA\nsequencing, for NGS and TGS platforms. We also present a compilation of\nsequencing datasets for Illumina, PacBio and ONT platforms that present\nchallenging scenarios for error-correction tools. Using these datasets and\nSPECTACLE, we evaluate the performance of 23 different error-correction tools\nand present unique and helpful insights into their strengths and weaknesses. We\nhope that our methodology will standardize the evaluation of DNA and RNA\nerror-correction tools in the future.\n"
  },
  {
    "year": 2020,
    "title": "Dynamics of B-cell repertoires and emergence of cross-reactive responses\n  in COVID-19 patients with different disease severity",
    "summary": "  COVID-19 patients show varying severity of the disease ranging from\nasymptomatic to requiring intensive care. Although a number of SARS-CoV-2\nspecific monoclonal antibodies have been identified, we still lack an\nunderstanding of the overall landscape of B-cell receptor (BCR) repertoires in\nCOVID-19 patients. Here, we used high-throughput sequencing of bulk and plasma\nB-cells collected over multiple time points during infection to characterize\nsignatures of B-cell response to SARS-CoV-2 in 19 patients. Using principled\nstatistical approaches, we determined differential features of BCRs associated\nwith different disease severity. We identified 38 significantly expanded clonal\nlineages shared among patients as candidates for specific responses to\nSARS-CoV-2. Using single-cell sequencing, we verified reactivity of BCRs shared\namong individuals to SARS-CoV-2 epitopes. Moreover, we identified natural\nemergence of a BCR with cross-reactivity to SARS-CoV-1 and SARS-CoV-2 in a\nnumber of patients. Our results provide important insights for development of\nrational therapies and vaccines against COVID-19.\n"
  },
  {
    "year": 2020,
    "title": "Learning the heterogeneous hypermutation landscape of immunoglobulins\n  from high-throughput repertoire data",
    "summary": "  Somatic hypermutations of immunoglobulin (Ig) genes occuring during affinity\nmaturation drive B-cell receptors' ability to evolve strong binding to their\nantigenic targets. The landscape of these mutations is highly heterogeneous,\nwith certain regions of the Ig gene being preferentially targeted. However, a\nrigorous quantification of this bias has been difficult because of phylogenetic\ncorrelations between sequences and the interference of selective forces. Here,\nwe present an approach that corrects for these issues, and use it to learn a\nmodel of hypermutation preferences from a recently published large IgH\nrepertoire dataset. The obtained model predicts mutation profiles accurately\nand in a reproducible way, including in the previously uncharacterized\nComplementarity Determining Region 3, revealing that both the sequence context\nof the mutation and its absolute position along the gene are important. In\naddition, we show that hypermutations occurring concomittantly along B-cell\nlineages tend to co-localize, suggesting a possible mechanism for accelerating\naffinity maturation.\n"
  },
  {
    "year": 2020,
    "title": "On the Transcriptomic Signature and General Stress State Associated with\n  Aneuploidy",
    "summary": "  Whether aneuploid cells with diverse karyotypes have any properties in common\nhas a been a subject of intense interest. A recent study by Terhorst et al. (1)\nreinvestigated the common aneuploidy gene expression (CAGE), disputing the\nconclusion of our recent work (2). In this short article, which has been\nsubmitted to PNAS as a Letter to the Editor, we explain our major concerns\nabout Terhorst et al. and why we believe that our previous conclusion stands\nvalid.\n"
  },
  {
    "year": 2020,
    "title": "Structural representations of DNA regulatory substrates can enhance\n  sequence-based algorithms by associating functional sequence variants",
    "summary": "  The nucleotide sequence representation of DNA can be inadequate for resolving\nprotein-DNA binding sites and regulatory substrates, such as those involved in\ngene expression and horizontal gene transfer. Considering that sequence-like\nrepresentations are algorithmically very useful, here we fused over 60\ncurrently available DNA physicochemical and conformational variables into\ncompact structural representations that can encode single DNA binding sites to\nwhole regulatory regions. We find that the main structural components reflect\nkey properties of protein-DNA interactions and can be condensed to the amount\nof information found in a single nucleotide position. The most accurate\nstructural representations compress functional DNA sequence variants by 30% to\n50%, as each instance encodes from tens to thousands of sequences. We show that\na structural distance function discriminates among groups of DNA substrates\nmore accurately than nucleotide sequence-based metrics. As this opens up a\nvariety of implementation possibilities, we develop and test a distance-based\nalignment algorithm, demonstrating the potential of using the structural\nrepresentations to enhance sequence-based algorithms. Due to the bias of most\ncurrent bioinformatic methods to nucleotide sequence representations, it is\npossible that considerable performance increases might still be achievable with\nsuch solutions.\n"
  },
  {
    "year": 2020,
    "title": "SARS-CoV-2 and miRNA-like inhibition power",
    "summary": "  (1) Background: RNA viruses and especially coronaviruses could act inside\nhost cells not only by building their own proteins, but also by perturbing the\ncell metabolism. We show the possibility of miRNA-like inhibitions by the\nSARS-CoV-2 concerning for example the hemoglobin and type I interferons\nsyntheses, hence highly perturbing oxygen distribution in vital organs and\nimmune response as described by clinicians; (2) Methods: We compare RNA\nsubsequences of SARS-CoV-2 protein S and RNA-dependent RNA polymerase genes to\nmRNA sequences of beta-globin and type I interferons; (3) Results: RNA\nsubsequences longer than eight nucleotides from SARS-CoV-2 genome could\nhybridize subsequences of the mRNA of beta-globin and of type I interferons;\n(4) Conclusions: Beyond viral protein production, Covid-19 might affect vital\nprocesses like host oxygen transport and immune response.\n"
  },
  {
    "year": 2020,
    "title": "Strategies to integrate multi-omics data for patient survival prediction",
    "summary": "  Genomics, especially multi-omics, has made precision medicine feasible. The\ncompletion and publicly accessible multi-omics resource with clinical outcome,\nsuch as The Cancer Genome Atlas (TCGA) is a great test bed for developing\ncomputational methods that integrate multi-omics data to predict patient cancer\nphenotypes. We have been utilizing TCGA multi-omics data to predict cancer\npatient survival, using a variety of approaches, including prior-biological\nknowledge (such as pathways), and more recently, deep-learning methods. Over\ntime, we have developed methods such as Cox-nnet, DeepProg, and two-stage\nCox-nnet, to address the challenges due to multi-omics and multi-modality.\nDespite the limited sample size (hundreds to thousands) in the training\ndatasets as well as the heterogeneity nature of human populations, these\nmethods have shown significance and robustness at predicting patient survival\nin independent population cohorts. In the following, we would describe in\ndetail these methodologies, the modeling results, and important biological\ninsights revealed by these methods.\n"
  },
  {
    "year": 2020,
    "title": "MOSGA: Modular Open-Source Genome Annotator",
    "summary": "  The generation of high-quality assemblies, even for large eukaryotic genomes,\nhas become a routine task for many biologists thanks to recent advances in\nsequencing technologies. However, the annotation of these assemblies - a\ncrucial step towards unlocking the biology of the organism of interest - has\nremained a complex challenge that often requires advanced bioinformatics\nexpertise. Here we present MOSGA, a genome annotation framework for eukaryotic\ngenomes with a user-friendly web-interface that generates and integrates\nannotations from various tools. The aggregated results can be analyzed with a\nfully integrated genome browser and are provided in a format ready for\nsubmission to NCBI. MOSGA is built on a portable, customizable, and easily\nextendible Snakemake backend, and thus, can be tailored to a wide range of\nusers and projects. We provide MOSGA as a publicly free available web service\nat https://mosga.mathematik.uni-marburg.de and as a docker container at\nregistry.gitlab.com/mosga/mosga:latest. Source code can be found at\nhttps://gitlab.com/mosga/mosga\n"
  },
  {
    "year": 2020,
    "title": "Genome-wide association and transcriptome analysis reveals serum ghrelin\n  to be linked with GFRAL",
    "summary": "  Objective: Ghrelin is an orexigenic peptide hormone involved in the\nregulation of energy homeostasis, food intake and glucose metabolism. Serum\nlevels increase anticipating a meal and fall afterwards. Underlying genetic\nmechanisms of the ghrelin secretion are unknown. Methods: Total serum ghrelin\nwas measured in 1501 subjects selected from the population-based\nLIFE-ADULT-sample after an overnight fast. A genome-wide association study\n(GWAS) was performed. Gene-based expression association analyses\n(transcriptome-wide association study (TWAS)) were done using MetaXcan.\nResults: In the GWAS, three loci reached genome-wide significance: the\nWW-domain containing the oxidoreductase-gene (WWOX; p=1.80E-10) on chromosome\n16q23.3-24.1 (SNP: rs76823993); the Contactin-Associated Protein-Like 2 gene\n(CNTNAP2; p=9.0E-9) on chromosome 7q35-q36 (SNP: rs192092592) and the Ghrelin\nAnd Obestatin Prepropeptide gene (GHRL; p=2.72E-8) on chromosome 3p25.3 (SNP:\nrs143729751). In the TWAS, serum ghrelin was negatively associated with RNA\nexpression of the GDNF Family Receptor Alpha Like (GFRAL), receptor of the\nanorexigenic Growth Differentiation Factor-15 (GDF15), (z-score=-4.288,\np=1.81E-05). Furthermore, ghrelin was positively associated with Ribosomal\nProtein L36 (RPL36; z-score=4.848, p=1.25E-06). Conclusions: Our findings\nprovide evidence of a functional link between two major players of weight\nregulation, the ghrelin system and the GDF15/GFRAL-pathway.\n"
  },
  {
    "year": 2020,
    "title": "Did circoviruses intermediate the recombination between bat and pangolin\n  coronaviruses, yielding SARS-CoV-2?",
    "summary": "  Since the first reports of a coronavirus (CoV) disease 2019 (COVID-19) caused\nby severe acute respiratory syndrome virus (SARS-CoV-2) in Wuhan, Hubei\nprovince, China, scientists are working around the clock to find sound answers\nto the issue of its origin. While the number of scientific articles on\nSARS-CoV-2 is increasing, there are still many gaps as to its origin. All\nstudies failed to find a coronavirus in other animals that is more similar to\nhuman SARS-COV2 than the bat virus, considered to be the primary reservoir. In\nthis paper we address a new hypothesis, based on a possible recombination\nbetween a DNA and SARS-CoV viruses, to explain the rise of SRAS-CoV-2. By\ncomparing SARS-CoV-2 and related CoVs with circoviruses (CVs), we found strong\nsequence similarity of the genomic region at the 3-end of Bat-CoV ORF1a and the\norigin of replication (Ori) of porcine CV type 2 (PCV2), as well as similar RNA\nsecondary structures of the region encompassing the cleavage site of CoV S gene\nwith the PCV2 Ori. This constitutes a primary evidence that supports a possible\nrecombination, which occurrence might explain the origin of SARS-CoV-2.\n"
  },
  {
    "year": 2020,
    "title": "Computational tools for the multiscale analysis of Hi-C data in\n  bacterial chromosomes",
    "summary": "  Just as in eukaryotes, high-throughput chromosome conformation capture (Hi-C)\ndata have revealed nested organizations of bacterial chromosomes into\noverlapping interaction domains. In this chapter, we present a multiscale\nanalysis framework aiming at capturing and quantifying these properties. These\ninclude both standard tools (e.g. contact laws) and novel ones such as an index\nthat allows identifying loci involved in domain formation independently of the\nstructuring scale at play. Our objective is two-fold. On the one hand, we aim\nat providing a full, understandable Python/Jupyter-based code which can be used\nby both computer scientists as well as biologists with no advanced\ncomputational background. On the other hand, we discuss statistical issues\ninherent to Hi-C data analysis, focusing more particularly on how to properly\nassess the statistical significance of results. As a pedagogical example, we\nanalyze data produced in {\\it Pseudomonas aeruginosa}, a model pathogenetic\nbacterium. All files (codes and input data) can be found on a github\nrepository. We have also embedded the files into a Binder package so that the\nfull analysis can be run on any machine through internet.\n"
  },
  {
    "year": 2020,
    "title": "Genome Compression Against a Reference",
    "summary": "  Being able to store and transmit human genome sequences is an important part\nin genomic research and industrial applications. The complete human genome has\n3.1 billion base pairs (haploid), and storing the entire genome naively takes\nabout 3 GB, which is infeasible for large scale usage.\n  However, human genomes are highly redundant. Any given individual's genome\nwould differ from another individual's genome by less than 1%. There are tools\nlike DNAZip, which express a given genome sequence by only noting down the\ndifferences between the given sequence and a reference genome sequence. This\nallows losslessly compressing the given genome to ~ 4 MB in size.\n  In this work, we demonstrate additional improvements on top of the DNAZip\nlibrary, where we show an additional ~ 11% compression on top of DNAZip's\nalready impressive results. This would allow further savings in disk space and\nnetwork costs for transmitting human genome sequences.\n"
  },
  {
    "year": 2020,
    "title": "RNA-seq data science: From raw data to effective interpretation",
    "summary": "  RNA-sequencing (RNA-seq) has become an exemplar technology in modern biology\nand clinical applications over the past decade. It has gained immense\npopularity in the recent years driven by continuous efforts of the\nbioinformatics community to develop accurate and scalable computational tools.\nRNA-seq is a method of analyzing the RNA content of a sample using the modern\nsequencing platforms. It generates enormous amounts of transcriptomic data in\nthe form of nucleotide sequences, known as reads. RNA-seq analysis enables the\nprobing of genes and corresponding transcripts which is essential for answering\nimportant biological questions, such as detecting novel exons, transcripts,\ngene expressions, and studying alternative splicing structure. However,\nobtaining meaningful biological signals from raw data using computational\nmethods is challenging due to the limitations of modern sequencing\ntechnologies. The need to leverage these technological challenges have pushed\nthe rapid development of many novel computational tools which have evolved and\ndiversified in accordance with technological advancements, leading to the\ncurrent myriad population of RNA-seq tools. Our review provides a systemic\noverview of RNA-seq technology and 235 available RNA-seq tools across various\ndomains published from 2008 to 2020, discussing the interdisciplinary nature of\nbioinformatics involved in RNA sequencing, analysis, and software development.\n"
  },
  {
    "year": 2020,
    "title": "Likelihood Models for Forensic Genealogy",
    "summary": "  In the idealized Morgan model of crossover, we study the probability\ndistributions of shared DNA (identical by descent) between individuals having a\nwide range of relationships (not just lineal descendants), especially cases for\nwhich previous work produces inaccurate results. Using Monte Carlo simulation,\nwe show that a particular, complicated functional form with just one continuous\nfitted parameter accurately approximates the distributions in all cases tried.\nAnalysis of that functional form shows that it is close to a normal\ndistribution, not in shared fraction f, but in the square-root of f. We\ndescribe a multivariate normal model in this variable for use as a practical\nframework for several general tasks in forensic genealogy that are currently\ndone by less-accurate and less well-founded methods.\n"
  },
  {
    "year": 2020,
    "title": "Deep Learning Prediction of Adverse Drug Reactions Using Open TG-GATEs\n  and FAERS Databases",
    "summary": "  With the advancements in Artificial intelligence (AI) and the accumulation of\nhealthrelated big data, it has become increasingly feasible and commonplace to\nleverage machine learning technologies to analyze clinical and omics metadata\nto assess the possibility of adverse drug reactions or events (ADRs) in the\ncourse of drug discovery. Here, we have described a novel approach that\ncombined drug-induced gene expression profile from Open TG-GATEs\n(Toxicogenomics Project-Genomics Assisted Toxicity Evaluation Systems) and ADR\noccurrence information from FAERS (FDA [Food and Drug Administration] Adverse\nEvents Reporting System) database to predict the likelihood of ADRs. We\ngenerated a total of 14 models using Deep Neural Networks (DNN) to predict\ndifferent ADRs; in the validation tests, our models achieved a mean accuracy of\n85.71%, indicating that our approach successfully and consistently predicted\nADRs for a wide range of drugs. As an example, we have described the ADR model\nin the context of Duodenal ulcer. We believe that our models will help predict\nthe likelihood of ADRs while testing novel pharmaceutical compounds, and will\nbe useful for researchers in drug discovery.\n"
  },
  {
    "year": 2020,
    "title": "Diversity in immunogenomics: the value and the challenge",
    "summary": "  With the advent of high-throughput sequencing technologies, the fields of\nimmunogenomics and adaptive immune receptor repertoire research are facing both\nopportunities and challenges. Adaptive immune receptor repertoire sequencing\n(AIRR-seq) has become an increasingly important tool to characterize T and B\ncell responses in settings of interest. However, the majority of AIRR-seq\nstudies conducted so far were performed in individuals of European ancestry,\nrestricting the ability to identify variation in human adaptive immune\nresponses across populations and limiting their applications. As AIRR-seq\nstudies depend on the ability to assign VDJ sequence reads to the correct\ngermline gene segments, efforts to characterize the genomic loci that encode\nadaptive immune receptor genes in different populations are urgently needed.\nThe availability of comprehensive germline gene databases and further\napplications of AIRR-seq studies to individuals of non-European ancestry will\nsubstantially enhance our understanding of human adaptive immune responses,\npromote the development of effective diagnostics and treatments, and eventually\nadvance precision medicine.\n"
  },
  {
    "year": 2020,
    "title": "Extraction of long k-mers using spaced seeds",
    "summary": "  The extraction of k-mers from sequencing reads is an important task in many\nbioinformatics applications, such as all DNA sequence analysis methods based on\nde Bruijn graphs. These methods tend to be more accurate when the used k-mers\nare unique in the analyzed DNA, and thus the use of longer k-mers is preferred.\nWhen the read lengths of short read sequencing technologies increase, the error\nrate will become the determining factor for the largest possible value of k.\nHere we propose LoMeX which uses spaced seeds to extract long k-mers accurately\neven in the presence of sequencing errors. Our experiments show that LoMeX can\nextract long k-mers from current Illumina reads with a higher recall than a\nstandard k-mer counting tool. Furthermore, our experiments on simulated data\nshow that when the read length further increases, the performance of standard\nk-mer counters declines, whereas LoMeX still extracts long k-mers successfully.\n"
  },
  {
    "year": 2020,
    "title": "Whole-Genome Sequence of the Trypoxylus dichotomus Japanese rhinoceros\n  beetle",
    "summary": "  The draft whole-genome sequence of the Japanese rhinoceros beetle, Trypoxylus\ndichotomus was obtained using long-read PacBio sequence technology. The final\nassembled genome consisted of 739 Mbp in 2,347 contigs, with 24.5x mean\ncoverage and a G+C content of 35.99%.\n"
  },
  {
    "year": 2020,
    "title": "Expanding the phenotype of SCA19/22: Parkinsonism, cognitive impairment\n  and epilepsy",
    "summary": "  BACKGROUND: Spinocerebellar ataxia types 19 and 22 (SCA19/22) are rare\nconditions in which relatively isolated cerebellar involvement is frequently\nassociated with cognitive impairment. Here, we report on new clinical features\nand provide details of the cognitive profile in two SCA19/22 families.METHODS:\nTwo families displaying an autosomal-dominant form of cerebellar ataxia\nunderwent clinical examinations and genetic testing.RESULTS: In addition to the\nclassical clinical features of SCA, a wide spectrum of cognitive disorders\n(including visuospatial impairments) was observed. Eight patients had mild\nParkinsonism, and five had epilepsy. Genetic testing showed that the KCND3\nmutation (c.679_681delTTC, p.F227del) was present in both families.CONCLUSIONS:\nOur findings broaden the phenotypic spectrum of SCA19/22, and suggest that\nKCND3 should be included in the list of candidate genes for epilepsy,\nParkinsonism and cognitive impairment.\n"
  },
  {
    "year": 2020,
    "title": "TMEM240 mutations cause spinocerebellar ataxia 21 with mental\n  retardation and severe cognitive impairment",
    "summary": "  Autosomal dominant cerebellar ataxia corresponds to a clinically and\ngenetically heterogeneous group of neurodegenerative disorders that primarily\naffect the cerebellum. Here, we report the identification of the causative gene\nin spinocerebellar ataxia 21, an autosomal-dominant disorder previously mapped\nto chromosome 7p21.3-p15.1. This ataxia was firstly characterized in a large\nFrench family with slowly progressive cerebellar ataxia, accompanied by severe\ncognitive impairment and mental retardation in two young children. Following\nthe recruitment of 12 additional young family members, linkage analysis enabled\nus to definitively map the disease locus to chromosome 1p36.33-p36.32. The\ncausative mutation, (c.509C4T/p.P170L) in the transmembrane protein gene\nTMEM240, was identified by whole exome sequencing and then was confirmed by\nSanger sequencing and co-segregation analyses. Index cases from 368 French\nfamilies with autosomal-dominant cerebellar ataxia were also screened for\nmutations. In seven cases, we identified a range of missense mutations\n(c.509C4T/p.P170L, c.239C4T/p.T80M, c.346C4T/p.R116C, c.445G4A/p.E149K,\nc.511C4T/p.R171W), and a stop mutation (c.489C4G/p.Y163*) in the same gene.\nTMEM240 is a small, strongly conserved transmembrane protein of unknown\nfunction present in cerebellum and brain. Spinocerebellar ataxia 21 may be a\nparticular early-onset disease associated with severe cognitive impairment.\n"
  },
  {
    "year": 2020,
    "title": "The bat coronavirus RmYN02 is characterized by a 6-nucleotide deletion\n  at the S1/S2 junction, and its claimed PAA insertion is highly doubtful",
    "summary": "  Zhou et al. reported the discovery of RmYN02, a strain closely related to\nSARS-CoV-2, which is claimed to contain a natural PAA amino acid insertion at\nthe S1/S2 junction of the spike protein at the same position of the PRRA\ninsertion that has created a polybasic furin cleavage site in SARS-CoV-2. The\nauthors support with their findings the theory that the furin cleavage site\ninsertion present in SARS-CoV-2 is natural. Because no nucleotide alignment\nwith closely related strains of the region coding for the supposed insertion is\nprovided by Zhou et al., we have applied several alignment algorithms to search\nfor the most parsimonious alignments. We conclude that RmYN02 does not contain\nan insertion at the S1/S2 junction when compared to its closest relatives at\nthe nucleotide level, but rather a 6-nucleotide deletion and that the claimed\nPAA insertion is more likely to be the result of mutations. A close examination\nof RmYN02 sequencing records and assembly methods is wishful. In conclusion,\nSARS-CoV-2, with its 12-nucleotide insertion at the S1/S2 junction remains\nunique among its sarbecovirus relatives.\n"
  },
  {
    "year": 2020,
    "title": "Towards a robust out-of-the-box neural network model for genomic data",
    "summary": "  The accurate prediction of biological features from genomic data is paramount\nfor precision medicine and sustainable agriculture. For decades, neural network\nmodels have been widely popular in fields like computer vision, astrophysics\nand targeted marketing given their prediction accuracy and their robust\nperformance under big data settings. Yet neural network models have not made a\nsuccessful transition into the medical and biological world due to the\nubiquitous characteristics of biological data such as modest sample sizes,\nsparsity, and extreme heterogeneity. Here, we investigate the robustness,\ngeneralization potential and prediction accuracy of widely used convolutional\nneural network and natural language processing models with a variety of\nheterogeneous genomic datasets. Mainly, recurrent neural network models\noutperform convolutional neural network models in terms of prediction accuracy,\noverfitting and transferability across the datasets under study. While the\nperspective of a robust out-of-the-box neural network model is out of reach, we\nidentify certain model characteristics that translate well across datasets and\ncould serve as a baseline model for translational researchers.\n"
  },
  {
    "year": 2020,
    "title": "Twelve years of SAMtools and BCFtools",
    "summary": "  Background\n  SAMtools and BCFtools are widely used programs for processing and analysing\nhigh-throughput sequencing data.\n  Findings\n  The first version appeared online twelve years ago and has been maintained\nand further developed ever since, with many new features and improvements added\nover the years. The SAMtools and BCFtools packages represent a unique\ncollection of tools that have been used in numerous other software projects and\ncountless genomic pipelines.\n  Conclusion\n  Both SAMtools and BCFtools are freely available on GitHub under the\npermissive MIT licence, free for both non-commercial and commercial use. Both\npackages have been installed over a million times via Bioconda. The source code\nand documentation are available from http://www.htslib.org.\n"
  },
  {
    "year": 2020,
    "title": "EPIHC: Improving Enhancer-Promoter Interaction Prediction by using\n  Hybrid features and Communicative learning",
    "summary": "  Enhancer-promoter interactions (EPIs) regulate the expression of specific\ngenes in cells, and EPIs are important for understanding gene regulation, cell\ndifferentiation and disease mechanisms. EPI identification through the wet\nexperiments is costly and time-consuming, and computational methods are in\ndemand. In this paper, we propose a deep neural network-based method EPIHC\nbased on sequence-derived features and genomic features for the EPI prediction.\nEPIHC extracts features from enhancer and promoter sequences respectively using\nconvolutional neural networks (CNN), and then design a communicative learning\nmodule to captures the communicative information between enhancer and promoter\nsequences. EPIHC also take the genomic features of enhancers and promoters into\naccount. At last, EPIHC combines sequence-derived features and genomic features\nto predict EPIs. The computational experiments show that EPIHC outperforms the\nexisting state-of-the-art EPI prediction methods on the benchmark datasets and\nchromosome-split datasets, and the study reveal that the communicative learning\nmodule can bring explicit information about EPIs, which is ignore by CNN.\nMoreover, we consider two strategies to improve performances of EPIHC in the\ncross-cell line prediction, and experimental results show that EPIHC\nconstructed on training cell lines exhibit improved performances for the other\ncell lines.\n"
  },
  {
    "year": 2020,
    "title": "Interactive SARS-CoV-2 mutation timemaps",
    "summary": "  As the year 2020 draws to an end, several new strains have been reported for\nthe SARS-CoV-2 coronavirus, the agent responsible for the COVID-19 pandemic\nthat has afflicted us all this past year. However, it is difficult to\ncomprehend the scale, in sequence space, geographical location and time, at\nwhich SARS-CoV-2 mutates and evolves in its human hosts. To get an appreciation\nfor the rapid evolution of the coronavirus, we built interactive scalable\nvector graphics maps that show daily nucleotide variations in genomes from the\nsix most populated continents compared to that of the initial, ground-zero\nSARS-CoV-2 isolate sequenced at the beginning of the year. Availability:\nMutation time maps are available from https://bcgsc.github.io/SARS2/\n"
  },
  {
    "year": 2021,
    "title": "Evolution of default genetic control mechanisms",
    "summary": "  We present a model of the evolution of control systems in a genome under\nenvironmental constraints. The model conceptually follows the Jacob and Monod\nmodel of gene control. Genes contain control elements which respond to the\ninternal state of the cell as well as the environment to control expression of\na coding region. Control and coding regions evolve to maximize a fitness\nfunction between expressed coding sequences and the environment. 118 runs of\nthe model run to an average of 1.4 x 10^6 `generations' each with a range of\nstarting parameters probed the conditions under which genomes evolved a\n`default style' of control. Unexpectedly, the control logic that evolved was\nnot significantly correlated to the complexity of the environment. Genetic\nlogic was strongly correlated with genome complexity and with the fraction of\ngenes active in the cell at any one time. More complex genomes correlated with\nthe evolution of genetic controls in which genes were active (`default on'),\nand a low fraction of genes being expressed correlated with a genetic logic in\nwhich genes were biased to being inactive unless positively activated (`default\noff' logic). We discuss how this might relate to the evolution of the complex\neukaryotic genome, which operates in a `default off' mode.\n"
  },
  {
    "year": 2021,
    "title": "Partition Quantitative Assessment (PQA): A quantitative methodology to\n  assess the embedded noise in clustered omics and systems biology data",
    "summary": "  Identifying groups that share common features among datasets through\nclustering analysis is a typical problem in many fields of science,\nparticularly in post-omics and systems biology research. In respect of this,\nquantifying how a measure can cluster or organize intrinsic groups is important\nsince currently there is no statistical evaluation of how ordered is, or how\nmuch noise is embedded in the resulting clustered vector. Many of the\nliterature focuses on how well the clustering algorithm orders the data, with\nseveral measures regarding external and internal statistical measures; but none\nmeasure has been developed to statistically quantify the noise in an arranged\nvector posterior a clustering algorithm, i.e., how much of the clustering is\ndue to randomness. Here, we present a quantitative methodology, based on\nautocorrelation, to assess this problem.\n"
  },
  {
    "year": 2021,
    "title": "Genes predisposing to syndromic and nonsyndromic infertility: a\n  narrative review",
    "summary": "  Background: Advanced biological techniques have helped produce more\ninsightful findings on the genetic etiology of infertility that may lead to\nbetter management of the condition. This review provides an update on genes\npredisposing to syndromic and nonsyndromic infertility.\n  Main body: The review identified 65 genes linked with infertility and\ninfertility-related disorders. These genes regulate fertility. However,\nmutational loss of the functions of the genes predisposes to infertility.\nTwenty-three (23) genes representing 35% were linked with syndromic\ninfertility, while 42 genes (65%) cause nonsyndromic infertility. Of the 42\nnonsyndromic genes, 26 predispose to spermatogenic failure and sperm\nmorphological abnormalities, 11 cause ovarian failures, and 5 cause sex\nreversal and puberty delay. Overall, 31 genes (48%) predispose to male\ninfertility, 15 genes (23%) cause female infertility, and 19 genes (29%)\npredispose to both. The common feature of male infertility was spermatogenic\nfailure and sperm morphology abnormalities, while ovarian failure has been the\nmost frequently reported among infertile females. The mechanisms leading to\nthese pathologies are gene-specific, which, if targeted in the affected, may\nlead to improved treatment.\n  Conclusions: Mutational loss of the functions of some genes involved in the\ndevelopment and maintenance of fertility may predispose to syndromic or\nnonsyndromic infertility via gene-specific mechanisms. A treatment procedure\nthat targets the affected gene(s) in individuals expressing infertility may\nlead to improved treatment.\n"
  },
  {
    "year": 2021,
    "title": "From Genotype to Phenotype: polygenic prediction of complex human traits",
    "summary": "  Decoding the genome confers the capability to predict characteristics of the\norganism(phenotype) from DNA (genotype). We describe the present status and\nfuture prospects of genomic prediction of complex traits in humans. Some highly\nheritable complex phenotypes such as height and other quantitative traits can\nalready be predicted with reasonable accuracy from DNA alone. For many\ndiseases, including important common conditions such as coronary artery\ndisease, breast cancer, type I and II diabetes, individuals with outlier\npolygenic scores (e.g., top few percent) have been shown to have 5 or even 10\ntimes higher risk than average. Several psychiatric conditions such as\nschizophrenia and autism also fall into this category. We discuss related\ntopics such as the genetic architecture of complex traits, sibling validation\nof polygenic scores, and applications to adult health, in vitro fertilization\n(embryo selection), and genetic engineering.\n"
  },
  {
    "year": 2021,
    "title": "Genes predisposing to type 1 diabetes mellitus and pathophysiology: a\n  narrative review",
    "summary": "  The possibility of targeting the causal genes along with the mechanisms of\npathogenically complex diseases has led to numerous studies on the genetic\netiology of some diseases. In particular, studies have added more genes to the\nlist of type 1 diabetes mellitus (T1DM) suspect genes, necessitating an update\nfor the interest of all stakeholders. Therefore this review articulates T1DM\nsuspect genes and their pathophysiology. Notable electronic databases,\nincluding Medline, Scopus, PubMed, and Google-Scholar were searched for\nrelevant information. The search identified over 73 genes suspected in the\npathogenesis of T1DM, with human leukocyte antigen, insulin gene, and cytotoxic\nT lymphocyte-associated antigen 4 accounting for most of the cases. Mutations\nin these genes, along with environmental factors, may produce a defective\nimmune response in the pancreas, resulting in \\b{eta}-cell autoimmunity,\ninsulin deficiency, and hyperglycemia. The mechanisms leading to these cellular\nreactions are gene-specific and, if targeted in diabetic individuals, may lead\nto improved treatment. Medical practitioners are advised to formulate treatment\nprocedures that target these genes in patients with T1DM.\n"
  },
  {
    "year": 2021,
    "title": "Genetics and Pathophysiology of Maturity-onset Diabetes of the Young\n  (MODY): A Review of Current Trends",
    "summary": "  Single gene mutations have been implicated in the pathogenesis of a form of\ndiabetes mellitus (DM) known as the maturity-onset diabetes of the young\n(MODY). However, there are diverse opinions on the suspect genes and\npathophysiology, necessitating the need to review and communicate the genes to\nraise public awareness. We used the Google search engine to retrieve relevant\ninformation from reputable sources such as PubMed and Google Scholar. We\nidentified 14 classified MODY genes as well as three new and unclassified genes\nlinked with MODY. These genes are fundamentally embedded in the beta cells, the\nmost common of which are HNF1A, HNF4A, HNF1B, and GCK genes. Mutations in these\ngenes cause beta-cell dysfunction, resulting in decreased insulin production\nand hyperglycemia. MODY genes have distinct mechanisms of action and phenotypic\npresentations compared with type 1 and type 2 DM and other forms of DM.\nHealthcare professionals are therefore advised to formulate drugs and treatment\nbased on the causal genes rather than the current generalized treatment for all\ntypes of DM. This will increase the effectiveness of diabetes drugs and\ntreatment and reduce the burden of the disease.\n"
  },
  {
    "year": 2021,
    "title": "Update on the genetic and epigenetic etiology of gestational diabetes\n  mellitus: a review",
    "summary": "  Background: Many studies have been conducted on the genetic and epigenetic\netiology of gestational diabetes mellitus (GDM) in the last two decades because\nof the diseases increasing prevalence and role in the global diabetes mellitus\n(DM) explosion. An update on the genetic and epigenetic etiology of GDM then\nbecomes imperative to better understand and stem the rising incidence of the\ndisease. This review, therefore, articulated GDM candidate genes and their\npathophysiology for the awareness of stakeholders. Main body (genetic and\nepigenetic etiology, GDM): The search discovered 83 GDM candidate genes, of\nwhich TCF7L2, MTNR1B, CDKAL1, IRS1, and KCNQ1 are the most prevalent. Certain\npolymorphisms of these genes can modulate beta-cell dysfunction, adiposity,\nobesity, and insulin resistance through several mechanisms. Environmental\ntriggers such as diets, pollutants, and microbes may also cause epigenetic\nchanges in these genes, resulting in a loss of insulin-boosting and glucose\nmetabolism functions. Early detection and adequate management may resolve the\ncondition after delivery; otherwise, it will progress to maternal type 2\ndiabetes mellitus (T2DM) and fetal configuration to future obesity and DM. This\nshows that GDM is a strong risk factor for T2DM and, in rare cases, type 1\ndiabetes mellitus (T1DM) and maturity-onset diabetes of the young (MODY). This\nfurther shows that GDM significantly contributes to the rising incidence and\nburden of DM worldwide and its prevention may reverse the trend. Conclusion:\nMutations and epigenetic changes in certain genes are strong risk factors for\nGDM. For affected individuals with such etiologies, medical practitioners\nshould formulate drugs and treatment procedures that target these genes and\ntheir pathophysiology.\n"
  },
  {
    "year": 2021,
    "title": "Protocol for Executing and Benchmarking Eight Computational\n  Doublet-Detection Methods in Single-Cell RNA Sequencing Data Analysis",
    "summary": "  The existence of doublets is a key confounder in single-cell RNA sequencing\n(scRNA-seq) data analysis. Computational methods have been developed for\ndetecting doublets from scRNA-seq data. We developed an R package\nDoubletCollection to integrate the installation and execution of eight\ndoublet-detection methods. DoubletCollection also provides a unified interface\nto perform and visualize downstream analysis after doublet detection. Here, we\npresent a protocol of using DoubletCollection to benchmark doublet-detection\nmethods. This protocol can automatically accommodate new doublet-detection\nmethods in the fast-growing scRNA-seq field.\n"
  },
  {
    "year": 2021,
    "title": "NoisET: Noise learning and Expansion detection of T-cell receptors",
    "summary": "  High-throughput sequencing of T- and B-cell receptors makes it possible to\ntrack immune repertoires across time, in different tissues, in acute and\nchronic diseases and in healthy individuals. However quantitative comparison\nbetween repertoires is confounded by variability in the read count of each\nreceptor clonotype due to sampling, library preparation, and expression noise.\nWe review methods for accounting for both biological and experimental noise and\npresent an easy-to-use python package NoisET that implements and generalizes a\npreviously developed Bayesian method. It can be used to learn experimental\nnoise models for repertoire sequencing from replicates, and to detect\nresponding clones following a stimulus. We test the package on different\nrepertoire sequencing technologies and datasets. We review how such approaches\nhave been used to identify responding clonotypes in vaccination and disease\ndata. Availability: NoisET is freely available to use with source code at\ngithub.com/statbiophys/NoisET.\n"
  },
  {
    "year": 2021,
    "title": "Performance Evaluation of Transcriptomics Data Normalization for\n  Survival Risk Prediction",
    "summary": "  One pivotal feature of transcriptomics data is the unwanted variations caused\nby disparate experimental handling, known as handling effects. Various data\nnormalization methods were developed to alleviate the adverse impact of\nhandling effects in the setting of differential expression analysis. However,\nlittle research has been done to evaluate their performance in the setting of\nsurvival outcome prediction, an important analysis goal for transcriptomics\ndata in biomedical research. Leveraging a unique pair of datasets for the same\nset of tumor samples-one with handling effects and the other without, we\ndeveloped a benchmarking tool for conducting such an evaluation in microRNA\nmicroarrays. We applied this tool to evaluate the performance of three popular\nnormalization methods-quantile normalization, median normalization, and\nvariance stabilizing normalization-in survival prediction using various\napproaches for model building and designs for sample assignment. We showed that\nhandling effects can have a strong impact on survival prediction, and that\nquantile normalization, a most popular method in current practice, tends to\nunderperform median normalization and variance stabilizing normalization. We\ndemonstrated with a small example the reason for quantile normalization's poor\nperformance in this setting. Our finding highlights the importance of putting\nnormalization evaluation in the context of the downstream analysis setting and\nthe potential of improving the development of survival predictors by applying\nmedian normalization. We make available our benchmarking tool for performing\nsuch evaluation on additional normalization methods in connection with\nprediction modeling approaches.\n"
  },
  {
    "year": 2021,
    "title": "Modern tools for annotation of small genomes of non-model eukaryotes",
    "summary": "  Nowadays, due to the increasing amount of experimental data obtained by\nsequencing, the most interest is focused on determining the functions and\ncharacteristics of its individual parts of the genome instead of determining\nthe nucleotide sequence of the genome. The genome annotation includes the\nidentification of coding and non-coding sequences, determining the structure of\nthe gene and determining the functions of these sequences. Despite the\nsignificant achievements in computational technologies working with sequencing\ndata, there is no general approach to the functional annotation of the genome\nin the reason of the large number of unresolved molecular determination of the\nfunction of some genomes parts. Nevertheless, the scientific community is\ntrying to solve this problem. This review analyzed existing approaches to\neukaryotic genome annotation. This work includes 3 main parts: introduction,\nmain body and discussion. The introduction reflects the development of\nindependent tools and automatic pipelines for annotation of eukaryotic genomes,\nwhich are associated with existing achievements in annotating prokaryotic ones.\nThe main body consists of two distinguished parts, the first one is devoted to\ninstructions for annotating genomes of non-model eukaryotes, and the second\nblock is about recent versions of automatic pipelines that require minimal\nuser's curation. The question of assessing the quality and completeness of the\nannotated genome is noted briefly, and the tools to conduct this analysis are\ndiscussed. Currently, there is no universal automatic software for eukaryotic\ngenome annotation, covering the whole list of tasks, without manual curation or\nusing additional external tools and resources. Thus it leads to the task of\ndeveloping a wider functional and universal protocol for automatic annotation\nof small eukaryotic genomes.\n"
  },
  {
    "year": 2021,
    "title": "Polygenic Risk Score in Africa Population: Progress and challenges",
    "summary": "  Polygenic risk score (PRS) analysis is a powerful method been used to\nestimate an individual's genetic risk towards targeted traits. PRS analysis\ncould be used to obtain evidence of a genetic effect beyond Genome-Wide\nAssociation Studies (GWAS) results i.e. when there are no significant markers.\nPRS analysis has been widely applied to investigate the genetic basis of\nseveral traits including rare diseases. However, the accuracy of PRS analysis\ndepends on the genomic data of the underlying population. For instance, several\nstudies showed that obtaining higher prediction power of PRS analysis is\nchallenging for non-Europeans. In this manuscript, we reviewed the conventional\nPRS methods and their application to sub-saharan Africa communities. We\nconcluded that the limiting factor of applying PRS analysis to sub-saharan\npopulations is the lack of sufficient GWAS data. Also, we recommended\ndeveloping African-specific PRS tools\n"
  },
  {
    "year": 2021,
    "title": "LRez: C++ API and toolkit for analyzing and managing Linked-Reads data",
    "summary": "  Linked-Reads technologies, such as 10x Genomics, combine both the\nhigh-quality and low cost of short-reads sequencing and a long-range\ninformation, through the use of barcodes able to tag reads which originate from\na common long DNA fragment. This technology has been employed in a broad range\nof applications including assembly or phasing of genomes, and structural\nvariant calling. However, to date, no tool or API dedicated to the manipulation\nof Linked-Reads data exist. We introduce LRez, a C++ API and toolkit which\nallows easy management of Linked-Reads data. LRez includes various\nfunctionalities, for computing number of common barcodes between genomic\nregions, extracting barcodes from BAM files, as well as indexing and querying\nboth BAM and FASTQ files to quickly fetch reads or alignments sharing one or\nmultiple barcodes. LRez can thus be used in a broad range of applications\nrequiring barcode processing, in order to improve their performances. LRez is\nimplemented in C++, supported on Linux platforms, and available under AGPL-3.0\nLicense at https://github.com/morispi/LRez.\n"
  },
  {
    "year": 2021,
    "title": "COSINE: A Web Server for Clonal and Subclonal Structure Inference and\n  Evolution in Cancer Genomics",
    "summary": "  Cancers evolve from mutation of a single cell with sequential clonal and\nsubclonal expansion of somatic mutation acquisition. Inferring clonal and\nsubclonal structures from bulk or single cell tumor genomic sequencing data has\na huge impact on cancer evolution studies. Clonal state and mutational order\ncan provide detailed insight into tumor origin and its future development. In\nthe past decade, a variety of methods have been developed for subclonal\nreconstruction using bulk tumor sequencing data. As these methods have been\ndeveloped in different programming languages and using different input data\nformats, their use and comparison can be problematic. Therefore, we established\na web server for clonal and subclonal structure inference and evolution of\ncancer genomic data (COSINE), which included 12 popular subclonal\nreconstruction methods. We decomposed each method via a detailed workflow of\nsingle processing steps with a user-friendly interface. To the best of our\nknowledge, this is the first web server providing online subclonal inference,\nincluding the most popular subclonal reconstruction methods. COSINE is freely\naccessible at www.clab-cosine.net or http://bio.rj.run:48996/cun-web.\n"
  },
  {
    "year": 2021,
    "title": "grenepipe: A flexible, scalable, and reproducible pipeline to automate\n  variant and frequency calling from sequence reads",
    "summary": "  Processing high-throughput DNA sequencing data of individuals or populations\nrequires stringing together independent software tools with many parameters,\noften leading to non-reproducible pipelines and datasets. We developed\ngrenepipe to streamline this data processing, an all-in-one Snakemake workflow\nfrom raw sequencing data to the end product of a table of individuals'\ngenotypes or population frequencies. Our pipeline allows users to select among\na range of popular software tools within a single configuration file,\nautomatically downloads and installs software and dependencies, and runs with\ntwo command calls: to prepare and to run. It is highly optimized for\nscalability in cluster environments and parallel computing, splitting data\ntasks into manageable genomic sections and automatically consolidating the\noutputs. grenepipe is published under the GPL-3 license, and freely available\nat https://github.com/moiexpositoalonsolab/grenepipe.\n"
  },
  {
    "year": 2021,
    "title": "Unexpected novel Merbecovirus discoveries in agricultural sequencing\n  datasets from Wuhan, China",
    "summary": "  In this study we document the unexpected discovery of multiple coronaviruses\nand a BSL-3 pathogen in agricultural cotton and rice sequencing datasets. In\nparticular, we have identified a novel HKU5-related Merbecovirus in a cotton\ndataset sequenced by the Huazhong Agricultural University in 2017. We have also\nfound an infectious clone sequence containing a novel HKU4-related Merbecovirus\nrelated to MERS coronavirus in a rice dataset sequenced by the Huazhong\nAgricultural University in early 2020. Another HKU5-related Merbecovirus, as\nwell as Japanese encephalitis virus, were identified in a cotton dataset\nsequenced by the Huazhong Agricultural University in 2018. An HKU3-related\nBetacoronavirus was found in a Mus musculus sequencing dataset from the Wuhan\nInstitute of Virology in 2017. Finally, a SARS-WIV1-like Betacoronavirus was\nfound in a rice dataset sequenced by the Fujian Agriculture and Forestry\nUniversity in 2017. Using the contaminating reads we have extracted from the\nabove datasets, we were able to assemble complete genomes of two novel\ncoronaviruses which we disclose herein. In light of our findings, we raise\nconcerns about biosafety protocol breaches, as indicated by our discovery of\nmultiple dangerous human pathogens in agricultural sequencing laboratories in\nWuhan and Fouzou City, China.\n"
  },
  {
    "year": 2021,
    "title": "Functional annotation of creeping bentgrass protein sequences based on\n  convolutional neural network",
    "summary": "  Background: Creeping bentgrass (Agrostis soionifera) is a perennial grass of\nGramineae, belonging to cold season turfgrass, but has poor disease resistance.\nUp to now, little is known about the induced systemic resistance (ISR)\nmechanism, especially the relevant functional proteins, which is important to\ndisease resistance of turfgrass. Achieving more information of proteins of\ninfected creeping bentgrass is helpful to understand the ISR mechanism.\nResults: With BDO treatment, creeping bentgrass seedlings were grown, and the\nISR response was induced by infecting Rhizoctonia solani. High-quality protein\nsequences of creeping bentgrass seedlings were obtained. Some of protein\nsequences were functionally annotated according to the database alignment while\na large part of the obtained protein sequences was left non-annotated. To treat\nthe non-annotated sequences, a prediction model based on convolutional neural\nnetwork was established with the dataset from Uniport database in three domains\nto acquire good performance, especially the higher false positive control rate.\nWith established model, the non-annotated protein sequences of creeping\nbentgrass were analyzed to annotate proteins relevant to disease-resistance\nresponse and signal transduction. Conclusions: The prediction model based on\nconvolutional neural network was successfully applied to select good candidates\nof the proteins with functions relevant to the ISR mechanism from the protein\nsequences which cannot be annotated by database alignment. The waste of\nsequence data can be avoided, and research time and labor will be saved in\nfurther research of protein of creeping bentgrass by molecular biology\ntechnology. It also provides reference for other sequence analysis of turfgrass\ndisease-resistance research.\n"
  },
  {
    "year": 2021,
    "title": "Integration of Unpaired Single-cell Chromatin Accessibility and Gene\n  Expression Data via Adversarial Learning",
    "summary": "  Deep learning has empowered analysis for single-cell sequencing data in many\nways and has generated deep understanding about a range of complex cellular\nsystems. As the booming single-cell sequencing technologies brings the surge of\nhigh dimensional data that come from different sources and represent cellular\nsystems with different features, there is an equivalent rise and challenge of\nintegrating single-cell sequence across modalities. Here, we present a novel\nadversarial approach to integrate single-cell chromatin accessibility and gene\nexpression data in a semi-supervised manner. We demonstrate that our method\nsubstantially improves data integration from a simple adversarial domain\nadaption approach, and it also outperforms two state-of-the-art (SOTA) methods.\n"
  },
  {
    "year": 2021,
    "title": "snpQT: flexible, reproducible, and comprehensive quality control and\n  imputation of genomic data",
    "summary": "  Motivation: Quality control of genomic data is an essential but complicated\nmulti-step procedure, often requiring separate installation and expert\nfamiliarity with a combination of disparate bioinformatics tools. Results: To\nprovide an automated solution that retains comprehensive quality checks and\nflexible workflow architecture, we have developed snpQT, a scalable,\nstand-alone software pipeline, offering some 36 discrete quality filters or\ncorrection steps, with plots before-and-after user-modifiable thresholding.\nThis includes build conversion, population stratification against 1,000 Genomes\ndata, population outlier removal, and built-in imputation with its own pre- and\npost- quality controls. Common input formats are used and users need not be\nsuperusers nor have any prior coding experience. A comprehensive online\ntutorial and installation guide is provided through to GWAS\n(https://snpqt.readthedocs.io/en/latest/), introducing snpQT using a synthetic\ndemonstration dataset and a real-world Amyotrophic Lateral Sclerosis SNP-array\ndataset. Availability: snpQT is open source and freely available at\nhttps://github.com/nebfield/snpQT Contact: Vasilopoulou-C@ulster.ac.uk,\nw.duddy@ulster.ac.uk\n"
  },
  {
    "year": 2021,
    "title": "maplet: An extensible R toolbox for modular and reproducible omics\n  pipelines",
    "summary": "  This paper presents maplet, an open-source R package for the creation of\nhighly customizable, fully reproducible statistical pipelines for omics data\nanalysis, with a special focus on metabolomics-based methods. It builds on the\nSummarizedExperiment data structure to create a centralized pipeline framework\nfor storing data, analysis steps, results, and visualizations. maplet's key\ndesign feature is its modularity, which offers several advantages, such as\nensuring code quality through the individual maintenance of functions and\npromoting collaborative development by removing technical barriers to code\ncontribution. With over 90 functions, the package includes a wide range of\nfunctionalities, covering many widely used statistical approaches and data\nvisualization techniques.\n"
  },
  {
    "year": 2021,
    "title": "Accelerating SARS-CoV-2 low frequency variant calling on ultra deep\n  sequencing datasets",
    "summary": "  With recent advances in sequencing technology it has become affordable and\npractical to sequence genomes to very high depth-of-coverage, allowing\nresearchers to discover low-frequency variants in the genome. However, due to\nthe errors in sequencing it is an active area of research to develop algorithms\nthat can separate noise from the true variants. LoFreq is a state of the art\nalgorithm for low-frequency variant detection but has a relatively long runtime\ncompared to other tools. In addition to this, the interface for running in\nparallel could be simplified, allowing for multithreading as well as\ndistributing jobs to a cluster. In this work we describe some specific\ncontributions to LoFreq that remedy these issues.\n"
  },
  {
    "year": 2021,
    "title": "Survival prediction of head and neck squamous cell carcinoma using\n  machine learning models",
    "summary": "  Head and Neck Squamous Cell Carcinoma (HNSCC) is one of cancer type that is\nmost distressing leading to acute pain, effecting speech and primary survival\nfunctions such as swallowing and breathing. The morbidity and mortality of\nHNSCC patients have not significantly improved even tough there has been\nadvancement in surgical and radiotherapy treatments. The high mortality may be\nattributed to the complexity and significant changes in the clinical outcomes.\nTherefore, it is important to increase the accuracy of predicting the outcome\nof cancer survival. Few cancer survival prediction models of HNSCC have been\nproposed so far. In this study, genomic data (whole exome sequencing) are\nintegrated with clinical data to improve the performance of prediction model.\nThe somatic mutations of every patient is processed using Multifractal\nDeterended Fluctuation Analysis (MFDFA) algorithm and the parameter values of\nFractal Dimension (Dq) is included along with clinical data for cancer survival\nprediction. Feature ranking proves that the new engineered feature is one of\nthe important feature in prediction model. In order to improve the performance\nindex of models, hyperparameters were also tuned in all the classifiers\nconsidered. 10-Fold cross validation is implemented and XGBoost (98% AUROC, 94%\nprecision, and 93% recall) proves to be best model classifier followed by\nRandom Forest 93% AUROC, 93% precision, and 93% recall), Support Vector Machine\n(84% AUCROC, 79% precision, and 79% recall) and Logistic Regression (80% AUROC,\n77% precision, and 76% recall).\n"
  },
  {
    "year": 2021,
    "title": "Prospects for Multi-omics in the Microbial Ecology of Water Engineering",
    "summary": "  Advances in high-throughput sequencing technologies and bioinformatics\napproaches over almost the last three decades have substantially increased our\nability to explore microorganisms and their functions-including those that have\nyet to be cultivated in pure isolation. Genome-resolved metagenomic approaches\nhave enabled linking powerful functional predictions to specific taxonomical\ngroups with increasing fidelity. Additionally, whole community gene expression\nsurveys and metabolite profiling have permitted direct surveys of\ncommunity-scale functions in specific environmental settings. These advances\nhave allowed for a shift in microbiome science away from descriptive studies\nand towards mechanistic and predictive frameworks for designing and harnessing\nmicrobial communities for desired beneficial outcomes. Here, we review how\nmodern genome-resolved metagenomic approaches have been applied to a variety of\nwater engineering applications from lab-scale bioreactors to full-scale\nsystems. We describe integrated omics analysis across engineered water systems\nand the foundations for pairing these insights with modeling approaches.\nLastly, we summarize emerging omics-based technologies that we believe will be\npowerful tools for water engineering applications. Overall, we provide a\nframework for microbial ecologists specializing in water engineering to apply\ncutting-edge omics approaches to their research questions to achieve novel\nfunctional insights. Successful adoption of predictive frameworks in engineered\nwater systems could enable more economically and environmentally sustainable\nbioprocesses as demand for water and energy resources increases.\n"
  },
  {
    "year": 2021,
    "title": "High rate of SARS-CoV2 nonsense spike genomes coding for prematurely\n  truncated proteins",
    "summary": "  Replication of SARS-CoV2 virions is an error-prone process which may\neventually generate a percentage of impaired protein copies with complete lack\nof functionality. For instance, after RNA mis-replication, a very premature\nstop codon (UAG, UAA, UGA) coding for a prematurely truncated\n(nonsense-mutated) spike protein may occur.\n  In the natural virus replication process via cell infection, the nonsense\ngenomes are corrected by the proofreading enzymes of the virus, strongly\npenalized by natural selection and condemned to a very short life by the host\ncell's mRNA watching mechanisms. However, for the very long spike genome of\n1273 codons, a truncated non-functional spike protein may potentially still\noccur with a high frequency, even in presence of a low mutation rate per single\nnucleotide.\n  With this paper, a hi-fidelity post-processing of SARS-CoV2 spike sequences\nis provided: in ex-vivo samples from patients, an impressively high rate of\n26\\% of prematurely-stopped (nonsense-mutated) spike genomes sequences due to\ninsertions/deletions is found, compared with a 9.7\\% obtained from in-vitro\ncell culture.\n  A general warning on the possible high rate of prematurely-stopped spike\nprotein sequences is also raised for \"artificial\" de novo DNA synthesis\nprocesses of SARS-CoV2 spike genomes with no associated natural\nproofreading/selection, possibly including vaccine preparations.\n  Finally, a metric based on the ratio between prematurely stopped and \"normal\"\ngenomes is proposed as a potential host-independent variant-watching tool, able\nto classify the infectivity of new spike mutations.\n"
  },
  {
    "year": 2021,
    "title": "A Multi-task Deep Feature Selection Method for Brain Imaging Genetics",
    "summary": "  Using brain imaging quantitative traits (QTs) to identify the genetic risk\nfactors is an important research topic in imaging genetics. Many efforts have\nbeen made via building linear models, e.g. linear regression (LR), to extract\nthe association between imaging QTs and genetic factors such as single\nnucleotide polymorphisms (SNPs). However, to the best of our knowledge, these\nlinear models could not fully uncover the complicated relationship due to the\nloci's elusive and diverse impacts on imaging QTs. Though deep learning models\ncan extract the nonlinear relationship, they could not select relevant genetic\nfactors. In this paper, we proposed a novel multi-task deep feature selection\n(MTDFS) method for brain imaging genetics. MTDFS first adds a multi-task\none-to-one layer and imposes a hybrid sparsity-inducing penalty to select\nrelevant SNPs making significant contributions to abnormal imaging QTs. It then\nbuilds a multi-task deep neural network to model the complicated associations\nbetween imaging QTs and SNPs. MTDFS can not only extract the nonlinear\nrelationship but also arms the deep neural network with the feature selection\ncapability. We compared MTDFS to both LR and single-task DFS (DFS) methods on\nthe real neuroimaging genetic data. The experimental results showed that MTDFS\nperformed better than both LR and DFS in terms of the QT-SNP relationship\nidentification and feature selection. In a word, MTDFS is powerful for\nidentifying risk loci and could be a great supplement to the method library for\nbrain imaging genetics.\n"
  },
  {
    "year": 2021,
    "title": "Sramm: short read alignment mapping metrics",
    "summary": "  Short Read Alignment Mapping Metrics (SRAMM): is an efficient and versatile\ncommand line tool providing additional short read mapping metrics, filtering,\nand graphs. Short read aligners report MAPing Quality (MAPQ), but these methods\ngenerally are neither standardized nor well described in literature or software\nmanuals. Additionally, third party mapping quality programs are typically\ncomputationally intensive or designed for specific applications. SRAMM\nefficiently generates multiple different concept-based mapping scores to\nprovide for an informative post alignment examination and filtering process of\naligned short reads for various downstream applications. SRAMM is compatible\nwith Python 2.6+ and Python 3.6+ on all operating systems. It works with any\nshort read aligner that generates SAM/BAM/CRAM file outputs and reports 'AS'\ntags. It is freely available under the MIT license at\nhttp://github.com/achon/sramm.\n"
  },
  {
    "year": 2021,
    "title": "Stool Studies Don't Pass the Sniff Test: A Systematic Review of Human\n  Gut Microbiome Research Suggests Widespread Misuse of Machine Learning",
    "summary": "  In the machine learning culture, an independent test set is required for\nproper model verification. Failures in model verification, including test set\nomission and test set leakage, make it impossible to know whether or not a\ntrained model is fit for purpose. In this article, we present a systematic\nreview and quantitative analysis of human gut microbiome classification\nstudies, conducted to measure the frequency and impact of test set omission and\ntest set leakage on area under the receiver operating curve (AUC) reporting.\nAmong 102 articles included for analysis, we find that only 12% of studies\nreport a bona fide test set AUC, meaning that the published AUCs for 88% of\nstudies cannot be trusted at face value. Our findings cast serious doubt on the\ngeneral validity of research claiming that the gut microbiome has high\ndiagnostic or prognostic potential in human disease.\n"
  },
  {
    "year": 2021,
    "title": "Variant-driven multi-wave pattern of COVID-19 via a Machine Learning\n  analysis of spike protein mutations",
    "summary": "  Applying a ML approach to the temporal variability of the Spike protein\nsequence enables us to identify, classify and track emerging virus variants.\nOur analysis is unbiased, in the sense that it does not require any prior\nknowledge of the variant characteristics, and our results are validated by\nother informed methods that define variants based on the complete genome.\nFurthermore, correlating persistent variants of our approach to epidemiological\ndata, we discover that each new wave of the COVID-19 pandemic is driven and\ndominated by a new emerging variant. Our results are therefore indispensable\nfor further studies on the evolution of SARS-CoV-2 and the prediction of\nevolutionary patterns that determine current and future mutations of the Spike\nproteins, as well as their diversification and persistence during the viral\nspread. Moreover, our ML algorithm works as an efficient early warning system\nfor the emergence of new persistent variants that may pose a threat of\ntriggering a new wave of COVID-19. Capable of a timely identification of\npotential new epidemiological threats when the variant only represents 1% of\nthe new sequences, our ML strategy is a crucial tool for decision makers to\ndefine short and long term strategies to curb future outbreaks. The same\nmethodology can be applied to other viral diseases, influenza included, if\nsufficient sequencing data is available.\n"
  },
  {
    "year": 2021,
    "title": "Variant interpretation using population databases: lessons from gnomAD",
    "summary": "  Reference population databases are an essential tool in variant and gene\ninterpretation. Their use guides the identification of pathogenic variants\namidst the sea of benign variation present in every human genome, and supports\nthe discovery of new disease-gene relationships. The Genome Aggregation\nDatabase (gnomAD) is currently the largest and most widely used publicly\navailable collection of population variation from harmonized sequencing data.\nThe data is available through the online gnomAD browser\n(https://gnomad.broadinstitute.org/) that enables rapid and intuitive variant\nanalysis. This review provides guidance on the content of the gnomAD browser,\nand its usage for variant and gene interpretation. We introduce key features\nincluding allele frequency, per-base expression levels, constraint scores, and\nvariant co-occurrence, alongside guidance on how to use these in analysis, with\na focus on the interpretation of candidate variants and novel genes in rare\ndisease.\n"
  },
  {
    "year": 2021,
    "title": "Bam-readcount -- rapid generation of basepair-resolution sequence\n  metrics",
    "summary": "  Bam-readcount is a utility for generating low-level information about\nsequencing data at specific nucleotide positions. Originally designed to help\nfilter genomic mutation calls, the metrics it outputs are useful as input for\nvariant detection tools and for resolving ambiguity between variant callers .\nIn addition, it has found broad applicability in diverse fields including tumor\nevolution, single-cell genomics, climate change ecology, and tracking community\nspread of SARS-CoV-2. Here we report on the release of version 1.0 of this\ntool, which adds CRAM support, among other improvements. It is released under a\npermissive MIT license and available at\nhttps://github.com/genome/bam-readcount.\n"
  },
  {
    "year": 2021,
    "title": "OncoEnrichR: cancer-dedicated gene set interpretation",
    "summary": "  Genome-scale screening experiments in cancer produce long lists of candidate\ngenes that require extensive interpretation for biological insight and\nprioritization for follow-up studies. Interrogation of gene lists frequently\nrepresents a significant and time-consuming undertaking, in which experimental\nbiologists typically combine results from a variety of bioinformatics resources\nin an attempt to portray and understand cancer relevance. As a means to\nsimplify and strengthen the support for this endeavor, we have developed\noncoEnrichR, a flexible bioinformatics tool that allows cancer researchers to\ncomprehensively interrogate a given gene list along multiple facets of cancer\nrelevance. oncoEnrichR differs from general gene set analysis frameworks\nthrough the integration of an extensive set of prior knowledge specifically\nrelevant for cancer, including ranked gene-tumor type associations,\nliterature-supported proto-oncogene and tumor suppressor gene annotations,\ntarget druggability data, regulatory interactions, synthetic lethality\npredictions, as well as prognostic associations, gene aberrations, and\nco-expression patterns across tumor types. The software produces a structured\nand user-friendly analysis report as its main output, where versions of all\nunderlying data resources are explicitly logged, the latter being a critical\ncomponent for reproducible science. We demonstrate the usefulness of\noncoEnrichR through interrogation of two candidate lists from proteomic and\nCRISPR screens. oncoEnrichR is freely available as a web-based workflow hosted\nby the Galaxy platform (https://oncotools.elixir.no), and can also be accessed\nas a stand-alone R package (https://github.com/sigven/oncoEnrichR).\n"
  },
  {
    "year": 2021,
    "title": "New strategies to improve minimap2 alignment accuracy",
    "summary": "  Summary: We present several recent improvements to minimap2, a versatile\npairwise aligner for nucleotide sequences. Now minimap2 v2.22 can more\naccurately map long reads to highly repetitive regions and align through\ninsertions or deletions up to 100kb by default, addressing major weakness in\nminimap2 v2.18 or earlier.\n  Availability and implementation: https://github.com/lh3/minimap2\n"
  },
  {
    "year": 2021,
    "title": "SquiggleFilter: An Accelerator for Portable Virus Detection",
    "summary": "  The MinION is a recent-to-market handheld nanopore sequencer. It can be used\nto determine the whole genome of a target virus in a biological sample. Its\nRead Until feature allows us to skip sequencing a majority of non-target reads\n(DNA/RNA fragments), which constitutes more than 99% of all reads in a typical\nsample. However, it does not have any on-board computing, which significantly\nlimits its portability.\n  We analyze the performance of a Read Until metagenomic pipeline for detecting\ntarget viruses and identifying strain-specific mutations. We find new sources\nof performance bottlenecks (basecaller in classification of a read) that are\nnot addressed by past genomics accelerators.\n  We present SquiggleFilter, a novel hardware accelerated dynamic time warping\n(DTW) based filter that directly analyzes MinION's raw squiggles and filters\neverything except target viral reads, thereby avoiding the expensive\nbasecalling step. We show that our 14.3W 13.25mm2 accelerator has 274X greater\nthroughput and 3481X lower latency than existing GPU-based solutions while\nconsuming half the power, enabling Read Until for the next generation of\nnanopore sequencers.\n"
  },
  {
    "year": 2021,
    "title": "Analysis of pangolin metagenomic datasets reveals significant\n  contamination, raising concerns for pangolin CoV host attribution",
    "summary": "  Metagenomic datasets from pangolin tissue specimens have previously yielded\nSARS-related coronaviruses which show high homology in their receptor binding\ndomain to SARS-CoV-2, suggesting a potential zoonotic source for this feature\nof the human virus, possibly via recombination (Liu et al. 2019, Lam et al.\n2020, Xiao et al. 2020, Liu et al. 2020). Here we re-examine these published\ndatasets. We report that only a few pangolin samples were found to contain\ncoronavirus reads, and even then in low abundance, while other non-pangolin\nhosted viruses were present in higher abundance. We also discovered extensive\ncontamination with human, rodent, and other mammalian gene sequences, which was\na surprising finding. Furthermore, we uncovered a number of pangolin CoV\nsequences embedded in standard laboratory cloning vectors, which suggests the\npangolin specimens could have been contaminated with sequences derived from\nsynthetic biology experiments. Finally, we discover a third pangolin dataset\n(He et al. 2022) with low levels of SARSr-CoV sequences and unambiguous\nextensive contamination of several pangolin samples. For these reasons, we find\nit unlikely that the pangolins in question had a coronavirus infection while\nalive, and all current versions of the cited papers claiming a zoonotic\ninfection of pangolins with a SARS-r CoV require substantial corrections and\nshould be retracted until such corrections are made.\n"
  },
  {
    "year": 2021,
    "title": "A review of computational tools for generating metagenome-assembled\n  genomes from metagenomic sequencing data",
    "summary": "  Microbes are essentially yet convolutedly linked with human lives on the\nearth. They critically interfere in different physiological processes and thus\ninfluence overall health status. Studying microbial species is used to be\nconstrained to those that can be cultured in the lab. But it excluded a huge\nportion of the microbiome that could not survive on lab conditions. In the past\nfew years, the culture-independent metagenomic sequencing enabled us to explore\nthe complex microbial community coexisting within and on us. Metagenomics has\nequipped us with new avenues of investigating the microbiome, from studying a\nsingle species to a complex community in a dynamic ecosystem. Thus, identifying\nthe involved microbes and their genomes becomes one of the core tasks in\nmetagenomic sequencing. Metagenome-assembled genomes are groups of contigs with\nsimilar sequence characteristics from de novo assembly and could represent the\nmicrobial genomes from metagenomic sequencing. In this paper, we reviewed a\nspectrum of tools for producing and annotating metagenome-assembled genomes\nfrom metagenomic sequencing data and discussed their technical and biological\nperspectives.\n"
  },
  {
    "year": 2021,
    "title": "Robust haplotype-resolved assembly of diploid individuals without\n  parental data",
    "summary": "  Routine single-sample haplotype-resolved assembly remains an unresolved\nproblem. Here we describe a new algorithm that combines PacBio HiFi reads and\nHi-C chromatin interaction data to produce a haplotype-resolved assembly\nwithout the sequencing of parents. Applied to human and other vertebrate\nsamples, our algorithm consistently outperforms existing single-sample assembly\npipelines and generates assemblies of comparable quality to the best\npedigree-based assemblies.\n"
  },
  {
    "year": 2021,
    "title": "Deep learning tackles single-cell analysis A survey of deep learning for\n  scRNA-seq analysis",
    "summary": "  Since its selection as the method of the year in 2013, single-cell\ntechnologies have become mature enough to provide answers to complex research\nquestions. With the growth of single-cell profiling technologies, there has\nalso been a significant increase in data collected from single-cell profilings,\nresulting in computational challenges to process these massive and complicated\ndatasets. To address these challenges, deep learning (DL) is positioning as a\ncompetitive alternative for single-cell analyses besides the traditional\nmachine learning approaches. Here we present a processing pipeline of\nsingle-cell RNA-seq data, survey a total of 25 DL algorithms and their\napplicability for a specific step in the processing pipeline. Specifically, we\nestablish a unified mathematical representation of all variational autoencoder,\nautoencoder, and generative adversarial network models, compare the training\nstrategies and loss functions for these models, and relate the loss functions\nof these models to specific objectives of the data processing step. Such\npresentation will allow readers to choose suitable algorithms for their\nparticular objective at each step in the pipeline. We envision that this survey\nwill serve as an important information portal for learning the application of\nDL for scRNA-seq analysis and inspire innovative use of DL to address a broader\nrange of new challenges in emerging multi-omics and spatial single-cell\nsequencing.\n"
  },
  {
    "year": 2021,
    "title": "Metagenome assembly of high-fidelity long reads with hifiasm-meta",
    "summary": "  Current metagenome assemblers developed for short sequence reads or noisy\nlong readswere not optimized for accurate long reads. Here we describe\nhifiasm-meta, a new metagenome assembler that exploits the high accuracy of\nrecent data. Evaluated on seven empirical datasets, hifiasm-meta reconstructed\ntens to hundreds of complete circular bacterial genomes per dataset,\nconsistently outperforming other metagenome assemblers.\n"
  },
  {
    "year": 2021,
    "title": "An optimized protocol for single cell transcriptional profiling by\n  combinatorial indexing",
    "summary": "  Single cell combinatorial indexing RNA sequencing (sci-RNA-seq) is a powerful\nmethod for recovering gene expression data from an exponentially scalable\nnumber of individual cells or nuclei. However, sci-RNA-seq is a complex\nprotocol that has historically exhibited variable performance on different\ntissues, as well as lower sensitivity than alternative methods. Here we report\na simplified, optimized version of the three-level sci-RNA-seq protocol that is\nfaster, higher yield, more robust, and more sensitive, than the original\nsci-RNA-seq3 protocol, with reagent costs on the order of 1 cent per cell or\nless. We showcase the optimized protocol via whole organism analysis of an\nE16.5 mouse embryo, profiling ~380,000 nuclei in a single experiment. Finally,\nwe introduce a \"tiny sci-*\" protocol for experiments where input is extremely\nlimited.\n"
  },
  {
    "year": 2021,
    "title": "SARS-CoV-2's closest relative, RaTG13, was generated from a bat\n  transcriptome not a fecal swab: implications for the origin of COVID-19",
    "summary": "  RaTG13 is the closest related coronavirus genome phylogenetically to\nSARS-CoV-2, consequently understanding its provenance is of key importance to\nunderstanding the origin of the COVID-19 pandemic. The RaTG13 NGS dataset is\nattributed to a fecal swab from the intermediate horseshoe bat Rhinolophus\naffinis. However, sequence analysis reveals that this is unlikely. Metagenomic\nanalysis using Metaxa2 shows that only 10.3 % of small subunit (SSU) rRNA\nsequences in the dataset are bacterial, inconsistent with a fecal sample, which\nare typically dominated by bacterial sequences. In addition, the bacterial taxa\npresent in the sample are inconsistent with fecal material. Assembly of\nmitochondrial SSU rRNA sequences in the dataset produces a contig 98.7 %\nidentical to R.affinis mitochondrial SSU rRNA, indicating that the sample was\ngenerated from this or a closely related species. 87.5 % of the NGS reads map\nto the Rhinolophus ferrumequinum genome, the closest bat genome to R.affinis\navailable. In the annotated genome assembly, 62.2 % of mapped reads map to\nprotein coding genes. These results clearly demonstrate that the dataset\nrepresents a Rhinolophus sp. transcriptome, and not a fecal swab sample.\nOverall, the data show that the RaTG13 dataset was generated by the Wuhan\nInstitute of Virology (WIV) from a transcriptome derived from Rhinolophus sp.\ntissue or cell line, indicating that RaTG13 was in live culture. This raises\nthe question of whether the WIV was culturing additional unreported\ncoronaviruses closely related to SARS-CoV-2 prior to the pandemic. The\nimplications for the origin of the COVID-19 pandemic are discussed.\n"
  },
  {
    "year": 2021,
    "title": "CRISPR SWAPnDROP -- A multifunctional system for genome editing and\n  large-scale interspecies gene transfer",
    "summary": "  The need for diverse chromosomal modifications in biotechnology, synthetic\nbiology and basic research requires the development of new technologies. With\nCRISPR SWAPnDROP, we extend the limits of genome editing to large-scale in-vivo\nDNA transfer between bacterial species. Its modular platform approach\nfacilitates species specific adaptation to confer genome editing in various\nspecies. In this study, we show the implementation of the CRISPR SWAPnDROP\nconcept for the model organism Escherichia coli and the currently fastest\ngrowing and biotechnologically relevant organism Vibrio natriegens. We\ndemonstrate the excision, transfer and integration of 151kb chromosomal DNA\nbetween E. coli strains and from E. coli to V. natriegens without size-limiting\nintermediate DNA extraction. With the transfer of the E. coli MG1655 wild type\nlac operon, we establish a functional lactose and galactose degradation pathway\nin V. natriegens to extend its biotechnological spectrum. We also transfer the\nE. coli DH5alpha lac operon and make V. natriegens capable of\nalpha-complementation - a step towards an ultra-fast cloning strain.\nFurthermore, CRISPR SWAPnDROP is designed to be the swiss army knife of genome\nengineering. Its spectrum of application comprises scarless, marker-free,\niterative and parallel insertions and deletions, genome rearrangements, as well\nas gene transfer between strains and across species. The modular character\nfacilitates DNA library applications and the recycling of standardized parts.\nIts novel multi-color scarless co-selection system significantly improves\nediting efficiency to 92% for single edits and 83% for quadruple edits and\nprovides visual quality controls throughout the assembly and editing process.\n"
  },
  {
    "year": 2021,
    "title": "EndHiC: assemble large contigs into chromosomal-level scaffolds using\n  the Hi-C links from contig ends",
    "summary": "  Motivation: The application of PacBio HiFi and ultra-long ONT reads have\nachieved huge progress in the contig-level assembly, but it is still\nchallenging to assemble large contigs into chromosomes with available Hi-C\nscaffolding software, which all compute the contact value between contigs using\nthe Hi-C links from the whole contig regions. As the Hi-C links of two adjacent\ncontigs concentrate only at the neighbor ends of the contigs, larger contig\nsize will reduce the power to differentiate adjacent (signal) and non-adjacent\n(noise) contig linkages, leading to a higher rate of mis-assembly.\n  Results: We present a software package EndHiC, which is suitable to assemble\nlarge contigs (> 1-Mb) into chromosomal-level scaffolds, using Hi-C links from\nonly the contig end regions instead of the whole contig regions. Benefiting\nfrom the increased signal to noise ratio, EndHiC achieves much higher\nscaffolding accuracy compared to existing software LACHESIS, ALLHiC, and\n3D-DNA. Moreover, EndHiC has few parameters, runs 10-1000 times faster than\nexisting software, needs trivial memory, provides robustness evaluation, and\nallows graphic viewing of the scaffold results. The high scaffolding accuracy\nand user-friendly interface of EndHiC, liberate the users from labor-intensive\nmanual checks and revision works.\n  Availability and implementation: EndHiC is written in Perl, and is freely\navailable at https://github.com/fanagislab/EndHiC. Contact: fanwei@caas.cn and\nmilrazhang@163.com Supplementary information: Supplementary data are available\nat Bioinformatics online.\n"
  },
  {
    "year": 2021,
    "title": "Classification of genetic variants using machine learning",
    "summary": "  Recent advances in genomic sequencing technology have resulted in an\nabundance of genome sequence data. Despite the progress in interpreting those\ndata, there remains a broad scope for their translation into clinical and\nsocietal benefits. Loss-of-function variations in the human genome can be\ncausal in disease development. Precise identification of such variations and\npathogenicity prediction may lead to better drug targeting, among other\nbenefits. Machine learning comes across as a promising method for its proven\npredictive ability. We have curated a novel dataset for the classification of\nLOF variants using high-quality databases of genetic variation. We trained and\nvalidated seven different classification algorithms using the new dataset to\nclassify the variants as Benign, Pathogenic and Likely pathogenic. We recorded\nthe best overall performance using the XG-Boost algorithm with an F1-score of\n0.88 on the test set. We observed fair performance on Pathogenic samples with\nhigh recall and moderate precision and subpar performance on Likely pathogenic\nclass, albeit with moderate precision. Overall, the encouraging results make\nour final model a promising candidate for further real-world tests.\n"
  },
  {
    "year": 2021,
    "title": "Laotian Bat Sarbecovirus BANAL-236 Uses ACE2 to Infect Cells by an\n  Unknown Mechanism",
    "summary": "  A manuscript identified bat sarbecoviruses with high sequence homology to\nSARS-CoV-2 found in caves in Laos that can directly infect human cells via the\nhuman ACE2 receptor (Coronaviruses with a SARS-CoV-2-like receptor binding\ndomain allowing ACE2-mediated entry into human cells isolated from bats of\nIndochinese peninsula, Temmam S., et al.). Here, I examine the genomic sequence\nof one of these viruses, BANAL-236, and show it has 5-UTR and 3-UTR secondary\nstructures that are non-canonical and, in fact, have never been seen in an\ninfective coronavirus. Specifically, the 5-UTR has a 177 nt copy-back extension\nwhich forms an extended, highly stable duplex RNA structure. Because of this\ncopy-back, the four obligate Stem Loops (SL) -1, -2, -3, and -4 cis-acting\nelements found in all currently known replicating coronaviruses are buried in\nthe extended duplex. The 3-UTR has a similar fold-back duplex of 144 nts and is\nmissing the obligate poly-A tail. Taken together, these findings demonstrate\nBANAL-236 is missing eight obligate UTR cis-acting elements; each one of which\nhas previously been lethal to replication when modified individually. Neither\nduplex copyback has ever been observed in an infective sarbecovirus, although\nsome of the features have been seen in defective interfering particles, which\ncan be found in co-infections with non-defective, replicating viruses. They are\nalso a common error seen during synthetic genome assembly in a laboratory.\nBANAL-236 must have evolved an entirely unique mechanism for replication, RNA\ntranslation, and RNA packaging never seen in a coronavirus and because it is a\nbat sarbecovirus closely related to SARS-CoV-2, it is imperative that we\nunderstand its unique mode of infectivity by a collaborative, international\nresearch effort.\n"
  },
  {
    "year": 2021,
    "title": "Replication of SARS-CoV-2 mutation analysis suggests differences in\n  per-protein mutation characteristics",
    "summary": "  The increasing spread of COVID-19, caused by the virus SARS-CoV-2, raises\nconcerns about the extent to which mutations have occurred across the viral\ngenome. We present a partial replication of an earlier 2021 study by Wang, R.\net al. that determined the presence of four substrains and eleven top mutations\nin the United States. We analyze a portion of the authors' data set in order to\nrecreate Figure S1 from the paper, recapitulating the same features observed in\nthe original figure. We further generate a summary of mutation characteristics\nfor each of the 26 named proteins and confirm the significance of the spike\nprotein at roughly 24% of all recorded mutations. Our analysis suggests that\nadditional factors may affect per-protein mutation rate besides protein length.\n"
  },
  {
    "year": 2021,
    "title": "Learning the statistics and landscape of somatic mutation-induced\n  insertions and deletions in antibodies",
    "summary": "  Affinity maturation is crucial for improving the binding affinity of\nantibodies to antigens. This process is mainly driven by point substitutions\ncaused by somatic hypermutations of the immunoglobulin gene. It also includes\ndeletions and insertions of genomic material known as indels. While the\nlandscape of point substitutions has been extensively studied, a detailed\nstatistical description of indels is still lacking. Here we present a\nprobabilistic inference tool to learn the statistics of indels from repertoire\nsequencing data, which overcomes the pitfalls and biases of standard annotation\nmethods. The model includes antibody-specific maturation ages to account for\nvariable mutational loads in the repertoire. After validation on synthetic\ndata, we applied our tool to a large dataset of human immunoglobulin heavy\nchains. The inferred model allows us to identify universal statistical features\nof indels in heavy chains. We report distinct insertion and deletion hotspots,\nand show that the distribution of lengths of indels follows a geometric\ndistribution, which puts constraints on future mechanistic models of the\nhypermutation process.\n"
  },
  {
    "year": 2021,
    "title": "BLEND: A Fast, Memory-Efficient, and Accurate Mechanism to Find Fuzzy\n  Seed Matches in Genome Analysis",
    "summary": "  Generating the hash values of short subsequences, called seeds, enables\nquickly identifying similarities between genomic sequences by matching seeds\nwith a single lookup of their hash values. However, these hash values can be\nused only for finding exact-matching seeds as the conventional hashing methods\nassign distinct hash values for different seeds, including highly similar\nseeds. Finding only exact-matching seeds causes either 1) increasing the use of\nthe costly sequence alignment or 2) limited sensitivity.\n  We introduce BLEND, the first efficient and accurate mechanism that can\nidentify both exact-matching and highly similar seeds with a single lookup of\ntheir hash values, called fuzzy seed matches. BLEND 1) utilizes a technique\ncalled SimHash, that can generate the same hash value for similar sets, and 2)\nprovides the proper mechanisms for using seeds as sets with the SimHash\ntechnique to find fuzzy seed matches efficiently.\n  We show the benefits of BLEND when used in read overlapping and read mapping.\nFor read overlapping, BLEND is faster by 2.4x - 83.9x (on average 19.3x), has a\nlower memory footprint by 0.9x - 14.1x (on average 3.8x), and finds higher\nquality overlaps leading to accurate de novo assemblies than the\nstate-of-the-art tool, minimap2. For read mapping, BLEND is faster by 0.8x -\n4.1x (on average 1.7x) than minimap2. Source code is available at\nhttps://github.com/CMU-SAFARI/BLEND.\n"
  },
  {
    "year": 2021,
    "title": "Temporal epistasis inference from more than 3,500,000 SARS-CoV-2 Genomic\n  Sequences",
    "summary": "  We use Direct Coupling Analysis (DCA) to determine epistatic interactions\nbetween loci of variability of the SARS-CoV-2 virus, segmenting genomes by\nmonth of sampling. We use full-length, high-quality genomes from the GISAID\nrepository up to October 2021, in total over 3,500,000 genomes. We find that\nDCA terms are more stable over time than correlations, but nevertheless change\nover time as mutations disappear from the global population or reach fixation.\nCorrelations are enriched for phylogenetic effects, and in particularly\nstatistical dependencies at short genomic distances, while DCA brings out links\nat longer genomic distance. We discuss the validity of a DCA analysis under\nthese conditions in terms of a transient Quasi-Linkage Equilibrium state. We\nidentify putative epistatic interaction mutations involving loci in Spike.\n"
  },
  {
    "year": 2021,
    "title": "Cell-in-cell structures are involved in the competition between cells in\n  breast cancer",
    "summary": "  Breast cancer is the most common cancer in women worldwide, and discovering\nthe biomarkers of this disease became so vital nowadays and Cell in Cell\nstructure could be one of them, and it may be used as an available proxy for\ntumor malignancy. (CICs) are unusual in that keep morphologically healthy cells\nwithin another cell. They are found in various human cancers and result from\nactive cell-cell interaction, and it has different kinds. In this study, we\nanalyzed the microarray data from GEO (GSE103865) to genetically evaluate CICs'\nincidence in samples obtained from breast cancer patients to understand the\nrelationship between the rate of CIC and the prognosis of breast cancer. The\npreprocessing was performed using R software. The DAVID website was used to\nanalyze gene ontology (GO) and Gene and Genome (KEGG) pathways. The\nprotein-protein interactions (PPIs) of the obtained DEGs were assessed using\nthe STRING website, and hub modules in Cytoscape and cytoHubba were screened.\nAccording to the results from analyzing the 20 hub genes, we understood that\noverexpression of our Top genes is effective in focal adhesion, ECM-receptor\ninteraction, platelet activation and PI3K-Akt signaling pathway, which shows\nthat changes in these pathways could be the reason the overexpression of CICs\nin breast cancer. These data and research by many others have uncovered various\ngenes involved in CIC formation and have started to give us an idea of why they\nare formed and how they could contribute to breast cancer\n"
  },
  {
    "year": 2022,
    "title": "Accurate identification of bacteriophages from metagenomic data using\n  Transformer",
    "summary": "  Motivation: Bacteriophages are viruses infecting bacteria. Being key players\nin microbial communities, they can regulate the composition/function of\nmicrobiome by infecting their bacterial hosts and mediating gene transfer.\nRecently, metagenomic sequencing, which can sequence all genetic materials from\nvarious microbiome, has become a popular means for new phage discovery.\nHowever, accurate and comprehensive detection of phages from the metagenomic\ndata remains difficult. High diversity/abundance, and limited reference genomes\npose major challenges for recruiting phage fragments from metagenomic data.\nExisting alignment-based or learning-based models have either low recall or\nprecision on metagenomic data. Results: In this work, we adopt the\nstate-of-the-art language model, Transformer, to conduct contextual embedding\nfor phage contigs. By constructing a protein-cluster vocabulary, we can feed\nboth the protein composition and the proteins' positions from each contig into\nthe Transformer. The Transformer can learn the protein organization and\nassociations using the self-attention mechanism and predicts the label for test\ncontigs. We rigorously tested our developed tool named PhaMer on multiple\ndatasets with increasing difficulty, including quality RefSeq genomes, short\ncontigs, simulated metagenomic data, mock metagenomic data, and the public\nIMG/VR dataset. All the experimental results show that PhaMer outperforms the\nstate-of-the-art tools. In the real metagenomic data experiment, PhaMer\nimproves the F1-score of phage detection by 27\\%.\n"
  },
  {
    "year": 2022,
    "title": "On the influence of several factors on pathway enrichment analysis",
    "summary": "  Pathway enrichment analysis has become a widely used knowledge-based approach\nfor the interpretation of biomedical data. Its popularity has led to an\nexplosion of both enrichment methods and pathway databases. While the elegance\nof pathway enrichment lies in its simplicity, multiple factors can impact the\nresults of such an analysis which may not be accounted for. Researchers may\nfail to give influential aspects their due, resorting instead to popular\nmethods and gene set collections, or default settings. Despite ongoing efforts\nto establish set guidelines, meaningful results are still hampered by a lack of\nconsensus or gold standards around how enrichment analysis should be conducted.\nNonetheless, such concerns have prompted a series of benchmark studies\nspecifically focused on evaluating the influence of various factors on pathway\nenrichment results. In this review, we organize and summarize the findings of\nthese benchmarks to provide a comprehensive overview on the influence of these\nfactors. Our work covers a broad spectrum of factors, spanning from\nmethodological assumptions to those related to prior biological knowledge, such\nas pathway definitions and database choice. In doing so, we aim to shed light\non how these aspects can lead to insignificant, uninteresting, or even\ncontradictory results. Finally, we conclude the review by proposing future\nbenchmarks as well as solutions to overcome some of the challenges which\noriginate from the outlined factors.\n"
  },
  {
    "year": 2022,
    "title": "FastRemap: A Tool for Quickly Remapping Reads between Genome Assemblies",
    "summary": "  A genome read data set can be quickly and efficiently remapped from one\nreference to another similar reference (e.g., between two reference versions or\ntwo similar species) using a variety of tools, e.g., the commonly-used CrossMap\ntool. With the explosion of available genomic data sets and references,\nhigh-performance remapping tools will be even more important for keeping up\nwith the computational demands of genome assembly and analysis.\n  We provide FastRemap, a fast and efficient tool for remapping reads between\ngenome assemblies. FastRemap provides up to a 7.82$\\times$ speedup\n(6.47$\\times$, on average) and uses as low as 61.7% (80.7%, on average) of the\npeak memory consumption compared to the state-of-the-art remapping tool,\nCrossMap.\n  FastRemap is written in C++. The source code and user manual are freely\navailable at: github.com/CMU-SAFARI/FastRemap. Docker image available at:\nhttps://hub.docker.com/r/alkanlab/fast. Also available in Bioconda at:\nhttps://anaconda.org/bioconda/fastremap-bio.\n"
  },
  {
    "year": 2022,
    "title": "Computational Methods for Single-Cell Multi-Omics Integration and\n  Alignment",
    "summary": "  Recently developed technologies to generate single-cell genomic data have\nmade a revolutionary impact in the field of biology. Multi-omics assays offer\neven greater opportunities to understand cellular states and biological\nprocesses. However, the problem of integrating different -omics data with very\ndifferent dimensionality and statistical properties remains quite challenging.\nA growing body of computational tools are being developed for this task,\nleveraging ideas ranging from machine translation to the theory of networks and\nrepresenting a new frontier on the interface of biology and data science. Our\ngoal in this review paper is to provide a comprehensive, up-to-date survey of\ncomputational techniques for the integration of multi-omics and alignment of\nmultiple modalities of genomics data in the single cell research field.\n"
  },
  {
    "year": 2022,
    "title": "Client applications and Server Side docker for management of RNASeq\n  and/or VariantSeq workflows and pipelines of the GPRO Suite",
    "summary": "  The GPRO suite is an in-progress bioinformatic project for -omic data\nanalyses. As part of the continued growth of this project, we introduce a\nclient side & server side solution for comparative transcriptomics and analysis\nof variants. The client side consists of two Java applications called \"RNASeq\"\nand \"VariantSeq\" to manage workflows for RNA-seq and Variant-seq analysis,\nrespectively, based on the most common command line interface tools for each\ntopic. Both applications are coupled with a Linux server infrastructure (named\nGPRO Server Side) that hosts all dependencies of each application (scripts,\ndatabases, and command line interface tools). Implementation of the server side\nrequires a Linux operating system, PHP, SQL, Python, bash scripting, and\nthird-party software. The GPRO Server Side can be deployed via a Docker\ncontainer that can be installed in the user's PC using any operating system or\non remote servers as a cloud solution. The two applications are available as\ndesktop and cloud applications and provide two execution modes: a Step-by-Step\nmode enables each step of a workflow to be executed independently and a\nPipeline mode allows all steps to be run sequentially. The two applications\nalso feature an experimental support system called GENIE that consists of a\nvirtual chatbot/assistant and a pipeline jobs panel coupled with an expert\nsystem. The chatbot can troubleshoot issues with the usage of each tool, the\npipeline job panel provides information about the status of each task executed\nin the GPRO Server Side, and the expert provides the user with a potential\nrecommendation to identify or fix failed analyses. The two applications and the\nGPRO Server Side combine the user-friendliness and security of client software\nwith the efficiency of front-end & back-end solutions to manage command line\ninterface software for RNA-seq and variant-seq analysis via interface\nenvironments.\n"
  },
  {
    "year": 2022,
    "title": "Investigating the genomic background of CRISPR-Cas genomes for\n  CRISPR-based antimicrobials",
    "summary": "  CRISPR-Cas systems are an adaptive immunity that protects prokaryotes against\nforeign genetic elements. Genetic templates acquired during past infection\nevents enable DNA-interacting enzymes to recognize foreign DNA for destruction.\nDue to the programmability and specificity of these genetic templates,\nCRISPR-Cas systems are potential alternative antibiotics that can be engineered\nto self-target antimicrobial resistance genes on the chromosome or plasmid.\nHowever, several fundamental questions remain to repurpose these tools against\ndrug-resistant bacteria. For endogenous CRISPR-Cas self-targeting,\nantimicrobial resistance genes and functional CRISPR-Cas systems have to\nco-occur in the target cell. Furthermore, these tools have to outplay DNA\nrepair pathways that respond to the nuclease activities of Cas proteins, even\nfor exogenous CRISPR-Cas delivery. Here, we conduct a comprehensive survey of\nCRISPR-Cas genomes. First, we address the co-occurrence of CRISPR-Cas systems\nand antimicrobial resistance genes in the CRISPR-Cas genomes. We show that the\naverage number of these genes varies greatly by the CRISPR-Cas type, and some\nCRISPR-Cas types (IE and IIIA) have over 20 genes per genome. Next, we\ninvestigate the DNA repair pathways of these CRISPR-Cas genomes, revealing that\nthe diversity and frequency of these pathways differ by the CRISPR-Cas type.\nThe interplay between CRISPR-Cas systems and DNA repair pathways is essential\nfor the acquisition of new spacers in CRISPR arrays. We conduct simulation\nstudies to demonstrate that the efficiency of these DNA repair pathways may be\ninferred from the time-series patterns in the RNA structure of CRISPR repeats.\nThis bioinformatic survey of CRISPR-Cas genomes elucidates the necessity to\nconsider multifaceted interactions between different genes and systems to\ndesign effective CRISPR-based antimicrobials.\n"
  },
  {
    "year": 2022,
    "title": "Correspondence on ACMG STATEMENT: ACMG SF v3.0 list for reporting of\n  secondary findings in clinical exome and genome sequencing: a policy\n  statement of the American College of Medical Genetics and Genomics (ACMG) by\n  Miller et al",
    "summary": "  We were interested to read the recent update on recommendations for reporting\nof secondary findings in clinical sequencing1, and the accompanying updated\nlist of genes in which secondary findings should be sought (ACMG SF v3.0)2.\nThough the authors discuss challenges around incomplete penetrance in\nconsiderable detail, we are concerned that the recommendations do not fully\nconvey the degree of uncertainty regarding the penetrance of variants in genes\nassociated with inherited cardiomyopathies, which make up almost a quarter of\nthe list. Since penetrance is incomplete and age-related, individuals found to\ncarry variants will often require surveillance, rather than a one-off\ndefinitive diagnostic assessment. There is a lack of evidence regarding\nbenefits, harms, and healthcare costs associated with opportunistic screening.\n"
  },
  {
    "year": 2022,
    "title": "graph-GPA 2.0: A Graphical Model for Multi-disease Analysis of GWAS\n  Results with Integration of Functional Annotation Data",
    "summary": "  Genome-wide association studies (GWAS) have successfully identified a large\nnumber of genetic variants associated with traits and diseases. However, it\nstill remains challenging to fully understand functional mechanisms underlying\nmany associated variants. This is especially the case when we are interested in\nvariants shared across multiple phenotypes. To address this challenge, we\npropose graph-GPA 2.0 (GGPA 2.0), a novel statistical framework to integrate\nGWAS datasets for multiple phenotypes and incorporate functional annotations\nwithin a unified framework. We conducted simulation studies to evaluate GGPA\n2.0. The results indicate that incorporating functional annotation data using\nGGPA 2.0 does not only improve detection of disease-associated variants, but\nalso allows to identify more accurate relationships among diseases. We analyzed\nfive autoimmune diseases and five psychiatric disorders with the functional\nannotations derived from GenoSkyline and GenoSkyline-Plus and the prior disease\ngraph generated by biomedical literature mining. For autoimmune diseases, GGPA\n2.0 identified enrichment for blood, especially B cells and regulatory T cells\nacross multiple diseases. Psychiatric disorders were enriched for brain,\nespecially prefrontal cortex and inferior temporal lobe for bipolar disorder\n(BIP) and schizophrenia (SCZ), respectively. Finally, GGPA 2.0 successfully\nidentified the pleiotropy between BIP and SCZ. These results demonstrate that\nGGPA 2.0 can be a powerful tool to identify associated variants associated with\neach phenotype or those shared across multiple phenotypes, while also promoting\nunderstanding of functional mechanisms underlying the associated variants.\n"
  },
  {
    "year": 2022,
    "title": "GeoTyper: Automated Pipeline from Raw scRNA-Seq Data to Cell Type\n  Identification",
    "summary": "  The cellular composition of the tumor microenvironment can directly impact\ncancer progression and the efficacy of therapeutics. Understanding immune cell\nactivity, the body's natural defense mechanism, in the vicinity of cancerous\ncells is essential for developing beneficial treatments. Single cell RNA\nsequencing (scRNA-seq) enables the examination of gene expression on an\nindividual cell basis, providing crucial information regarding both the\ndisturbances in cell functioning caused by cancer and cell-cell communication\nin the tumor microenvironment. This novel technique generates large amounts of\ndata, which require proper processing. Various tools exist to facilitate this\nprocessing but need to be organized to standardize the workflow from data\nwrangling to visualization, cell type identification, and analysis of changes\nin cellular activity, both from the standpoint of malignant cells and immune\nstromal cells that eliminate them. We aimed to develop a standardized pipeline\n(GeoTyper, https://github.com/celineyayifeng/GeoTyper) that integrates multiple\nscRNA-seq tools for processing raw sequence data extracted from NCBI GEO,\nvisualization of results, statistical analysis, and cell type identification.\nThis pipeline leverages existing tools, such as Cellranger from 10X Genomics,\nAlevin, and Seurat, to cluster cells and identify cell types based on gene\nexpression profiles. We successfully tested and validated the pipeline on\nseveral publicly available scRNA-seq datasets, resulting in clusters\ncorresponding to distinct cell types. By determining the cell types and their\nrespective frequencies in the tumor microenvironment across multiple cancers,\nthis workflow will help quantify changes in gene expression related to\ncell-cell communication and identify possible therapeutic targets.\n"
  },
  {
    "year": 2022,
    "title": "CAGI, the Critical Assessment of Genome Interpretation, establishes\n  progress and prospects for computational genetic variant interpretation\n  methods",
    "summary": "  The Critical Assessment of Genome Interpretation (CAGI) aims to advance the\nstate of the art for computational prediction of genetic variant impact,\nparticularly those relevant to disease. The five complete editions of the CAGI\ncommunity experiment comprised 50 challenges, in which participants made blind\npredictions of phenotypes from genetic data, and these were evaluated by\nindependent assessors. Overall, results show that while current methods are\nimperfect, they have major utility for research and clinical applications.\nMissense variant interpretation methods are able to estimate biochemical\neffects with increasing accuracy. Performance is particularly strong for\nclinical pathogenic variants, including some difficult-to-diagnose cases, and\nextends to interpretation of cancer-related variants. Assessment of methods for\nregulatory variants and complex trait disease risk is less definitive, and\nindicates performance potentially suitable for auxiliary use in the clinic.\nEmerging methods and increasingly large, robust datasets for training and\nassessment promise further progress ahead.\n"
  },
  {
    "year": 2022,
    "title": "SODA: a TypeScript/JavaScript Library for Visualizing Biological\n  Sequence Annotation",
    "summary": "  We present SODA, a lightweight and open-source visualization library for\nbiological sequence annotations that enables straightforward development of\nflexible, dynamic, and interactive web graphics. SODA is implemented in\nTypeScript and can be used as a library within TypeScript and JavaScript.\n"
  },
  {
    "year": 2022,
    "title": "Functional and evolutionary genomics of the Streptomyces metabolism",
    "summary": "  This thesis is focused in the study of the evolution of the metabolic\nrepertoire of Streptomyces, which are renowned as proficient producers of\nbioactive Natural Products (NPs). The main goal of my work was to contribute\ninto the understanding of the evolutionary mechanisms behind the evolution of\nNP biosynthetic pathways. Specifically, the development of a bioinformatic\nmethod that helps into the discovery of new NP biosynthetic pathways from\nactinobacterial genome sequences with emphasis on members of the genus\nStreptomyces. I developed this method using a comparative and functional\ngenomics perspective. My studies indicate that central metabolic enzymes were\nexpanded in a genus-specific manner in Actinobacteria, and that they have been\nextensively recruited for the biosynthesis of NPs. Based in these observations,\nI developed EvoMining, a bioinformatic pipeline for the identificatoon of novel\nbiosynthetic pathways in microbial genomes. Using EvoMining several new NP\nbiosynthetic pathways have been predicted in different members of the phylum\nActinobacteria, including the model organism S. lividans 66. To test this\napproach, the genome sequence of this model strain was obtained, and its\nanalysis led to the discovery of an unprecedented system for peptide bond\nformation, as well as a biosynthetic pathway for an arsenic-containing\nmetabolite. Moreover, this work also led to the identification of expansions on\na conserved metabolic node in the glycolytic pathway of Streptomyces. These\nexpansions occurred before the radiation of Streptomyces and are concomitant\nwith the evolution of their capability to produce NPs. Experimental analyses\nindicate that this node evolved to mediate the interplay between central an NP\nmetabolism.\n"
  },
  {
    "year": 2022,
    "title": "LAPIS is a fast web API for massive open virus sequencing databases",
    "summary": "  Background: Recent epidemic outbreaks such as the SARS-CoV-2 pandemic and the\nmpox outbreak in 2022 have demonstrated the value of genomic sequencing data\nfor tracking the origin and spread of pathogens. Laboratories around the globe\ngenerated new sequences at unprecedented speed and volume and bioinformaticians\ndeveloped new tools and dashboards to analyze this wealth of data. However, a\nmajor challenge that remains is the lack of simple and efficient approaches for\naccessing and processing sequencing data.\n  Results: The Lightweight API for Sequences (LAPIS) facilitates rapid\nretrieval and analysis of genomic sequencing data through a REST API. It\nsupports complex mutation- and metadata-based queries and can perform\naggregation operations on massive datasets. LAPIS is optimized for typical\nquestions relevant to genomic epidemiology. Using a newly-developed in-memory\ndatabase engine, it has a high speed and throughput: between 25 January and 4\nFebruary 2023, the SARS-CoV-2 instance of LAPIS, which contains 14.5 million\nsequences, processed over 20 million requests with a mean response time of 411\nms and a median response time of 1 ms. LAPIS is the core engine behind our\ndashboards on genspectrum.org and we currently maintain public LAPIS instances\nfor SARS-CoV-2 and mpox.\n  Conclusions: Powered by an optimized database engine and available through a\nweb API, LAPIS enhances the accessibility of genomic sequencing data. It is\ndesigned to serve as a common backend for dashboards and analyses with the\npotential to be integrated into common database platforms such as GenBank.\n"
  },
  {
    "year": 2022,
    "title": "PhaTYP: Predicting the lifestyle for bacteriophages using BERT",
    "summary": "  Bacteriophages (or phages), which infect bacteria, have two distinct\nlifestyles: virulent and temperate. Predicting the lifestyle of phages helps\ndecipher their interactions with their bacterial hosts, aiding phages'\napplications in fields such as phage therapy. Because experimental methods for\nannotating the lifestyle of phages cannot keep pace with the fast accumulation\nof sequenced phages, computational method for predicting phages' lifestyles has\nbecome an attractive alternative. Despite some promising results, computational\nlifestyle prediction remains difficult because of the limited known annotations\nand the sheer amount of sequenced phage contigs assembled from metagenomic\ndata. In particular, most of the existing tools cannot precisely predict\nphages' lifestyles for short contigs. In this work, we develop PhaTYP (Phage\nTYPe prediction tool) to improve the accuracy of lifestyle prediction on short\ncontigs. We design two different training tasks, self-supervised and\nfine-tuning tasks, to overcome lifestyle prediction difficulties. We rigorously\ntested and compared PhaTYP with four state-of-the-art methods: DeePhage,\nPHACTS, PhagePred, and BACPHLIP. The experimental results show that PhaTYP\noutperforms all these methods and achieves more stable performance on short\ncontigs. In addition, we demonstrated the utility of PhaTYP for analyzing the\nphage lifestyle on human neonates' gut data. This application shows that PhaTYP\nis a useful means for studying phages in metagenomic data and helps extend our\nunderstanding of microbial communities.\n"
  },
  {
    "year": 2022,
    "title": "Further analysis of metagenomic datasets containing GD and GX pangolin\n  CoVs indicates widespread contamination, undermining pangolin host\n  attribution",
    "summary": "  The only animals other than bats reported to have been infected with\nSARS-CoV-2-related coronaviruses (SARS2r-CoVs) prior to the COVID-19 pandemic\nare pangolins. In early 2020 multiple papers reported the identification of two\nclades of SARS2r-CoVs, GD and GX, infecting pangolins. However the RNA-Seq\ndatasets supporting pangolin genome assembly were widely contaminated,\ncontained synthetic vectors or were heavily enriched or filtered with little\nbut coronavirus sequences left in the datasets. Here we investigate two\npangolin fecal samples sequenced by Li et al. (2021) provided in support of GD\nPCoV infection of pangolins in Guangdong and find the read distribution\nconsistent with PCR amplicon contamination and SARS-CoV-2 contamination, and\nfurther identify the presence of synthetic plasmid sequences. We also build\nupon our previous work to further analyze the dataset GX/P3B by Lam et al.\n(2020), which is the only non enriched/heavily filtered pangolin tissue dataset\nsequenced by Lam et al. (2020). We identify synthetic vectors and confirm human\ngenomic origin samples in the dataset. Finally, we find human mitochondrial\nsequences in all pangolin organ datasets and mouse and tiger mitochondrial\nsequences in selected pangolin organ datasets sequenced by Liu et al. (2019).\nWe infer that human and mouse genomic origin sequences were probably sourced\nfrom contamination prior to sequencing, while tiger origin sequence\ncontamination may have occurred due to index hopping during sequencing. These\nobservations are problematic for attributing pangolins as SARS2r-CoV hosts in\nthe datasets examined. The forensic methods developed and used here can be\napplied to examine any third party SRA data sets.\n"
  },
  {
    "year": 2022,
    "title": "A Bayesian method for estimating gene-level polygenicity under the\n  framework of transcriptome-wide association study",
    "summary": "  Polygnicity refers to the phenomenon that multiple genetic variants have a\nnon-zero effect on a complex trait. It is defined as the proportion of genetic\nvariants that have a nonzero effect on the trait. Evaluation of polygenicity\ncan provide valuable insights into the genetic architecture of the trait.\nSeveral recent works have attempted to estimate polygenicity at the SNP level.\nHowever, evaluating polygenicity at the gene level can be biologically more\nmeaningful. We propose the notion of gene-level polygenicity, defined as the\nproportion of genes having a non-zero effect on the trait under the framework\nof transcriptome-wide association study. We introduce a Bayesian approach\npolygene to estimate this quantity for a trait. The method is based on spike\nand slab prior and simultaneously provides an optimal subset of non-null genes.\nOur simulation study shows that polygene efficiently estimates gene-level\npolygenicity. The method produces downward bias for small choices of trait\nheritability due to a non-null gene, which diminishes rapidly with an increase\nin the GWAS sample size. While identifying the optimal subset of non-null\ngenes, polygene offers a high level of specificity and an overall good level of\nsensitivity -- the sensitivity increases as the sample size of the reference\npanel expression and GWAS data increase. We applied the method to seven\nphenotypes in the UK Biobank, integrating expression data. We find height to be\nmost polygenic and asthma to be the least polygenic. Our analysis suggests that\nboth HDL and triglycerides are more polygenic than LDL.\n"
  },
  {
    "year": 2022,
    "title": "Therapeutic algebra of immunomodulatory drug responses at single-cell\n  resolution",
    "summary": "  Therapeutic modulation of immune states is central to the treatment of human\ndisease. However, how drugs and drug combinations impact the diverse cell types\nin the human immune system remains poorly understood at the transcriptome\nscale. Here, we apply single-cell mRNA-seq to profile the response of human\nimmune cells to 502 immunomodulatory drugs alone and in combination. We develop\na unified mathematical model that quantitatively describes the transcriptome\nscale response of myeloid and lymphoid cell types to individual drugs and drug\ncombinations through a single inferred regulatory network. The mathematical\nmodel reveals how drug combinations generate novel, macrophage and T-cell\nstates by recruiting combinations of gene expression programs through both\nadditive and non-additive drug interactions. A simplified drug response algebra\nallows us to predict the continuous modulation of immune cell populations\nbetween activated, resting and hyper-inhibited states through combinatorial\ndrug dose titrations. Our results suggest that transcriptome-scale mathematical\nmodels could enable the design of therapeutic strategies for programming the\nhuman immune system using combinations of therapeutics.\n"
  },
  {
    "year": 2022,
    "title": "Mapache: a flexible pipeline to map ancient DNA",
    "summary": "  Summary: Mapping ancient DNA to a reference genome is challenging as it\ninvolves numerous steps, is time-consuming and has to be repeated within a\nstudy to assess the quality of extracts and libraries; as a result, the mapping\nneeds to be automatized to handle large amounts of data in a reproducible way.\nWe present mapache, a flexible, robust, and scalable pipeline to map, quantify\nand impute ancient and present-day DNA in a reproducible way. Mapache is\nimplemented in the workflow manager Snakemake and is optimized for low-space\nconsumption, allowing to efficiently (re)map large data sets such as reference\npanels and multiple extracts and libraries.\n"
  },
  {
    "year": 2022,
    "title": "genomepy: genes and genomes at your fingertips",
    "summary": "  Analyzing a functional genomics experiment, such as ATAC-, ChIP- or\nRNA-sequencing, requires reference data including a genome assembly and gene\nannotation. These resources can generally be retrieved from different\norganizations and in different versions. Most bioinformatic workflows require\nthe user to supply this genomic data manually, which can be a tedious and\nerror-prone process.\n  Here we present genomepy, which can search, download, and preprocess the\nright genomic data for your analysis. Genomepy can search genomic data on NCBI,\nEnsembl, UCSC and GENCODE, and compare available gene annotations to enable an\ninformed decision. The selected genome and gene annotation can be downloaded\nand preprocessed with sensible, yet controllable, defaults. Additional\nsupporting data can be automatically generated or downloaded, such as aligner\nindexes, genome metadata and blacklists.\n  Genomepy is freely available at https://github.com/vanheeringen-lab/genomepy\nunder the MIT license and can be installed through pip or bioconda.\n"
  },
  {
    "year": 2022,
    "title": "Phage family classification under Caudoviricetes: a review of current\n  tools using the latest ICTV classification framework",
    "summary": "  Bacteriophages, which are viruses infecting bacteria, are the most ubiquitous\nand diverse entities in the biosphere. There is accumulating evidence revealing\ntheir important roles in shaping the structure of various microbiomes. Thanks\nto (viral) metagenomic sequencing, a large number of new bacteriophages have\nbeen discovered. However, lacking a standard and automatic virus classification\npipeline, the taxonomic characterization of new viruses seriously lag behind\nthe sequencing efforts. In particular, according to the latest version of ICTV,\nseveral large phage families in the previous classification system are removed.\nTherefore, a comprehensive review and comparison of taxonomic classification\ntools under the new standard are needed to establish the state-of-the-art. In\nthis work, we retrained and tested four recently published tools on newly\nlabeled databases. We demonstrated their utilities and tested them on multiple\ndatasets, including the RefSeq, short contigs, simulated metagenomic datasets,\nand low-similarity datasets. This study provides a comprehensive review of\nphage family classification in different scenarios and a practical guidance for\nchoosing appropriate taxonomic classification pipelines. To our best knowledge,\nthis is the first review conducted under the new ICTV classification framework.\nThe results show that the new family classification framework overall leads to\nbetter-conserved groups and thus makes family-level classification more\nfeasible.\n"
  },
  {
    "year": 2022,
    "title": "HLA predictions from long sequence read alignments, streamed directly\n  into HLAminer",
    "summary": "  The rapidly changing landscape of sequencing technologies brings new\nopportunities to genomics research. Longer sequence reads and higher sequence\nthroughput coupled with ever-improving base accuracy and decreasing per-base\ncost is now making long reads suitable for analyzing polymorphic regions of the\nhuman genome, such as those of the human leucocyte antigen (HLA) gene complex.\nHere I present a simple protocol for predicting HLA signatures from whole\ngenome shotgun (WGS) long sequencing reads, by directly streaming sequence\nalignments into HLAminer. The method is as simple as running minimap2, it\nscales with the number of sequences to align, and can be used with any read\naligner capable of sam format output without the need to store bulky alignment\nfiles to disk. I show how the predictions are robust even with older and less\n[base] accurate WGS nanopore datasets and relatively low (10X) sequence\ncoverage and present a step-by-step protocol to predict HLA class I and II\ngenes from the long sequencing reads of modern third-generation technologies.\n"
  },
  {
    "year": 2022,
    "title": "On the unknown proteins of eukaryotic proteomes",
    "summary": "  In order to study unknown proteins on a large scale, a reference system has\nbeen set up for the three major eukaryotic lineages, built with 36 proteomes as\ntaxonomically diverse as possible. Proteins from 362 eukaryotic proteomes with\nno known homologue in this set were then analyzed, focusing noteworthy on\nsingletons, that is, on unknown proteins with no known homologue in their own\nproteome. Consistently, according to Uniprot, for a given species, no more than\n12% of the singletons thus found are known at the protein level. Also, since\nthey rely on the information found in the alignment of homologous sequences,\npredictions of AlphaFold2 for their tridimensional structure are usually poor.\nIn the case of metazoan species, the number of singletons seems to increase as\na function of the evolutionary distance from the reference system.\nInterestingly, no such trend is found in the cases of viridiplantae and fungi,\nas if the timescale on which singletons are added to proteomes were different\nin metazoa and in other eukaryotic kingdoms. In order to confirm this\nphenomenon, further studies of proteomes closer to those of the reference\nsystem are however needed.\n"
  },
  {
    "year": 2022,
    "title": "NLP-based classification of software tools for metagenomics sequencing\n  data analysis into EDAM semantic annotation",
    "summary": "  Motivation: The rapid growth of metagenomics sequencing data makes\nmetagenomics increasingly dependent on computational and statistical methods\nfor fast and efficient analysis. Consequently, novel analysis tools for\nbig-data metagenomics are constantly emerging. One of the biggest challenges\nfor researchers occurs in the analysis planning stage: selecting the most\nsuitable metagenomics software tool to gain valuable insights from sequencing\ndata. The building process of data analysis pipelines is often laborious and\ntime-consuming since it requires a deep and critical understanding of how to\napply a particular tool to complete a specified metagenomics task.\n  Results: We have addressed this challenge by using machine learning methods\nto develop a classification system of metagenomics software tools into 13\nclasses (11 semantic annotations of EDAM and two virus-specific classes) based\non the descriptions of the tools. We trained three classifiers (Naive Bayes,\nLogistic Regression, and Random Forest) using 15 text feature extraction\ntechniques (TF-IDF, GloVe, BERT-based models, and others). The manually curated\ndataset includes 224 software tools and contains text from the abstract and the\nmethods section of the tools' publications. The best classification\nperformance, with an Area Under the Precision-Recall Curve score of 0.85, is\nachieved using Logistic regression, BioBERT for text embedding, and text from\nabstracts only. The proposed system provides accurate and unified\nidentification of metagenomics data analysis tools and tasks, which is a\ncrucial step in the construction of metagenomics data analysis pipelines.\n"
  },
  {
    "year": 2022,
    "title": "Towards complete representation of bacterial contents in metagenomic\n  samples",
    "summary": "  Background: In the metagenome assembly of a microbiome community, we may\nthink abundant species would be easier to assemble due to their deeper\ncoverage. However, this conjucture is rarely tested. We often do not know how\nmany abundant species we are missing and do not have an approach to recover\nthese species.\n  Results: Here we proposed k-mer based and 16S RNA based methods to measure\nthe completeness of metagenome assembly. We showed that even with PacBio\nHigh-Fidelity (HiFi) reads, abundant species are often not assembled as high\nstrain diversity may lead to fragmented contigs. We developed a novel algorithm\nto recover abundant metagenome-assembled genomes (MAGs) by identifying circular\nassembly subgraphs. Our algorithm is reference-free and complement to standard\nmetagenome binning. Evaluated on 14 real datasets, it rescued many abundant\nspecies that would be missing with existing methods.\n  Conclusions: Our work stresses the importance of metagenome completeness\nwhich is often overlooked before. Our algorithm generates more circular MAGs\nand moves a step closer to the complete representation of microbiome\ncommunities.\n"
  },
  {
    "year": 2022,
    "title": "An RNA Sequencing Analysis of Glaucoma Genesis in Mice",
    "summary": "  Glaucoma is the leading cause of irreversible blindness in people over the\nage of 60, accounting for 6.6 to 8% of all blindness in 2010, but there is\nstill much to be learned about the genetic origins of the eye disease. With the\nmodern development of Next-Generation Sequencing (NGS) technologies, scientists\nare starting to learn more about the genetic origins of Glaucoma. This research\nuses differential expression (DE) and gene ontology (GO) analyses to study the\ngenetic differences between mice with severe Glaucoma and multiple control\ngroups. Optical nerve head (ONH) and retina data samples of genome-wide RNA\nexpression from NCBI (NIH) are used for pairwise comparison experimentation. In\naddition, principal component analysis (PCA) and dispersion visualization\nmethods are employed to perform quality control tests of the sequenced data.\nGenes with skewed gene counts are also identified, as they may be marker genes\nfor a particular severity of Glaucoma. The gene ontologies found in this\nexperiment support existing knowledge of Glaucoma genesis, providing confidence\nthat the results were valid. Future researchers can thoroughly study the gene\nlists generated by the DE and GO analyses to find potential activator or\nprotector genes for Glaucoma in mice to develop drug treatments or gene\ntherapies to slow or stop the progression of the disease. The overall goal is\nthat in the future, such treatments can be made for humans as well to improve\nthe quality of life for human patients with Glaucoma and reduce Glaucoma\nblindness rates.\n"
  },
  {
    "year": 2022,
    "title": "KGP: An R Package with Metadata from the 1000 Genomes Project",
    "summary": "  The 1000 Genomes Project provides sequencing data on 3,202 samples from 26\npopulations spanning five continental regions with no access or use\nrestrictions. The kgp R package provides consistent and comprehensive metadata\nabout samples and populations in the 1000 Genomes Project and other population\nsequencing data in the International Genome Sample Resource collection. The kgp\npackage is distributed via the Comprehensive R Archive Network (CRAN) at\nhttps://cran.r-project.org/package=kgp. Source code is available on GitHub at\nhttps://github.com/stephenturner/kgp. Further documentation is online at\nhttps://stephenturner.github.io/kgp/.\n"
  },
  {
    "year": 2022,
    "title": "Deep Learning in Spatially Resolved Transcriptomics: A Comprehensive\n  Technical View",
    "summary": "  Spatially resolved transcriptomics (SRT) has evolved rapidly through various\ntechnologies, enabling scientists to investigate both morphological contexts\nand gene expression profiling at single-cell resolution in parallel. SRT data\nare complex and multi-modal, comprising gene expression matrices, spatial\ninformation, and often high-resolution histology images. Because of this\ncomplexity and multi-modality, sophisticated computational algorithms are\nrequired to accurately analyze SRT data. Most efforts in this domain have been\nmade to utilize conventional machine learning and statistical approaches,\nexhibiting sub-optimal results due to the complicated nature of SRT datasets.\nTo address these shortcomings, researchers have recently employed deep learning\nalgorithms including various state-of-the-art methods mainly in spatial\nclustering, spatially variable gene identification, and alignment. While great\nprogress has been made in developing deep learning-based models for SRT data\nanalysis, further improvement is still needed to create more biologically aware\nmodels that consider aspects such as phylogeny-aware clustering or the analysis\nof small histology image patches. Additionally, strategies for batch effect\nremoval, normalization, and handling overdispersion and zero inflation patterns\nof gene expression are still needed in the analysis of SRT data using deep\nlearning methods. In this paper, we provide a comprehensive overview of these\ndeep learning methods, including their strengths and limitations. We also\nhighlight new frontiers, current challenges, limitations, and open questions in\nthis field. Also, we provide a comprehensive list of all available SRT\ndatabases that can be used as an extensive resource for future studies.\n"
  },
  {
    "year": 2022,
    "title": "Protein-to-genome alignment with miniprot",
    "summary": "  Motivation: Protein-to-genome alignment is critical to annotating genes in\nnon-model organisms. While there are a few tools for this purpose, all of them\nwere developed over ten years ago and did not incorporate the latest advances\nin alignment algorithms. They are inefficient and could not keep up with the\nrapid production of new genomes and quickly growing protein databases.\n  Results: Here we describe miniprot, a new aligner for mapping protein\nsequences to a complete genome. Miniprot integrates recent techniques such as\nk-mer sketch and SIMD-based dynamic programming. It is tens of times faster\nthan existing tools while achieving comparable accuracy on real data.\n  Availability and implementation: https://github.com/lh3/miniprot\n"
  },
  {
    "year": 2022,
    "title": "MVP: Detection of motif-making and -breaking mutations",
    "summary": "  Background: DNA, RNA, and protein sequence motifs can be recognition sites\nfor biological functions such as regulation, DNA base modification, and\nmolecular binding in general. The gain and loss of such motifs can carry\nimportant consequences. When comparing sequences, the analysis of individual\nvariants does not impart an understanding of the impact on these sites. Rather,\nonly when these variants considered together with their neighbors and the\noriginal sequence context does this become possible.\n  Results: The motif-variant probe (mvp) makes this consideration, counting\ninstances of user-specified sequence motifs before and after mutation and\nreports those that result in motif gain or loss. mvp can perform a similar\nanalysis for proteins with amino acid variant data. The software is freely\navailable at https://lpcdrp.gitlab.io/mvp and also installable with the conda\npackage manager.\n  Conclusions: The ability to easily search for variants affecting any motif,\ntogether with the simultaneous consideration of neighboring variants makes mvp\na versatile tool to aid in a less-frequented dimension of comparative genomics.\n"
  },
  {
    "year": 2022,
    "title": "Integrative Pan-Cancer Analysis of RNMT: a Potential Prognostic and\n  Immunological Biomarker",
    "summary": "  Background: RNA guanine-7 methyltransferase (RNMT) is one of the main\nregulators of N7-methylguanosine, and the deregulation of RNMT correlated with\ntumor development and immune metabolism. However, the specific function of RNMT\nin pan-cancer remains unclear.\n  Methods: RNMT expression in different cancers was analyzed using multiple\ndatabases, including Cancer Cell Line Encyclopedia (CCLE), Genotype-Tissue\nExpression Project (GTEx), and The Cancer Genome Atlas (TCGA). Cox regression\nanalysis and Kaplan-Meier analysis were used to estimate the correlation of\nRNMT expression to prognosis. The data was also used to research the\nrelationship between RNMT expression and common immunoregulators, tumor\nmutation burden (TMB), microsatellite instability (MSI), mismatch repair (MMR),\nand DNA methyltransferase (DNMT). Additionally, the cBioPortal website was used\nto evaluate the characteristics of RNMT alteration. The TISDB database was used\nto obtain the expression of different subtypes. The Tumor Immune Estimation\nResource (TIMER) database was used to analyze the association between RNMT and\ntumor immune infiltration. Gene set enrichment analysis (GSEA) was used to\nidentify the relevant pathways.\n  Results: RNMT was ubiquitously highly expressed across cancers and survival\nanalysis revealed that its expression was highly associated with the clinical\nprognosis of various cancer types. Remarkably, RNMT participates in immune\nregulation and plays a crucial part in the tumor microenvironment. A positive\nassociation was found between RNMT expression and six immune cell types\nexpression in colon adenocarcinoma, kidney renal clear cell carcinoma, and\nliver hepatocellular carcinoma. Moreover, RNMT expression was highly associated\nwith immunoregulators in most cancer types, and correlated to TMB, MSI, MMR,\nand DNMT. Finally, GSEA indicated that RNMT may correlate with tumor immunity.\n"
  },
  {
    "year": 2022,
    "title": "Transcriptome Complexities Across Eukaryotes",
    "summary": "  Genomic complexity is a growing field of evolution, with case studies for\ncomparative evolutionary analyses in model and emerging non-model systems.\nUnderstanding complexity and the functional components of the genome is an\nuntapped wealth of knowledge ripe for exploration. With the \"remarkable lack of\ncorrespondence\" between genome size and complexity, there needs to be a way to\nquantify complexity across organisms. In this study we use a set of complexity\nmetrics that allow for evaluation of changes in complexity using TranD. We\nascertain if complexity is increasing or decreasing across transcriptomes and\nat what structural level, as complexity is varied. We define three metrics --\nTpG, EpT, and EpG in this study to quantify the complexity of the transcriptome\nthat encapsulate the dynamics of alternative splicing. Here we compare\ncomplexity metrics across 1) whole genome annotations, 2) a filtered subset of\northologs, and 3) novel genes to elucidate the impacts of ortholog and novel\ngenes in transcriptome analysis. We also derive a metric from Hong et al.,\n2006, Effective Exon Number (EEN), to compare the distribution of exon sizes\nwithin transcripts against random expectations of uniform exon placement. EEN\naccounts for differences in exon size, which is important because novel genes\ndifferences in complexity for orthologs and whole transcriptome analyses are\nbiased towards low complexity genes with few exons and few alternative\ntranscripts. With our metric analyses, we are able to implement changes in\ncomplexity across diverse lineages with greater precision and accuracy than\nprevious cross-species comparisons under ortholog conditioning. These analyses\nrepresent a step forward toward whole transcriptome analysis in the emerging\nfield of non-model evolutionary genomics, with key insights for evolutionary\ninference of complexity changes on deep timescales across the tree of life. We\nsuggest a means to quantify biases generated in ortholog calling and correct\ncomplexity analysis for lineage-specific effects. With these metrics, we\ndirectly assay the quantitative properties of newly formed lineage-specific\ngenes as they lower complexity in transcriptomes.\n"
  },
  {
    "year": 2022,
    "title": "Reconstructing gene expression and knockout effect scores from DNA\n  mutation (Mut2Ex): methodology and application to cancer prediction problems",
    "summary": "  Building prediction models for outcomes of clinical relevance when only a\nlimited number of mutational features are available causes considerable\nchallenges due to the sparseness and low-dimensionality of the data. In this\narticle, we present a method to augment the predictive power of these features\nby leveraging multi-modal associative relationships between an individual's\nmutational profile and their corresponding gene expression or knockout effect\nprofiles. We can thus reconstruct expression or effect scores for genes of\ninterest from the available mutation features and then use this reconstructed\nrepresentation directly to model and predict clinical outcomes. We show that\nour method produces significant improvements in predictive accuracy compared to\nmodels utilizing only the raw mutational data, and results in conclusions\ncomparable to those obtained using real expression or effect profiles.\n"
  },
  {
    "year": 2022,
    "title": "Neural Replicator Analysis for virus genomes binomial systematics in\n  metagenomics",
    "summary": "  We have presented some arguments to substantiate the usefulness of neural\nreplicator analysis (NRA) for constructing variants of the natural binomial\nclassification of virus genomes based only on knowledge of their complete\ngenomic sequences, without involving other data on the phenotype, functions,\nencoded proteins, etc., and also without the need of genomic sequences\nalignment. Perhaps this will make sense when processing metagenomic data. This\nmakes it possible to construct the binomial classification accepted for the\nviruses themselves. We restrict ourselves to three families of viruses having\ndsDNA circular genomes (Papillomaviridae, Polyomaviridae and Caulimoviridae)\nand partly to the family Geminiviridae having ssDNA genomes though the approach\npresented can be also applied to genomes of other dsDNA, ssDNA and ssRNA\nviruses, including linear ones (some results for Mitoviridae are also\npresented). It is argued that binomial classification of virus genomes which is\ndifficult to apply in all cases can nevertheless be informative tool of\nrevealing virus properties, areal of hosts, forms of diseases and can also show\nthe connections of the viruses belonging to different families and even to\ndifferent kingdoms.\n"
  },
  {
    "year": 2022,
    "title": "Sequential Labelling and DNABERT For Splice Site Prediction in Homo\n  Sapiens DNA",
    "summary": "  Genome sequencing technology has improved significantly in few last years and\nresulted in abundance genetic data. Artificial intelligence has been employed\nto analyze genetic data in response to its sheer size and variability. Gene\nprediction on single DNA has been conducted using various deep learning\narchitectures to discover splice sites and therefore discover intron and exon\nregion. Recent predictions are carried out with models trained on sequence with\nfixed splice site location which eliminates possibility of multiple splice\nsites existence in single sequence. This paper proposes sequential labelling to\npredict splice sites regardless their position in sequence. Sequential\nlabelling is carried out on DNA to determine intron and exon region and thus\ndiscover splice sites. Sequential labelling models used are based on pretrained\nDNABERT-3 which has been trained on human genome. Both fine-tuning and\nfeature-based approach are tested. Proposed model is benchmarked against latest\nsequential labelling model designed for mutation type and location prediction.\nWhile achieving high F1 scores on validation data, both baseline and proposed\nmodel perform poorly on test data. Error and test results analysis reveal that\nmodel experience overfitting and therefore, model is deemed not suitable for\nsplice site prediction.\n"
  },
  {
    "year": 2022,
    "title": "Metabolomics of Aging and Alzheimer's Disease: From Single-Omics to\n  Multi-Omics",
    "summary": "  Aging is a multifactorial process and a key factor of morbidity and\nmortality. Alzheimer's disease (AD) is an age-related disorder and a main cause\nof worldwide disability. Both aging and AD can be characterized by metabolic\ndysfunction. Metabolomics can quantify the complete set of metabolites in a\nstudied sample and is helpful for studying metabolic alterations in aging and\nAD. In this review, we summarize the metabolomic changes regarding aging and\nAD, discuss their biological functions, and highlight their potential\napplication as diagnostic biomarkers or therapeutic targets. Recent advances in\nmulti-omics approaches for understanding the metabolic mechanism of aging and\nAD are also reviewed.\n"
  },
  {
    "year": 2022,
    "title": "COGEDAP: A COmprehensive GEnomic Data Analysis Platform",
    "summary": "  Non-sharable sensitive data collection and analysis in large-scale consortia\nfor genomic research is complicated. Time consuming issues in installing\nsoftware arise due to different operating systems, software dependencies and\nrunning the software. Therefore, easier, more standardized, automated protocols\nand platforms can be a solution to overcome these issues. We have developed one\nsuch solution for genomic data analysis using software container technologies.\nThe platform, COGEDAP, consists of different software tools placed into\nSingularity containers with corresponding pipelines and instructions on how to\nperform genome-wide association studies (GWAS) and other genomic data analysis\nvia corresponding tools. Using a provided helper script written in Python,\nusers can obtain auto-generated scripts to conduct the desired analysis both on\nhigh-performance computing (HPC) systems and on personal computers. The\nanalyses can be done by running these auto-generated scripts with the software\ncontainers. The helper script also performs minor re-formatting of the\ninput/output data, so that the end user can work with a unified file format\nregardless of which genetic software is used for the analysis. COGEDAP is\nactively being used by users from different countries/projects to conduct their\ngenomic data analyses. Thanks to this platform, users can easily run GWAS and\nother genomic analyses without spending much effort on software installation,\ndata formats, and other technical requirements.\n"
  },
  {
    "year": 2023,
    "title": "Statistical Power Analysis for Designing Bulk, Single-Cell, and Spatial\n  Transcriptomics Experiments: Review, Tutorial, and Perspectives",
    "summary": "  Gene expression profiling technologies have been used in various applications\nsuch as cancer biology. The development of gene expression profiling has\nexpanded the scope of target discovery in transcriptomic studies, and each\ntechnology produces data with distinct characteristics. In order to guarantee\nbiologically meaningful findings using transcriptomic experiments, it is\nimportant to consider various experimental factors in a systematic way through\nstatistical power analysis. In this paper, we review and discuss the power\nanalysis for three types of gene expression profiling technologies from a\npractical standpoint, including bulk RNA-seq, single-cell RNA-seq, and\nhigh-throughput spatial transcriptomics. Specifically, we describe the existing\npower analysis tools for each research objective for each of the bulk RNA-seq\nand scRNA-seq experiments, along with recommendations. On the other hand, since\nthere are no power analysis tools for high-throughput spatial transcriptomics\nat this point, we instead investigate the factors that can influence power\nanalysis.\n"
  },
  {
    "year": 2023,
    "title": "Algorithms for the uniqueness of the longest common subsequence",
    "summary": "  Given several number sequences, determining the longest common subsequence is\na classical problem in computer science. This problem has applications in\nbioinformatics, especially determining transposable genes. Nevertheless,\nrelated works only consider how to find one longest common subsequence. In this\npaper, we consider how to determine the uniqueness of the longest common\nsubsequence. If there are multiple longest common subsequences, we also\ndetermine which number appears in all/some/none of the longest common\nsubsequences. We focus on four scenarios: (1) linear sequences without\nduplicated numbers; (2) circular sequences without duplicated numbers; (3)\nlinear sequences with duplicated numbers; (4) circular sequences with\nduplicated numbers. We develop corresponding algorithms and apply them to gene\nsequencing data.\n"
  },
  {
    "year": 2023,
    "title": "Beyond the exome: what's next in diagnostic testing for Mendelian\n  conditions",
    "summary": "  Despite advances in clinical genetic testing, including the introduction of\nexome sequencing (ES), more than 50% of individuals with a suspected Mendelian\ncondition lack a precise molecular diagnosis. Clinical evaluation is\nincreasingly undertaken by specialists outside of clinical genetics, often\noccurring in a tiered fashion and typically ending after ES. The current\ndiagnostic rate reflects multiple factors, including technical limitations,\nincomplete understanding of variant pathogenicity, missing genotype-phenotype\nassociations, complex gene-environment interactions, and reporting differences\nbetween clinical labs. Maintaining a clear understanding of the rapidly\nevolving landscape of diagnostic tests beyond ES, and their limitations,\npresents a challenge for non-genetics professionals. Newer tests, such as\nshort-read genome or RNA sequencing, can be challenging to order and emerging\ntechnologies, such as optical genome mapping and long-read DNA or RNA\nsequencing, are not available clinically. Furthermore, there is no clear\nguidance on the next best steps after inconclusive evaluation. Here, we review\nwhy a clinical genetic evaluation may be negative, discuss questions to be\nasked in this setting, and provide a framework for further investigation,\nincluding the advantages and disadvantages of new approaches that are nascent\nin the clinical sphere. We present a guide for the next best steps after\ninconclusive molecular testing based upon phenotype and prior evaluation,\nincluding when to consider referral to a consortium such as GREGoR, which is\nfocused on elucidating the underlying cause of rare unsolved genetic disorders.\n"
  },
  {
    "year": 2023,
    "title": "ntLink: a toolkit for de novo genome assembly scaffolding and mapping\n  using long reads",
    "summary": "  With the increasing affordability and accessibility of genome sequencing\ndata, de novo genome assembly is an important first step to a wide variety of\ndownstream studies and analyses. Therefore, bioinformatics tools that enable\nthe generation of high-quality genome assemblies in a computationally efficient\nmanner are essential. Recent developments in long-read sequencing technologies\nhave greatly benefited genome assembly work, including scaffolding, by\nproviding long-range evidence that can aid in resolving the challenging\nrepetitive regions of complex genomes. ntLink is a flexible and\nresource-efficient genome scaffolding tool that utilizes long-read sequencing\ndata to improve upon draft genome assemblies built from any sequencing\ntechnologies, including the same long reads. Instead of using read alignments\nto identify candidate joins, ntLink utilizes minimizer-based mappings to infer\nhow input sequences should be ordered and oriented into scaffolds. Recent\nimprovements to ntLink have added important features such as overlap detection,\ngap-filling and in-code scaffolding iterations. Here, we present three basic\nprotocols demonstrating how to use each of these new features to yield highly\ncontiguous genome assemblies, while still maintaining ntLink's proven\ncomputational efficiency. Further, as we illustrate in the alternate protocols,\nthe lightweight minimizer-based mappings that enable ntLink scaffolding can\nalso be utilized for other downstream applications, such as misassembly\ndetection. With its modularity and multiple modes of execution, ntLink has\nbroad benefit to the genomics community, from genome scaffolding and beyond.\nntLink is an open-source project and is freely available from\nhttps://github.com/bcgsc/ntLink.\n"
  },
  {
    "year": 2023,
    "title": "RawHash: Enabling Fast and Accurate Real-Time Analysis of Raw Nanopore\n  Signals for Large Genomes",
    "summary": "  Nanopore sequencers generate electrical raw signals in real-time while\nsequencing long genomic strands. These raw signals can be analyzed as they are\ngenerated, providing an opportunity for real-time genome analysis. An important\nfeature of nanopore sequencing, Read Until, can eject strands from sequencers\nwithout fully sequencing them, which provides opportunities to computationally\nreduce the sequencing time and cost. However, existing works utilizing Read\nUntil either 1) require powerful computational resources that may not be\navailable for portable sequencers or 2) lack scalability for large genomes,\nrendering them inaccurate or ineffective.\n  We propose RawHash, the first mechanism that can accurately and efficiently\nperform real-time analysis of nanopore raw signals for large genomes using a\nhash-based similarity search. To enable this, RawHash ensures the signals\ncorresponding to the same DNA content lead to the same hash value, regardless\nof the slight variations in these signals. RawHash achieves an accurate\nhash-based similarity search via an effective quantization of the raw signals\nsuch that signals corresponding to the same DNA content have the same quantized\nvalue and, subsequently, the same hash value.\n  We evaluate RawHash on three applications: 1) read mapping, 2) relative\nabundance estimation, and 3) contamination analysis. Our evaluations show that\nRawHash is the only tool that can provide high accuracy and high throughput for\nanalyzing large genomes in real-time. When compared to the state-of-the-art\ntechniques, UNCALLED and Sigmap, RawHash provides 1) 25.8x and 3.4x better\naverage throughput and 2) significantly better accuracy for large genomes,\nrespectively. Source code is available at\nhttps://github.com/CMU-SAFARI/RawHash.\n"
  },
  {
    "year": 2023,
    "title": "PhaVIP: Phage VIrion Protein classification based on chaos game\n  representation and Vision Transformer",
    "summary": "  Motivation: As viruses that mainly infect bacteria, phages are key players\nacross a wide range of ecosystems. Analyzing phage proteins is indispensable\nfor understanding phages' functions and roles in microbiomes. High-throughput\nsequencing enables us to obtain phages in different microbiomes with low cost.\nHowever, compared to the fast accumulation of newly identified phages, phage\nprotein classification remains difficult. In particular, a fundamental need is\nto annotate virion proteins, the structural proteins such as major tail,\nbaseplate etc. Although there are experimental methods for virion protein\nidentification, they are too expensive or time-consuming, leaving a large\nnumber of proteins unclassified. Thus, there is a great demand to develop a\ncomputational method for fast and accurate phage virion protein classification.\nResults: In this work, we adapted the state-of-the-art image classification\nmodel, Vision Transformer, to conduct virion protein classification. By\nencoding protein sequences into unique images using chaos gaming\nrepresentation, we can leverage Vision Transformer to learn both local and\nglobal features from sequence ``images''. Our method, PhaVIP, has two main\nfunctions: classifying PVP and non-PVP sequences and annotating the types of\nPVP, such as capsid and tail. We tested PhaVIP on several datasets with\nincreasing difficulty and benchmarked it against alternative tools. The\nexperimental results show that PhaVIP has superior performance. After\nvalidating the performance of PhaVIP, we investigated two applications that can\nuse the output of PhaVIP: phage taxonomy classification and phage host\nprediction. The results show the benefit of using classified proteins rather\nthan all proteins.\n"
  },
  {
    "year": 2023,
    "title": "The big challenge for livestock genomics is to make sequence data pay",
    "summary": "  This paper will argue that one of the biggest challenges for livestock\ngenomics is to make whole-genome sequencing and functional genomics applicable\nto breeding practice. It discusses potential explanations for why it is so\ndifficult to consistently improve the accuracy of genomic prediction by means\nof whole-genome sequence data, and three potential attacks on the problem.\n"
  },
  {
    "year": 2023,
    "title": "Unambiguosly expressing expectations about the content of prokaryotic\n  genomes",
    "summary": "  In recent years, the sequencing, assembling and annotation of prokaryotic\ngenomes has become increasingly easy and cheap. Thus it becomes increasingly\nfeasible and interesting to perform comparative genomics analyses of new\ngenomes to those of related organisms. Thereby related organisms can be defined\nby different criteria, such as taxonomy or phenotype.\n  Expectations regarding the contents of genomes are often expressed in\nscientific articles describing group of organisms. Evaluating such\nexpectations, when a new genome becomes available, requires analysing the text\nsnippets which express such expectations, extracting the logical elements of\nthe text and enabling a formal expression, more suitable for further automated\nanalyses.\n  Hereby we present a theoretical framework, alongside practical consideration\nfor expressing expectations about the content of genomes, with the purpose of\nenabling such comparative genomics analyses. The components of the framework\ninclude a system for the definition of groups of organisms, supported by a\nProkaryotic Group Types Ontology, a system for the definition of genomic\ncontents, supported by a Prokaryotic Genomic Contents Definition Ontology.\nFinally we discuss how the combination of these two systems may enable an\nunambiguous definition of absolute and relative genome content expectation\nrules.\n"
  },
  {
    "year": 2023,
    "title": "Early Risk Prediction of Chronic Myeloid Leukemia with Protein Sequences\n  using Machine Learning-based Meta-Ensemble",
    "summary": "  Leukemia, the cancer of blood cells, originates in the blood-forming cells of\nthe bone marrow. In Chronic Myeloid Leukemia (CML) conditions, the cells\npartially become mature that look like normal white blood cells but do not\nresist infection effectively. Early detection of CML is important for effective\ntreatment, but there is a lack of routine screening tests. Regular check-ups\nand monitoring of symptoms are the best way to detect CML in the early stages.\nIn the study, we developed a multi-layer-perception-based meta-ensemble system\nusing protein amino acid sequences for early risk prediction of CML. The\ndeleterious mutation analysis of protein sequences provides 7discriminant\ninformation in amino acid sequences causing CML. The protein sequences are\nexpressed into molecular descriptors using the values of hydrophobicity and\nhydrophilicity of the amino acids. 9 These descriptors are transformed in\nvarious statistical and correlation-based feature spaces. These 10 features\ninformation is given to several diverse types of base learners. The preliminary\npredictions of 11 base-learners are employed to develop Multi-Layered\nPerceptron (MLP) based meta-ensemble. The 12 proposed learning approach\neffectively utilizes the discriminant information to classify CML/non- 13 CML\nprotein sequences. The proposed prediction system has given improved results\nand it can be 14 employed as a potential biomarker for early diagnosis of CML.\n"
  },
  {
    "year": 2023,
    "title": "Russel and Rao Coefficient is a Suitable Substitute for Dice Coefficient\n  in Studying Restriction Mapped Genetic Distances of Escherichia coli",
    "summary": "  Escherichia coli is one of many bacterial inhabitants found in human\nintestines and any adaptation as a result of mutations may affect its host. A\ncommonly used technique employed to study these mutations is Restriction\nFragment Length Polymorphism (RFLP) and is proceeded with a suitable distance\ncoefficient to quantify genetic differences between 2 samples. Dice is\nconsidered a suitable distance coefficient in RFLP analyses, while others were\nleft unstudied in its suitability for use. Hence, this study aims to identify\nsubstitutes for Dice. Experimental data was obtained by subculturing E. coli\nfor 72 passages in 8 different adaptation media and RFLP profiles analyzed\nusing 20 distance coefficients. Our results suggest that Dennis, Fossum,\nMatching and Russel and Rao to work as well or better than Dice. Dennis,\nMatching and Fossum coefficients had highest discriminatory abilities but are\nlimited by the lack of upper or lower boundaries. Russel and Rao coefficient is\nhighly correlated with Dice coefficient (r2 = 0.998), with both higher and\nlower boundaries, suggesting that Russel and Rao coefficient can be used to\nsubstitute Dice coefficient in studying genetic distances in E. coli.\n"
  },
  {
    "year": 2023,
    "title": "Quantifying the common genetic variability of bacterial traits",
    "summary": "  The study of common heritability, or co-heritability, among multiple traits\nhas been widely established in quantitative and molecular genetics. However, in\nbacteria, genome-based estimation of heritability has only been considered very\nrecently and no methods are currently available for considering\nco-heritability. Here we introduce such a method and demonstrate its usefulness\nby multi-trait analyses of the three major human pathogens \\textit{Escherichia\ncoli}, \\textit{Neisseria gonorrhoeae} and \\textit{Streprococcus pneumoniae}. We\nanticipate that the increased availability of high-throughput genomic and\nphenotypic screens of bacterial populations will spawn ample future\nopportunities to understand the common molecular basis of different traits in\nbacteria.\n"
  },
  {
    "year": 2023,
    "title": "eQTL Studies: from Bulk Tissues to Single Cells",
    "summary": "  An expression quantitative trait locus (eQTL) is a chromosomal region where\ngenetic variants are associated with the expression levels of certain genes\nthat can be both nearby or distant. The identifications of eQTLs for different\ntissues, cell types, and contexts have led to better understanding of the\ndynamic regulations of gene expressions and implications of functional genes\nand variants for complex traits and diseases. Although most eQTL studies to\ndate have been performed on data collected from bulk tissues, recent studies\nhave demonstrated the importance of cell-type-specific and context-dependent\ngene regulations in biological processes and disease mechanisms. In this\nreview, we discuss statistical methods that have been developed to enable the\ndetections of cell-type-specific and context-dependent eQTLs from bulk tissues,\npurified cell types, and single cells. We also discuss the limitations of the\ncurrent methods and future research opportunities.\n"
  },
  {
    "year": 2023,
    "title": "Deciphering a Sleeping Pathogen: Uncovering Novel Transcriptional\n  Regulators of Hypoxia-Induced Dormancy in Mycobacterium Tuberculosis",
    "summary": "  Along the pathogenesis of Mycobacterium Tuberculosis (MTB), hypoxia-induced\ndormancy is a process involving the oxygen-depleted environment encountered\ninside the lung granuloma, where bacilli enter a viable, non-replicating state\ntermed as latency. Affecting nearly two billion people, latent TB can linger in\nthe host for indefinite periods of time before resuscitating, which\nsignificantly strains the accuracy of treatment options and patient prognosis.\nTranscriptional factors thought to mediate this process have only conferred\nmild growth defects, signaling that our current understanding of the MTB\ngenetic architecture is highly insufficient. In light of these inconsistencies,\nthe objective of this study was to characterize regulatory mechanisms\nunderlying the transition of MTB into dormancy. The project methodology\ninvolved a three-part approach - constructing an aggregate hypoxia dataset,\ninferring a gene regulatory network based on those observations, and leveraging\nseveral downstream network analyses to make sense of it all. Results indicated\ndormancy to be functionally associated with cell redox homeostasis, metal ion\ncycling, and cell wall metabolism, all of which modulate essential\nhost-pathogen interactions. Additionally, the crosstalk between individual\nregulons (Rv0821c and Rv0144; Rv1152 and Rv2359) was shown to be critical in\nfacilitating bacterial persistence and allowing MTB to gain control over key\nmicronutrients within the cell. Defense antioxidants and nutritional immunity\nwere also identified as future avenues to explore further. In providing some of\nthe first insights into the methods utilized by MTB to endure in a hypoxic\nstate, this research suggests a range of strategies that might aid in improved\nclinical outcomes of TB treatment.\n"
  },
  {
    "year": 2023,
    "title": "Molecular detection and antimicrobial activity of Endophytic fungi\n  isolated from a medical plant Rosmarinus officinalis",
    "summary": "  Endophytes are tiny organisms present in living tissues of distinct plants\nand have been extensively studied for their endophytic microbial complement.\nRoots of Rosmarinus officinalis were subjected to the isolation of endophytic\nfungi and screened for antimicrobial activity against Gram-positive\n(Staphylococcus aureus and Bacillus subtilis) and Gram-negative (Escherichia\ncoli, Pseudomonas aeruginosa, Klebsiella pneumoniae) bacteria. Genomic DNA from\nactive fungal strain of Trichoderma harzianum was isolated, and the internal\ntranscribed spacer (ITS) region was amplified using ITS4 and ITS5 primers and\nsequenced for genetic inference in fungus. The crude extract of T. harzianum\nisolate with Ethyl acetate was showed significant antimicrobial activity\nagainst P. aeruginosa, S. aureus, K. pneumonia, B. subtilis and E. coli. The\nantimicrobial activity was highest against P. aeruginosa at concentration of 40\nmicrogram/ ml, followed by S. aureus and K. pneumonia at the same\nconcentration. The lowest antimicrobial activity was against by S. aureus at\nconcentration of 60 microgram/ ml. The current study is confirmed that the\nantimicrobial activity is due to bioactive compounds founded in endophytic\nfungi.\n"
  },
  {
    "year": 2023,
    "title": "Molecular characterization of wild Pleurotus ostreatus (MW457626) and\n  evaluation of $\u03b2$-glucans polysaccharide activities",
    "summary": "  Pleurotus ostreatus is a common cultivated edible mushroom worldwide. The\nfruiting bodies of P. ostreatus is a rich source of a $\\beta$-glucans\npolysaccharide. The current study aimed to investigate the effectiveness of\n$\\beta$-glucans as a natural polysaccharide produced by P. ostreatus as an\nantioxidant, antimicrobial, and anticancer. The molecular identification of P.\nostreatus isolate was confirmed by Internal Transcribed Spacer (ITS) sequence.\nThe sequence alignment and phylogenetic evolutionary relationship of studied\nITS sequence were performed against some deposited sequences in GenBank. The\nanalysis of high-performance liquid chromatography (HPLC) as well as the result\nof fourier transform infrared spectroscopy (FTIR) has confirmed the presence of\n$\\beta$-glucans polysaccharide in the tested samples. The percentage of\nantioxidant activity of $\\beta$-glucans showed a gradual increase from 8.59% to\n12.36, 18.56, 23.69, 44.66 and 80.36% at the concentrations of 31.2, 64.4, 125,\n250, 500, and 800 $\\mu$g/ml, respectively. In addition, all concentrations of\n$\\beta$-glucans showed higher antioxidant activities when compared with\nstandard antioxidant (Vitamin C). The highest antimicrobial activity of\n$\\beta$-glucans polysaccharide was against P. aeruginosa with a zone of\ninhibition (45 mm), while the lowest activity was against S. aureus (13 mm)\nboth at 100 mg/mL. The percentage of growth-inhibiting of MCF-7 a humanbreast\ncancer cell line and normal WRL-68 cell line affected by $\\beta$-glucans were\ndetermined by 3-(4,5)-dimethylthiazol (-z-y1)-3,5-di-phenytetrazoliumromide\n(MTT assay).\n"
  },
  {
    "year": 2023,
    "title": "A primer on correlation-based dimension reduction methods for\n  multi-omics analysis",
    "summary": "  The continuing advances of omic technologies mean that it is now more\ntangible to measure the numerous features collectively reflecting the molecular\nproperties of a sample. When multiple omic methods are used, statistical and\ncomputational approaches can exploit these large, connected profiles.\nMulti-omics is the integration of different omic data sources from the same\nbiological sample. In this review, we focus on correlation-based dimension\nreduction approaches for single omic datasets, followed by methods for pairs of\nomics datasets, before detailing further techniques for three or more omic\ndatasets. We also briefly detail network methods when three or more omic\ndatasets are available and which complement correlation-oriented tools. To aid\nreaders new to this area, these are all linked to relevant R packages that can\nimplement these procedures. Finally, we discuss scenarios of experimental\ndesign and present road maps that simplify the selection of appropriate\nanalysis methods. This review will guide researchers navigate the emerging\nmethods for multi-omics and help them integrate diverse omic datasets\nappropriately and embrace the opportunity of population multi-omics.\n"
  },
  {
    "year": 2023,
    "title": "EGC: a format for expressing prokaryotic genomes content expectations",
    "summary": "  The number of available genomes of prokaryotic organisms is rapidly growing\nenabling comparative genomics studies. The comparison of genomes of organisms\nwith a common phenotype, habitat or phylogeny often shows that these genomes\nshare some common contents. Collecting rules expressing common genome traits\ndepending on given factors is useful, as such rules could be used for quality\ncontrol or for identifying interesting exceptions and formulating hypothesis.\nAutomatizing the rules verification using computation tools requires the\ndefinition of a representation schema. In this study, we present EGC (Expected\nGenome Contents), a flat-text file format for the representation of expectation\nrules about the content of prokaryotic genomes. A parser for the EGC format has\nbeen implemented using the TextFormats software library, accompanied by a set\nof related Python packages.\n"
  },
  {
    "year": 2023,
    "title": "PhaBOX: A web server for identifying and characterizing phage contigs in\n  metagenomic data",
    "summary": "  Motivation: There is accumulating evidence showing the important roles of\nbacteriophages (phages) in regulating the structure and functions of the\nmicrobiome. However, lacking an easy-to-use and integrated phage analysis\nsoftware hampers microbiome-related research from incorporating phages in the\nanalysis. Results: In this work, we developed a web server, PhaBOX, which can\ncomprehensively identify and analyze phage contigs in metagenomic data. It\nsupports integrated phage analysis, including phage contig identification from\nthe metagenomic assembly, lifestyle prediction, taxonomic classification, and\nhost prediction. Instead of treating the algorithms as a black box, PhaBOX also\nsupports visualization of the essential features for making predictions. The\nweb server is designed with a user-friendly graphical interface that enables\nboth informatics-trained and non-specialist users to analyze phages in\nmicrobiome data with ease. Availability: The web server of PhaBOX is available\nvia: https://phage.ee.cityu.edu.hk. The source code of PhaBOX is available at:\nhttps://github.com/KennthShang/PhaBOX Contact: yannisun@cityu.edu.hk\n"
  },
  {
    "year": 2023,
    "title": "Prediction of cancer driver genes and mutations: the potential of\n  integrative computational frameworks",
    "summary": "  The vast amount of sequencing data presently available allow the scientific\ncommunity to explore a range of genetic variables that may drive and progress\ncancer. A myriad of predictive tools has been proposed, allowing researchers\nand clinicians to compare and prioritize driver genes and mutations and their\nrelative pathogenicity. However, there is little consensus on the computational\napproach or a golden standard for comparison. Hence, benchmarking the different\ntools depends highly on the input data, indicating that overfitting is still a\nmassive problem. One of the solutions is to limit the scope and usage of\nspecific tool. However, such limitations forces researchers to walk on a\ntightrope between creating and using high-quality tools for a specific purpose\nand describing the complex alterations driving cancer. While the knowledge of\ncancer development increases every day, many bioinformatic pipelines rely on\nsingle nucleotide variants or alterations in a vacuum without accounting for\ncellular compartment, mutational burden, or disease progression. Even within\nbioinformatics and computational cancer biology, the research fields work in\nsilos, risking overlooking potential synergies or breakthroughs. Here, we\nprovide an overview of databases and datasets for building or testing\npredictive tools for discovery of cancer drivers. We introduce predictive tools\nfor driver genes, driver mutations, and the impact of these based on structural\nanalysis. Additionally, we suggest and recommend directions in the field to\navoid silo-research, moving in the direction of integrative frameworks.\n"
  },
  {
    "year": 2023,
    "title": "Buffalo Genome Projects: Current Situation and Future Perspective in\n  Improving Breeding Programs",
    "summary": "  Buffaloes are farm animals that contribute to food security by providing high\nquality meat and milk. They can better tolerate the adverse effects of global\nclimate change on their meat and milk production. Despite their advantages,\nbuffaloes are heavily neglected animals with fewer studies compared to other\nfarm animals, hence, the real potential of buffaloes has never been realized.\nThe complete genome sequencing projects of buffaloes are essential to better\nunderstanding the buffalos biology and production since they allow scientists\nto identify important genes and understand how the gene networks interact to\ndetermine the critical features of buffaloes. The genome projects are also\nvaluable for gaining better knowledge of growth, development, maintenance, and\ndetermining factors associated with increased meat and milk production.\nFurthermore, having access to a complete genome of high quality and\ncomprehensive annotations provides a powerful tool in breeding programs. The\ncurrent review surveyed the publicly available buffalo genome projects and\nstudied the impact of incorporating genomic selection into the buffalo breeding\nprogram. Our survey of the publicly available buffalo genome projects showed\nthe promise of genomic selection in developing water buffalo science and\ntechnology for food security on a global scale.\n"
  },
  {
    "year": 2023,
    "title": "MIK2 is a candidate gene of the S-locus for sporophytic\n  self-incompatibility (SSI) in chicory (Cichorium intybus, Asteraceae)",
    "summary": "  The Cichorium genus offers a unique opportunity to study the sporophytic self\nincompatibility (SSI) system, being composed of species characterized by highly\nefficient SI (C. intybus) and complete self compatibility (C. endivia). The\nchicory genome was used to map 7 previously identified SSI locus-associated\nmarkers. The region containing the S locus was restricted to an 4 M bp window\non chromosome 5. Among the genes predicted in this region, MDIS1 INTERACTING\nRECEPTOR LIKE KINASE 2 (MIK2) was promising as a candidate for SSI. Its\northolog in Arabidopsis is involved in pollen stigma recognition reactions, and\nits protein structure is similar to that of S-receptor kinase (SRK), a key\ncomponent of the SSI in the Brassica genus. The sequencing of MIK2 in chicory\nand endive accessions revealed two contrasting scenarios. In C. endivia, MIK2\nwas fully conserved even comparing different botanical varieties (smooth and\ncurly). In C. intybus, 387 SNPs and 3 INDELs were identified when comparing\naccessions of different biotypes from the same botanical variety (radicchio).\nThe SNP distribution throughout the gene was uneven, with hypervariable domains\npreferentially localized in the LRR-rich extracellular region, putatively\nidentified as the receptor domain. The gene was hypothesized to be under\npositive selection, as the nonsynonymous mutations were more than double the\nsynonymous ones (dN / dS = 2.17). An analogous situation was observed analyzing\nthe first 500 bp of the MIK2 promoter: no SNPs were observed among the endive\nsamples, whereas 44 SNPs and 6 INDELs were detected among the chicory samples.\nFurther analyses are needed to confirm the role of MIK2 in SSI and to\ndemonstrate whether the 23 species-specific nonsynonymous SNPs in the CDS\nand/or the species-specific 10 bp INDEL found in a CCAAT box region of the\npromoter are responsible for the contrasting sexual behaviors of the two\nspecies.\n"
  },
  {
    "year": 2023,
    "title": "A Method for Improving the Detection Accura-cy of MSIsensor Based on\n  Downsampling",
    "summary": "  Motivation: Microsatellite instability (MSI) is a cancer biomarker associated\nwith cancer prognosis and chemotherapy sensitivity. Since the discovery of MSI,\npolymerase chain reaction (PCR)-based testing has been considered the gold\nstandard for MSI detection. However, with the decrease in sequencing costs,\nsoftware that calculates MSI based on next-generation sequencing (NGS) data has\nbeen widely applied. Results: In this study, we evaluated the performance of\nthe MSIsensor detection software, focusing on the limitations of the chi-square\ntest algorithm in determining microsatellite stability under high-depth\nsequencing data. We demonstrated that the chi-square test algorithm is\ninsufficient for accurately as-sessing microsatellite stability in this\ncontext. Furthermore, we explored the application of downsampling techniques to\nenhance the accuracy of MSIsensor detection. Our findings provide insight into\nthe limita-tions of current methods and offer potential improvements for more\nreliable MSI detection based on NGS data.\n"
  },
  {
    "year": 2023,
    "title": "De novo reconstruction of satellite repeat units from sequence data",
    "summary": "  Satellite DNA are long tandemly repeating sequences in a genome and may be\norganized as high-order repeats (HORs). They are enriched in centromeres and\nare challenging to assemble. Existing algorithms for identifying satellite\nrepeats either require the complete assembly of satellites or only work for\nsimple repeat structures without HORs. Here we describe Satellite Repeat Finder\n(SRF), a new algorithm for reconstructing satellite repeat units and HORs from\naccurate reads or assemblies without prior knowledge on repeat structures.\nApplying SRF to real sequence data, we showed that SRF could reconstruct known\nsatellites in human and well-studied model organisms. We also found satellite\nrepeats are pervasive in various other species, accounting for up to 12% of\ntheir genome contents but are often underrepresented in assemblies. With the\nrapid progress on genome sequencing, SRF will help the annotation of new\ngenomes and the study of satellite DNA evolution even if such repeats are not\nfully assembled.\n"
  },
  {
    "year": 2023,
    "title": "Advancing regulatory genomics with machine learning",
    "summary": "  In recent years, several machine learning approaches have been proposed to\npredict gene expression and epigenetic signals from the DNA sequence alone.\nThese models are often used to deduce, and, to some extent, assess putative new\nbiological insights about gene regulation, and they have led to very\ninteresting advances in regulatory genomics. This article reviews a selection\nof these methods, ranging from linear models to random forests, kernel methods,\nand more advanced deep learning models. Specifically, we detail the different\ntechniques and strategies that can be used to extract new gene-regulation\nhypotheses from these models. Furthermore, because these putative insights need\nto be validated with wet-lab experiments, we emphasize that it is important to\nhave a measure of confidence associated with the extracted hypotheses. We\nreview the procedures that have been proposed to measure this confidence for\nthe different types of machine learning models, and we discuss the fact that\nthey do not provide the same kind of information.\n"
  },
  {
    "year": 2023,
    "title": "Representing and extracting knowledge from single cell data",
    "summary": "  Single-cell analysis is currently one of the most high-resolution techniques\nto study biology. The large complex datasets that have been generated have\nspurred numerous developments in computational biology, in particular the use\nof advanced statistics and machine learning. This review attempts to explain\nthe deeper theoretical concepts that underpin current state-of-the-art analysis\nmethods. Single-cell analysis is covered from cell, through instruments, to\ncurrent and upcoming models. A minimum of mathematics and statistics has been\nused, but the reader is assumed to either have basic knowledge of single-cell\nanalysis workflows, or have a solid knowledge of statistics. The aim of this\nreview is to spread concepts which are not yet in common use, especially from\ntopology and generative processes, and how new statistical models can be\ndeveloped to capture more of biology. This opens epistemological questions\nregarding our ontology and models, and some pointers will be given to how\nnatural language processing (NLP) may help overcome our cognitive limitations\nfor understanding single-cell data.\n"
  },
  {
    "year": 2023,
    "title": "SinglePointRNA, an user-friendly application implementing single cell\n  RNA-seq analysis software",
    "summary": "  Single-cell transcriptomics techniques, such as scRNA-seq, attempt to\ncharacterize gene expression profiles in each cell of a heterogeneous sample\nindividually. Due to growing amounts of data generated and the increasing\ncomplexity of the computational protocols needed to process the resulting\ndatasets, the demand for dedicated training in mathematical and programming\nskills may preclude the use of these powerful techniques by many teams.\n  In order to help close that gap between wet-lab and dry-lab capabilities we\nhave developed SinglePointRNA, a shiny-based R application that provides a\ngraphic interface for different publicly available tools to analyze single cell\nRNA-seq data.\n  The aim of SinglePointRNA is to provide an accessible and transparent tool\nset to researchers that allows them to perform detailed and custom analysis of\ntheir data autonomously. SinglePointRNA is structured in a context-driven\nframework that prioritizes providing the user with solid qualitative guidance\nat each step of the analysis process and interpretation of the results.\nAdditionally, the rich user guides accompanying the software are intended to\nserve as a point of entry for users to learn more about computational\ntechniques applied to single cell data analysis.\n  The SinglePointRNA app, as well as case datasets for the different tutorials\nare available at www.github.com/ScienceParkMadrid/SinglePointRNA\n"
  },
  {
    "year": 2023,
    "title": "Prokaryotic genome editing based on the subtype I-B-Svi CRISPR-Cas\n  system",
    "summary": "  Type I CRISPR-Cas systems are the most common among six types of CRISPR-Cas\nsystems, however, non-self-targeting genome editing based on a single Cas3 of\ntype I CRISPR-Cas systems has not been reported. Here, we present the subtype\nI-B-Svi CRISPR-Cas system (with three confirmed CRISPRs and a cas gene cluster)\nand genome editing based on this system found in Streptomyces virginiae IBL14.\nImportantly, like the animal-derived bacterial protein SpCas9 (1368\namino-acids), the single, compact, non-animal-derived bacterial protein SviCas3\n(771 amino-acids) can also direct template-based microbial genome editing\nthrough the target cell's own homology-directed repair system, which breaks the\nview that the genome editing based on type I CRISPR-Cas systems requires a full\nCascade. Notably, no off-target changes or indel-formation were detected in the\nanalysis of potential off-target sites. This discovery broadens our\nunderstanding of the diversity of type I CRISPR-Cas systems and will facilitate\nnew developments in genome editing tools.\n"
  },
  {
    "year": 2023,
    "title": "Template-based eukaryotic genome editing directed by SviCas3",
    "summary": "  RNA-guided gene editing based on the CRISPR-Cas system is currently the most\neffective genome editing technique. Here, we report that the SviCas3 from the\nsubtype I-B-Svi Cas system in Streptomyces virginiae IBL14 is an RNA-guided and\nDNA-guided DNA endonuclease suitable for the HDR-directed gene and/or base\nediting of eukaryotic cell genomes. The genome editing efficiency of SviCas3\nguided by DNA is no less than that of SviCas3 guided by RNA. In particular,\nt-DNA, as a template and a guide, does not require a proto-spacer-adjacent\nmotif, demonstrating that CRISPR, as the basis for crRNA design, is not\nrequired for the SviCas3-mediated gene and base editing. This discovery will\nbroaden our understanding of enzyme diversity in CRISPR-Cas systems, will\nprovide important tools for the creation and modification of living things and\nthe treatment of human genetic diseases, and will usher in a new era of\nDNA-guided gene editing and base editing.\n"
  },
  {
    "year": 2023,
    "title": "The evolution of next-generation sequencing technologies",
    "summary": "  The genetic information that dictates the structure and function of all life\nforms is encoded in the DNA. In 1953, Watson and Crick first presented the\ndouble helical structure of a DNA molecule. Their findings unearthed the desire\nto elucidate the exact composition and sequence of DNA molecules. Discoveries\nand the subsequent development and optimization of techniques that allowed for\ndeciphering the DNA sequence has opened new doors in research, biotech, and\nhealthcare. The application of high-throughput sequencing technologies in these\nindustries has positively impacted and will continue to contribute to the\nbetterment of humanity and the global economy. Improvements, such as the use of\nradioactive molecules for DNA sequencing to the use of florescent dyes and the\nimplementation of polymerase chain reaction (PCR) for amplification, led to\nsequencing a few hundred base pairs in days, to automation, where sequencing of\nthousands of base pairs in hours became possible. Significant advances have\nbeen made, but there is still room for improvement. Here, we look at the\nhistory and the technology of the currently available next-generation\nsequencing platforms and the possible applications of such technologies to\nbiomedical research and beyond.\n"
  },
  {
    "year": 2023,
    "title": "New Sequence Alignment Algorithm using AI Rules and Dynamic Seeds",
    "summary": "  DNA sequence alignment is important today as it is usually the first step in\nfinding gene mutation, evolutionary similarities, protein structure, drug\ndevelopment and cancer treatment. Covid-19 is one recent example. There are\nmany sequencing algorithms developed over the past decades but the sequence\nalignment using expert systems is quite new. To find DNA sequence alignment,\ndynamic programming was used initially. Later faster algorithms used small DNA\nsequence length of fixed size to find regions of similarity, and then build the\nfinal alignment using these regions. Such systems were not sensitive but were\nfast. To improve the sensitivity, we propose a new algorithm which is based on\nfinding maximal matches between two sequences, find seeds between them, employ\nrules to find more seeds of varying length, and then employ a new stitching\nalgorithm, and weighted seeds to solve the problem\n"
  },
  {
    "year": 2023,
    "title": "pgMAP: a pipeline to enable guide RNA read mapping from dual-targeting\n  CRISPR screens",
    "summary": "  We developed pgMAP, an analysis pipeline to map gRNA sequencing reads from\ndual-targeting CRISPR screens. pgMAP output includes a dual gRNA read counts\ntable and quality control metrics including the proportion of correctly-paired\nreads and CRISPR library sequencing coverage across all time points and\nsamples. pgMAP is implemented using Snakemake and is available open-source\nunder the MIT license at https://github.com/fredhutch/pgmap_pipeline.\n"
  },
  {
    "year": 2023,
    "title": "A FAIR platform for reproducing mutational signature detection on tumor\n  sequencing data",
    "summary": "  This paper presents a portable, privacy-preserving, in-browser platform for\nthe reproducible assessment of mutational signature detection methods from\nsparse sequencing data generated by targeted gene panels. The platform aims to\naddress the reproducibility challenges in mutational signature research by\nadhering to the FAIR principles, making it findable, accessible, interoperable,\nand reusable. Our approach focuses on the detection of specific mutational\nsignatures, such as SBS3, which have been linked to specific mutagenic\nprocesses. The platform relies on publicly available data, simulation,\ndownsampling techniques, and machine learning algorithms to generate training\ndata and labels and to train and evaluate models. The key achievement of our\nplatform is its transparency, reusability, and privacy preservation, enabling\nresearchers and clinicians to analyze mutational signatures with the guarantee\nthat no data circulates outside the client machine.\n"
  },
  {
    "year": 2023,
    "title": "Scalable telomere-to-telomere assembly for diploid and polyploid genomes\n  with double graph",
    "summary": "  Despite recent advances in the length and the accuracy of long-read data,\nbuilding haplotype-resolved genome assemblies from telomere to telomere still\nrequires considerable computational resources. In this study, we present an\nefficient de novo assembly algorithm that combines multiple sequencing\ntechnologies to scale up population-wide telomere-to-telomere assemblies. By\nutilizing twenty-two human and two plant genomes, we demonstrate that our\nalgorithm is around an order of magnitude cheaper than existing methods, while\nproducing better diploid and haploid assemblies. Notably, our algorithm is the\nonly feasible solution to the haplotype-resolved assembly of polyploid genomes.\n"
  },
  {
    "year": 2023,
    "title": "Collection of prokaryotic genome contents expectation rules from\n  scientific literature",
    "summary": "  Shaped by natural selection and other evolutionary forces, an organism's\nevolutionary history is reflected through its genome sequence, content of\nfunctional elements and organization. Consequently, organisms connected through\nphylogeny, metabolic or morphological traits, geographical proximity, or\nhabitat features are likely to exhibit similarities in their genomes. These\nsimilarities give rise to expectations about the content of genomes within\nthese organism groups.\n  Such expectations are often informally expressed in scientific literature,\nfocusing on the analysis of individual genomes or comparisons among related\ngroups of organisms. Our objective is to develop a system for formalized\nexpectations as rules, facilitating automated verification, and evaluation of\nnewly sequenced genomes.\n  In this study, we present a database comprising rules manually extracted from\nscientific literature. Furthermore, we explore the feasibility of automatizing\nthe extraction and analysis process using large language models, such as GPT3.5\nand GPT4.\n  We have developed a web application, EGCWebApp, which enables users to\nvisualize and edit the rules. Additionally, we provided a Python library and\ncommand-line tools collection, egctools, to further extend the functionality\nfor processing and managing these rules.\n"
  },
  {
    "year": 2023,
    "title": "A Novel Approach to Encode Two-Way Epistatic Interactions Between Single\n  Nucleotide Polymorphisms",
    "summary": "  Modelling gene-gene epistatic interactions when computing genetic risk scores\nis not a well-explored subfield of genetics and could have potential to improve\nrisk stratification in practice. Though applications of machine learning (ML)\nshow promise as an avenue of improvement for current genetic risk assesments,\nthey frequently suffer from the problem of two many features and to little\ndata. We propose a method that when combined with ML allows information from\nindividual genetic contributors to be preserved while incorporating information\non their interactions in a single feature. This allows second-order analysis,\nwhile simultaneously increasing the number of input features to ML models as\nlittle as possible. We presented three methods that can be utilized to account\nfor genetic interactions. We found that interaction methods that preserved\ninformation from the constituent SNPs performed significantly better than the\nsimplest interaction method. Since the currently available ML methods are able\nto account for complex interactions, utilizing raw SNP genotypes alone is\nsufficient because the simplest model outperforms all the interaction methods\nGiven that understanding and accounting for epistatic interactions is one of\nthe most promising avenues for increasing explained variability in heritable\ndisease, this work represents a first step toward an algorithmic interaction\nmethod that preserves the information in each component. This is relevant not\nonly because of potential improvements in model quality, but also because\nexplicit interaction terms allow a human readable interpretation of potential\ninteraction pathways within the disease.\n"
  },
  {
    "year": 2023,
    "title": "Prognostic Biomarker Identification for Pancreatic Cancer by Analyzing\n  Multiple mRNA Microarray and microRNA Expression Datasets",
    "summary": "  Possessing the five-year durability rate of nearly 5%, currently, the fourth\nleading cause for cancer-related deaths is pancreatic cancer. Previously,\nseveral works have resolved that early diagnosis performs a meaningful function\nin enhancing the durability rate and diverse online tools have been utilized to\ndistinguish prognostic biomarker which is a lengthy process. We believe that\nthe statistical feature selection method can produce a better and faster result\nhere. To authenticate our statement, we picked three different mRNA microarray\n(GSE15471, GSE28735, and GSE16515) and a microRNA (GSE41372) dataset for\nidentification of differentially expressed genes (DEGs) and differentially\nexpressed microRNAs (DEMs). By adopting some feature selecting methods, 178\nDEGs and 16 DEMs were elected. After identifying target genes of DEMs, we\nselected two DEGs (ECT2 and NRP2) which were also identified among DEMs target\ngenes. Moreover, overall durability report established that ECT2 and NRP2 were\nassociated with poor overall survival. Hence, we concluded that for pancreatic\ncancer, statistical feature selection approaches certainly perform better for\nbiomarker identification than pre-defined online programs, and here, ECT2 and\nNRP2 can act as possible prognostic biomarkers. All the resources, programs and\nsnippets of our literature can be discovered at\nhttps://github.com/Srizon143005/PancreaticCancerBiomarkers.\n"
  },
  {
    "year": 2023,
    "title": "SumVg: Total heritability explained by all variants in genome-wide\n  association studies based on summary statistics with standard error estimates",
    "summary": "  Genome-wide association studies (GWAS) are commonly employed to study the\ngenetic basis of complex traits and diseases, and a key question is how much\nheritability could be explained by all variants in GWAS. One widely used\napproach that relies on summary statistics only is LD score regression (LDSC),\nhowever the approach requires certain assumptions on the SNP effects (all SNPs\ncontribute to heritability and each SNP contributes equal variance). More\nflexible modeling methods may be useful. We previously developed an approach\nrecovering the true z-statistics from a set of observed z-statistics with an\nempirical Bayes approach, using only summary statistics. However, methods for\nstandard error (SE) estimation are not available yet, limiting the\ninterpretation of results and applicability of the approach. In this study we\ndeveloped several resampling-based approaches to estimate the SE of SNP-based\nheritability, including two jackknife and three parametric bootstrap methods.\nSimulations showed that delete-d-jackknife and parametric bootstrap approaches\nprovide good estimates of the SE. Particularly, the parametric bootstrap\napproaches yield the lowest root-mean-squared-error (RMSE) of the true SE. In\naddition, we applied our method to estimate SNP-based heritability of 12\nimmune-related traits (levels of cytokines and growth factors) to shed light on\ntheir genetic architecture. We also implemented the methods to compute the sum\nof heritability explained and the corresponding SE in an R package SumVg,\navailable at https://github.com/lab-hcso/Estimating-SE-of-total-heritability/ .\nIn conclusion, SumVg may provide a useful alternative tool for SNP heritability\nand SE estimates, which does not rely on distributional assumptions of SNP\neffects.\n"
  },
  {
    "year": 2023,
    "title": "Comparative study of under-expressed prognostic biomarkers and pivotal\n  signaling pathways in colon cancer and ulcerative colitis using integrated\n  bioinformatics approach",
    "summary": "  Colon cancer is a prevalent gastrointestinal malignancy arising in the colon.\nUlcerative colitis(UC) is one of the risk factors of colorectal cancer. The\ndetection of under-expressed biomarkers and molecular mechanisms in UC and\ncolon cancer can lead to effective management of colitis-associated cancer. A\ntotal of two mRNA expression datasets (GSE87473 and GSE44076) were downloaded\nfrom the Gene Expression Omnibus (GEO) database. GEO2R was used to screen\ndifferentially expressed genes (DEGs) between extensive ulcerative colitis\nsamples and healthy samples, limited ulcerative colitis samples and healthy\nsamples, and colon cancer samples and healthy samples. In extensive ulcerative\ncolitis, limited ulcerative colitis and colon cancer groups, 95,69 and 635\nunder-expressed genes with adjusted p-value<0.05 and log(2) fold change<-2 were\ndetected respectively. Using Cytoscape software, the genes with degree> 15\nincluding CLCA1, SLC26A3, SI, KIT, HPGDS, NR1H4, ADIPOQ, PPARGC1A, GCG, MS4A12,\nGUCA2A and FABP1 were screened as hub under-expressed genes in colon cancer. In\nextensive ulcerative colitis, the genes with degree>5 including ABCB1, ABCG2,\nUGT1A6, CYP2B6 and AQP8 were identified as hub genes. Moreover, the genes\nincluding NR1H4, CYP2B6, ABCB1, ABCG2, UGT2A3 and PLA2G12B were detected as hub\ngenes with degree>5 in limited ulcerative colitis. According to inclusion\ncriteria and venn diagram, the downregulated gene NR1H4 was common gene in\nlimited ulcerative colitis and colon cancer. The current in silico study showed\nthat downregulation of CLCA1, PPARGC1A and AQP8 genes may increase cancer cell\ninvasion and metastasis ability.\n"
  },
  {
    "year": 2023,
    "title": "Root Causal Inference from Single Cell RNA Sequencing with the Negative\n  Binomial",
    "summary": "  Accurately inferring the root causes of disease from sequencing data can\nimprove the discovery of novel therapeutic targets. However, existing root\ncausal inference algorithms require perfectly measured continuous random\nvariables. Single cell RNA sequencing (scRNA-seq) datasets contain large\nnumbers of cells but non-negative counts measured by an error prone process. We\ntherefore introduce an algorithm called Root Causal Inference with Negative\nBinomials (RCI-NB) that accounts for count-based measurement error by\nseparating negative binomial distributions into their gamma and Poisson\ncomponents; the gamma distributions form a fully identifiable but latent post\nnon-linear causal model representing the true RNA expression levels, which we\nonly observe with Poisson corruption. RCI-NB identifies patient-specific root\ncausal contributions from scRNA-seq datasets by integrating novel sparse\nregression and goodness of fit testing procedures that bypass Poisson\nmeasurement error. Experiments demonstrate significant improvements over\nexisting alternatives.\n"
  },
  {
    "year": 2023,
    "title": "ProSt: computing, storing and visualizing attributes of prokaryotic\n  genomes",
    "summary": "  Prokaryotic organisms usually possess compact genomes, which are particularly\nsuitable to complete sequencing with existing technologies, which led to an\nescalating accumulation of available genome data. In response to this\never-expanding repository of information, we introduce ProSt, a computational\nsystem designed for the batch computation, storage, and interactive\nvisualization of the values of attributes of prokaryotic genomes. The system\nallows for parallel attribute value batch computation, dynamically designed to\nincrementally integrate new attribute values as additional genomes become\navailable.\n  ProSt is flexible permitting the definition of attributes by implementing\nattribute value computation plugins, supporting several languages (Python, Nim,\nRust and Bash). This allows the system to continually evolve in accordance with\nchanging research needs and developments. Additionally, our computation and\nstorage systems maintain comprehensive metadata, thereby enabling data\nprovenance tracking for the computed attribute values.\n"
  },
  {
    "year": 2023,
    "title": "Somatic mutations in human ageing: New insights from DNA sequencing and\n  inherited mutations",
    "summary": "  The accumulation of somatic mutations is a driver of cancer and has long been\nassociated with ageing. Due to limitations in quantifying mutation burden with\nage in non-cancerous tissues, the impact of somatic mutations in other ageing\nphenotypes is unclear. Recent advances in DNA sequencing technologies have\nallowed the large-scale quantification of somatic mutations in ageing. These\nstudies have revealed a gradual accumulation of mutations in most normal\ntissues with age as well as a substantial clonal expansion driven mostly by\ncancer-related mutations. Nevertheless, because of the relatively modest burden\nof age-related somatic mutations identified so far and their stochastic nature,\nit is difficult to envision how somatic mutation accumulation alone can explain\nmost ageing phenotypes that develop gradually. Studies across species have also\nfound that longer-lived species have lower somatic mutation rates, though these\ncould be explained by selective pressures to reduce or postpone cancer as\nlongevity increases. Overall, with a few exceptions like cancer, results from\nrecent DNA sequencing studies do not add weight to the idea that somatic\nmutations with age drive ageing phenotypes and the phenotypic role, if any, of\nsomatic mutations in ageing remains unclear. Recent studies in patients with\nsomatic mutation burden and no signs of accelerated ageing further question the\nrole of somatic mutations in ageing.\n"
  },
  {
    "year": 2023,
    "title": "Enhancing Cell Proliferation and Migration by MIR-Carbonyl Vibrational\n  Coupling: Insights from Transcriptome Profiling",
    "summary": "  Cell proliferation and migration highly relate to normal tissue self-healing,\ntherefore it is highly significant for artificial controlling. Recently,\nvibrational strong coupling between biomolecules and Mid-infrared (MIR) light\nphotons has been successfully used to modify in vitro bioreactions, neuronal\nsignaling and even animal behavior. However, the synergistic effects from\nmolecules to cells remains unclear, and the regulation of MIR on cells needs to\nbe explained from the molecular level. Herein, the proliferation rate and\nmigration capacity of fibroblasts were increased by 156% and 162.5%,\nrespectively, by vibratory coupling of 5.6 micrometers photons with carbonyl\ngroups in biomolecules. Through transcriptome sequencing analysis, the\nregulatory mechanism of infrared light in 5.6 micrometers was explained from\nthe level of signal pathway and cell components. 5.6 micrometers optical high\npower lasers can regulate cell function through vibrational strong coupling\nwhile minimizing photothermal damage. This work not only sheds light on the\nnon-thermal effect on MIR light-based on wound healing, but also provides new\nevidence to future frequency medicine.\n"
  },
  {
    "year": 2023,
    "title": "mSigSDK -- private, at scale, computation of mutation signatures",
    "summary": "  In our previous work, we demonstrated that it is feasible to perform analysis\non mutation signature data without the need for downloads or installations and\nanalyze individual patient data at scale without compromising privacy. Building\non this foundation, we developed a Software Development Kit (SDK) called\nmSigSDK to facilitate the orchestration of distributed data processing\nworkflows and graphic visualization of mutational signature analysis results.\nWe strictly adhered to modern web computing standards, particularly the\nmodularization standards set by the ECMAScript ES6 framework (JavaScript\nmodules). Our approach allows for computation to be entirely performed by\nsecure delegation to the computational resources of the user's own machine\n(in-browser), without any downloads or installations. The mSigSDK was developed\nprimarily as a companion library to the mSig Portal resource of the National\nCancer Institute Division of Cancer Epidemiology and Genetics (NIH/NCI/DCEG),\nwith a focus on its FAIR extensibility as components of other researchers'\ncomputational constructs. Anticipated extensions include the programmatic\noperation of other mutation signature API ecosystems such as SIGNAL and COSMIC,\nadvancing towards a data commons for mutational signature research (Grossman et\nal., 2016).\n"
  },
  {
    "year": 2023,
    "title": "Genome assembly in the telomere-to-telomere era",
    "summary": "  De novo assembly is the process of reconstructing the genome sequence of an\norganism from sequencing reads. Genome sequences are essential to biology, and\nassembly has been a central problem in bioinformatics for four decades. Until\nrecently, genomes were typically assembled into fragments of a few megabases at\nbest but technological advances in long-read sequencing now enable near\ncomplete chromosome-level assembly, also known as telomere-to-telomere\nassembly, for many organisms. Here we review recent progress on assembly\nalgorithms and protocols. We focus on how to derive near telomere-to-telomere\nassemblies and discuss potential future developments.\n"
  },
  {
    "year": 2023,
    "title": "Genomic reproducibility in the bioinformatics era",
    "summary": "  In biomedical research, validation of a new scientific discovery is tied to\nthe reproducibility of its experimental results. However, in genomics, the\ndefinition and implementation of reproducibility still remain imprecise. Here,\nwe argue that genomic reproducibility, defined as the ability of bioinformatics\ntools to maintain consistent genomics results across technical replicates, is\nkey to generating scientific knowledge and enabling medical applications. We\nfirst discuss different concepts of reproducibility and then focus on\nreproducibility in the context of genomics, aiming to establish clear\ndefinitions of relevant terms. We then focus on the role of bioinformatics\ntools and their impact on genomic reproducibility and assess methods of\nevaluating bioinformatics tools in terms of genomic reproducibility. Lastly, we\nsuggest best practices for enhancing genomic reproducibility, with an emphasis\non assessing the performance of bioinformatics tools through rigorous testing\nacross multiple technical replicates.\n"
  },
  {
    "year": 2023,
    "title": "SARS-CoV-2 Wastewater Genomic Surveillance: Approaches, Challenges, and\n  Opportunities",
    "summary": "  During the SARS-CoV-2 pandemic, wastewater-based genomic surveillance (WWGS)\nemerged as an efficient viral surveillance tool that takes into account\nasymptomatic cases and can identify known and novel mutations and offers the\nopportunity to assign known virus lineages based on the detected mutations\nprofiles. WWGS can also hint towards novel or cryptic lineages, but it is\ndifficult to clearly identify and define novel lineages from wastewater (WW)\nalone. While WWGS has significant advantages in monitoring SARS-CoV-2 viral\nspread, technical challenges remain, including poor sequencing coverage and\nquality due to viral RNA degradation. As a result, the viral RNAs in wastewater\nhave low concentrations and are often fragmented, making sequencing difficult.\nWWGS analysis requires advanced computational tools that are yet to be\ndeveloped and benchmarked. The existing bioinformatics tools used to analyze\nwastewater sequencing data are often based on previously developed methods for\nquantifying the expression of transcripts or viral diversity. Those methods\nwere not developed for wastewater sequencing data specifically, and are not\noptimized to address unique challenges associated with wastewater. While\nspecialized tools for analysis of wastewater sequencing data have also been\ndeveloped recently, it remains to be seen how they will perform given the\nongoing evolution of SARS-CoV-2 and the decline in testing and patient-based\ngenomic surveillance. Here, we discuss opportunities and challenges associated\nwith WWGS, including sample preparation, sequencing technology, and\nbioinformatics methods.\n"
  },
  {
    "year": 2023,
    "title": "Categorization and analysis of 14 computational methods for estimating\n  cell potency from single-cell RNA-seq data",
    "summary": "  In single-cell RNA sequencing (scRNA-seq) analysis, a key challenge is\ninferring hidden cellular dynamics from static cell snapshots. Various\ncomputational methods have been developed to address this, focusing on\nperspectives like pseudotime trajectories, RNA velocities, and estimating the\ndifferentiation potential of cells, often referred to as \"cell potency.\" This\nreview summarizes 14 methods for defining cell potency from scRNA-seq data,\ncategorizing them into average-based, entropy-based, and correlation-based\nmethods based on how they summarize gene expression levels into a potency\nmeasure. We highlight the key similarities and differences within and between\nthese categories, offering a high-level intuition for each method.\nAdditionally, we use unified mathematical notations to detail each method's\nmethodology and summarize their usage complexities, including parameters,\nrequired inputs, and differences between published descriptions and software\nimplementations. We conclude that cell potency estimation remains an open\nquestion without a consensus on the optimal approach, emphasizing the need for\nbenchmark datasets and studies. This review aims to provide a foundation for\nfuture benchmark studies, while also addressing the broader challenge of\ncomparing methods that infer cellular dynamics from scRNA-seq data through\nvarious perspectives, including pseudotime trajectories, RNA velocities, and\ncell potency.\n"
  },
  {
    "year": 2023,
    "title": "A rigorous benchmarking of methods for SARS-CoV-2 lineage abundance\n  estimation in wastewater",
    "summary": "  In light of the continuous transmission and evolution of SARS-CoV-2 coupled\nwith a significant decline in clinical testing, there is a pressing need for\nscalable, cost-effective, long-term, passive surveillance tools to effectively\nmonitor viral variants circulating in the population. Wastewater genomic\nsurveillance of SARS-CoV-2 has arrived as an alternative to clinical genomic\nsurveillance, allowing to continuously monitor the prevalence of viral lineages\nin communities of various size at a fraction of the time, cost, and logistic\neffort and serving as an early warning system for emerging variants, critical\nfor developed communities and especially for underserved ones. Importantly,\nlineage prevalence estimates obtained with this approach aren't distorted by\nbiases related to clinical testing accessibility and participation. However,\nthe relative performance of bioinformatics methods used to measure relative\nlineage abundances from wastewater sequencing data is unknown, preventing both\nthe research community and public health authorities from making informed\ndecisions regarding computational tool selection. Here, we perform\ncomprehensive benchmarking of 18 bioinformatics methods for estimating the\nrelative abundance of SARS-CoV-2 (sub)lineages in wastewater by using data from\n36 in vitro mixtures of synthetic lineage and sublineage genomes. In addition,\nwe use simulated data from 78 mixtures of lineages and sublineages co-occurring\nin the clinical setting with proportions mirroring their prevalence ratios\nobserved in real data. Importantly, we investigate how the accuracy of the\nevaluated methods is impacted by the sequencing technology used, the associated\nerror rate, the read length, read depth, but also by the exposure of the\nsynthetic RNA mixtures to wastewater, with the goal of capturing the effects\ninduced by the wastewater matrix, including RNA fragmentation and degradation.\n"
  },
  {
    "year": 2023,
    "title": "GOLEM: distribution of Gene regulatOry eLEMents within the plant\n  promoters",
    "summary": "  Motivation: The regulation of gene expression during tissue development is\nextremely complex. One of the key regulatory mechanisms of gene expression\ninvolves the recognition of regulatory motifs by various proteins in the\npromoter regions of many genes. Localisation of these motifs in proximity to\nthe transcription start site (TSS) or translation start site (ATG) is critical\nfor regulating the initiation and rate of transcription. The levels of\ntranscription of individual genes, regulated by these motifs, can vary signifi\ncantly in diff erent tissues and developmental stages, especially during\ntightly regulated processes such as sexual reproduction. However, the precise\nlocalisation and visualisation of the regulatory motifs within gene promoters\nwith respect to gene transcription in specifi c tissues, can be challenging.\nResults: Here, we introduce a program called GOLEM (Gene regulatOry eLEMents)\nwhich enables users to precisely locate any motif of interest with respect to\nTSS or ATG within the relevant plant genomes across the plant Tree of Life\n(Marchantia, Physcomitrium, Amborella, Oryza, Zea, Solanum and Arabidopsis).\nThe visualisation of the motifs is performed with respect to the transcript\nlevels of particular genes in leaves and male reproductive tissues, and can be\ncompared with genome-wide distribution regardless of the transcription level.\nAvailability and implementation: GOLEM is freely available at\nhttps://golem.ncbr.muni.cz and its source codes are provided under the MIT\nlicence at GitHub at https://github.com/sb-ncbr/golem.\n"
  },
  {
    "year": 2023,
    "title": "Comparative Analysis of Plastid Genomes Using Pangenome Research ToolKit\n  (PGR-TK)",
    "summary": "  Plastid genomes (plastomes) of angiosperms are of great interest among\nbiologists. High-throughput sequencing is making many such genomes accessible,\nincreasing the need for tools to perform rapid comparative analysis. This\nexploratory analysis investigates whether the Pangenome Research Tool Kit\n(PGR-TK) is suitable for analyzing plastomes. After determining the optimal\nparameters for this tool on plastomes, we use it to compare sequences from each\nof the genera - Magnolia, Solanum, Fragaria and Cotoneaster, as well as a\ncombined set from 20 rosid genera. PGR-TK recognizes large-scale plastome\nstructures, such as the inverted repeats, among combined sequences from distant\nrosid families. If the plastid genomes are rotated to the same starting point,\nit also correctly groups different species from the same genus together in a\ngenerated cladogram. The visual approach of PGR-TK provides insights into\ngenome evolution without requiring gene annotations.\n"
  },
  {
    "year": 2023,
    "title": "scX: A user-friendly tool for scRNA-seq exploration",
    "summary": "  Single-cell RNA sequencing (scRNA-seq) has transformed our ability to explore\nbiological systems. Nevertheless, proficient expertise is essential for\nhandling and interpreting the data. In this paper, we present scX, an R package\nbuilt on the Shiny framework that streamlines the analysis, exploration, and\nvisualization of single-cell experiments. With an interactive graphic\ninterface, implemented as a web application, scX provides easy access to key\nscRNAseq analyses, including marker identification, gene expression profiling,\nand differential gene expression analysis. Additionally, scX seamlessly\nintegrates with commonly used single-cell Seurat and SingleCellExperiment R\nobjects, resulting in efficient processing and visualization of varied\ndatasets. Overall, scX serves as a valuable and user-friendly tool for\neffortless exploration and sharing of single-cell data, simplifying some of the\ncomplexities inherent in scRNAseq analysis.\n"
  },
  {
    "year": 2023,
    "title": "Generating site saturation mutagenesis libraries and transferring them\n  to broad host range plasmids using type IIS restriction enzymes",
    "summary": "  Protein engineering is an established method for tailoring enzymatic\nreactivity. A commonly used method is directed evolution, where the mutagenesis\nand natural selection process is mimicked and accelerated in the laboratory.\nHere, we describe a reliable method for generating saturation mutagenesis\nlibraries by golden gate cloning in a broad host range plasmid containing the\npBBR1 replicon. The applicability is demonstrated by generating a mutant\nlibrary of the iron nitrogenase gene cluster (anfHDGK) of Rhodobacter\ncapsulatus, which is subsequently screened for the improved formation of\nmolecular hydrogen.\n"
  },
  {
    "year": 2023,
    "title": "Probing omics data via harmonic persistent homology",
    "summary": "  Identifying molecular signatures from complex disease patients with\nunderlying symptomatic similarities is a significant challenge in the analysis\nof high dimensional multi-omics data. Topological data analysis (TDA) provides\na way of extracting such information from the geometric structure of the data\nand identifying multiway higher-order relationships. Here, we propose an\napplication of Harmonic persistent homology, which overcomes the limitations of\nambiguous assignment of the topological information to the original elements in\na representative topological cycle from the data. When applied to multi-omics\ndata, this leads to the discovery of hidden patterns highlighting the\nrelationships between different omic profiles, while allowing for common tasks\nin multi-omics analyses, such as disease subtyping, and most importantly\nbiomarker identification for similar latent biological pathways that are\nassociated with complex diseases. Our experiments on multiple cancer data show\nthat harmonic persistent homology effectively dissects multi-omics data to\nidentify biomarkers by detecting representative cycles predictive of disease\nsubtypes.\n"
  },
  {
    "year": 2023,
    "title": "Accelerating ILP solvers for Minimum Flow Decompositions through search\n  space and dimensionality reductions",
    "summary": "  Given a flow network, the Minimum Flow Decomposition (MFD) problem is finding\nthe smallest possible set of weighted paths whose superposition equals the\nflow. It is a classical, strongly NP-hard problem that is proven to be useful\nin RNA transcript assembly and applications outside of Bioinformatics. We\nimprove an existing ILP (Integer Linear Programming) model by Dias et al.\n[RECOMB 2022] for DAGs by decreasing the solver's search space using solution\nsafety and several other optimizations. This results in a significant speedup\ncompared to the original ILP, of up to 55-90x on average on the hardest\ninstances. Moreover, we show that our optimizations apply also to MFD problem\nvariants, resulting in similar speedups, going up to 123x on the hardest\ninstances. We also developed an ILP model of reduced dimensionality for an MFD\nvariant in which the solution path weights are restricted to a given set. This\nmodel can find an optimal MFD solution for most instances, and overall, its\naccuracy significantly outperforms that of previous greedy algorithms while\nbeing up to an order of magnitude faster than our optimized ILP.\n"
  },
  {
    "year": 2023,
    "title": "Using Guided Transfer Learning to Predispose AI Agent to Learn\n  Efficiently from Small RNA-sequencing Datasets",
    "summary": "  Given the increasing availability of RNA-seq data and its complex and\nheterogeneous nature, there has been growing interest in applying AI/machine\nlearning methodologies to work with such data modalities. However, because\nomics data is characterized by high dimensionality and low sample size (HDLSS),\ncurrent attempts at integrating AI in this domain require significant human\nguidance and expertise to mitigate overfitting. In this work we look at how\ntransfer learning can be improved to learn from small RNA-seq sample sizes\nwithout significant human interference. The strategy is to gain general prior\nknowledge about a particular domain of data (e.g. RNA-seq data) by pre-training\non a general task with a large aggregate of data, then fine-tuning to various\nspecific, downstream target tasks in the same domain. Because previous attempts\nhave shown traditional transfer learning failing on HLDSS, we propose to\nimprove performance by using Guided Transfer Learning (GTL). Collaborating with\nRobots Go Mental, the AI we deploy here not only learns good initial parameters\nduring pre-training, but also learns inductive biases that affect how the AI\nlearns downstream tasks. In this approach, we first pre-trained on recount3\ndata, a collection of over 400,000 mouse RNA-seq samples sourced from thousands\nof individual studies. With such a large collection, patterns of expression\nbetween the ~30,000 genes in mammalian systems were pre-determined. Such\npatterns were sufficient for the pre-trained AI agent to efficiently learn new\ndownstream tasks involving RNA-seq datasets with very low sample sizes and\nperformed notably better on few-shot learning tasks compared to the same model\nwithout pre-training.\n"
  },
  {
    "year": 2023,
    "title": "A selective review of recent developments in spatially variable gene\n  detection for spatial transcriptomics",
    "summary": "  With the emergence of advanced spatial transcriptomic technologies, there has\nbeen a surge in research papers dedicated to analyzing spatial transcriptomics\ndata, resulting in significant contributions to our understanding of biology.\nThe initial stage of downstream analysis of spatial transcriptomic data has\ncentered on identifying spatially variable genes (SVGs) or genes expressed with\nspecific spatial patterns across the tissue. SVG detection is an important task\nsince many downstream analyses depend on these selected SVGs. Over the past few\nyears, a plethora of new methods have been proposed for the detection of SVGs,\naccompanied by numerous innovative concepts and discussions. This article\nprovides a selective review of methods and their practical implementations,\noffering valuable insights into the current literature in this field.\n"
  },
  {
    "year": 2023,
    "title": "Identifying topologically associating domains using differential kernels",
    "summary": "  Chromatin is a polymer complex of DNA and proteins that regulates gene\nexpression. The three-dimensional structure and organization of chromatin\ncontrols DNA transcription and replication. High-throughput chromatin\nconformation capture techniques generate Hi-C maps that can provide insight\ninto the 3D structure of chromatin. Hi-C maps can be represented as a symmetric\nmatrix where each element represents the average contact probability or number\nof contacts between two chromatin loci. Previous studies have detected\ntopologically associating domains (TADs), or self-interacting regions in Hi-C\nmaps within which the contact probability is greater than that outside the\nregion. Many algorithms have been developed to identify TADs within Hi-C maps.\nHowever, most TAD identification algorithms are unable to identify nested or\noverlapping TADs and for a given Hi-C map there is significant variation in the\nlocation and number of TADs identified by different methods. We develop a novel\nmethod, KerTAD, using a kernel-based technique from computer vision and image\nprocessing that is able to accurately identify nested and overlapping TADs. We\nbenchmark this method against state-of-the-art TAD identification methods on\nboth synthetic and experimental data sets. We find that KerTAD consistently has\nhigher true positive rates (TPR) and lower false discovery rates (FDR) than all\ntested methods for both synthetic and manually annotated experimental Hi-C\nmaps. The TPR for KerTAD is also largely insensitive to increasing noise and\nsparsity, in contrast to the other methods. We also find that KerTAD is\nconsistent in the number and size of TADs identified across replicate\nexperimental Hi-C maps for several organisms. KerTAD will improve automated TAD\nidentification and enable researchers to better correlate changes in TADs to\nbiological phenomena, such as enhancer-promoter interactions and disease\nstates.\n"
  },
  {
    "year": 2023,
    "title": "DSBplot: Indels in DNA Double-strand Break Repair Experiments",
    "summary": "  Double-strand breaks (DSBs) in DNA are naturally occurring destructive events\nin all organisms that may lead to genome instability. Cells employ various\nrepair methods known as non-homologous end joining (NHEJ), microhomology\nmediated end joining (MMEJ), and homology-directed recombination (HDR). These\nrepair processes may lead to DNA sequence variations (e.g., nucleotide\ninsertions, deletions, and substitutions) at the location of the break.\nStudying DNA DSB repair processes often involves the use of high throughput\nsequencing assays to precisely quantify the sequence variations near the break\nwith software tools. Often methods of assessing and visualizing these data have\nnot taken into account the full complexity of the sequencing data, such as the\nfrequency, type, and position of the sequence variations in a single\ncomprehensive representation. Here we present a method that allows\nvisualization of the overall variation pattern as well as comparison of these\npatterns among experimental setups.\n"
  },
  {
    "year": 2024,
    "title": "Approximating a linear dynamical system from non-sequential data",
    "summary": "  Given non-sequential snapshots from instances of a dynamical system, we\ndesign a compressed sensing based algorithm that reconstructs the dynamical\nsystem. We formally prove that successful reconstruction is possible under the\nassumption that we can construct an approximate clock from a subset of the\ncoordinates of the underlying system.\n  As an application, we argue that our assumption is likely true for genomic\ndatasets, and we recover the underlying nuclear receptor networks and predict\npathways, as opposed to genes, that may differentiate phenotypes in some\npublicly available datasets.\n"
  },
  {
    "year": 2024,
    "title": "Enhancing Cardiovascular Disease Risk Prediction with Machine Learning\n  Models",
    "summary": "  Cardiovascular disease remains a leading global cause of mortality,\nnecessitating accurate risk prediction tools. Traditional methods, such as\nQRISK and the Framingham heart score, exhibit limitations in their ability to\nincorporate comprehensive patient data, potentially resulting in incomplete\nrisk factor consideration. To address these shortcomings, this study conducts a\nmeticulous review focusing on the application of machine learning models to\nenhance predictive accuracy. Machine learning models, such as support vector\nmachines, and Random Forest, as well as deep learning techniques like\nconvolutional neural networks and recurrent neural networks, have emerged as\npromising alternatives. These models offer superior performance, accommodating\na broader spectrum of variables and providing precise subgroup-specific\npredictions. While machine learning integration holds promise for enhancing\nrisk assessment, it presents challenges such as data requirements and\ncomputational constraints. Additionally, large language models have\nrevolutionised healthcare applications, augmenting diagnostic precision and\npatient care. This study examines the core aspects of cardiovascular disease\nevent risk and presents a thorough review of traditional and machine learning\nmodels, alongside deep learning techniques, for improved accuracy. It offers a\ncomprehensive survey of relevant datasets, critically compares ML models with\nconventional approaches, and synthesizes key findings, highlighting their\nimplications for clinical practice. Furthermore, the potential of machine\nlearning and large language models in cardiovascular medicine is undeniable.\nHowever, rigorous validation and optimisation are imperative before widespread\napplication in healthcare. This integration promises more accurate and\npersonalised cardiovascular care.\n"
  },
  {
    "year": 2024,
    "title": "Evaluation of simulation methods for tumor subclonal reconstruction",
    "summary": "  Most neoplastic tumors originate from a single cell, and their evolution can\nbe genetically traced through lineages characterized by common alterations such\nas small somatic mutations (SSMs), copy number alterations (CNAs), structural\nvariants (SVs), and aneuploidies. Due to the complexity of these alterations in\nmost tumors and the errors introduced by sequencing protocols and calling\nalgorithms, tumor subclonal reconstruction algorithms are necessary to\nrecapitulate the DNA sequence composition and tumor evolution in silico. With a\ngrowing number of these algorithms available, there is a pressing need for\nconsistent and comprehensive benchmarking, which relies on realistic tumor\nsequencing generated by simulation tools. Here, we examine the current\nsimulation methods, identifying their strengths and weaknesses, and provide\nrecommendations for their improvement. Our review also explores potential new\ndirections for research in this area. This work aims to serve as a resource for\nunderstanding and enhancing tumor genomic simulations, contributing to the\nadvancement of the field.\n"
  },
  {
    "year": 2024,
    "title": "Predicting Breast Cancer Phenotypes from Single-cell RNA-seq Data Using\n  CloudPred",
    "summary": "  Numerous tools have been recently developed to predict disease phenotypes\nusing single-cell RNA sequencing (RNA-seq) data. CloudPred is an end-to-end\ndifferentiable learning algorithm coupled with a biologically informed mixture\nmodel, originally tested on lupus data. This study extends CloudPred's\napplications to breast cancer disease phenotype prediction to test its\nrobustness and applicability on untested and unrelated biological data. When\napplying a breast cancer single-cell RNA seq dataset, CloudPred achieved an\narea under the ROC curve (AUC) of 1 in predicting cancer status and performed\nbetter than a linear and Deepset models.\n"
  },
  {
    "year": 2024,
    "title": "Exploring gene content with pangene graphs",
    "summary": "  Motivation: The gene content regulates the biology of an organism. It varies\nbetween species and between individuals of the same species. Although tools\nhave been developed to identify gene content changes in bacterial genomes, none\nis applicable to collections of large eukaryotic genomes such as the human\npangenome.\n  Results: We developed pangene, a computational tool to identify gene\norientation, gene order and gene copy-number changes in a collection of\ngenomes. Pangene aligns a set of input protein sequences to the genomes,\nresolves redundancies between protein sequences and constructs a gene graph\nwith each genome represented as a walk in the graph. It additionally finds\nsubgraphs, which we call bibubbles, that capture gene content changes. Applied\nto the human pangenome, pangene identifies known gene-level variations and\nreveals complex haplotypes that are not well studied before. Pangene also works\nwith high-quality bacterial pangenome and reports similar numbers of core and\naccessory genes in comparison to existing tools.\n  Availability and implementation: Source code at\nhttps://github.com/lh3/pangene; pre-built pangene graphs can be downloaded from\nhttps://zenodo.org/records/8118576 and visualized at\nhttps://pangene.bioinweb.org\n"
  },
  {
    "year": 2024,
    "title": "Graph-based variant discovery reveals novel dynamics in the human\n  microbiome",
    "summary": "  Sequence differences between the strains of bacteria comprising\nhost-associated and environmental microbiota may play a role in community\nassembly and influence the resilience of microbial communities to disturbances.\nTools for characterizing strain-level variation within microbial communities,\nhowever, are limited in scope, focusing on just single nucleotide\npolymorphisms, or relying on reference-based analyses that miss complex\nfunctional and structural variants. Here, we demonstrate the power of assembly\ngraph analysis to detect and characterize structural variants in almost 1,000\nmetagenomes generated as part of the Human Microbiome Project. We identify over\nnine million variants comprising insertion/deletion events, repeat copy-number\nchanges, and mobile elements such as plasmids. We highlight some of the\npotential functional roles of these genomic changes. Our analysis revealed\nstriking differences in the rate of variation across body sites, highlighting\nniche-specific mechanisms of bacterial adaptation. The structural variants we\ndetect also include potentially novel prophage integration events, highlighting\nthe potential use of graph-based analyses for phage discovery.\n"
  },
  {
    "year": 2024,
    "title": "A genome-scale deep learning model to predict gene expression changes of\n  genetic perturbations from multiplex biological networks",
    "summary": "  Systematic characterization of biological effects to genetic perturbation is\nessential to the application of molecular biology and biomedicine. However, the\nexperimental exhaustion of genetic perturbations on the genome-wide scale is\nchallenging. Here, we show that TranscriptionNet, a deep learning model that\nintegrates multiple biological networks to systematically predict\ntranscriptional profiles to three types of genetic perturbations based on\ntranscriptional profiles induced by genetic perturbations in the L1000 project:\nRNA interference (RNAi), clustered regularly interspaced short palindromic\nrepeat (CRISPR) and overexpression (OE). TranscriptionNet performs better than\nexisting approaches in predicting inducible gene expression changes for all\nthree types of genetic perturbations. TranscriptionNet can predict\ntranscriptional profiles for all genes in existing biological networks and\nincreases perturbational gene expression changes for each type of genetic\nperturbation from a few thousand to 26,945 genes. TranscriptionNet demonstrates\nstrong generalization ability when comparing predicted and true gene expression\nchanges on different external tasks. Overall, TranscriptionNet can systemically\npredict transcriptional consequences induced by perturbing genes on a\ngenome-wide scale and thus holds promise to systemically detect gene function\nand enhance drug development and target discovery.\n"
  },
  {
    "year": 2024,
    "title": "The use of next-generation sequencing in personalized medicine",
    "summary": "  The revolutionary progress in development of next-generation sequencing (NGS)\ntechnologies has made it possible to deliver accurate genomic information in a\ntimely manner. Over the past several years, NGS has transformed biomedical and\nclinical research and found its application in the field of personalized\nmedicine. Here we discuss the rise of personalized medicine and the history of\nNGS. We discuss current applications and uses of NGS in medicine, including\ninfectious diseases, oncology, genomic medicine, and dermatology. We provide a\nbrief discussion of selected studies where NGS was used to respond to wide\nvariety of questions in biomedical research and clinical medicine. Finally, we\ndiscuss the challenges of implementing NGS into routine clinical use.\n"
  },
  {
    "year": 2024,
    "title": "Genetic diversity of barley accessions and their response under abiotic\n  stresses using different approaches",
    "summary": "  In this investigation, five separate experiments were carried out. The first\nexperiments were examined the molecular characteristics of 59 barley accessions\ncollected from different regions in Iraq using three different molecular\nmarkers (ISSR, CDDP, and Scot). A total of 391 amplified polymorphic bands were\ngenerated using forty-four ISSR, nine CDDP, and twelve Scot primers, which they\ntotally observed 255, 35, and 101 polymorphic bands respectively. The mean\nvalues of PIC for ISSR, CDDP, and Scot markers were 0.74, 0.63, and 0.80,\nrespectively, indicating the efficiency of the underlying markers in detecting\npolymorphic status among the studied barley accessions. Based on the respective\nmarkers, the barley accessions were classified and clustered into two main\ngroups using the UPGMA and population structure analysis. Results of claustral\nanalyses showed that the variation patterns corresponded with the geographical\ndistribution of barley accessions.\n"
  },
  {
    "year": 2024,
    "title": "Just-DNA-Seq, open-source personal genomics platform: longevity science\n  for everyone",
    "summary": "  Genomic data has become increasingly accessible to the general public with\nthe advent of companies offering whole genome sequencing at a relatively low\ncost. However, their reports are not verifiable due to a lack of crucial\ndetails and transparency: polygenic risk scores do not always mention all the\npolymorphisms involved. Simultaneously, tackling the manual investigation and\ninterpretation of data proves challenging for individuals lacking a background\nin genetics. Currently, there is no open-source or commercial solution that\nprovides comprehensive longevity reports surpassing a limited number of\npolymorphisms. Additionally, there are no ready-made, out-of-the-box solutions\navailable that require minimal expertise to generate reports independently. To\naddress these issues, we have developed the Just-DNA-Seq open-source genomic\nplatform. Just-DNA-Seq aims to provide a user-friendly solution to genome\nannotation by allowing users to upload their own VCF files and receive\nannotations of their genetic variants and polygenic risk scores related to\nlongevity. We also created GeneticsGenie custom GPT that can answer genetics\nquestions based on our modules. With the Just-DNA-Seq platform, we want to\nprovide full information regarding the genetics of long life:\ndisease-predisposing variants, that can reduce lifespan and manifest at\ndifferent age (cardiovascular, oncological, neurodegenerative diseases, etc.),\npro-longevity variants and longevity drug pharmacokinetics. In this research\narticle, we will discuss the features and capabilities of Just-DNA-Seq, and how\nit can benefit individuals looking to understand and improve their health. It's\ncrucial to note that the Just-DNA-Seq platform is exclusively intended for\nscientific and informational purposes and is not suitable for medical\napplications.\n"
  },
  {
    "year": 2024,
    "title": "Navigating Eukaryotic Genome Annotation Pipelines: A Route Map to\n  BRAKER, Galba, and TSEBRA",
    "summary": "  Annotating the structure of protein-coding genes represents a major challenge\nin the analysis of eukaryotic genomes. This task sets the groundwork for\nsubsequent genomic studies aimed at understanding the functions of individual\ngenes. BRAKER and Galba are two fully automated and containerized pipelines\ndesigned to perform accurate genome annotation. BRAKER integrates the\nGeneMark-ETP and AUGUSTUS gene finders, employing the TSEBRA combiner to attain\nhigh sensitivity and precision. BRAKER is adept at handling genomes of any\nsize, provided that it has access to both transcript expression sequencing data\nand an extensive protein database from the target clade. In particular, BRAKER\ndemonstrates high accuracy even with only one type of these extrinsic evidence\nsources, although it should be noted that accuracy diminishes for larger\ngenomes under such conditions. In contrast, Galba adopts a distinct methodology\nutilizing the outcomes of direct protein-to-genome spliced alignments using\nminiprot to generate training genes and evidence for gene prediction in\nAUGUSTUS. Galba has superior accuracy in large genomes if protein sequences are\nthe only source of evidence. This chapter provides practical guidelines for\nemploying both pipelines in the annotation of eukaryotic genomes, with a focus\non insect genomes.\n"
  },
  {
    "year": 2024,
    "title": "Guide to k-mer approaches for genomics across the tree of life",
    "summary": "  The wide array of currently available genomes display a wonderful diversity\nin size, composition and structure with many more to come thanks to several\nglobal biodiversity genomics initiatives starting in recent years. However,\nsequencing of genomes, even with all the recent advances, can still be\nchallenging for both technical (e.g. small physical size, contaminated samples,\nor access to appropriate sequencing platforms) and biological reasons (e.g.\ngermline restricted DNA, variable ploidy levels, sex chromosomes, or very large\ngenomes). In recent years, k-mer-based techniques have become popular to\novercome some of these challenges. They are based on the simple process of\ndividing the analysed sequences (e.g. raw reads or genomes) into a set of\nsub-sequences of length k, called k-mers. Despite this apparent simplicity,\nk-mer-based analysis allows for a rapid and intuitive assessment of complex\nsequencing datasets. Here, we provide the first comprehensive review to the\ntheoretical properties and practical applications of k-mers in biodiversity\ngenomics, serving as a reference manual for this powerful approach.\n"
  },
  {
    "year": 2024,
    "title": "A Systematic Overview of Single-Cell Transcriptomics Databases, their\n  Use cases, and Limitations",
    "summary": "  Rapid advancements in high-throughput single-cell RNA-seq (scRNA-seq)\ntechnologies and experimental protocols have led to the generation of vast\namounts of genomic data that populates several online databases and\nrepositories. Here, we systematically examined large-scale scRNA-seq databases,\ncategorizing them based on their scope and purpose such as general,\ntissue-specific databases, disease-specific databases, cancer-focused\ndatabases, and cell type-focused databases. Next, we discuss the technical and\nmethodological challenges associated with curating large-scale scRNA-seq\ndatabases, along with current computational solutions. We argue that\nunderstanding scRNA-seq databases, including their limitations and assumptions,\nis crucial for effectively utilizing this data to make robust discoveries and\nidentify novel biological insights. Furthermore, we propose that bridging the\ngap between computational and wet lab scientists through user-friendly\nweb-based platforms is needed for democratizing access to single-cell data.\nThese platforms would facilitate interdisciplinary research, enabling\nresearchers from various disciplines to collaborate effectively. This review\nunderscores the importance of leveraging computational approaches to unravel\nthe complexities of single-cell data and offers a promising direction for\nfuture research in the field.\n"
  },
  {
    "year": 2024,
    "title": "Heterogeneity analysis provides evidence for a genetically homogeneous\n  subtype of bipolar-disorder",
    "summary": "  Bipolar Disorder (BD) is a complex disease. It is heterogeneous, both at the\nphenotypic and genetic level, although the extent and impact of this\nheterogeneity is not fully understood. In this paper, we leverage recent\nadvances in heterogeneity analysis to look for genetically-driven subgroups\n(i.e., biclusters) within the broad phenotype of Bipolar Disorder. We first\napply this covariate-corrected biclustering algorithm to a cohort of 2524 BD\ncases and 4106 controls from the Bipolar Disease Research Network (BDRN) within\nthe Psychiatric Genomics Consortium (PGC). We find evidence of genetic\nheterogeneity delineating a statistically significant bicluster comprising a\nsubset of BD cases which exhibits a disease-specific pattern of\ndifferential-expression across a subset of SNPs. This disease-specific genetic\npattern (i.e., 'genetic subgroup') replicates across the remaining data-sets\ncollected by the PGC containing 5781/8289, 3581/7591, and 6825/9752\ncases/controls, respectively. This genetic subgroup (discovered without using\nany BD subtype information) was more prevalent in Bipolar type-I than in\nBipolar type-II. Our methodology has successfully identified a replicable\nhomogeneous genetic subgroup of bipolar disorder. This subgroup may represent a\ncollection of correlated genetic risk-factors for BDI. By investigating the\nsubgroup's bicluster-informed polygenic-risk-scoring (PRS), we find that the\ndisease-specific pattern highlighted by the bicluster can be leveraged to\neliminate noise from our GWAS analyses and improve risk prediction. This\nimprovement is particularly notable when using only a relatively small subset\nof the available SNPs, implying improved SNP replication. Though our primary\nfocus is only the analysis of disease-related signal, we also identify\nreplicable control-related heterogeneity.\n"
  },
  {
    "year": 2024,
    "title": "The Canadian VirusSeq Data Portal & Duotang: open resources for\n  SARS-CoV-2 viral sequences and genomic epidemiology",
    "summary": "  The COVID-19 pandemic led to a large global effort to sequence SARS-CoV-2\ngenomes from patient samples to track viral evolution and inform public health\nresponse. Millions of SARS-CoV-2 genome sequences have been deposited in global\npublic repositories. The Canadian COVID-19 Genomics Network (CanCOGeN -\nVirusSeq), a consortium tasked with coordinating expanded sequencing of\nSARS-CoV-2 genomes across Canada early in the pandemic, created the Canadian\nVirusSeq Data Portal, with associated data pipelines and procedures, to support\nthese efforts. The goal of VirusSeq was to allow open access to Canadian\nSARS-CoV-2 genomic sequences and enhanced, standardized contextual data that\nwere unavailable in other repositories and that meet FAIR standards (Findable,\nAccessible, Interoperable and Reusable). The Portal data submission pipeline\ncontains data quality checking procedures and appropriate acknowledgement of\ndata generators that encourages collaboration. Here we also highlight Duotang,\na web platform that presents genomic epidemiology and modeling analyses on\ncirculating and emerging SARS-CoV-2 variants in Canada. Duotang presents\ndynamic changes in variant composition of SARS-CoV-2 in Canada and by province,\nestimates variant growth, and displays complementary interactive\nvisualizations, with a text overview of the current situation. The VirusSeq\nData Portal and Duotang resources, alongside additional analyses and resources\ncomputed from the Portal (COVID-MVP, CoVizu), are all open-source and freely\navailable. Together, they provide an updated picture of SARS-CoV-2 evolution to\nspur scientific discussions, inform public discourse, and support communication\nwith and within public health authorities. They also serve as a framework for\nother jurisdictions interested in open, collaborative sequence data sharing and\nanalyses.\n"
  },
  {
    "year": 2024,
    "title": "Characterizing virulence differences in a parasitoid wasp through\n  comparative transcriptomic and proteomic",
    "summary": "  Background: Two strains of the endoparasitoid Cotesia typhae present a\ndifferential parasitism success on the host, Sesamia nonagrioides. One is\nvirulent on both permissive and resistant host populations, and the other only\non the permissive host. This interaction provides a very interesting frame for\nstudying virulence factors. Here, we used a combination of comparative\ntranscriptomic and proteomic analyses to unravel the molecular basis underlying\nvirulence differences between the strains.Results: First, we report that\nvirulence genes are mostly expressed during the nymphal stage of the\nparasitoid. Especially, proviral genes are broadly up-regulated at this stage,\nwhile their expression is only expected in the host. Parasitoid gene expression\nin the host increases with time, indicating the production of more virulence\nfactors. Secondly, comparison between strains reveals differences in venom\ncomposition, with 12 proteins showing differential abundance. Proviral\nexpression in the host displays a strong temporal variability, along with\ndifferential patterns between strains. Notably, a subset of proviral genes\nincluding protein-tyrosine phosphatases is specifically over-expressed in the\nresistant host parasitized by the less virulent strain, 24 hours after\nparasitism. This result particularly hints at host modulation of proviral\nexpression.Conclusions: This study sheds light on the temporal expression of\nvirulence factors of Cotesia typhae, both in the host and in the parasitoid. It\nalso identifies potential molecular candidates driving differences in\nparasitism success between two strains. Together, those findings provide a path\nfor further exploration of virulence mechanisms in parasitoid wasps, and offer\ninsights into host-parasitoid coevolution.\n"
  },
  {
    "year": 2024,
    "title": "Accurate and efficient protein embedding using multi-teacher\n  distillation learning",
    "summary": "  Motivation: Protein embedding, which represents proteins as numerical\nvectors, is a crucial step in various learning-based protein\nannotation/classification problems, including gene ontology prediction,\nprotein-protein interaction prediction, and protein structure prediction.\nHowever, existing protein embedding methods are often computationally expensive\ndue to their large number of parameters, which can reach millions or even\nbillions. The growing availability of large-scale protein datasets and the need\nfor efficient analysis tools have created a pressing demand for efficient\nprotein embedding methods.\n  Results: We propose a novel protein embedding approach based on multi-teacher\ndistillation learning, which leverages the knowledge of multiple pre-trained\nprotein embedding models to learn a compact and informative representation of\nproteins. Our method achieves comparable performance to state-of-the-art\nmethods while significantly reducing computational costs and resource\nrequirements. Specifically, our approach reduces computational time by ~70\\%\nand maintains almost the same accuracy as the original large models. This makes\nour method well-suited for large-scale protein analysis and enables the\nbioinformatics community to perform protein embedding tasks more efficiently.\n"
  },
  {
    "year": 2024,
    "title": "Range-Limited Heaps' Law for Functional DNA Words in the Human Genome",
    "summary": "  Heaps' or Herdan's law is a linguistic law describing the relationship\nbetween the vocabulary/dictionary size (type) and word counts (token) to be a\npower-law function. Its existence in genomes with certain definition of DNA\nwords is unclear partly because the dictionary size in genome could be much\nsmaller than that in a human language. We define a DNA word as a coding region\nin a genome that codes for a protein domain. Using human chromosomes and\nchromosome arms as individual samples, we establish the existence of Heaps' law\nin the human genome within limited range. Our definition of words in a genomic\nor proteomic context is different from other definitions such as\nover-represented k-mers which are much shorter in length. Although an\napproximate power-law distribution of protein domain sizes due to gene\nduplication and the related Zipf's law is well known, their translation to the\nHeaps' law in DNA words is not automatic. Several other animal genomes are\nshown herein also to exhibit range-limited Heaps' law with our definition of\nDNA words, though with various exponents. When tokens were randomly sampled and\nsample sizes reach to the maximum level, a deviation from the Heaps' law was\nobserved, but a quadratic regression in log-log type-token plot fits the data\nperfectly. Investigation of type-token plot and its regression coefficients\ncould provide an alternative narrative of reusage and redundancy of protein\ndomains as well as creation of new protein domains from a linguistic\nperspective.\n"
  },
  {
    "year": 2024,
    "title": "Metadata-guided Feature Disentanglement for Functional Genomics",
    "summary": "  With the development of high-throughput technologies, genomics datasets\nrapidly grow in size, including functional genomics data. This has allowed the\ntraining of large Deep Learning (DL) models to predict epigenetic readouts,\nsuch as protein binding or histone modifications, from genome sequences.\nHowever, large dataset sizes come at a price of data consistency, often\naggregating results from a large number of studies, conducted under varying\nexperimental conditions. While data from large-scale consortia are useful as\nthey allow studying the effects of different biological conditions, they can\nalso contain unwanted biases from confounding experimental factors. Here, we\nintroduce Metadata-guided Feature Disentanglement (MFD) - an approach that\nallows disentangling biologically relevant features from potential technical\nbiases. MFD incorporates target metadata into model training, by conditioning\nweights of the model output layer on different experimental factors. It then\nseparates the factors into disjoint groups and enforces independence of the\ncorresponding feature subspaces with an adversarially learned penalty. We show\nthat the metadata-driven disentanglement approach allows for better model\nintrospection, by connecting latent features to experimental factors, without\ncompromising, or even improving performance in downstream tasks, such as\nenhancer prediction, or genetic variant discovery. The code for our\nimplemementation is available at https://github.com/HealthML/MFD\n"
  },
  {
    "year": 2024,
    "title": "pVACview: an interactive visualization tool for efficient neoantigen\n  prioritization and selection",
    "summary": "  Neoantigen targeting therapies including personalized vaccines have shown\npromise in the treatment of cancers. Accurate identification/prioritization of\nneoantigens is highly relevant to designing clinical trials, predicting\ntreatment response, and understanding mechanisms of resistance. With the advent\nof massively parallel sequencing technologies, it is now possible to predict\nneoantigens based on patient-specific variant information. However, numerous\nfactors must be considered when prioritizing neoantigens for use in\npersonalized therapies. Complexities such as alternative transcript\nannotations, various binding, presentation and immunogenicity prediction\nalgorithms, and variable peptide lengths/registers all potentially impact the\nneoantigen selection process. While computational tools generate numerous\nalgorithmic predictions for neoantigen characterization, results from these\npipelines are difficult to navigate and require extensive knowledge of the\nunderlying tools for accurate interpretation. Due to the intricate nature and\nnumber of salient neoantigen features, presenting all relevant information to\nfacilitate candidate selection for downstream applications is a difficult\nchallenge that current tools fail to address. We have created pVACview, the\nfirst interactive tool designed to aid in the prioritization and selection of\nneoantigen candidates for personalized neoantigen therapies. pVACview has a\nuser-friendly and intuitive interface where users can upload, explore, select\nand export their neoantigen candidates. The tool allows users to visualize\ncandidates using variant, transcript and peptide information. pVACview will\nallow researchers to analyze and prioritize neoantigen candidates with greater\nefficiency and accuracy in basic and translational settings. The application is\navailable as part of the pVACtools pipeline at pvactools.org and as an online\nserver at pvacview.org.\n"
  },
  {
    "year": 2024,
    "title": "skandiver: a divergence-based analysis tool for identifying\n  intercellular mobile genetic elements",
    "summary": "  Mobile genetic elements (MGEs) are as ubiquitous in nature as they are varied\nin type, ranging from viral insertions to transposons to incorporated plasmids.\nHorizontal transfer of MGEs across bacterial species may also pose a\nsignificant threat to global health due to their capability to harbour\nantibiotic resistance genes. However, despite cheap and rapid whole genome\nsequencing, the varied nature of MGEs makes it difficult to fully characterize\nthem, and existing methods for detecting MGEs often don't agree on what should\ncount. In this manuscript, we first define and argue in favor of a\ndivergence-based characterization of mobile-genetic elements. Using that\nparadigm, we present skandiver, a tool designed to efficiently detect MGEs from\nwhole genome assemblies without the need for gene annotation or markers.\nskandiver determines mobile elements via genome fragmentation, average\nnucleotide identity (ANI), and divergence time. By building on the scalable\nskani software for ANI computation, skandiver can query hundreds of complete\nassemblies against $>$65,000 representative genomes in a few minutes and 19 GB\nmemory, providing scalable and efficient method for elucidating mobile element\nprofiles in incomplete, uncharacterized genomic sequences. For isolated and\nintegrated large plasmids (>10kbp), skandiver's recall was 48\\% and 47\\%,\nMobileElementFinder was 59\\% and 17\\%, and geNomad was 86\\% and 32\\%,\nrespectively. For isolated large plasmids, skandiver's recall (48\\%) is lower\nthan state-of-the-art reference-based methods geNomad (86\\%) and\nMobileElementFinder (59\\%). However, skandiver achieves higher recall on\nintegrated plasmids and, unlike other methods, without comparing against a\ncurated database, making skandiver suitable for discovery of novel MGEs.\n  Availability: https://github.com/YoukaiFromAccounting/skandiver\n"
  },
  {
    "year": 2024,
    "title": "A mapping-free NLP-based technique for sequence search in Nanopore\n  long-reads",
    "summary": "  In unforeseen situations, such as nuclear power plant's or civilian radiation\naccidents, there is a need for effective and computationally inexpensive\nmethods to determine the expression level of a selected gene panel, allowing\nfor rough dose estimates in thousands of donors. The new generation in-situ\nmapper, fast and of low energy consumption, working at the level of single\nnanopore output, is in demand. We aim to create a sequence identification tool\nthat utilizes Natural Language Processing (NLP) techniques and ensures a high\nlevel of negative predictive value (NPV) compared to the classical approach.\nThe training dataset consisted of RNASeq data from 6 samples. Having tested\nmultiple NLP models, the best configuration analyses the entire sequence and\nuses a word length of 3 base pairs with one-word neighbor on each side. For the\nconsidered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and\nNPV 99.25%, compared to minimap2's performance in a cross-validation scenario.\nReducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to\n98.15%. Obtained NLP model, validated on an external independent genome\nsequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced\ndictionary. The salmon-estimated read counts differed from the classical\napproach on average by 3.48% for the complete dictionary and by 5.82% for the\nreduced one. We conclude that for long Oxford Nanopore reads, an NLP-based\napproach can successfully replace classical mapping in case of emergency. The\ndeveloped NLP model can be easily retrained to identify selected transcripts\nand/or work with various long-read sequencing techniques. Our results of the\nstudy clearly demonstrate the potential of applying techniques known from\nclassical text processing to nucleotide sequences and represent a significant\nadvancement in this field of science.\n"
  },
  {
    "year": 2024,
    "title": "Metagenomic analysis revealed significant changes in cattle rectum\n  microbiome and antimicrobial resistome under fescue toxicosis",
    "summary": "  Fescue toxicity causes reduced growth and reproductive issues in cattle\ngrazing endophyte-infected tall fescue. To characterize the gut microbiota and\nits response to fescue toxicosis, we collected fecal samples before and after a\n30-days toxic fescue seeds supplementation from eight Angus Simmental pregnant\ncows and heifers. We sequenced the 16 metagenomes using the whole-genome\nshotgun approach and generated 157 Gbp of metagenomic sequences. Through de\nnovo assembly and annotation, we obtained a 13.1 Gbp reference contig assembly\nand identified 22 million microbial genes for cattle rectum microbiota. We\ndiscovered a significant reduction of microbial diversity after toxic seed\ntreatment (P<0.01), suggesting dysbiosis of the microbiome. Six bacterial\nfamilies and 31 species are significantly increased in the fecal microbiota\n(P-adj<0.05), including members of the top abundant rumen core taxa. This\nglobal elevation of rumen microbes in the rectum microbiota suggests a\npotential impairment of rumen microbiota under fescue toxicosis. Among these,\nRuminococcaceae bacterium P7, an important species accounting for ~2% of rumen\nmicrobiota, was the most impacted with a 16-fold increase from 0.17% to 2.8% in\nfeces (P<0.01). We hypothesized that rumen Ruminococcaceae bacterium P7\nre-adapted to the large intestine environment under toxic fescue stress,\ncausing this dramatic increase in abundance. Functional enrichment analysis\nrevealed that the overrepresented pathways shifted from energy metabolism to\nantimicrobial resistance and DNA replication. In conclusion, we discovered\ndramatic microbiota alterations in composition, abundance, and functional\ncapacities under fescue toxicosis, and our results suggest Ruminococcaceae\nbacterium P7 as a potential biomarker for fescue toxicosis management.\n"
  },
  {
    "year": 2024,
    "title": "Metagenomic analysis reveals shared and distinguishing features in horse\n  and donkey gut microbiome and maternal resemblance of the microbiota in\n  hybrid equids",
    "summary": "  Mammalian gut microbiomes are essential for host functions like digestion,\nimmunity, and nutrient utilization. This study examines the gut microbiome of\nhorses, donkeys, and their hybrids, mules and hinnies, to explore the role of\nmicrobiomes in hybrid vigor. We performed whole-genome sequencing on rectal\nmicrobiota from 18 equids, generating detailed microbiome assemblies. Our\nanalysis revealed significant differences between horse and donkey microbiomes,\nwith hybrids showing a pronounced maternal resemblance. Notably, Firmicutes\nwere more abundant in the horse-maternal group, while Fibrobacteres were richer\nin the donkey-maternal group, indicating distinct digestive processes.\nFunctional annotations indicated metabolic differences, such as protein\nsynthesis in horses and energy metabolism in donkeys. Machine learning\npredictions of probiotic species highlighted potential health benefits for each\nmaternal group. This study provides a high-resolution view of the equid gut\nmicrobiome, revealing significant taxonomic and metabolic differences\ninfluenced by maternal lineage, and offers insights into microbial\ncontributions to hybrid vigor.\n"
  },
  {
    "year": 2024,
    "title": "PyamilySeq: A Python Tool for Interpretable Gene (Re)Clustering and\n  Pangenomic Inference Across Species and Genera",
    "summary": "  PyamilySeq is a Python-based tool designed for interpretable gene clustering\nand pangenomic inference, supporting analyses at both species and genus levels.\nIt facilitates the clustering of gene sequences into families based on sequence\nsimilarity using CD-HIT, and can take the output of tried-and-tested sequence\nclustering tools such as CD-HIT, BLAST, DIAMOND, and MMseqs2. PyamilySeq is\ndistinctive in its ability to integrate new sequences into existing clusters,\nproviding a robust framework for iterative analysis while preserving the\noriginal clusters, useful when reannotating genomes. In addition to the\nstandard Species mode which as with other tools performs core-gene analysis\nacross a species range, PyamilySeq can be run in Genus mode where it detects\nthe presence of gene families shared across multiple genera. These features\nenhance the tools applicability for ongoing and past genomic studies and\ncomparative analyses. PyamilySeq generates comprehensive outputs, including\ngene presence-absence matrices and aligned sequence data, enabling downstream\nanalysis and interpretation of the identified gene groups and pangenomic data.\n"
  },
  {
    "year": 2024,
    "title": "Are gene-by-environment interactions leveraged in multi-modality neural\n  networks for breast cancer prediction?",
    "summary": "  Polygenic risk scores (PRSs) can significantly enhance breast cancer risk\nprediction when combined with clinical risk factor data. While many studies\nhave explored the value-add of PRSs, little is known about the potential impact\nof gene-by-gene or gene-by-environment interactions towards enhancing the risk\ndiscrimination capabilities of multi-modal models combining PRSs with clinical\ndata. In this study, we integrated data on 318 individual genotype variants\nalong with clinical data in a neural network to explore whether gene-by-gene\n(i.e., between individual variants) and/or gene-by-environment (between\nclinical risk factors and variants) interactions could be leveraged jointly\nduring training to improve breast cancer risk prediction performance. We\nbenchmarked our approach against a baseline model combining traditional\nunivariate PRSs with clinical data in a logistic regression model and ran an\ninterpretability analysis to identify feature interactions.\n  While our model did not demonstrate improved performance over the baseline,\nwe discovered 248 (<1%) statistically significant gene-by-gene and\ngene-by-environment interactions out of the ~53.6k possible feature pairs, the\nmost contributory of which included rs6001930 (MKL1) and rs889312 (MAP3K1),\nwith age and menopause being the most heavily interacting non-genetic risk\nfactors. We also modeled the significant interactions as a network of highly\nconnected features, suggesting that potential higher-order interactions are\ncaptured by the model. Although gene-by-environment (or gene-by-gene)\ninteractions did not enhance breast cancer risk prediction performance in\nneural networks, our study provides evidence that these interactions can be\nleveraged by these models to inform their predictions. This study represents\nthe first application of neural networks to screen for interactions impacting\nbreast cancer risk using real-world data.\n"
  },
  {
    "year": 2024,
    "title": "Integrating spatially-resolved transcriptomics data across tissues and\n  individuals: challenges and opportunities",
    "summary": "  Advances in spatially-resolved transcriptomics (SRT) technologies have\npropelled the development of new computational analysis methods to unlock\nbiological insights. As the cost of generating these data decreases, these\ntechnologies provide an exciting opportunity to create large-scale atlases that\nintegrate SRT data across multiple tissues, individuals, species, or phenotypes\nto perform population-level analyses. Here, we describe unique challenges of\nvarying spatial resolutions in SRT data, as well as highlight the opportunities\nfor standardized preprocessing methods along with computational algorithms\namenable to atlas-scale datasets leading to improved sensitivity and\nreproducibility in the future.\n"
  },
  {
    "year": 2024,
    "title": "Refinement of genetic variants needs attention",
    "summary": "  Variant calling refinement is crucial for distinguishing true genetic\nvariants from technical artifacts in high-throughput sequencing data. Manual\nreview is time-consuming while heuristic filtering often lacks optimal\nsolutions. Traditional variant calling methods often struggle with accuracy,\nespecially in regions of low read coverage, leading to false-positive or\nfalse-negative calls. Here, we introduce VariantTransformer, a\nTransformer-based deep learning model, designed to automate variant calling\nrefinement directly from VCF files in low-coverage data (10-15X).\nVariantTransformer, trained on two million variants, including SNPs and short\nInDels, from low-coverage sequencing data, achieved an accuracy of 89.26% and a\nROC AUC of 0.88. When integrated into conventional variant calling pipelines,\nVariantTransformer outperformed traditional heuristic filters and approached\nthe performance of state-of-the-art AI-based variant callers like DeepVariant.\nComparative analysis demonstrated VariantTransformer's superiority in\nfunctionality, variant type coverage, training size, and input data type.\nVariantTransformer represents a significant advancement in variant calling\nrefinement for low-coverage genomic studies.\n"
  },
  {
    "year": 2024,
    "title": "Insights, opportunities and challenges provided by large cell atlases",
    "summary": "  The field of single-cell biology is growing rapidly and is generating large\namounts of data from a variety of species, disease conditions, tissues, and\norgans. Coordinated efforts such as CZI CELLxGENE, HuBMAP, Broad Institute\nSingle Cell Portal, and DISCO, allow researchers to access large volumes of\ncurated datasets. Although the majority of the data is from scRNAseq\nexperiments, a wide range of other modalities are represented as well. These\nresources have created an opportunity to build and expand the computational\nbiology ecosystem to develop tools necessary for data reuse, and for extracting\nnovel biological insights. Here, we highlight achievements made so far, areas\nwhere further development is needed, and specific challenges that need to be\novercome.\n"
  },
  {
    "year": 2024,
    "title": "Comparison of algorithms used in single-cell transcriptomic data\n  analysis",
    "summary": "  Single-cell analysis is an increasingly relevant approach in \"omics''\nstudies. In the last decade, it has been applied to various fields, including\ncancer biology, neuroscience, and, especially, developmental biology. This rise\nin popularity has been accompanied with creation of modern software,\ndevelopment of new pipelines and design of new algorithms. Many established\nalgorithms have also been applied with varying levels of effectiveness.\nCurrently, there is an abundance of algorithms for all steps of the general\nworkflow. While some scientists use ready-made pipelines (such as Seurat),\nmanual analysis is popular, too, as it allows more flexibility. Scientists who\nperform their own analysis face multiple options when it comes to the choice of\nalgorithms. We have used two different datasets to test some of the most\nwidely-used algorithms. In this paper, we are going to report the main\ndifferences between them, suggest a minimal number of algorithms for each step,\nand explain our suggestions. In certain stages, it is impossible to make a\nclear choice without further context. In these cases, we are going to explore\nthe major possibilities, and make suggestions for each one of them.\n"
  },
  {
    "year": 2024,
    "title": "Superimposed Hi-C: A Solution Proposed for Identifying Single Cell's\n  Chromosomal Interactions",
    "summary": "  Hi-C sequencing is widely used for analyzing chromosomal interactions. In\nthis study, we propose \"superimposed Hi-C\" which features paired EcoP15I sites\nin a linker to facilitate sticky-end ligation with target DNAs. Superimposed\nHi-C overcomes Hi-C's technical limitations, enabling the identification of\nsingle cell's chromosomal interactions.\n"
  },
  {
    "year": 2024,
    "title": "HEK-Omics: The promise of omics to optimize HEK293 for recombinant\n  adeno-associated virus (rAAV) gene therapy manufacturing",
    "summary": "  Gene therapy is poised to transition from niche to mainstream medicine, with\nrecombinant adeno-associated virus (rAAV) as the vector of choice. However,\nthis requires robust, scalable, industrialized production to meet demand and\nprovide affordable patient access, which has thus far failed to materialize.\nClosing the chasm between demand and supply requires innovation in\nbiomanufacturing to achieve the essential step change in rAAV product yield and\nquality. Omics provides a rich source of mechanistic knowledge that can be\napplied to HEK293, the prevailing cell line for rAAV production. In this\nreview, the findings from a growing number of disparate studies that apply\ngenomics, epigenomics, transcriptomics, proteomics, and metabolomics to HEK293\nbioproduction are explored. Learnings from CHO-Omics, application of omics\napproaches to improve CHO bioproduction, provide context for the potential of\n\"HEK-Omics\" as a multiomics-informed approach providing actionable mechanistic\ninsights for improved transient and stable production of rAAV and other\nrecombinant products in HEK293.\n"
  },
  {
    "year": 2024,
    "title": "BWT construction and search at the terabase scale",
    "summary": "  Motivation: Burrows-Wheeler Transform (BWT) is a common component in\nfull-text indices. Initially developed for data compression, it is particularly\npowerful for encoding redundant sequences such as pangenome data. However, BWT\nconstruction is resource intensive and hard to be parallelized, and many\nmethods for querying large full-text indices only report exact matches or their\nsimple extensions. These limitations have hampered the biological applications\nof full-text indices.\n  Results: We developed ropebwt3 for efficient BWT construction and query.\nRopebwt3 indexed 320 assembled human genomes in 65 hours and indexed 7.3\nterabases of commonly studied bacterial assemblies in 26 days. This was\nachieved using up to 170 gigabytes of memory at the peak without working disk\nspace. Ropebwt3 can find maximal exact matches and inexact alignments under\naffine-gap penalties, and can retrieve similar local haplotypes matching a\nquery sequence. It demonstrates the feasibility of full-text indexing at the\nterabase scale.\n  Availability and implementation: https://github.com/lh3/ropebwt3\n"
  },
  {
    "year": 2024,
    "title": "Advances in practical k-mer sets: essentials for the curious",
    "summary": "  This paper provides a comprehensive survey of data structures for\nrepresenting k-mer sets, which are fundamental in high-throughput sequencing\nanalysis. It categorizes the methods into two main strategies: those using\nfingerprinting and hashing for compact storage, and those leveraging\nlexicographic properties for efficient representation. The paper reviews key\noperations supported by these structures, such as membership queries and\ndynamic updates, and highlights recent advancements in memory efficiency and\nquery speed. A companion paper explores colored k-mer sets, which extend these\nconcepts to integrate multiple datasets or genomes.\n"
  },
  {
    "year": 2024,
    "title": "Advances in colored k-mer sets: essentials for the curious",
    "summary": "  This paper provides a comprehensive review of recent advancements in\nk-mer-based data structures representing collections of several samples\n(sometimes called colored de Bruijn graphs) and their applications in\nlarge-scale sequence indexing and pangenomics. The review explores the\nevolution of k-mer set representations, highlighting the trade-offs between\nexact and inexact methods, as well as the integration of compression strategies\nand modular implementations. I discuss the impact of these structures on\npractical applications and describe recent utilization of these methods for\nanalysis. By surveying the state-of-the-art techniques and identifying emerging\ntrends, this work aims to guide researchers in selecting and developing methods\nfor large scale and reference-free genomic data. For a broader overview of\nk-mer set representations and foundational data structures, see the\naccompanying article on practical k-mer sets.\n"
  },
  {
    "year": 2024,
    "title": "Selecting Differential Splicing Methods: Practical Considerations",
    "summary": "  Alternative splicing is crucial in gene regulation, with significant\nimplications in clinical settings and biotechnology. This review article\ncompiles bioinformatics RNA-seq tools for investigating differential splicing;\noffering a detailed examination of their statistical methods, case\napplications, and benefits. A total of 22 tools are categorised by their\nstatistical family (parametric, non-parametric, and probabilistic) and level of\nanalysis (transcript, exon, and event). The central challenges in quantifying\nalternative splicing include correct splice site identification and accurate\nisoform deconvolution of transcripts. Benchmarking studies show no consensus on\ntool performance, revealing considerable variability across different\nscenarios. Tools with high citation frequency and continued developer\nmaintenance, such as DEXSeq and rMATS, are recommended for prospective\nresearchers. To aid in tool selection, a guide schematic is proposed based on\nvariations in data input and the required level of analysis. Additionally,\nadvancements in long-read RNA sequencing are expected to drive the evolution of\ndifferential splicing tools, reducing the need for isoform deconvolution and\nprompting further innovation.\n"
  },
  {
    "year": 2024,
    "title": "wgatools: an ultrafast toolkit for manipulating whole genome alignments",
    "summary": "  Summary: With the rapid development of long-read sequencing technologies, the\nera of individual complete genomes is approaching. We have developed wgatools,\na cross-platform, ultrafast toolkit that supports a range of whole genome\nalignment (WGA) formats, offering practical tools for conversion, processing,\nstatistical evaluation, and visualization of alignments, thereby facilitating\npopulation-level genome analysis and advancing functional and evolutionary\ngenomics. Availability and Implementation: wgatools supports diverse formats\nand can process, filter, and statistically evaluate alignments, perform\nalignment-based variant calling, and visualize alignments both locally and\ngenome-wide. Built with Rust for efficiency and safe memory usage, it ensures\nfast performance and can handle large datasets consisting of hundreds of\ngenomes. wgatools is published as free software under the MIT open-source\nlicense, and its source code is freely available at\nhttps://github.com/wjwei-handsome/wgatools. Contact: weiwenjie@westlake.edu.cn\n(W.W.) or liuhaijun@yzwlab.cn (H.-J.L.).\n"
  },
  {
    "year": 2024,
    "title": "Allium Vegetables Intake and Digestive System Cancer Risk: A Study Based\n  on Mendelian Randomization, Network Pharmacology and Molecular Docking",
    "summary": "  Background: Allium vegetables (garlic and onion) are one of the flavorings in\npeople's daily diets. Observational studies suggest that intake of allium\nvegetables may be correlated with a lower incidence of digestive system\ncancers. However, the existence of a causal relationship is still controversial\ndue to confounding factors and reverse causation. Therefore, we explored the\ncausal relationship between intake of allium vegetables and digestive system\ncancers using Mendelian randomization approach. Methods: First, we performed\nMendelian randomization analyses using inverse variance weighting (IVW),\nweighted median, and MR-Egger approaches, and demonstrated the reliability of\nthe results in the sensitivity step. Second, Multivariable Mendelian\nrandomization was applied to adjust for smoking and alcohol consumption. Third,\nwe explored the molecular mechanisms behind the positive results through\nnetwork pharmacology and molecular docking methods. Results: The study suggests\nthat increased intake of garlic reduced gastric cancer risk. However, onion\nintake was not statistically associated with digestive system cancer.\nConclusion: Garlic may have a protective effect against gastric cancer.\n"
  },
  {
    "year": 2024,
    "title": "FlexLMM: a Nextflow linear mixed model framework for GWAS",
    "summary": "  Summary: Linear mixed models are a commonly used statistical approach in\ngenome-wide association studies when population structure is present. However,\nnaive permutations to empirically estimate the null distribution of a statistic\nof interest are not appropriate in the presence of population structure,\nbecause the samples are not exchangeable with each other. For this reason we\ndeveloped FlexLMM, a Nextflow pipeline that runs linear mixed models while\nallowing for flexibility in the definition of the exact statistical model to be\nused. FlexLMM can also be used to set a significance threshold via\npermutations, thanks to a two-step process where the population structure is\nfirst regressed out, and only then are the permutations performed. We envision\nthis pipeline will be particularly useful for researchers working on\nmulti-parental crosses among inbred lines of model organisms or farm animals\nand plants.\n  Availability and implementation: The source code and documentation for the\nFlexLMM is available at https://github.com/birneylab/flexlmm.\n"
  },
  {
    "year": 2024,
    "title": "Annotation of protein-coding genes in 49 diatom genomes from the\n  Bacillariophyta clade",
    "summary": "  Diatoms, a major group of microalgae, play a critical role in global carbon\ncycling and primary production. Despite their ecological significance,\ncomprehensive genomic resources for diatoms are limited. To address this, we\nhave annotated previously unannotated genome assemblies of 49 diatom species.\nGenome assemblies were obtained from NCBI Datasets and processed for repeat\nelements using RepeatModeler2 and RepeatMasker. For gene prediction, BRAKER2\nwas employed in the absence of transcriptomic data, while BRAKER3 was utilized\nwhen transcriptome short read data were available from the Sequence Read\nArchive. The quality of genome assemblies and predicted protein sets was\nevaluated using BUSCO, ensuring high-quality genomic resources. Functional\nannotation was performed using EnTAP, providing insights into the biological\nroles of the predicted proteins. Our study enhances the genomic toolkit\navailable for diatoms, facilitating future research in diatom biology, ecology,\nand evolution.\n"
  },
  {
    "year": 2024,
    "title": "The Mitochondrial Genome of Cathaya argyrophylla Reaches 18.99 Mb:\n  Analysis of Super-Large Mitochondrial Genomes in Pinaceae",
    "summary": "  Mitochondrial genomes in the Pinaceae family are notable for their large size\nand structural complexity. In this study, we sequenced and analyzed the\nmitochondrial genome of Cathaya argyrophylla, an endangered and endemic\nPinaceae species, uncovering a genome size of 18.99 Mb, meaning the largest\nmitochondrial genome reported to date. To investigate the mechanisms behind\nthis exceptional size, we conducted comparative analyses with other Pinaceae\nspecies possessing both large and small mitochondrial genomes, as well as with\nother gymnosperms. We focused on repeat sequences, transposable element\nactivity, RNA editing events, chloroplast-derived sequence transfers (mtpts),\nand sequence homology with nuclear genomes. Our findings indicate that while\nCathaya argyrophylla and other extremely large Pinaceae mitochondrial genomes\ncontain substantial amounts of repeat sequences and show increased activity of\nLINEs and LTR retrotransposons, these factors alone do not fully account for\nthe genome expansion. Notably, we observed a significant incorporation of\nchloroplast-derived sequences in Cathaya argyrophylla and other large\nmitochondrial genomes, suggesting that extensive plastid-to-mitochondrial DNA\ntransfer may play a crucial role in genome enlargement. Additionally, large\nmitochondrial genomes exhibited distinct patterns of RNA editing and limited\nsimilarity with nuclear genomes compared to smaller genomes. These results\nsuggest that the massive mitochondrial genomes in Pinaceae are likely the\nresult of multiple contributing factors, including repeat sequences, transposon\nactivity, and extensive plastid sequence incorporation. Our study enhances the\nunderstanding of mitochondrial genome evolution in plants and provides valuable\ngenetic information for the conservation and study of Cathaya argyrophylla.\n"
  },
  {
    "year": 2024,
    "title": "Zimin patterns in genomes",
    "summary": "  Zimin words are words that have the same prefix and suffix. They are\nunavoidable patterns, with all sufficiently large strings encompassing them.\nHere, we examine for the first time the presence of k-mers not containing any\nZimin patterns, defined hereafter as Zimin avoidmers, in the human genome. We\nreport that in the reference human genome all k-mers above 104 base-pairs\ncontain Zimin words. We find that Zimin avoidmers are most enriched in coding\nand Human Satellite 1 regions in the human genome. Zimin avoidmers display a\ndepletion of germline insertions and deletions relative to surrounding genomic\nareas. We also apply our methodology in the genomes of another eight model\norganisms from all three domains of life, finding large differences in their\nZimin avoidmer frequencies and their genomic localization preferences. We\nobserve that Zimin avoidmers exhibit the highest genomic density in prokaryotic\norganisms, with E. coli showing particularly high levels, while the lowest\ndensity is found in eukaryotic organisms, with D. rerio having the lowest.\nAmong the studied genomes the longest k-mer length at which Zimin avoidmers are\nobserved is that of S. cerevisiae at k-mer length of 115 base-pairs. We\nconclude that Zimin avoidmers display inhomogeneous distributions in organismal\ngenomes, have intricate properties including lower insertion and deletion\nrates, and disappear faster than the theoretical expected k-mer length, across\nthe organismal genomes studied.\n"
  },
  {
    "year": 2024,
    "title": "Hierarchical Classification for Predicting Metastasis Using Elastic-Net\n  Regularization on Gene Expression Data",
    "summary": "  Metastasis is a leading cause of cancer-related mortality and remains\nchallenging to detect during early stages. Accurate identification of cancers\nlikely to metastasize can improve treatment strategies and patient outcomes.\nThis study leverages publicly available gene expression profiles from primary\ncancers, with and without distal metastasis, to build predictive models. We\nutilize elastic net regularization within a hierarchical classification\nframework to predict both the tissue of origin and the metastasis status of\nprimary tumors. Our elastic net-based hierarchical classification achieved a\ntissue-of-origin prediction accuracy of 97%, and a metastasis prediction\naccuracy of 90%. Notably, mitochondrial gene expression exhibited significant\nnegative correlations with metastasis, providing potential biological insights\ninto the underlying mechanisms of cancer progression.\n"
  },
  {
    "year": 2024,
    "title": "Rawsamble: Overlapping and Assembling Raw Nanopore Signals using a\n  Hash-based Seeding Mechanism",
    "summary": "  Raw nanopore signal analysis is a common approach in genomics to provide fast\nand resource-efficient analysis without translating the signals to bases (i.e.,\nwithout basecalling). However, existing solutions cannot interpret raw signals\ndirectly if a reference genome is unknown due to a lack of accurate mechanisms\nto handle increased noise in pairwise raw signal comparison. Our goal is to\nenable the direct analysis of raw signals without a reference genome. To this\nend, we propose Rawsamble, the first mechanism that can 1) identify regions of\nsimilarity between all raw signal pairs, known as all-vs-all overlapping, using\na hash-based search mechanism and 2) use these to construct genomes from\nscratch, called de novo assembly.\n  Our extensive evaluations across multiple genomes of varying sizes show that\nRawsamble provides a significant speedup (on average by 16.36x and up to\n41.59x) and reduces peak memory usage (on average by 11.73x and up to by\n41.99x) compared to a conventional genome assembly pipeline using the\nstate-of-the-art tools for basecalling (Dorado's fastest mode) and overlapping\n(minimap2) on a CPU. We find that 36.57% of overlapping pairs generated by\nRawsamble are identical to those generated by minimap2. Using the overlaps from\nRawsamble, we construct the first de novo assemblies directly from raw signals\nwithout basecalling. We show that we can construct contiguous assembly segments\n(unitigs) up to 2.7 million bases in length (half the genome length of E.\ncoli). We identify previously unexplored directions that can be enabled by\nfinding overlaps and constructing de novo assemblies. Rawsamble is available at\nhttps://github.com/CMU-SAFARI/RawHash. We also provide the scripts to fully\nreproduce our results on our GitHub page.\n"
  },
  {
    "year": 2024,
    "title": "GATES: Graph Attention Network with Global Expression Fusion for\n  Deciphering Spatial Transcriptome Architectures",
    "summary": "  Single-cell spatial transcriptomics (ST) offers a unique approach to\nmeasuring gene expression profiles and spatial cell locations simultaneously.\nHowever, most existing ST methods assume that cells in closer spatial proximity\nexhibit more similar gene expression patterns. Such assumption typically\nresults in graph structures that prioritize local spatial information while\noverlooking global patterns, limiting the ability to fully capture the broader\nstructural features of biological tissues. To overcome this limitation, we\npropose GATES (Graph Attention neTwork with global Expression fuSion), a novel\nmodel designed to capture structural details in spatial transcriptomics data.\nGATES first constructs an expression graph that integrates local and global\ninformation by leveraging both spatial proximity and gene expression\nsimilarity. The model then employs an autoencoder with adaptive attention to\nassign proper weights for neighboring nodes, enhancing its capability of\nfeature extraction. By fusing features of both the spatial and expression\ngraphs, GATES effectively balances spatial context with gene expression data.\nExperimental results across multiple datasets demonstrate that GATES\nsignificantly outperforms existing methods in identifying spatial domains,\nhighlighting its potential for analyzing complex biological tissues. Our code\ncan be accessed on GitHub at https://github.com/xiaoxiongtao/GATES.\n"
  },
  {
    "year": 2024,
    "title": "gggenomes: effective and versatile visualizations for comparative\n  genomics",
    "summary": "  The effective visualization of genomic data is crucial for exploring and\ninterpreting complex relationships within and across genes and genomes. Despite\nadvances in developing dedicated bioinformatics software, common visualization\ntools often fail to efficiently integrate the diverse datasets produced in\ncomparative genomics, lack intuitive interfaces to construct complex plots and\nare missing functionalities to inspect the underlying data iteratively and at\nscale. Here, we introduce gggenomes, a versatile R package designed to overcome\nthese challenges by extending the widely used ggplot2 framework for comparative\ngenomics. gggenomes is available from CRAN and GitHub, accompanied by detailed\nand user-friendly documentation (https://thackl.github.io/gggenomes).\n"
  },
  {
    "year": 2024,
    "title": "Upcycling Human Excrement: The Gut Microbiome to Soil Microbiome Axis",
    "summary": "  Human excrement composting (HEC) is a sustainable strategy for human\nexcrement (HE) management that recycles nutrients and mitigates health risks\nwhile reducing reliance on freshwater, fossil fuels, and fertilizers. We\npresent a comprehensive microbial time series analysis of HEC and show that the\ninitial gut-like microbiome of HEC systems transitions to a microbiome similar\nto soil and traditional compost in fifteen biological replicates tracked weekly\nfor one year.\n"
  },
  {
    "year": 2024,
    "title": "Perspective on recent developments and challenges in regulatory and\n  systems genomics",
    "summary": "  Predicting how genetic variation affects phenotypic outcomes at the\norganismal, cellular, and molecular levels requires deciphering the\ncis-regulatory code, the sequence rules by which non-coding regions regulate\ngenes. In this perspective, we discuss recent computational progress and\nchallenges towards solving this fundamental problem. We describe how\ncis-regulatory elements are mapped and how their sequence rules can be learned\nand interpreted with sequence-to-function neural networks, with the goal of\nidentifying genetic variants in human disease. We also discuss how studies of\nthe 3D chromatin organization could help identifying long-range regulatory\neffects and how current methods for mapping gene regulatory networks could\nbetter describe biological processes. We point out current gaps in knowledge\nalong with technical limitations and benchmarking challenges of computational\nmethods. Finally, we discuss newly emerging technologies, such as spatial\ntranscriptomics, and outline strategies for creating a more general model of\nthe cis-regulatory code that is more broadly applicable across cell types and\nindividuals.\n"
  },
  {
    "year": 2024,
    "title": "Use of 3D chaos game representation to quantify DNA sequence similarity\n  with applications for hierarchical clustering",
    "summary": "  A 3D chaos game is shown to be a useful way for encoding DNA sequences. Since\nmatching subsequences in DNA converge in space in 3D chaos game encoding, a DNA\nsequence's 3D chaos game representation can be used to compare DNA sequences\nwithout prior alignment and without truncating or padding any of the sequences.\nTwo proposed methods inspired by shape-similarity comparison techniques show\nthat this form of encoding can perform as well as alignment-based techniques\nfor building phylogenetic trees. The first method uses the volume overlap of\nintersecting spheres and the second uses shape signatures by summarizing the\ncoordinates, oriented angles, and oriented distances of the 3D chaos game\ntrajectory. The methods are tested using: (1) the first exon of the beta-globin\ngene for 11 species, (2) mitochondrial DNA from four groups of primates, and\n(3) a set of synthetic DNA sequences. Simulations show that the proposed\nmethods produce distances that reflect the number of mutation events;\nadditionally, on average, distances resulting from deletion mutations are\ncomparable to those produced by substitution mutations.\n"
  },
  {
    "year": 2024,
    "title": "Leveraging genomic deep learning models for non-coding variant effect\n  prediction",
    "summary": "  The majority of genetic variants identified in genome-wide association\nstudies of complex traits are non-coding, and characterizing their function\nremains an important challenge in human genetics. Genomic deep learning models\nhave emerged as a promising approach to enable in silico prediction of variant\neffects. These include supervised sequence-to-activity models, which predict\ngenome-wide chromatin states or gene expression levels directly from DNA\nsequence, and self-supervised genomic language models. Here, we review progress\nin leveraging these models for non-coding variant effect prediction. We\ndescribe practical considerations for making such predictions and categorize\nthe types of ground truth data that have been used to evaluate deep\nlearning-based variant effect predictions, providing insight into the settings\nin which current models are most useful. We also discuss downstream\napplications of such models to understanding disease-relevant non-coding\nvariants. Our review highlights key considerations for practitioners and\nopportunities for future improvements in model development and evaluation.\n"
  },
  {
    "year": 2024,
    "title": "ukbFGSEA: an R Package for Applying Fast Preranked Gene Set Enrichment\n  Analysis to UK Biobank Exome Data",
    "summary": "  The Genebass dataset, released by Karczewski et al. (2022), provides a\ncomprehensive resource elucidating associations between genes and 4,529\nphenotypes based on nearly 400,000 exomes from the UK Biobank. This extensive\ndataset enables the evaluation of gene set enrichment across a wide range of\nphenotypes, facilitating the inference of associations between specified gene\nsets and phenotypic traits. Despite its potential, no established method for\napplying gene set enrichment analysis (GSEA) to Genebass data exists. To\naddress this gap, we propose utilizing fast pre-ranked gene set enrichment\nanalysis (FGSEA) as a novel approach to determine whether a specified set of\ngenes is significantly enriched in phenotypes within the UK Biobank. We\ndeveloped an R package, ukbFGSEA, to implement this analysis, completed with a\nhands-on tutorial. Our approach has been validated by analyzing gene sets\nassociated with autism spectrum disorder, developmental disorder, and\nneurodevelopmental disorders, demonstrating its capability to reveal\nestablished and novel associations.\n"
  },
  {
    "year": 2024,
    "title": "Evidence of epigenetic oncogenesis: a turning point in cancer research",
    "summary": "  In cancer research, the term epigenetics was used in the 1970s in its modern\nsense encompassing non-genetic events modifying the chromatin state, mainly to\noppose the emerging oncogene paradigm. However, starting from the establishment\nof this prominent concept, the importance of these epigenetic phenomena in\ncancer rarely led to questioning the causal role of genetic alterations. Only\nin the last 10 years, the accumulation of problematic data, better experimental\ntechnologies, and some ambitious models pushed the idea that epigenetics could\nbe at least as important as genetics in early oncogenesis. Until this year, a\ndirect demonstration of epigenetic oncogenesis was still lacking. Now Parreno,\nCavalli and colleagues, using a refined experimental model in the fruit fly\nDrosophila melanogaster, enforced the initiation of tumours solely by imposing\na transient loss of Polycomb repression, leading to a purely epigenetic\noncogenesis phenomenon. Despite a few caveats that we discuss, this pioneering\nwork represents a major breakpoint in cancer research that leads us to consider\nthe theoretical and conceptual implications on oncogenesis and to search for\nlinks between this artificial experimental model and naturally occurring\nprocesses, while revisiting cancer theories that were previously proposed as\nalternatives to the oncogene-centered paradigm.\n"
  }
]