{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ru007471\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import socket\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import httpx\n",
    "import h5py\n",
    "import os\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the NLTK stopwords downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from scipy.spatial import ConvexHull\n",
    "from datetime import datetime \n",
    "\n",
    "import openai\n",
    "from bertopic.representation import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERTOpic Modelling\n",
    "def create_topic_model(docs, embeddings, nr_topics=36, n_gram_range=(1, 3), min_cluster_size=50):\n",
    "    \"\"\"\n",
    "    Create and apply a BERTopic model to a given set of documents and embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list of str): The documents to be analyzed.\n",
    "    embeddings (ndarray): Precomputed embeddings for the documents.\n",
    "    nr_topics (int): Maximum number of topics to extract.\n",
    "    n_gram_range (tuple): The range of n-grams to consider for topic tokenization.\n",
    "    min_cluster_size (int): Minimum size of clusters.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the topic information for each document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 - Extract embeddings\n",
    "    embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(n_neighbors=30,  # Neighboring sample points (Increasing means more global view)\n",
    "                      n_components=20, # Reduced dimensions of the embeddings\n",
    "                      min_dist=0.0, \n",
    "                      metric='euclidean')\n",
    "\n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                            metric='euclidean', \n",
    "                            cluster_selection_method='eom', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\",  # Removes stopwords\n",
    "                                       ngram_range=n_gram_range)  # No. of words in a topic\n",
    "\n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "    # Step 6 - (Optional) Fine-tune topic representations with \n",
    "    # a `bertopic.representation` model\n",
    "    representation_model = {\n",
    "      \"KeyBERET\": KeyBERTInspired(\n",
    "        top_n_words=15,  # The top n words to extract per topic (default 10)\n",
    "        nr_repr_docs=12,  # The number of representative documents to extract per cluster (default 5)\n",
    "        nr_samples=300  # The number of candidate documents to extract per cluster (default 500)\n",
    "      )\n",
    "    }\n",
    "\n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "      embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "      umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "      hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "      vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "      ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "      representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic representations\n",
    "      nr_topics=nr_topics,                      # Maximum number of topics\n",
    "      n_gram_range=n_gram_range                 # No. of words per topic\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    topics, ini_probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "    # Making the outliers part of the dataset\n",
    "    new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\")\n",
    "    topic_model.update_topics(docs, topics=new_topics)\n",
    "        \n",
    "    # Create result DataFrame\n",
    "    #df_res = pd.DataFrame({'Topic': topics, 'Probability': ini_probs})\n",
    "    df_res = pd.DataFrame({'Topic': new_topics, 'Probability': ini_probs})\n",
    "    \n",
    "    # Convert the list of topic labels to a DataFrame\n",
    "    df_topic = pd.DataFrame({'Topic': range(len(topic_model.get_topic_info())), 'Name': topic_model.generate_topic_labels(nr_words=10, topic_prefix=True, word_length=None, separator='_', aspect=None)})\n",
    "    \n",
    "    df_res = df_res.merge(df_topic, how='left', on='Topic')\n",
    "    df_res.rename(columns={'Name': 'label',\n",
    "                           'Topic': 'cluster'}, inplace=True)\n",
    "\n",
    "    return df_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
