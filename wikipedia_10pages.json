[{"title": "Bank for International Settlements", "text": "The Bank for International Settlements (BIS) is an international financial institution which is owned by member central banks. Its primary goal is to foster international monetary and financial cooperation while serving as a bank for central banks. With its establishment in 1930 it is the oldest international financial institution. Its initial purpose was to oversee the settlement of World War I war reparations.\nThe BIS carries out its work through its meetings, programmes and through the Basel Process, hosting international groups pursuing global financial stability and facilitating their interaction. It also provides banking services, but only to central banks and other international organizations. \nThe BIS is based in Basel, Switzerland, with representative offices in Hong Kong and Mexico City.\n\nHistory\nBackground\nInternational monetary cooperation started to develop tentatively in the course of the 19th century. An early case was a \u00a3400,000 loan in gold coins from the Bank of France to the Bank of England which was facing a bank run, made in 1825 and facilitated by the Rothschilds. The Bank of England again borrowed from its French counterpart (and from the Hamburger Bank) in 1836 and 1839, and lent to it in return in 1847. \nIn 1860-1861, because of the disruption from the incipient American Civil War, the Bank of France entered a series of swap agreements on specie with the Bank of England as well as the State Bank of the Russian Empire and De Nederlandsche Bank. That episode was recorded as the \"war of the banks\", ostensibly because of frictions between the Bank of France and the Bank of England about the transaction.:\u200a66\u201367\u200a\nA few years later, monetary cooperation took a novel form with a series of international monetary conferences devoted to better coordination of the coinage system, even though these initiatives, like the Latin Monetary Union started in 1865, did not extend to money other than coins, and therefore involved treasury and mint officials rather than bankers.:\u200a69\u200a \nAt the Brussels Conference in 1892, German academic Julius Wolff submitted a blueprint for an international currency that would be used for emergency lending to national central banks and would be issued by an institution based in a neutral country. In 1893, French economist Rapha\u00ebl-Georges L\u00e9vy suggested to establish an international central bank in Bern. \nIn 1907, Italian statesman Luigi Luzzatti published an article in the Austrian Neue Freie Presse, referencing past examples of bilateral cooperation between central banks and emphasizing the need for more institutionalized cooperation at the international level.:\u200a21\u200a\nThe practice of formalized central bank cooperation made unprecedented advances among allies in the course of World War I. In 1916, the Bank of England and Bank of France made agreements on bilateral lending and established a direct telegraph line between their respective offices to facilitate communication. Similar formal agreements were made that year between the two banks and the Federal Reserve Bank of New York, and in 1917 the Bank of Italy opened an office in New York.:\u200a17\u200a\nIn the war's immediate aftermath, Dutch central banker Gerard Vissering advocated an international currency without reliance of a common gold pool.:\u200a22\u200a Similar ideas burgeoned at the Brussels Conference of 1920, the first major discussion of international financial challenges following the war, endorsed by luminaries such as Belgian prime minister L\u00e9on Delacroix and American banker Frank A. Vanderlip, who suggested reorganizing Europe's national central banks along similar lines as the U.S. Federal Reserve which he had helped establish in the previous decade. At the Genoa Conference of 1922, following advocacy by several experts that included Ralph Hawtrey, Robert Horne and John Maynard Keynes, a resolution was passed that recommended the creation of \"an association or permanent understanding for cooperation amongst central banks, not necessarily limited to Europe, to coordinate credit policies, without detriment to the freedom of each individual central bank.\":\u200a23\u200a\nThe decision to create the BIS took place in the context of negotiations over World War I reparations which plagued international relations in Europe throughout the 1920s. Following the Treaty of Versailles, a Reparation Commission was set up in January 1920 to determine the amount of German reparations. Conferences at Spa in July 1920 and London in March 1921 were followed by the occupation of the Ruhr in January 1923, and eventually the Dawes Plan approved at another London conference in July-August 1924. The latter allowed for a more constructive atmosphere, materialized in diplomacy by the Locarno Treaties in October 1925 and encouraging Montagu Norman, the influential governor of the Bank of England, to envisage the creation of what he described in September 1925 as \"a private and eclectic Central Banks' 'Club', small at first, larger in the future.\":\u200a30\u200a That vision had a first materialization at a meeting in early July 1927 which brought together Montagu, his friend Benjamin Strong, head of the Federal Reserve Bank of New York, Reichsbank president Hjalmar Schacht, and Bank of France vice governor Charles Rist at a private home on Long Island (the Bank of Italy had hoped for an invitation but was not included). A second meeting was scheduled in Algeciras, but was not held because of the bad health of Strong, who eventually died in October 1928.:\u200a31\n\nThe Young Plan and the Hague Conference\nA deadline for French repayment of its bilateral debt to the United States provided impetus for a new initiative, which took the form of a Committee of Experts appointed to work out a final settlement of the German reparations, known as the Young Committee for its chairman the American banker Owen D. Young. The committee first met at the Bank of France on 9 February 1929, then on 28 successive sessions ending on 7 June 1929 at the Hotel George V. The seven participating countries were Belgium, France, Germany, Italy, Japan, the United Kingdom and the United States. The need for a jointly governed bank emerged in these discussions as a means to overcome information asymmetries and increase the likelihood that commitments would be effectively met, not least by helping the creditors to act collectively and facilitating the reinvestment of German payments into the German economy.:\u200a34\u200a The first draft concept for the new bank was presented by Belgian banker \u00c9mile Francqui on 23 February 1929, and amended with suggestions from Bank of France governor \u00c9mile Moreau. It was envisaged as a private institution with shareholders from all participating countries (including Germany) that would settle reparation payments, issue bonds to be serviced by the reparation transfers, and (as advocated by Schacht) provide international long-term credit for countries in need, including Germany.:\u200a35\u200a In a memo to Young a few days later, Schacht first used the name \"International Settlements Bank\" while referring to the projected new institution.:\u200a36\u200a Young asked his American peers Warren Randolph Burgess, Shepard Morgan and Walter W. Stewart to sail promptly to Paris, and on 7 March 1929 they presented a compromise text that formed the basis for subsequent developments. \nUnder the Young Committee's consensus concept, made public on 10 March 1929, the bank would serve a threefold purpose as a trustee, bank, and international organization of central bankers: (1) receiving, managing, and distributing German reparation annuities as a trustee; (2) facilitating German transfers by issuing counterpart bills, notes, and bonds; and (3) serving national central banks by taking their deposits, granting them credit, and carrying out currency and gold transactions on their behalf. It would rely on nonpolitical staff located in a country not directly involved in the reparations disputes. Subsequent fine-tuning discussions revolved around the scope for the bank's lending to foster economic growth and trade which would have given it a role similar to that of the later World bank. Such a role was advocated by Schacht but opposed by France and by commercial bankers, on the grounds that it could be inflationary and create unfair competition to private-sector lenders. :\u200a37\u200a An overall agreement on the future bank, with draft statutes prepared by the Bank of England's Charles Stewart Addis, was achieved by the Young Committee on 25 March 1929.:\u200a38\u200a\nPolitical positions within the Herbert Hoover administration made it impossible for U.S. Federal Reserve System officials to be formally involved in the initiative, but the U.S. was still able to retain major influence in the proceedings because of a shared perception amongst negotiators that the project would fail without U.S. participation. Major figures of the U.S. financial world would participate in the joint bank, and act in close cooperation with the Federal Reserve Bank of New York. The leverage held by the U.S. allowed Young and J. P. Morgan Jr. to make sure that Americans would be in leadership position at the bank when it started operations, as indeed happened. \nThe BIS concept was agreed to in August 1929 at the first part of the Hague conference on reparations. The bank's Charter, Statutes, Trust Agreement, and Convention on its relations with the host country were subsequently drafted by a special Organisation Committee chaired by Jackson Reynolds, president of the First National Bank of New York,:\u200a47\u200a which met in the discreet location of H\u00f4tel St\u00e9phanie (part of which later became Brenners Park-Hotel & Spa) at Baden-Baden from 3 October to 13 November 1929; the intense work was marred by the death of Delacroix from a heart attack during the proceedings. Aside from Reynolds, American participants in Baden-baden also included Melvin Alvah Traylor, president of the First National Bank of Chicago, Warren Randolph Burgess, Shepard Morgan, and Leon Fraser (a legal expert with the Agent General for Reparation Payments), with J. P. Morgan Jr. monitoring the proceedings and advising from London.:\u200a616-618\u200a\nAs in the Paris discussions earlier in the year, the Baden-Baden committee had to reconcile the different visions for the future BIS, from purely a creation of central banks (as espoused by Italy's Alberto Beneduce and by Montagu Norman) to a supranational development bank with policy tasks such as developing world trade (as advocated by Schacht and UK Chancellor Philip Snowden). Other points of contention included the future institution's official language(s), for which the committee endorsed French, and location. For the latter, several delegates favored London, but that was vetoed by the French who proposed Brussels instead, which in turn was vetoed by the British; after Amsterdam failed to gain sufficient support, a consensus was eventually found on Basel, which combined neutral country status and good railway connections.:\u200a40-44\u200a The founding texts of the BIS were then approved at the second part of the Hague conference, on 20 January 1930, with only minor changes from the Baden-Baden drafts such as the addition of English to French as official language.:\u200a41\u200a These texts included the constituent Charter and Statutes for the bank, and a Convention (intergovernmental agreement) between Germany, Belgium, France, the United Kingdom, Italy, Japan, the United States, and Switzerland, establishing the bank's special status on Swiss soil and committing Switzerland to grant the Charter and approve the Statutes.\n\nCreation\nThe Convention and Charter were approved by the Swiss Federal Council and thus obtained force of law on 26 February 1930. The governors met that day and the next to formally approve and sign the statutes in Rome, out of consideration for the Bank of Italy's governor Bonaldo Stringher who was both the most senior in the group, and in poor health (he would pass away in December of 1930). Aside from Stringher, who chaired the meeting, the other participants were: Vincenzo Azzolini and Alberto Beneduce, for Italy; Bank of England governor Montagu Norman and Harry Arthur Siepmann, for the UK; Bank of France governor \u00c9mile Moreau, Cl\u00e9ment Moret and Pierre Quesnay, for France; Reichsbank governor Schacht, for Germany; National Bank of Belgium governor Louis Franck and Paul van Zeeland, for Belgium; and the Bank of Japan's London representative Tetsuzaburo Tanaka and diplomat Hiroshi Saito, for Japan.:\u200a61\u200a Thus the BIS was formally created in Rome on 27 February 1930. The BIS promptly opened its doors in Basel on 17 May 1930, ahead of the first German annuities under the Young Plan due in June.:\u200a61\u200a\nThe legal status of the bank combined features of a private-sector company and of a public international organization. It was a limited-liability company incorporated under Swiss law, whose shares could be held by individuals and non-governmental entities. However, the rights of voting and representation at the Bank's General Meeting were to be exercised exclusively by the central banks of the countries in which shares had been issued. At the same time, the BIS possessed de facto international legal personality, was exempted from Swiss taxation and banking supervision, and its senior management enjoyed diplomatic status. The Charter stipulated that \"The Bank, its property and assets and all deposits and other funds entrusted to it shall be immune in time of peace and in time of war from any measure such as expropriation, requisition, seizure, confiscation, prohibition or restriction of gold or currency export or import, and any other similar measures.\" Under the Statutes, the governor of each of the founding central banks was a member of the BIS Board of Directors ex officio, and had the right to appoint a second Board member, plus additional right for France and Germany to appoint a third Board member each or the duration of the Young Plan. In principle the Board could appoint up to nine additional directors, in practice however only the Dutch, Swedish and Swiss central bank governors in the BIS's first decades.:\u200a49-50\u200a The inaugural BIS Board had 16 members: Franck and Francqui (Belgium); Moreau, Georges Brincard and Melchior de Vog\u00fc\u00e9 (France); Hans Luther, Carl Melchior and Paul Reusch (Germany); Stringher and Beneduce (Italy); Tanaka and Daisuke Nohara (Japan); Norman and Addis (UK); and Gates McGarrah and Fraser (United States).:\u200a62\n\nEarly activity\nThe BIS had a quick start, even though its intended task of facilitating World War I reparation payments soon became obsolete. Even before the founding meeting in Rome, the Organisation Committee had secured a two-year lease of a convenient building in Basel, the former Grand H\u00f4tel et Savoy H\u00f4tel Univers across the street from Basel railway station. By mid-April, Fraser had arrived from the United States and was working on the BIS's behalf from the Paris office of the Agent General for Reparation Payments, joined by McGarrah in April. The Board members first met for an informal meeting in Basel on 22-23 April 1930, adopted the bank's Statutes. It unanimously elected McGarrah as President of the BIS and Chairman of its Board, Fraser as alternate President, and Addis, Melchior and Moreau as Vice-Chairmen \"with equal rank\". Quesnay was appointed General Manager, albeit with the three German Board members voting against \"for serious reasons of principle\" (i.e. objecting to the choice of a French national rather than to Quesnay as an individual). These foundational decisions were later ratified by the first formal Board meeting on 17 May 1930, as the April meeting had also agreed that the ordinary meetings of the Board would henceforth take place in Basel on the second Monday of each month. On 17 May the BIS opened for business and formally took over the funds, accounts, capital, and records of the Agent General for Reparation Payments. On 20 May, the bank's shares were publicly issued.:\u200a62-63\u200a\nReparation payments were first suspended for one year (Hoover moratorium, June 1931) and then stopped altogether after the Lausanne Agreement of July 1932 failed to be ratified. Instead, the BIS focused on its second statutory task, i.e. fostering the cooperation between its member central banks. It acted as a meeting forum for central banks and provided banking facilities to them. For instance, in the late 1930s, the BIS was instrumental in helping continental European central banks ship out part of their gold reserves to London. \nAs a purportedly apolitical organization, the BIS was unable to prevent transactions that reflected contemporaneous geopolitical realities, but were also widely regarded as unconscionable. As a result of the policy of appeasement of Nazi Germany by the UK and France, in March 1939, the BIS was obliged to transfer 23 tons of gold it held, on behalf of Czechoslovakia, to the German Reichsbank, following the German annexation of Czechoslovakia. The decision to carry out the transaction is still considered the most controversial by BIS.\n\nWorld War II\nAt the outbreak of World War II in September 1939, the BIS Board of Directors \u2013 on which the main European central banks were represented \u2013 decided that the Bank should remain open, but that, for the duration of hostilities, no meetings of the Board of Directors were to take place and that the Bank should maintain a neutral stance in the conduct of its business. However, as the war dragged on evidence mounted that the BIS conducted operations that were helpful to the Germans. Also, throughout the war, the Allies accused the Nazis of looting and pleaded with the BIS not to accept gold from the Reichsbank in payment for prewar obligations linked to the Young Plan. This was to no avail as remelted gold was either confiscated from prisoners or seized in victory and thus unacceptable as payment to the BIS.:\u200a245\u2013252\u200a Operations conducted by the BIS were viewed with increasing suspicion from London and Washington. The fact that top-level German industrialists and advisors sat on the BIS Board seemed to provide ample evidence of how the BIS might be used by Hitler throughout the war, with the help of American, British and French banks. Between 1933 and 1945 the BIS Board of directors included Walther Funk, a prominent Nazi official, and Emil Puhl responsible for processing dental gold looted from concentration camp victims, as well as Hermann Schmitz, the director of IG Farben, and Baron von Schroeder, the owner of the J. H. Stein Bank, all of whom were later convicted of war crimes or crimes against humanity.\nThe 1944 Bretton Woods Conference recommended the \"liquidation of the Bank for International Settlements at the earliest possible moment\". This resulted in the BIS being the subject of a disagreement between the U.S. and British delegations. The liquidation of the bank was supported by other European delegates, as well as Americans (including Harry Dexter White and Secretary of the Treasury Henry Morgenthau Jr.). Abolition was opposed by John Maynard Keynes, head of the British delegation.\nKeynes went to Morgenthau hoping to prevent or postpone the dissolution, but the next day it was approved; the liquidation of the bank was never actually undertaken. In April 1945, the new U.S. president Harry S. Truman ended U.S. involvement in the scheme. The British government suspended the dissolution and the decision to liquidate the BIS was officially reversed in 1948.\n\nPostwar decades\nAfter World War II, the BIS retained a distinct European focus. According to an announcement made by the Swiss Government on 26 December 1952, Japan renounced all rights, titles and interests in the BIS it had acquired under the Hague Convention of January 1930. The BIS acted as Agent for the European Payments Union (EPU, 1950\u201358), an intra-European clearing arrangement designed to help the European countries in restoring currency convertibility and free, multilateral trade. During the 1960s \u2013 the heyday of the Bretton Woods fixed exchange rate system \u2013 the BIS once again became the locus for transatlantic monetary cooperation. It coordinated the central banks' Gold Pool:\u200a416\u200a and a number of currency support operations (e.g. Sterling Group Arrangements of 1966 and 1968. The Group of Ten (G10), including the main European economies, Canada, Japan, and the United States, became the most prominent grouping.\nThe BIS acquired land near the Basel SBB railway station between 1966 and 1972. Architect Martin Burckhardt made three design proposals in 1969, among which the Board of the BIS selected an 82-meter high round tower. This was opposed by locals and their representation in the Swiss Heritage Society, which led to a public referendum in 1971 in which 69% of voters endorsed a revised design with reduced height. The BIS moved into the new premises, sometimes dubbed the \"Tower of Basel,\" in 1977. \nWith the end of the Bretton Woods system (1971\u201373) and the return to floating exchange rates, financial instability came to the fore. The collapse of some internationally active banks, such as Herstatt Bank (1974), highlighted the need for improved banking supervision at an international level. The G10 Governors created the Basel Committee on Banking Supervision (BCBS), which remains active. The BIS developed into a global meeting place for regulators and for developing international standards (Basel Concordat, Basel Capital Accord, Basel II and III). Through its member central banks, the BIS was actively involved in the resolution of the Latin American debt crisis (1982).\nFrom 1964 until 1993, the BIS provided the secretariat for the Committee of Governors of the Central Banks of the Member States of the European Community (Committee of Governors). This Committee had been created by the European Council decision to improve monetary cooperation among the EC central banks. Likewise, the BIS in 1988\u201389 hosted most of the meetings of the Delors Committee (Committee for the Study of Economic and Monetary Union), which produced a blueprint for monetary unification subsequently adopted in the Maastricht Treaty (1992). In 1993, when the Committee of Governors was replaced by the European Monetary Institute (EMI \u2013 the precursor of the ECB), it moved from Basel to Frankfurt, cutting its ties with the BIS.\nIn 1998, the BIS acquired a second building on Aeschenplatz 1 in Basel, designed in 1986 by Mario Botta and previously owned and used by UBS. Since then, the BIS has used that building to host its banking operations on behalf of member central banks.\n\n21st century\nIn the 1990s\u20132000s, the BIS successfully globalized, breaking out of its traditional European core. This was reflected in a gradual increase in its membership (from 33 shareholding central bank members in 1995 to 60 in 2013, which together represent roughly 95% of global GDP), and also in the much more global composition of the BIS Board of Directors. In 1998, the BIS opened a Representative Office for Asia and the Pacific in the Hong Kong SAR. A BIS Representative Office for the Americas was established in 2002 in Mexico City.\nThe BIS was originally owned by both central banks and private individuals, since the United States, Belgium and France had decided to sell all or some of the shares allocated to their central banks to private investors. BIS shares traded on stock markets, which made the bank an unusual organization: an international organization (in the technical sense of public international law), yet allowed for private shareholders. Many central banks had similarly started as such private institutions; for example, the Bank of England was privately owned until 1946. In more recent years the BIS has bought back its once publicly traded shares. It is now wholly owned by BIS members (central banks), but still operates in the private market as a counterparty, asset manager and lender for central banks and international financial institutions. Profits from its transactions are used, among other things, to fund the bank's other international activities.\nAfter the 2022 Russian invasion of Ukraine, in March 2022 the BIS suspended the Bank of Russia's membership.\nProject Nexus\nThe Bank for International Settlements signed an agreement with Central Bank of Malaysia, Bank of Thailand, Bangko Sentral ng Pilipinas, Monetary Authority of Singapore, and the Reserve Bank of India on 30 June 2024 as founding member of Project Nexus, a multilateral international initiative to enable retail cross-border payments. Bank Indonesia involved as a special observer. The platform, which is expected to go live by 2026, will interlink domestic fast payment systems of the member countries.\n\nMembership\nThe BIS members are central banks of 63 jurisdictions: 34 in Europe, 16 in Asia, 5 in South America, 3 in North America, 3 in Africa, and 2 in Oceania. The United States is represented by two members, the United States Federal Reserve System and Federal Reserve Bank of New York. The Central Bank of Russia is a member but its engagement with the BIS has been suspended since early March 2022 (see History section above). In the list below, (*) indicates members of the BIS Global Economy Meetings (see below) and (**) indicates observers to these meetings.\n\nBasel Meetings\nThe activity of the BIS has always revolved around the regular meetings of its membership in Basel. In the 1930s, these meetings were held every month, with two interruptions resulting in ten meetings per year.:\u200a4\u200a Since 1998, these meetings have been held every other month, so six times a year. The meetings always start on Sundays, when the dinner is a key moment for informal exchange and coordination, and extend over the next day or two. The meeting on Monday morning is the Global Economy Meeting (GEM), preceded by a meeting of the Economic Coordination Committee on Sunday. With the suspension of Russia since March 2022, 30 jurisdictions are members of the GEM and an additional 22 participate as observers.\nAs an organization of central banks, the BIS seeks to make monetary policy more predictable and transparent among its 60-member central banks, except in the case of Eurozone countries which forfeited the right to conduct monetary policy in order to implement the euro. While monetary policy is determined by most sovereign nations, it is subject to central and private banking scrutiny and potentially to speculation that affects foreign exchange rates and especially the fate of export economies. BIS aims to keep monetary policy in line with reality and to help implement monetary reforms in time, preferably as a simultaneous policy among all 60 member banks and also involving the International Monetary Fund.\nCentral banks do not unilaterally \"set\" rates, rather they set goals and intervene using their massive financial resources and regulatory powers to achieve monetary targets they set. One reason to coordinate policy closely is to ensure that this does not become too expensive and that opportunities for private arbitrage exploiting shifts in policy or difference in policy, are rare and quickly removed.\nThe stated mission of the BIS is to serve central banks in their pursuit of monetary and financial stability, to foster international cooperation in those areas and to act as a bank for central banks. The BIS pursues its mission by:\n\nfostering discussion and facilitating collaboration among central banks;\nsupporting dialogue with other authorities that are responsible for promoting financial stability;\ncarrying out research and policy analysis on issues of relevance for monetary and financial stability;\nacting as a prime counterparty for central banks in their financial transactions; and\nserving as an agent or trustee in connection with international financial operations.\nThe role that the BIS plays today goes beyond its historical role. The original goal of the BIS was \"to promote the co-operation of central banks and to provide additional facilities for international financial operations; and to act as trustee or agent in regard to international financial settlements entrusted to it under agreements with the parties concerned\", as stated in its Statutes of 1930.\n\nBasel Committee on Banking Supervision\nThe BIS hosts the Secretariat of the Basel Committee on Banking Supervision (BCBS), colloquially referred to simply as the \"Basel Committee\", and with it has played a central role in establishing the Basel Capital Accords (now commonly referred to as Basel I) of 1988, Basel II framework in 2004 and more recently Basel III framework in 2010-2017.\nCapital adequacy policy applies to equity and capital assets. These can be overvalued in many circumstances because they do not always reflect current market conditions or adequately assess the risk of every trading position. Accordingly, the Basel standards require the capital adequacy ratio of internationally active commercial banks to be above a prescribed minimum international standard, to improve the resilience of the banking sector.\n\nCommittee on the Global Financial System\nThe Committee on the Global Financial System (CGFS) was established in 1971 as the Euro-currency Standing Committee, and adopted its current name in 1999. It reports to the Global Economy Meeting. \nAs of 2023, it had 28 members: Central Bank of Argentina, Reserve Bank of Australia, National Bank of Belgium, Central Bank of Brazil, Bank of Canada, People's Bank of China, European Central Bank, Bank of France, Deutsche Bundesbank, Hong Kong Monetary Authority, Reserve Bank of India, Bank of Italy, Bank of Japan, Bank of Korea, Central Bank of Luxembourg, Bank of Mexico, De Nederlandsche Bank, Central Bank of Russia, Saudi Central Bank, Monetary Authority of Singapore, South African Reserve Bank, Bank of Spain, Sveriges Riksbank, Swiss National Bank, Bank of Thailand, Bank of England, Board of Governors of the Federal Reserve System, and Federal Reserve Bank of New York.\n\nMarkets Committee\nThe Markets Committee is the oldest of the BIS-hosted committees, originally established in 1962 as the Committee on Gold and Foreign Exchange. It also reports to the Global Economy Meeting. \nAs of 2023, it had 27 members: Reserve Bank of Australia, National Bank of Belgium, Central Bank of Brazil, Bank of Canada, People's Bank of China, European Central Bank, Bank of France, Deutsche Bundesbank, Hong Kong Monetary Authority, Reserve Bank of India, Bank of Indonesia, Bank of Italy, Bank of Japan, Bank of Korea, Central Bank of Malaysia, Bank of Mexico, De Nederlandsche Bank, Central Bank of Russia, Monetary Authority of Singapore, South African Reserve Bank, Bank of Spain, Sveriges Riksbank, Swiss National Bank, Central Bank of the Republic of T\u00fcrkiye, Bank of England, Board of Governors of the Federal Reserve System, and Federal Reserve Bank of New York.\n\nCommittee on Payments and Market Infrastructure\nAnother of the committees hosted at the BIS is the Committee on Payments and Market Infrastructures (CPMI). The Committee on Payment and Settlement Systems (CPSS) was established in 1990 and extended the prior work of the Group of Experts on Payment Systems (1980) and Committee on Interbank Netting Schemes (1989), and was in turn renamed to CPMI in 2014. Its membership was extended in 1997-98, 2009, and 2018 to reach the following 28 members: Central Bank of Argentina, Reserve Bank of Australia, National Bank of Belgium, Central Bank of Brazil, Bank of Canada, People's Bank of China, European Central Bank, Bank of France, Deutsche Bundesbank, Hong Kong Monetary Authority, Reserve Bank of India, Bank Indonesia, Bank of Italy, Bank of Japan, Bank of Korea, Bank of Mexico, De Nederlandsche Bank, Central Bank of Russia, Saudi Central Bank, Monetary Authority of Singapore, South African Reserve Bank, Bank of Spain, Sveriges Riksbank, Swiss National Bank, Central Bank of the Republic of T\u00fcrkiye, Bank of England, the Board of Governors of the Federal Reserve System and Federal Reserve Bank of New York. \nOne of the Group's first projects, a detailed review of payment system developments in the G10 countries, was published by the BIS in 1985 in the first of a series that has become known as \"Red Books\". Currently, the red books cover countries participating in the CPMI. A sample of statistical data in the red books appears in the table below, where local currency is converted to US dollars using end-of-year rates.\n\nIrving Fisher Committee\nThe Irving Fisher Committee on Central Bank Statistics gathers 100 members, mostly national central banks as well as a few regional organizations such as the Center for Latin American Monetary Studies (CEMLA), Central American Monetary Council, and South East Asian Central Banks Research and Training Centre (SEACEN). It is led by an 11-member executive elected by its members. \nReserve policy is also important, especially to consumers and the domestic economy. To ensure liquidity and limit liability to the larger economy, banks cannot create money in specific industries or regions without limit. To make bank depositing and borrowing safer for customers and reduce the risk of bank runs, banks are required to set aside or \"reserve\".\nReserve policy is harder to standardize, as it depends on local conditions and is often fine-tuned to make industry-specific or region-specific changes, especially within large developing nations. For instance, the People's Bank of China requires urban banks to hold 7% reserves while letting rural banks continue to hold only 6%, and simultaneously telling all banks that reserve requirements on certain overheated industries would rise sharply or penalties would be laid if investments in them did not stop completely. The PBoC is thus unusual in acting as a national bank focused on the country and not on the currency, but its desire to control asset inflation is increasingly shared among BIS members who fear \"bubbles\", and among exporting countries that find it difficult to manage the diverse requirements of the domestic economy, especially rural agriculture, and an export economy, especially in manufactured goods.\nEffectively, the PBoC sets different reserve levels for domestic and export styles of development. Historically, the United States also did this, by dividing federal monetary management into nine regions, in which the less-developed western United States had looser policies.\nFor various reasons, it has become quite difficult to accurately assess reserves on more than simple loan instruments, and this plus the regional differences has tended to discourage standardizing any reserve rules at the global BIS scale. Historically, the BIS did set some standards which favoured lending money to private landowners (at about 5 to 1) and for-profit corporations (at about 2 to 1) over loans to individuals. These distinctions reflecting classical economics were superseded by policies relying on undifferentiated market values \u2013 more in line with neoclassical economics.\n\nFinancial Stability Institute\nThe Financial Stability Institute is dedicated to debates and exchanges of practices among supervisors and financial stability policymakers. It was established in 1999 in the wake of the 1997 Asian financial crisis. It has been led by Josef To\u0161ovsk\u00fd from December 2000 to December 2016, and by Fernando Restoy since January 2017.\n\nBIS Innovation Hub\nThe BIS Innovation Hub, launched in 2019, extends the BIS mission of collaboration through digital innovation, developing technology-based public goods to support central banks and enhance the functioning of the financial system.\nThe BIS Innovation Hub has officies in Hong Kong SAR, Singapore, Switzerland, London, Stockholm and Toronto.\n\nOther associations hosted by the BIS\nThe BIS hosts the secretariats of the Financial Stability Board, the International Association of Insurance Supervisors, and International Association of Deposit Insurers. These entities, unlike the above listed committees, have no direct reporting links to the BIS.\n\nFinancial results\nBIS denominates its reserve in IMF special drawing rights. The balance sheet total of the BIS on 31 March 2019 was SDR 291.1 billion (US$403.7 billion) and a net profit of SDR 461.1 million (US$639.5 million).\n\nLeadership\nThe Chairmen concurrently held the role of President from April 1930 to May 1937. Johan Beyen of the Netherlands served as President from May 1937 to December 1939, succeeded by American national Thomas H. McKittrick from January 1940 to June 1946. The position of President remained vacant from June 1946 to June 1948, when the roles of President and Chair of the Board were again reunited until the former was abolished on 27 June 2005. Meanwhile, the Chair had been left vacant from May 1940 to December 1942.\n\nBIS Chairs\nBIS General Managers\nBoard of directors\nAs of August 2024:\n\nSee also\nBank regulation\nGlobal financial system\nSWIFT\nCentral bank digital currency\n\nReferences\nExternal links\n\nOfficial website\n\"They've Got a Secret\" by Michael Hirsh, The New York Times, 2013 (a book review of Tower of Basel, Adam LeBor, 2014)\n\"The Money Club\", by Edward Jay Epstein, Harper's, 1983.\nAndrew Crockett statement to the IMF\nAn account of the use of reserve policy and other central bank powers in China at the Wayback Machine (archived February 4, 2012), by Henry C K Liu in the Asia Times.\nBanking with Hitler on YouTube, Timewatch, Paul Elston, producer Laurence Rees, narrator Sean Barrett (UK), BBC, 1998 (a video documentary about the BIS role in financing Nazi Germany)\neabh (The European Association for Banking and Financial History e.V.)\nBank for International Settlements in the Dodis database of the Diplomatic Documents of Switzerland\nDocuments and clippings about Bank for International Settlements in the 20th Century Press Archives of the ZBW", "url": "https://en.wikipedia.org/wiki/Bank_for_International_Settlements"}, {"title": "United States", "text": "The United States of America (USA), commonly known as the United States (U.S.) or America, is a country primarily located in North America. It is a federal union of 50 states and a federal capital district, Washington, D.C. The 48 contiguous states border Canada to the north and Mexico to the south, with the states of Alaska to the northwest and the archipelagic Hawaii in the Pacific Ocean. The United States also asserts sovereignty over five major island territories and various uninhabited islands. The country has the world's third-largest land area, largest exclusive economic zone, and third-largest population, exceeding 334 million. Its three largest metropolitan areas are New York, Los Angeles, and Chicago, and its three most populous states are California, Texas, and Florida.\nPaleo-Indians migrated across the Bering land bridge more than 12,000 years ago, and formed various civilizations and societies. British colonization led to the first settlement of the Thirteen Colonies in Virginia in 1607. Clashes with the British Crown over taxation and political representation sparked the American Revolution, with the Second Continental Congress formally declaring independence on July 4, 1776. Following its victory in the 1775\u20131783 Revolutionary War, the country continued to expand westward across North America, resulting in the dispossession of native inhabitants. As more states  were admitted, a North\u2013South division over slavery led to the secession of the Confederate States of America, which fought states remaining in the Union in the 1861\u20131865 American Civil War. With the victory and preservation of the United States, slavery was abolished nationally. By 1900, the country had established itself as a great power, a status solidified after its involvement in World War I. After Japan's attack on Pearl Harbor in December 1941, the U.S. entered World War II. Its aftermath left the U.S. and the Soviet Union as the world's two superpowers and led to the Cold War, during which both countries struggled for ideological dominance and international influence. Following the Soviet Union's collapse and the end of the Cold War in 1991, the U.S. emerged as the world's sole superpower, wielding significant geopolitical influence globally.\nThe U.S. national government is a presidential constitutional federal republic and liberal democracy with three separate branches: legislative, executive, and judicial. It has a bicameral national legislature composed of the House of Representatives, a lower house based on population; and the Senate, an upper house based on equal representation for each state. Federalism provides substantial autonomy to the 50 states, while the country's political culture promotes liberty, equality, individualism, personal autonomy, and limited government.\nOne of the world's most developed countries, the United States has had the largest nominal GDP since about 1890 and accounted for over 15% of the global economy in 2023. It possesses by far the largest amount of wealth of any country and has the highest disposable household income per capita among OECD countries. The U.S. ranks among the world's highest in economic competitiveness, productivity, innovation, human rights, and higher education. Its hard power and cultural influence have a global reach. The U.S. is a founding member of the World Bank, Organization of American States, NATO, and United Nations, as well as a permanent member of the UN Security Council.\n\nEtymology\nThe first documented use of the phrase \"United States of America\" is a letter from January 2, 1776. Stephen Moylan, a Continental Army aide to General George Washington, wrote to Joseph Reed, Washington's aide-de-camp, seeking to go \"with full and ample powers from the United States of America to Spain\" to seek assistance in the Revolutionary War effort. The first known public usage is an anonymous essay published in the Williamsburg newspaper, The Virginia Gazette, on April 6, 1776. By June 1776, the \"United States of America\" appeared in the Articles of Confederation and the Declaration of Independence. The Second Continental Congress adopted the Declaration of Independence on July 4, 1776.\nThe term \"United States\" and the initialism \"U.S.\", used as nouns or as adjectives in English, are common short names for the country. The initialism \"USA\", a noun, is also common. \"United States\" and \"U.S.\" are the established terms throughout the U.S. federal government, with prescribed rules. In English, the term \"America\" rarely refers to topics unrelated to the United States, despite the usage of \"the Americas\" as the totality of North and South America. \"The States\" is an established colloquial shortening of the name, used particularly from abroad; \"stateside\" is sometimes used as an adjective or adverb.\n\nHistory\nIndigenous peoples\nThe first inhabitants of North America migrated from Siberia across the Bering land bridge about 12,000 years ago; the Clovis culture, which appeared around 11,000 BC, is believed to be the first widespread culture in the Americas. Over time, indigenous North American cultures grew increasingly sophisticated, and some, such as the Mississippian culture, developed agriculture, architecture, and complex societies. In the post-archaic period, the Mississippian cultures were located in the midwestern, eastern, and southern regions, and the Algonquian in the Great Lakes region and along the Eastern Seaboard, while the Hohokam culture and Ancestral Puebloans inhabited the southwest. Native population estimates of what is now the United States before the arrival of European immigrants range from around 500,000 to nearly 10 million.\n\nEuropean settlement and conflict (1607\u20131765)\nChristopher Columbus began exploring the Caribbean for Spain in 1492, leading to Spanish-speaking settlements and missions from Puerto Rico and Florida to New Mexico and California. France established its own settlements along the Great Lakes, Mississippi River and Gulf of Mexico. British colonization of the East Coast began with the Virginia Colony (1607) and Plymouth Colony (1620). The Mayflower Compact and the Fundamental Orders of Connecticut established precedents for representative self-governance and constitutionalism that would develop throughout the American colonies. While European settlers in what is now the United States experienced conflicts with Native Americans, they also engaged in trade, exchanging European tools for food and animal pelts. Relations ranged from close cooperation to warfare and massacres. The colonial authorities often pursued policies that forced Native Americans to adopt European lifestyles, including conversion to Christianity. Along the eastern seaboard, settlers trafficked African slaves through the Atlantic slave trade.\nThe original Thirteen Colonies that would later found the United States were administered as possessions of Great Britain, and had local governments with elections open to most white male property owners. The colonial population grew rapidly, eclipsing Native American populations; by the 1770s, the natural increase of the population was such that only a small minority of Americans had been born overseas. The colonies' distance from Britain allowed for the development of self-governance, and the First Great Awakening, a series of Christian revivals, fueled colonial interest in religious liberty.\nFor a century, the American colonists had been providing their own troops and materiel in conflicts with indigenous peoples allied with Britain's colonial rivals, especially France, and the Americans had begun to develop a sense of self-defense and self-reliance separate from Britain. The French and Indian War (1754\u20131763) took on new significance for all North American colonists after Parliament under William Pitt the Elder concluded that major military resources needed to be devoted to North America to win the war against France. For the first time, the continent became one of the main theaters of what could be termed a \"world war\". The British colonies' position as an integral part of the British Empire became more apparent during the war, with British military and civilian officials becoming a more significant presence in American life.\nThe war increased a sense of American identity as well. Men who otherwise never left their own colony now traveled across the continent to fight alongside men from decidedly different backgrounds but who were no less \"American\". British officers trained American officers for battle, most notably George Washington; these officers would lend their skills and expertise to the colonists' cause during the American Revolutionary War to come. In addition, colonial legislatures and officials found it necessary to cooperate intensively in pursuit of a coordinated, continent-wide military effort. Finally, deteriorating relations between the British military establishment and the colonists, relations that were already less than positive, set the stage for further distrust and dislike of British troops.\n\nAmerican Revolution and the early republic (1765\u20131800)\nFollowing their victory in the French and Indian War, Britain began to assert greater control over local colonial affairs, resulting in colonial political resistance; one of the primary colonial grievances was a denial of their rights as Englishmen, particularly the right to representation in the British government that taxed them. To demonstrate their dissatisfaction and resolve, the First Continental Congress met in 1774 and passed the Continental Association, a colonial boycott of British goods that proved effective. The British attempt to then disarm the colonists resulted in the 1775 Battles of Lexington and Concord, igniting the American Revolutionary War. At the Second Continental Congress, the colonies appointed George Washington commander-in-chief of the Continental Army, and created a committee that named Thomas Jefferson to draft the Declaration of Independence. Two days after passing the Lee Resolution to create an independent nation the Declaration was adopted on July 4, 1776. The political values of the American Revolution included liberty, inalienable individual rights; and the sovereignty of the people; supporting republicanism and rejecting monarchy, aristocracy, and all hereditary political power; civic virtue; and vilification of political corruption. The Founding Fathers of the United States, who included Washington, Jefferson, John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, James Madison, Thomas Paine,  and many others, were inspired by Greco-Roman, Renaissance, and Enlightenment philosophies and ideas.\nThe Articles of Confederation were ratified in 1781 and established a decentralized government that operated until 1789. After the British surrender at the siege of Yorktown in 1781 American sovereignty was internationally recognized by the Treaty of Paris (1783), through which the U.S. gained territory stretching west to the Mississippi River, north to present-day Canada, and south to Spanish Florida.  The Northwest Ordinance (1787) established the precedent by which the country's territory would expand with the admission of new states, rather than the expansion of existing states. The U.S. Constitution was drafted at the 1787 Constitutional Convention to overcome the limitations of the Articles. It went into effect in 1789, creating a federal republic governed by three separate branches that together ensured a system of checks and balances. George Washington was elected the country's first president under the Constitution, and the Bill of Rights was adopted in 1791 to allay skeptics' concerns about the power of the more centralized government. His resignation as commander-in-chief after the Revolutionary War and his later refusal to run for a third term as the country's first president established a precedent for the supremacy of civil authority in the United States and the peaceful transfer of power, respectively.\n\nWestward expansion and Civil War (1800\u20131865)\nThe Louisiana Purchase of 1803 from France nearly doubled the territory of the United States. Lingering issues with Britain remained, leading to the War of 1812, which was fought to a draw. Spain ceded Florida and its Gulf Coast territory in 1819. In the late 18th century, American settlers began to expand westward, many with a sense of manifest destiny. The Missouri Compromise attempted to balance the desire of northern states to prevent the expansion of slavery into new territories with that of southern states to extend it, admitting Missouri as a slave state and Maine as a free state. It further prohibited slavery in all other lands of the Louisiana Purchase north of the 36\u00b030\u2032 parallel. As Americans expanded further into land inhabited by Native Americans, the federal government often applied policies of Indian removal or assimilation. The Trail of Tears (1830\u20131850) was a U.S. government policy that forcibly removed and displaced most Native Americans living east of the Mississippi River to lands far to the west. These and earlier organized displacements prompted a long series of American Indian Wars west of the Mississippi. The Republic of Texas was annexed in 1845, and the 1846 Oregon Treaty led to U.S. control of the present-day American Northwest. Victory in the Mexican\u2013American War resulted in the 1848 Mexican Cession of California, Nevada, Utah, and much of present-day Colorado and the American Southwest. The California gold rush of 1848\u20131849 spurred a huge migration of white settlers to the Pacific coast, leading to even more confrontations with Native populations. One of the most violent, the California genocide of thousands of Native inhabitants, lasted into the early 1870s, just as additional western territories and states were created.\nDuring the colonial period, slavery had been legal in the American colonies, though the practice began to be significantly questioned during the American Revolution. States in the North enacted abolition laws, though support for slavery strengthened in Southern states, as inventions such as the cotton gin made the institution increasingly profitable for Southern elites. This sectional conflict regarding slavery culminated in the American Civil War (1861\u20131865). Eleven slave states seceded and formed the Confederate States of America, while the other states remained in the Union. War broke out in April 1861 after the Confederates bombarded Fort Sumter. After the January 1863 Emancipation Proclamation, many freed slaves joined the Union army. The war began to turn in the Union's favor following the 1863 Siege of Vicksburg and Battle of Gettysburg, and the Confederacy surrendered in 1865 after the Union's victory in the Battle of Appomattox Court House. The Reconstruction era followed the war. After the assassination of President Abraham Lincoln, Reconstruction Amendments were passed to protect the rights of African Americans. National infrastructure, including transcontinental telegraph and railroads, spurred growth in the American frontier.\n\nPost\u2013Civil War era (1865\u20131917)\nFrom 1865 through 1917 an unprecedented stream of immigrants arrived in the United States, including 24.4 million from Europe. Most came through the port of New York City, and New York City and other large cities on the East Coast became home to large Jewish, Irish, and Italian populations, while many Germans and Central Europeans moved to the Midwest. At the same time, about one million French Canadians migrated from Quebec to New England. During the Great Migration, millions of African Americans left the rural South for urban areas in the North. Alaska was purchased from Russia in 1867.\nThe Compromise of 1877 effectively ended Reconstruction and white supremacists took local control of Southern politics. African Americans endured a period of heightened, overt racism following Reconstruction, a time often called the nadir of American race relations. A series of Supreme Court decisions, including Plessy v. Ferguson, emptied the Fourteenth and Fifteenth Amendments of their force, allowing Jim Crow laws in the South to remain unchecked, sundown towns in the Midwest, and segregation in communities across the country, which would be reinforced by the policy of redlining later adopted by the federal Home Owners' Loan Corporation.\nAn explosion of technological advancement accompanied by the exploitation of cheap immigrant labor led to rapid economic expansion during the late 19th and early 20th centuries, allowing the United States to outpace the economies of England, France, and Germany combined. This fostered the amassing of power by a few prominent industrialists, largely by their formation of trusts and monopolies to prevent competition. Tycoons led the nation's expansion in the railroad, petroleum, and steel industries. The United States emerged as a pioneer of the automotive industry. These changes were accompanied by significant increases in economic inequality, slum conditions, and social unrest, creating the environment for labor unions to begin to flourish. This period eventually ended with the advent of the Progressive Era, which was characterized by significant reforms.\nPro-American elements in Hawaii overthrew the Hawaiian monarchy; the islands were annexed in 1898. That same year, Puerto Rico, the Philippines, and Guam were ceded to the U.S. by Spain after the latter's defeat in the Spanish\u2013American War. (The Philippines was granted full independence from the U.S. on July 4, 1946, following World War II. Puerto Rico and Guam have remained U.S. territories.) American Samoa was acquired by the United States in 1900 after the Second Samoan Civil War. The U.S. Virgin Islands were purchased from Denmark in 1917.\n\nRise as a superpower (1917\u20131945)\nThe United States entered World War I alongside the Allies of World War I, helping to turn the tide against the Central Powers. In 1920, a constitutional amendment granted nationwide women's suffrage. During the 1920s and '30s, radio for mass communication and the invention of early television transformed communications nationwide. The Wall Street Crash of 1929 triggered the Great Depression, which President Franklin D. Roosevelt responded to with the New Deal, a series of sweeping programs and public works projects combined with financial reforms and regulations. All were intended to protect against future economic depressions.\nInitially neutral during World War II, the U.S. began supplying war materiel to the Allies of World War II in March 1941 and entered the war in December after the Empire of Japan's attack on Pearl Harbor. The U.S. developed the first nuclear weapons and used them against the Japanese cities of Hiroshima and Nagasaki in August 1945, ending the war. The United States was one of the \"Four Policemen\" who met to plan the post-war world, alongside the United Kingdom, Soviet Union, and China. The U.S. emerged relatively unscathed from the war, with even greater economic power and international political influence.\n\nCold War (1945\u20131991)\nAfter World War II, the United States entered the Cold War, where geopolitical tensions between the U.S. and the Soviet Union led the two countries to dominate world affairs. The U.S. utilized the policy of containment to limit the USSR's sphere of influence, and prevailed in the Space Race, which culminated with the first crewed Moon landing in 1969. Domestically, the U.S. experienced economic growth, urbanization, and population growth following World War II. The civil rights movement emerged, with Martin Luther King Jr. becoming a prominent leader in the early 1960s. The Great Society plan of President Lyndon Johnson's administration resulted in groundbreaking and broad-reaching laws, policies and a constitutional amendment to counteract some of the worst effects of lingering institutional racism. The counterculture movement in the U.S. brought significant social changes, including the liberalization of attitudes toward recreational drug use and sexuality. It also encouraged open defiance of the military draft (leading to the end of conscription in 1973) and wide opposition to U.S. intervention in Vietnam (with the U.S. totally withdrawing in 1975). A societal shift in the roles of women was significantly responsible for the large increase in female paid labor participation during the 1970s, and by 1985 the majority of American women aged 16 and older were employed. The late 1980s and early 1990s saw the fall of communism and the collapse of the Soviet Union, which marked the end of the Cold War and left the United States as the world's sole superpower.\n\nContemporary (1991\u2013present)\nThe 1990s saw the longest recorded economic expansion in American history, a dramatic decline in U.S. crime rates, and advances in technology. Throughout this decade, technological innovations such as the World Wide Web, the evolution of the Pentium microprocessor in accordance with Moore's law, rechargeable lithium-ion batteries, the first gene therapy trial, and cloning either emerged in the U.S. or were improved upon there. The Human Genome Project was formally launched in 1990, while Nasdaq became the first stock market in the United States to trade online in 1998.\nIn the Gulf War of 1991, an American-led international coalition of states expelled an Iraqi invasion force that had occupied neighboring Kuwait. The September 11 attacks on the United States in 2001 by the pan-Islamist militant organization al-Qaeda led to the war on terror, and subsequent military interventions in Afghanistan and Iraq. The cultural impact of the attacks was profound and long-lasting.\nThe U.S. housing bubble culminated in 2007 with the Great Recession, the largest economic contraction since the Great Depression. Coming to a head in the 2010s, political polarization in the country increased between liberal and conservative factions. This polarization was capitalized upon in the January 2021 Capitol attack, when a mob of insurrectionists entered the U.S. Capitol and sought to prevent the peaceful transfer of power in an attempted self-coup d'\u00e9tat.\n\nGeography\nThe United States is the world's third-largest country by total area behind Russia and Canada. The 48 contiguous states and the District of Columbia occupy a combined area of 3,119,885 square miles (8,080,470 km2). The coastal plain of the Atlantic seaboard gives way to inland forests and rolling hills in the Piedmont plateau region.\nThe Appalachian Mountains and the Adirondack massif separate the East Coast from the Great Lakes and the grasslands of the Midwest. The Mississippi River System, the world's fourth-longest river system, runs predominantly north\u2013south through the heart of the country. The flat and fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.\n\nThe Rocky Mountains, west of the Great Plains, extend north to south across the country, peaking at over 14,000 feet (4,300 m) in Colorado. Farther west are the rocky Great Basin and Chihuahua, Sonoran, and Mojave deserts. In the northwest corner of Arizona, carved by the Colorado River over millions of years, is the Grand Canyon, a steep-sided canyon and popular tourist destination known for its overwhelming visual size and intricate, colorful landscape.\nThe Sierra Nevada and Cascade mountain ranges run close to the Pacific coast. The lowest and highest points in the contiguous United States are in the State of California, about 84 miles (135 km) apart. At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali is the highest peak in the country and continent. Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rocky Mountains, the Yellowstone Caldera, is the continent's largest volcanic feature. In 2021, the United States had 8% of global permanent meadows and pastures and 10% of cropland.\n\nClimate\nWith its large size and geographic variety, the United States includes most climate types. East of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south. The western Great Plains are semi-arid. Many mountainous areas of the American West have an alpine climate. The climate is arid in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon, Washington, and southern Alaska. Most of Alaska is subarctic or polar. Hawaii, the southern tip of Florida and U.S. territories in the Caribbean and Pacific are tropical.\nStates bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley. Overall, the United States receives more high-impact extreme weather incidents than any other country. Extreme weather became more frequent in the U.S. in the 21st century, with three times the number of reported heat waves as in the 1960s. In the American Southwest, droughts became more persistent and more severe.\n\nBiodiversity and conservation\nThe U.S. is one of 17 megadiverse countries containing large numbers of endemic species: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and over 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland. The United States is home to 428 mammal species, 784 birds, 311 reptiles, 295 amphibians, and around 91,000 insect species.\nThere are 63 national parks, and hundreds of other federally managed parks, forests, and wilderness areas, managed by the National Park Service and other agencies. About 28% of the country's land is publicly owned and federally managed, primarily in the Western States. Most of this land is protected, though some is leased for commercial use, and less than one percent is used for military purposes.\nEnvironmental issues in the United States include debates on non-renewable resources and nuclear energy, air and water pollution, biodiversity, logging and deforestation, and climate change. The U.S. Environmental Protection Agency (EPA) is the federal agency charged with addressing most environmental-related issues. The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act. The Endangered Species Act of 1973 provides a way to protect threatened and endangered species and their habitats. The United States Fish and Wildlife Service implements and enforces the Act. In 2024, the U.S. ranked 34th among 180 countries in the Environmental Performance Index. The country joined the Paris Agreement on climate change in 2016 and has many other environmental commitments.\n\nGovernment and politics\nThe United States is a federal republic of 50 states and a federal district, Columbia, which contains the national capitol city of Washington. It also asserts sovereignty over five unincorporated territories and several uninhabited island possessions. The world's oldest surviving federation, the Constitution of the United States is the world's oldest national constitution still in effect (from March 4, 1789). Its presidential system of national government has been adopted, in whole or in part, by many newly independent countries worldwide following decolonization. It is a liberal representative democracy \"in which majority rule is tempered by minority rights protected by law.\" The U.S. Constitution serves as the country's supreme legal document, also establishing the structure and responsibilities of the national federal government and its relationship with the individual states.\nAccording to V-Dem Institute's 2023 Human Rights Index, the United States ranks among the highest in the world for human rights.\n\nNational government\nComposed of three branches, all headquartered in national capitol, the federal government is the national government of the United States. It is regulated by a strong system of checks and balances.\n\nThe U.S. Congress, a bicameral legislature made up of the Senate and the House of Representatives, makes federal law, declares war, approves treaties, has the power of the purse, and has the power of impeachment. The Senate has 100 members (2 from each state), elected for a six-year term. The House of Representatives has 435 members, each elected for a two-year term; all representatives serve one congressional district of equivalent population. Congressional districts are drawn by each state legislature and are contiguous within the state. The Congress also organizes a collection of committees, each of which handles a specific task or duty. One of Congress's foremost non-legislative functions is the power to investigate and oversee the executive branch. Congressional oversight is usually delegated to committees and is facilitated by Congress's subpoena power. Appointment to a committee enables a member to develop specialized knowledge of the matters under its purview. The various committees monitor ongoing governmental operations, identify issues suitable for legislative review, gather and evaluate information, and recommend courses of action to the U.S. Congress, including but not limited to new legislation. The two major political parties have appointment power in deciding each committee's membership. Committee chairs are assigned to a member of the majority party.\nThe U.S. president is the head of state,  commander-in-chief of the military, chief executive of the federal government, and has the ability to veto legislative bills from the U.S. Congress before they become law. However, presidential vetoes can be overridden by a two-thirds supermajority vote in both chambers of Congress. The president appoints the members of the Cabinet, subject to Senate approval, and names other officials who administer and enforce federal laws through their respective agencies. The president also has clemency power for federal crimes and can issue pardons. Finally, the president has the right to issue expansive \"executive orders\", subject to judicial review, in a number of policy areas. Candidates for president campaign with a vice-presidential running mate. Both candidates are elected together, or defeated together, in a presidential election. Unlike other votes in American politics, this is technically an indirect election in which the winner will be determined by the U.S. Electoral College. There, votes are officially cast by individual electors selected by their state legislature. In practice, however, each of the 50 states chooses a group of presidential electors who are required to confirm the winner of their state's popular vote. Each state is allocated two electors plus one additional elector for each congressional district, which in effect combines to equal the number of elected officials that state sends to Congress. The District of Columbia, with no representatives or senators, is allocated three electoral votes. Both the president and the vice president serve a four-year term, and the president may be reelected to the office only once, for one additional four-year term.\nThe U.S. federal judiciary, whose judges are all appointed for life by the president with Senate approval, consists primarily of the U.S. Supreme Court, the U.S. courts of appeals, and the U.S. district courts. The U.S. Supreme Court interprets laws and overturn those they find unconstitutional. The Supreme Court has nine members led by the Chief Justice of the United States. The members are appointed by the sitting president when a vacancy becomes available. In a number of ways the federal court system operates differently than state courts. For civil cases that is apparent in the types of cases that can be heard in the federal system. Their limited jurisdiction restricts them to cases authorized by the United States Constitution or federal statutes. In criminal cases, states may only bring criminal prosecutions in state courts, and the federal government may only bring criminal prosecutions in federal court. The first level in the federal courts is federal district court for any case under \"original jurisdiction\", such as federal statutes, the Constitution, or treaties. There are twelve federal circuits that divide the country into different regions for federal appeals courts. After a federal district court has decided a case, it can then be appealed to a United States court of appeal.  The next and highest court in the system is the Supreme Court of the United States. It has the power to decide appeals on all cases brought in federal court or those brought in state court but dealing with federal law. Unlike circuit court appeals, however, the Supreme Court is usually not required to hear the appeal. A \"petition for writ of certiorari\" may be submitted to the court, asking it to hear the case. If it is granted, the Supreme Court will take briefs and conduct oral arguments. If it is not granted, the opinion of the lower court stands. Certiorari is not often granted, and less than 1% of appeals to the Supreme Court are actually heard by it. Usually, the Court only hears cases when there are conflicting decisions across the nation on a particular issue, or when there is an obvious error in a case.\nThe three-branch system is known as the presidential system, in contrast to the parliamentary system, where the executive is part of the legislative body. Many countries around the world imitated this aspect of the 1789 Constitution of the United States, especially in the Americas.\n\nPolitical parties\nThe Constitution is silent on political parties. However, they developed independently in the 18th century with the Federalist and Anti-Federalist parties. Since then, the United States has operated as a de facto two-party system, though the parties in that system have been different at different times. The two main national parties are presently the Democratic and the Republican. The former is perceived as relatively liberal in its political platform while the latter is perceived as relatively conservative.\n\nSubdivisions\nIn the American federal system, sovereign powers are shared between two levels of elected government: national and state. People in the states are also represented by local elected governments, which are administrative divisions of the states. States are subdivided into counties or county equivalents, and further divided into municipalities. The District of Columbia is a federal district that contains the United States capitol, the city of Washington. The territories and the District of Columbia are administrative divisions of the federal government. Federally recognized tribes govern 326 Indian reservations.\n\nForeign relations\nThe United States has an established structure of foreign relations, and it has the world's second-largest diplomatic corps as of 2024. It is a permanent member of the United Nations Security Council, and home to the United Nations headquarters. The United States is a member of the G7, G20, and OECD intergovernmental organizations. Almost all countries have embassies and many have consulates (official representatives) in the country. Likewise, nearly all countries host formal diplomatic missions with the United States, except Iran, North Korea, and Bhutan. Though Taiwan does not have formal diplomatic relations with the U.S., it maintains close unofficial relations. The United States regularly supplies Taiwan with military equipment to deter potential Chinese aggression. Its geopolitical attention also turned to the Indo-Pacific when the United States joined the Quadrilateral Security Dialogue with Australia, India, and Japan.\nThe United States has a \"Special Relationship\" with the United Kingdom and strong ties with Canada, Australia, New Zealand, the Philippines, Japan, South Korea, Israel, and several European Union countries (France, Italy, Germany, Spain, and Poland). The U.S. works closely with its NATO allies on military and national security issues, and with countries in the Americas through the Organization of American States and the United States\u2013Mexico\u2013Canada Free Trade Agreement. In South America, Colombia is traditionally considered to be the closest ally of the United States. The U.S. exercises full international defense authority and responsibility for Micronesia, the Marshall Islands, and Palau through the Compact of Free Association. It has increasingly conducted strategic cooperation with India, but its ties with China have steadily deteriorated. Since 2014, the U.S. has become a key ally of Ukraine; it has also provided the country with significant military equipment and other support in response to Russia's 2022 invasion.\n\nMilitary\nThe president is the commander-in-chief of the United States Armed Forces and appoints its leaders, the secretary of defense and the Joint Chiefs of Staff. The Department of Defense, which is headquartered at the Pentagon near Washington, D.C., administers five of the six service branches, which are made up of the U.S. Army, Marine Corps, Navy, Air Force, and Space Force. The Coast Guard is administered by the Department of Homeland Security in peacetime and can be transferred to the Department of the Navy in wartime.\nThe United States spent $916 billion on its military in 2023, which is by far the largest amount of any country, making up 37% of global military spending and accounting for 3.4% of the country's GDP. The U.S. has 42% of the world's nuclear weapons\u2014the second-largest share after Russia.\nThe United States has the third-largest combined armed forces in the world, behind the Chinese People's Liberation Army and Indian Armed Forces. The military operates about 800 bases and facilities abroad, and maintains deployments greater than 100 active duty personnel in 25 foreign countries.\nState defense forces (SDFs) are military units that operate under the sole authority of a state government. SDFs are authorized by state and federal law but are under the command of the state's governor.\nThey are distinct from the state's National Guard units in that they cannot become federalized entities. A state's National Guard personnel, however, may be federalized under the National Defense Act Amendments of 1933, which created the Guard and provides for the integration of Army National Guard units and personnel into the U.S. Army and (since 1947) the U.S. Air Force.\n\nLaw enforcement and criminal justice\nThere are about 18,000 U.S. police agencies from local to national level in the United States. Law in the United States is mainly enforced by local police departments and sheriff departments in their municipal or county jurisdictions. The state police departments have authority in their respective state, and federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have national jurisdiction and specialized duties, such as protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws. State courts conduct most civil and criminal trials, and federal courts handle designated crimes and appeals of state court decisions.\nThere is no unified \"criminal justice system\" in the United States. The American prison system is largely heterogenous, with thousands of relatively independent systems operating across federal, state, local, and tribal levels. In 2023, \"these systems [held] almost 2 million people in 1,566 state prisons, 98 federal prisons, 3,116 local jails, 1,323 juvenile correctional facilities, 181 immigration detention facilities, and 80 Indian country jails, as well as in military prisons, civil commitment centers, state psychiatric hospitals, and prisons in the U.S. territories.\"  Despite disparate systems of confinement, four main institutions dominate: federal prisons, state prisons, local jails, and juvenile correctional facilities. Federal prisons are run by the Federal Bureau of Prisons and hold people who have been convicted of federal crimes, including pretrial detainees. State prisons, run by the official department of correction of each state, hold sentenced people serving prison time (usually longer than one year) for felony offenses. Local jails are county or municipal facilities that incarcerate defendants prior to trial; they also hold those serving short sentences (typically under a year). Juvenile correctional facilities are operated by local or state governments and serve as longer-term placements for any minor adjudicated as delinquent and ordered by a judge to be confined.\nAs of January 2023, the United States has the sixth-highest per capita incarceration rate in the world\u2014531 people per 100,000 inhabitants\u2014and the largest prison and jail population in the world, with almost 2 million people incarcerated. An analysis of the World Health Organization Mortality Database from 2010 showed U.S. homicide rates \"were 7 times higher than in other high-income countries, driven by a gun homicide rate that was 25 times higher\".\n\nEconomy\nThe U.S. has been the world's largest economy nominally since about 1890. The 2023 nominal U.S. gross domestic product (GDP) of more than $27 trillion was the highest in the world, constituting over 25% of the global economy or 15% at purchasing power parity (PPP). From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7. The country ranks first in the world by nominal GDP, second when adjusted for purchasing power parities (PPP), and ninth by PPP-adjusted GDP per capita. It possesses the highest disposable household income per capita among OECD countries. As of February 2024, the total federal government debt was $34.4 trillion.\n\nOf the world's 500 largest companies by revenue, 136 are headquartered in the U.S. as of 2023, which is the highest number of any country. The U.S. dollar is the currency most used in international transactions and is the world's foremost reserve currency, backed by the country's dominant economy, its military, the petrodollar system, and its linked eurodollar and large U.S. treasuries market. Several countries use it as their official currency, and in others it is the de facto currency. It has free trade agreements with several countries, including the USMCA. The U.S. ranked second in the Global Competitiveness Report in 2019, after Singapore. Although the United States has reached a post-industrial level of development and is often  described as having a service economy, it remains a major industrial power. As of 2021, the U.S. is the second-largest manufacturing country after China.\n\nNew York City is the world's principal financial center and the epicenter of the world's largest metropolitan economy. The New York Stock Exchange and Nasdaq, both located in New York City, are the world's two largest stock exchanges by market capitalization and trade volume. The United States is at or near the forefront of technological advancement and innovation in many economic fields, especially in artificial intelligence; electronics and computers; pharmaceuticals; and medical, aerospace and military equipment. The country's economy is fueled by abundant natural resources, a well-developed infrastructure, and high productivity. The largest trading partners of the United States are the European Union, Mexico, Canada, China, Japan, South Korea, the United Kingdom, Vietnam, India, and Taiwan. The United States is the world's largest importer and the second-largest exporter. It is by far the world's largest exporter of services.\nAmericans have the highest average household and employee income among OECD member states, and the fourth-highest median household income, up from sixth-highest in 2013. With personal consumption expenditures of over $18.5 trillion in 2023, the U.S. has a heavily consumer-driven economy and is by far the world's largest consumer market. Wealth in the United States is highly concentrated; the richest 10% of the adult population own 72% of the country's household wealth, while the bottom 50% own just 2%. Income inequality in the U.S. remains at record highs, with the top fifth of earners taking home more than half of all income and giving the U.S. one of the widest income distributions among OECD members. The U.S. ranks first in the number of dollar billionaires and millionaires, with 735 billionaires and nearly 22 million millionaires as of 2023. There were about 582,500 sheltered and unsheltered homeless persons in the U.S. in 2022, with 60% staying in an emergency shelter or transitional housing program. In 2022, 6.4 million children experienced food insecurity. Feeding America estimates that around one in five, or approximately 13 million, children experience hunger in the U.S. and do not know where they will get their next meal or when. As of 2022, 37.9 million people, or 11.5% of the U.S. population, were living in poverty.\nThe United States has a smaller welfare state and redistributes less income through government action than most other high-income countries. It is the only advanced economy that does not guarantee its workers paid vacation nationally and is one of a few countries in the world without federal paid family leave as a legal right. The United States has a higher percentage of low-income workers than almost any other developed country, largely because of a weak collective bargaining system and lack of government support for at-risk workers.\n\nScience, technology, spaceflight and energy\nThe United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts and the establishment of a machine tool industry enabled the large-scale manufacturing of U.S. consumer products in the late 19th century. By the early 20th century, factory electrification, the introduction of the assembly line, and other labor-saving techniques created the system of mass production. The United States is widely considered to be the leading country in the development of artificial intelligence technology. In 2022, the United States was (after China) the country with the second-highest number of published scientific papers. In 2021, the U.S. ranked second (also after China) by the number of patent applications, and third by trademark and industrial design applications (after China and Germany), according to World Intellectual Property Indicators. In both 2023 and 2024, the United States ranked third (after Switzerland and Sweden) in the Global Innovation Index. The U.S. has the highest total research and development expenditure of any country and ranks ninth as a percentage of GDP. In 2023, the United States was ranked the second most technologically advanced country in the world  (after South Korea) by Global Finance magazine. \n\nThe United States has maintained a space program since the late 1950s, beginning with the establishment of the National Aeronautics and Space Administration (NASA) in 1958. NASA's Apollo program (1961\u20131972) achieved the first crewed Moon landing with the 1969 Apollo 11 mission; it remains one of the agency's most significant milestones. Other major endeavors by NASA include the Space Shuttle program (1981\u20132011), the Voyager program (1972\u2013present), the Hubble and James Webb space telescopes (launched in 1990 and 2021, respectively), and the multi-mission Mars Exploration Program (Spirit and Opportunity, Curiosity, and Perseverance). NASA is one of five agencies collaborating on the International Space Station (ISS); U.S. contributions to the ISS include several modules, including Destiny (2001), Harmony (2007), and Tranquility (2010), as well as ongoing logistical and operational support. The United States private sector dominates the global commercial spaceflight industry. Prominent American spaceflight contractors include Blue Origin, Boeing, Lockheed Martin, Northrop Grumman, and SpaceX. NASA programs such as the Commercial Crew Program, Commercial Resupply Services, Commercial Lunar Payload Services, and NextSTEP have facilitated growing private-sector involvement in American spaceflight.\nAs of 2023, the United States receives approximately 84% of its energy from fossil fuel, and the largest source of the country's energy came from petroleum (38%), followed by natural gas (36%), renewable sources (9%), coal (9%), and nuclear power (9%). The United States constitutes less than 4% of the world's population, but consumes around 16% of the world's energy. The U.S. ranks as the second-highest emitter of greenhouse gases.\n\nTransportation\nThe U.S. Department of Transportation and its divisions provide regulation, supervision, and funding for all aspects of transportation except for customs, immigration, and security. (The latter remain the responsibility of the U.S. Department of Homeland Security.) Each U.S. state has its own department of transportation, which builds and maintains  state highways. Depending upon the state, this department might also directly operate or supervise other modes of transportation.\nAviation law is almost entirely the jurisdiction of the federal government; the Federal Aviation Administration regulates all aspects of civil aviation, air traffic management, certification and compliance, and aviation safety. Vehicle traffic laws, however, are enacted and enforced by state and local authorities, with the exception of roads located on federal property (national parks, military bases) or in the unorganized U.S. territories. The United States Coast Guard is the primary enforcer of law and security on U.S. waterways, inland as well as coastal, but economic jurisdiction over coastal tidelands is shared between state and federal governments. The country's inland waterways are the world's fifth-longest, totaling 41,009 km (25,482 mi).\nPassenger and freight rail systems, bus systems, water ferries, and dams may be under either public or private ownership and operation. U.S. civilian airlines are all privately owned. Most U.S. airports are owned and operated by local government authorities, and there are also some private airports. The Transportation Security Administration has provided security at most major airports since 2001.\n\nPrivately owned railroads and trains were the dominant mode of transportation in the U.S. until the mid-twentieth century. The introduction of jet airplanes and airports serving the same major city routes accelerated a decline in demand for intercity rail passenger service by the 1960s. The completion of the Interstate Highway System also hastened the sharp curtailment of passenger service by the railroads. These significant developments led to the creation of the National Railroad Passenger Corporation, now called Amtrak, by the U.S. federal government in 1971. Amtrak helps to maintain limited intercity rail passenger service in most parts of the country. It serves most major U.S. cities, but outside the Northeast, California, and Illinois it typically runs only a few trains per day. More frequent Amtrak service is available in regional corridors between certain major cities, particularly the Northeast Corridor between Washington, D.C., Philadelphia, New York City and Boston; between New York City and Albany; in metropolitan Chicago; and in parts of California and the Pacific Northwest. Amtrak does not serve several major U.S. destinations, including Las Vegas and Phoenix, Arizona.\nThe American civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned. The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways. Of the world's 50 busiest passenger airports, 16 are in the United States, including the top five and the busiest, Hartsfield\u2013Jackson Atlanta International Airport. As of 2022, there are 19,969 airports in the U.S., of which 5,193 are designated as \"public use\", including for general aviation and other activities.\nThe overwhelming majority of roads in the United States are owned and maintained by state and local governments. Roads maintained only by the U.S. federal government are generally found on federal lands (such as national parks) or at federal facilities (like military bases).  The Interstate Highway System, with its large, open freeways linking the states, is partly funded by the federal government but owned and maintained by the state government hosting its section of the interstate. Some states fund and build their own large expressways\u2014often called \"parkways\" or \"turnpikes\"\u2014that generally use tolls to pay for construction and maintenance. Likewise, some privately owned roads may use tolls for this purpose.\nPublic transportation in the United States includes bus, commuter rail, ferry, and sometimes airline service. Public transit systems serve areas of higher population density where demand is greatest. Many U.S. cities, towns, and suburbs are car-dependent, however, and public transit there is less common and service far less frequent. Most U.S. urban areas have some form of public transit, notably city buses, while the largest (e.g. New York, Chicago, Atlanta, Philadelphia, Boston, San Francisco, and Portland, Oregon) operate extensive systems that also include subways or light rail. Most public transit service in the United States is run by local governments, but national and regional commuter lines serve major U.S. urban corridors.\nPersonal transportation in the United States is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads, making it the longest in the world. The country's rail transport network, also the longest in the world at 182,412.3 mi (293,564.2 km), handles mostly freight. Of the world's 50 busiest container ports, four are located in the United States. The busiest in the U.S. is the Port of Los Angeles.\nThe Oldsmobile Curved Dash and the Ford Model T, both American cars, are considered the first mass-produced and mass-affordable cars, respectively. As of 2023, the United States is the second-largest manufacturer of motor vehicles and is home to Tesla, the world's most valuable car company. American automotive company General Motors held the title of the world's best-selling automaker from 1931 to 2008. The American automotive industry is the world's second-largest automobile market by sales, having been overtaken by China in 2010, and the U.S. has the highest vehicle ownership per capita in the world, with 910 vehicles per 1000 people. By value, the U.S. was the world's largest importer and third-largest exporter of cars in 2022.\n\nDemographics\nPopulation\nThe U.S. Census Bureau reported 331,449,281 residents as of April 1, 2020, making the United States the third-most-populous country in the world, after China and India. According to the Bureau's U.S. Population Clock, on July 1, 2024, the U.S. population had a net gain of one person every 16 seconds, or about 5400 people per day. In 2023, 51% of Americans age 15 and over were married, 6% were widowed, 10% were divorced, and 34% had never been married. In 2023, the total fertility rate for the U.S. stood at 1.6 children per woman, and, at 23%, it had the world's highest rate of children living in single-parent households in 2019.\nThe United States has a diverse population; 37 ancestry groups have more than one million members. White Americans with ancestry from Europe, the Middle East, or North Africa form the largest racial and ethnic group at 57.8% of the United States population. Hispanic and Latino Americans form the second-largest group and are 18.7% of the United States population. African Americans constitute the country's third-largest ancestry group and are 12.1% of the total U.S. population. Asian Americans are the country's fourth-largest group, composing 5.9% of the United States population. The country's 3.7 million Native Americans account for about 1%, and some 574 native tribes are recognized by the federal government. In 2022, the median age of the United States population was 38.9 years.\n\nLanguage\nWhile many languages are spoken in the United States, English is by far the most commonly spoken and written. Although there is no official language at the federal level, some laws, such as U.S. naturalization requirements, standardize English, and most states have declared it the official language. Three states and four U.S. territories have recognized local or indigenous languages in addition to English, including Hawaii (Hawaiian), Alaska (twenty Native languages), South Dakota (Sioux), American Samoa (Samoan), Puerto Rico (Spanish), Guam (Chamorro), and the Northern Mariana Islands (Carolinian and Chamorro). In total, 169 Native American languages are spoken in the United States. In Puerto Rico, Spanish is more widely spoken than English.\nAccording to the American Community Survey (2020), some 245.4 million people out of the total U.S. population of 334 million spoke only English at home. About 41.2 million spoke Spanish at home, making it the second most commonly used language. Other languages spoken at home by one million people or more include Chinese (3.40 million), Tagalog (1.71 million), Vietnamese (1.52 million), Arabic (1.39 million), French (1.18 million), Korean (1.07 million), and Russian (1.04 million). German, spoken by 1 million people at home in 2010, fell to 857,000 total speakers in 2020.\n\nImmigration\nAmerica's immigrant population of nearly 51 million is by far the world's largest in absolute terms. In 2022, there were 87.7 million immigrants and U.S.-born children of immigrants in the United States, accounting for nearly 27% of the overall U.S. population. In 2017, out of the U.S. foreign-born population, some 45% (20.7 million) were naturalized citizens, 27% (12.3 million) were lawful permanent residents, 6% (2.2 million) were temporary lawful residents, and 23% (10.5 million) were unauthorized immigrants. In 2019, the top countries of origin for immigrants were Mexico (24% of immigrants), India (6%), China (5%), the Philippines (4.5%), and El Salvador (3%). In fiscal year 2022, over one million immigrants (most of whom entered through family reunification) were granted legal residence. The United States led the world in refugee resettlement for decades, admitting more refugees than the rest of the world combined.\n\nReligion\nThe First Amendment guarantees the free exercise of religion in the country and forbids Congress from passing laws respecting its establishment. Religious practice is widespread, among the most diverse in the world, and profoundly vibrant. The country has the world's largest Christian population.  Other notable faiths include Judaism, Buddhism, Hinduism, Islam, many New Age movements, and Native American religions. Religious practice varies significantly by region. \"Ceremonial deism\" is common in American culture.\nThe overwhelming majority of Americans believe in a higher power or spiritual force, engage in spiritual practices such as prayer, and consider themselves religious or spiritual. In the \"Bible Belt\", located within the Southern United States, evangelical Protestantism plays a significant role culturally, whereas New England and the Western United States tend to be more secular. Mormonism\u2014a Restorationist movement, whose members migrated westward from Missouri and Illinois under the leadership of Brigham Young in 1847 after the assassination of Joseph Smith\u2014remains the predominant religion in Utah to this day.\n\nUrbanization\nAbout 82% of Americans live in urban areas, including suburbs; about half of those reside in cities with populations over 50,000. In 2022, 333 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four cities\u2014New York City, Los Angeles, Chicago, and Houston\u2014had populations exceeding two million. Many U.S. metropolitan populations are growing rapidly, particularly in the South and West.\n\nHealth\nAccording to the Centers for Disease Control (CDC), average American life expectancy at birth was 77.5 years in 2022 (74.8 years for men and 80.2 years for women). This was a gain of 1.1 years from 76.4 years in 2021, but the CDC noted that the new average \"didn't fully offset the loss of 2.4 years between 2019 and 2021\". Higher overall mortality due especially to the health impact of the COVID-19 pandemic as well as opioid overdoses and suicides were held mostly responsible for the previous drop in life expectancy. The same report stated that the 2022 gains in average U.S. life expectancy were especially significant for men, Hispanics, and American Indian\u2013Alaskan Native people (AIAN). Starting in 1998, the life expectancy in the U.S. fell behind that of other wealthy industrialized countries, and Americans' \"health disadvantage\" gap has been increasing ever since. The U.S. has one of the highest suicide rates among high-income countries. Approximately one-third of the U.S. adult population is obese and another third is overweight. The U.S. healthcare system far outspends that of any other country, measured both in per capita spending and as a percentage of GDP, but attains worse healthcare outcomes when compared to peer countries for reasons that are debated. The United States is the only developed country without a system of universal healthcare, and a significant proportion of the population that does not carry health insurance. Government-funded healthcare coverage for the poor (Medicaid) and for those age 65 and older (Medicare) is available to Americans who meet the programs' income or age qualifications. In 2010, former President Obama passed the Patient Protection and Affordable Care Act. Abortion in the United States is not federally protected, and is illegal or restricted in 17 states.\n\nEducation\nAmerican primary and secondary education (known in the U.S. as K-12, \"kindergarten through 12th grade\") is decentralized. It is operated by state, territorial, and sometimes municipal governments and regulated by the U.S. Department of Education. In general, children are required to attend school or an approved homeschool from the age of five or six (kindergarten or first grade) until they are 18 years old. This often brings students through the 12th grade, the final year of a U.S. high school, but some states and territories allow them to leave school earlier, at age 16 or 17. The U.S. spends more on education per student than any country in the world, an average of $18,614 per year per public elementary and secondary school student in 2020\u20132021. Among Americans age 25 and older, 92.2% graduated from high school, 62.7% attended some college, 37.7% earned a bachelor's degree, and 14.2% earned a graduate degree. The U.S. literacy rate is near-universal. The country has the most Nobel Prize winners of any country, with 411 (having won 413 awards).\nU.S. tertiary or higher education has earned a global reputation. Many of the world's top universities, as listed by various ranking organizations, are in the United States, including 19 of the top 25. American higher education is dominated by state university systems, although the country's many private universities and colleges enroll about 20% of all American students. Local community colleges generally offer coursework and degree programs covering the first two years of college study. They often have more open admission policies, shorter academic programs, and lower tuition.\nAs for public expenditures on higher education, the U.S. spends more per student than the OECD average, and Americans spend more than all nations in combined public and private spending. Colleges and universities directly funded by the federal government do not charge tuition and are limited to military personnel and government employees, including: the U.S. service academies, the Naval Postgraduate School, and military staff colleges. Despite some student loan forgiveness programs in place, student loan debt increased by 102% between 2010 and 2020, and exceeded $1.7 trillion as of 2022.\n\nCulture and society\nAmericans have traditionally been characterized by a unifying political belief in an \"American Creed\" emphasizing consent of the governed, liberty, equality under the law, democracy, social equality, property rights, and a preference for limited government. Culturally, the country has been described as having the values of individualism and personal autonomy, as well as having a strong work ethic, competitiveness, and voluntary altruism towards others. According to a 2016 study by the Charities Aid Foundation, Americans donated 1.44% of total GDP to charity\u2014the highest rate in the world by a large margin. The United States is home to a wide variety of ethnic groups, traditions, and values. It has acquired significant cultural and economic soft power.\nNearly all present Americans or their ancestors came from Europe, Africa, or Asia (the \"Old World\") within the past five centuries. Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa. More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as a homogenizing melting pot, and a heterogeneous salad bowl, with immigrants contributing to, and often assimilating into, mainstream American culture. The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants. Whether this perception is accurate has been a topic of debate. While mainstream culture holds that the United States is a classless society, scholars identify significant differences between the country's social classes, affecting socialization, language, and values. Americans tend to greatly value socioeconomic achievement, but being ordinary or average is promoted by some as a noble condition as well.\nThe United States is considered to have the strongest protections of free speech of any country under the First Amendment, which protects flag desecration, hate speech, blasphemy, and lese-majesty as forms of protected expression. A 2016 Pew Research Center poll found that Americans were the most supportive of free expression of any polity measured. They are the \"most supportive of freedom of the press and the right to use the Internet without government censorship.\" The U.S. is a socially progressive country with permissive attitudes surrounding human sexuality. LGBT rights in the United States are advanced by global standards.\n\nLiterature\nColonial American authors were influenced by John Locke and various other Enlightenment philosophers. The American Revolutionary Period (1765\u20131783) is notable for the political writings of Benjamin Franklin, Alexander Hamilton, Thomas Paine, and Thomas Jefferson. Shortly before and after the Revolutionary War, the newspaper rose to prominence, filling a demand for anti-British national literature. An early novel is William Hill Brown's The Power of Sympathy, published in 1791. Writer and critic John Neal in the early- to mid-nineteenth century helped advance America toward a unique literature and culture by criticizing predecessors such as Washington Irving for imitating their British counterparts, and by influencing writers such as Edgar Allan Poe, who took American poetry and short fiction in new directions. Ralph Waldo Emerson and Margaret Fuller pioneered the influential Transcendentalism movement; Henry David Thoreau, author of Walden, was influenced by this movement. The conflict surrounding abolitionism inspired writers, like Harriet Beecher Stowe, and authors of slave narratives, such as Frederick Douglass. Nathaniel Hawthorne's The Scarlet Letter (1850) explored the dark side of American history, as did Herman Melville's Moby-Dick (1851). Major American poets of the nineteenth century American Renaissance include Walt Whitman, Melville, and Emily Dickinson. Mark Twain was the first major American writer to be born in the West. Henry James achieved international recognition with novels like The Portrait of a Lady (1881). As literacy rates rose, periodicals published more stories centered around industrial workers, women, and the rural poor. Naturalism, regionalism, and realism were the major literary movements of the period.\nWhile modernism generally took on an international character, modernist authors working within the United States more often rooted their work in specific regions, peoples, and cultures. Following the Great Migration to northern cities, African-American and black West Indian authors of the Harlem Renaissance developed an independent tradition of literature that rebuked a history of inequality and celebrated black culture. An important cultural export during the Jazz Age, these writings were a key influence on N\u00e9gritude, a philosophy emerging in the 1930s among francophone writers of the African diaspora. In the 1950s, an ideal of homogeneity led many authors to attempt to write the Great American Novel, while the Beat Generation rejected this conformity, using styles that elevated the impact of the spoken word over mechanics to describe drug use, sexuality, and the failings of society. Contemporary literature is more pluralistic than in previous eras, with the closest thing to a unifying feature being a trend toward self-conscious experiments with language. As of 2024 there have been 12 American laureates for the Nobel Prize in literature.\n\nMass media\nMedia is broadly uncensored, with the First Amendment providing significant protections, as reiterated in New York Times Co. v. United States. The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX). The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches. As of 2021, about 83% of Americans over age 12 listen to broadcast radio, while about 40% listen to podcasts. As of 2020, there were 15,460 licensed full-power radio stations in the U.S. according to the Federal Communications Commission (FCC). Much of the public radio broadcasting is supplied by NPR, incorporated in February 1970 under the Public Broadcasting Act of 1967.\nU.S. newspapers with a global reach and reputation include The Wall Street Journal, The New York Times, The Washington Post, and USA Today. About 800 publications are produced in Spanish. With few exceptions, newspapers are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or, in an increasingly rare situation, by individuals or families. Major cities often have alternative newspapers to complement the mainstream daily papers, such as The Village Voice in New York City and LA Weekly in Los Angeles. The five most popular websites used in the U.S. are Google, YouTube, Amazon, Yahoo, and Facebook\u2014all of them American-owned.\nAs of 2022, the video game market of the United States is the world's largest by revenue. There are 444 publishers, developers, and hardware companies in California alone.\n\nTheater\nThe United States is well known for its theater. Mainstream theater in the United States derives from the old European theatrical tradition and has been heavily influenced by the British theater. By the middle of the 19th century America had created new distinct dramatic forms in the Tom Shows, the showboat theater and the minstrel show. The central hub of the American theater scene is the Theater District in Manhattan, with its divisions of Broadway, off-Broadway, and off-off-Broadway.\nMany movie and television celebrities have gotten their big break working in New York productions. Outside New York City, many cities have professional regional or resident theater companies that produce their own seasons. The biggest-budget theatrical productions are musicals. U.S. theater has an active community theater culture.\nThe Tony Awards recognizes excellence in live Broadway theater and are presented at an annual ceremony in Manhattan. The awards are given for Broadway productions and performances. One is also given for regional theater. Several discretionary non-competitive awards are given as well, including a Special Tony Award, the Tony Honors for Excellence in Theatre, and the Isabelle Stevenson Award.\n\nVisual arts\nFolk art in colonial America grew out of artisanal craftsmanship in communities that allowed commonly trained people to individually express themselves. It was distinct from Europe's tradition of high art, which was less accessible and generally less relevant to early American settlers. Cultural movements in art and craftsmanship in colonial America generally lagged behind those of Western Europe. For example, the prevailing medieval style of woodworking and primitive sculpture became integral to early American folk art, despite the emergence of Renaissance styles in England in the late 16th and early 17th centuries. The new English styles would have been early enough to make a considerable impact on American folk art, but American styles and forms had already been firmly adopted. Not only did styles change slowly in early America, but there was a tendency for rural artisans there to continue their traditional forms longer than their urban counterparts did\u2014and far longer than those in Western Europe.\nThe Hudson River School was a mid-19th-century movement in the visual arts tradition of European naturalism. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene.\nGeorgia O'Keeffe, Marsden Hartley, and others experimented with new and individualistic styles, which would become known as American modernism. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. Major photographers include Alfred Stieglitz, Edward Steichen, Dorothea Lange, Edward Weston, James Van Der Zee, Ansel Adams, and Gordon Parks.\nThe tide of modernism and then postmodernism has brought global fame to American architects, including Frank Lloyd Wright, Philip Johnson, and Frank Gehry. The Metropolitan Museum of Art in Manhattan is the largest art museum in the United States and the fourth-largest in the world.\n\nMusic\nAmerican folk music encompasses numerous music genres, variously known as traditional music, traditional folk music, contemporary folk music, or roots music. Many traditional songs have been sung within the same family or folk group for generations, and sometimes trace back to such origins as the British Isles, mainland Europe, or Africa. The rhythmic and lyrical styles of African-American music in particular have influenced American music. Banjos were brought to America through the slave trade. Minstrel shows incorporating the instrument into their acts led to its increased popularity and widespread production in the 19th century. The electric guitar, first invented in the 1930s, and mass-produced by the 1940s, had an enormous influence on popular music, in particular due to the development of rock and roll. \n\nElements from folk idioms such as the blues and old-time music were adopted and transformed into popular genres with global audiences. Jazz grew from blues and ragtime in the early 20th century, developing from the innovations and recordings of composers such as W.C. Handy and Jelly Roll Morton. Louis Armstrong and Duke Ellington increased its popularity early in the 20th century. Country music developed in the 1920s, rock and roll in the 1930s, and bluegrass and rhythm and blues in the 1940s. In the 1960s, Bob Dylan emerged from the folk revival to become one of the country's most celebrated songwriters. The musical forms of punk and hip hop both originated in the United States in the 1970s.\nThe United States has the world's largest music market with a total retail value of $15.9 billion in 2022. Most of the world's major record companies are based in the U.S.; they are represented by the Recording Industry Association of America (RIAA). Mid-20th-century American pop stars, such as Frank Sinatra and Elvis Presley, became global celebrities and best-selling music artists, as have artists of the late 20th century, such as Michael Jackson, Madonna, Whitney Houston, and Prince, and the early 21st century, such as Taylor Swift and Beyonc\u00e9.\n\nFashion\nThe United States is the world's largest apparel market by revenue. Apart from professional business attire, American fashion is eclectic and predominantly informal. Americans' diverse cultural roots are reflected in their clothing; however, sneakers, jeans, T-shirts, and baseball caps are emblematic of American styles. New York, with its fashion week, is considered to be one of the \"Big Four\" global fashion capitals, along with Paris, Milan, and London. A study demonstrated that general proximity to Manhattan's Garment District has been synonymous with American fashion since its inception in the early 20th century.\nThe headquarters of many designer labels reside in Manhattan. Labels cater to niche markets, such as preteens. New York Fashion Week is one of the most influential fashion weeks in the world, and occurs twice a year; while the annual Met Gala in Manhattan is commonly known as the fashion world's \"biggest night\".\n\nCinema\nThe U.S. film industry has a worldwide influence and following. Hollywood, a district in northern Los Angeles, the nation's second-most populous city, is also metonymous for the American filmmaking industry. The major film studios of the United States are the primary source of the most commercially successful and most ticket-selling movies in the world. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization. The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929, and the Golden Globe Awards have been held annually since January 1944.\nThe industry peaked in what is commonly referred to as the \"Golden Age of Hollywood\", from the early sound period until the early 1960s, with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures. In the 1970s, \"New Hollywood\", or the \"Hollywood Renaissance\", was defined by grittier films influenced by French and Italian realist pictures of the post-war period. The 21st century has been marked by the rise of American streaming platforms, which came to rival traditional cinema.\n\nCuisine\nEarly settlers were introduced by Native Americans to foods such as turkey, sweet potatoes, corn, squash, and maple syrup. Of the most enduring and pervasive examples are variations of the native dish called succotash. Early settlers and later immigrants combined these with foods they were familiar with, such as wheat flour, beef, and milk, to create a distinctive American cuisine. New World crops, especially pumpkin, corn, potatoes, and turkey as the main course are part of a shared national menu on Thanksgiving, when many Americans prepare or purchase traditional dishes to celebrate the occasion.\nCharacteristic American dishes such as apple pie, fried chicken, doughnuts, french fries, macaroni and cheese, ice cream, hamburgers, hot dogs, and American pizza derive from the recipes of various immigrant groups. Mexican dishes such as burritos and tacos preexisted the United States in areas later annexed from Mexico, and adaptations of Chinese cuisine as well as pasta dishes freely adapted from Italian sources are all widely consumed. American chefs have had a significant impact on society both domestically and internationally. In 1946, the Culinary Institute of America was founded by Katharine Angell and Frances Roth. This would become the United States' most prestigious culinary school, where many of the most talented American chefs would study prior to successful careers.\nThe United States restaurant industry was projected at $899 billion in sales for 2020, and employed more than 15 million people, representing 10% of the nation's workforce directly. It is the country's second-largest private employer and the third-largest employer overall. The United States is home to over 220 Michelin Star-rated restaurants, 70 of which are in New York City alone. Wine has been produced in what is now the United States since the 1500s, with the first widespread production beginning in what is now New Mexico in 1628. In the modern U.S., wine production is undertaken in all fifty states, with California producing 84 percent of all U.S. wine. With more than 1,100,000 acres (4,500 km2) under vine, the United States is the fourth-largest wine-producing country in the world, after Italy, Spain, and France.\nThe American fast-food industry developed alongside the nation's car culture. American restaurants developed the drive-in format in the 1920s, which they began to replace with the drive-through format by the 1940s. American fast-food restaurant chains, such as McDonald's, Kentucky Fried Chicken, Dunkin' Donuts and many others, have numerous outlets around the world.\n\nSports\nThe most popular spectator sports in the U.S. are American football, basketball, baseball, soccer, and ice hockey. While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, many of which have become popular worldwide. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate European contact. The market for professional sports in the United States was approximately $69 billion in July 2013, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.\nAmerican football is by several measures the most popular spectator sport in the United States; the National Football League has the highest average attendance of any sports league in the world, and the Super Bowl is watched by tens of millions globally. However, baseball has been regarded as the U.S. \"national sport\" since the late 19th century. After American football, the next four most popular professional team sports are basketball, baseball, soccer, and ice hockey. Their premier leagues are, respectively, the National Basketball Association, Major League Baseball, Major League Soccer, and the National Hockey League. The most-watched individual sports in the U.S. are golf and auto racing, particularly NASCAR and IndyCar.\nOn the collegiate level, earnings for the member institutions exceed $1 billion annually, and college football and basketball attract large audiences, as the NCAA March Madness tournament and the College Football Playoff are some of the most watched national sporting events. In the U.S., the intercollegiate sports level serves as a feeder system for professional sports. This differs greatly from practices in nearly all other countries, where publicly and privately funded sports organizations serve this function.\nEight Olympic Games have taken place in the United States. The 1904 Summer Olympics in St. Louis, Missouri, were the first-ever Olympic Games held outside of Europe. The Olympic Games will be held in the U.S. for a ninth time when Los Angeles hosts the 2028 Summer Olympics. U.S. athletes have won a total of 2,968 medals (1,179 gold) at the Olympic Games, the most of any country.\nIn international professional competition, the U.S. men's national soccer team has qualified for eleven World Cups, while the women's national team has won the FIFA Women's World Cup and Olympic soccer tournament four times each. The United States hosted the 1994 FIFA World Cup and will co-host, along with Canada and Mexico, the 2026 FIFA World Cup. The 1999 FIFA Women's World Cup was also hosted by the United States. Its final match was watched by 90,185, setting the world record for most-attended women's sporting event at the time.\n\nSee also\nLists of U.S. state topics\nOutline of the United States\n\nNotes\nReferences\nSources\nExternal links\nKey Development Forecasts for the United States from International Futures\n\nGovernment\nOfficial U.S. Government web portal \u2013 gateway to government sites\nHouse \u2013 official website of the United States House of Representatives\nSenate \u2013 official website of the United States Senate\nWhite House \u2013 official website of the president of the United States\nSupreme Court \u2013 official website of the Supreme Court of the United States\n\nHistory\n\"Historical Documents\" \u2013 website from the National Center for Public Policy Research\n\"U.S. National Mottos: History and Constitutionality\". Religious Tolerance. Analysis by the Ontario Consultants on Religious Tolerance.\n\"Historical Statistics\" \u2013 links to U.S. historical data\n\nMaps\n\"National Atlas of the United States\" \u2013 official maps from the U.S. Department of the Interior\n Wikimedia Atlas of the United States\n Geographic data related to United States at OpenStreetMap\n\"Measure of America\" \u2013 a variety of mapped information relating to health, education, income, safety and demographics in the United States", "url": "https://en.wikipedia.org/wiki/United_States"}, {"title": "Machine Learning", "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n\nHistory\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\n\nRelationships to other fields\nArtificial intelligence\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.:\u200a488\u200a\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.:\u200a25\u200a\nMachine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\n\nData compression\nData mining\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).\n\nGeneralization\nCharacterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n\nStatistics\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\nLeo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\n\nStatistical physics\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\n\nTheory\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalization error.\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nApproaches\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\n\nSupervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\nTypes of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. \nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\nUnsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.  \n\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\nA special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.\n\nSemi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\nReinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\nDimensionality reduction\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\n\nOther types\nOther approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.\n\nSelf-learning\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s perform action a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.\n\nFeature learning\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\nSparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\nAnomaly detection\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\nRobot learning\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\n\nAssociation rules\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n  \n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\nModels\nA machine learning model is a type of mathematical model that, after being \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n\nArtificial neural networks\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\nDecision trees\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\nSupport-vector machines\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\nRegression analysis\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n\nBayesian networks\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\nGaussian processes\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\n\nGenetic algorithms\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\nBelief functions\nThe theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n\nTraining models\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n\nFederated learning\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\nApplications\nThere are many applications for machine learning, including:\n\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires.\n\nLimitations\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\n\nBias\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\nLanguage models learned from data have been shown to contain human-like biases. In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants.\" In 2015, Google Photos would often tag black people as gorillas, and in 2018, this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\nExplainability\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\nOverfitting\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.\n\nOther limitations and vulnerabilities\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\n\nModel assessments\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).\n\nEthics\nMachine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\nWhile responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\nAI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.\nOther forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\nHardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\nNeuromorphic/Physical Neural Networks\nA physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.\n\nEmbedded Machine Learning\nEmbedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more. Pruning, Quantization, Knowledge Distillation, Low-Rank Factorization, Network Architecture Search (NAS) & Parameter Sharing are few of the techniques used for optimization of machine learning models.\n\nSoftware\nSoftware suites containing a variety of machine learning algorithms include the following:\n\nFree and open-source software\nProprietary software with free and open-source editions\nKNIME\nRapidMiner\n\nProprietary software\nJournals\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\nConferences\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\nSee also\nAutomated machine learning \u2013 Process of automating the application of machine learning\nBig data \u2013 Extremely large or complex datasets\nDeep learning \u2014 branch of ML concerned with artificial neural networks\nDifferentiable programming \u2013 Programming paradigm\nList of datasets for machine-learning research \u2013 Machine learning based fault detection in Electronics Circuit\nM-theory (learning framework)\n\nReferences\nSources\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\n\nFurther reading\nExternal links\n\nInternational Machine Learning Society\nmloss is an academic database of open-source machine learning software.", "url": "https://en.wikipedia.org/wiki/Machine_learning"}, {"title": "Data Science", "text": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\nFoundations\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human\u2013computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\n\nRelationship to statistics\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.\nStanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n\nEtymology\nEarly usage\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.\nDuring the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".\n\nModern usage\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.\nThe professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.\nThere is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\n\nData science and data analysis\nData science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.\nData analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.\nData science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.\nWhile data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.\nDespite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.\nIn summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n\nCloud computing for data science\nCloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.\n\nEthical consideration in data science\nData science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts \nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.\n\nSee also\nPython (programming language)\nR (programming language)\nData engineering\nBig data\nMachine learning\n\n\n== References ==", "url": "https://en.wikipedia.org/wiki/Data_science"}, {"title": "Artificial Intelligence", "text": "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence\u2014the ability to complete any task performable by a human on an at least equal level\u2014is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\nPlanning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\nLearning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\nNatural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.\n\nSocial intelligence\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\nHowever, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.\n\nGeneral intelligence\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\nTechniques\nAI research uses a wide variety of techniques to accomplish the goals above.\n\nSearch and optimization\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\nState space search\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\n\nLocal search\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\nLogic\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\nProbabilistic methods for uncertain reasoning\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation\u2013maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nClassifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\nArtificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other\u2014this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.\n\nDeep learning\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023. The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\nGPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\nHardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster.\n\nApplications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n\nHealth and medicine\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\nGames\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\nMathematics\nIn mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.\nAlternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind, Llemma from eleuther or Julius.\nWhen natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks.\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\n\nFinance\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nWorld Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\nMilitary\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\n\nGenerative AI\nIn the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models, often in response to prompts.\nIn March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.\n\nAgents\nArtificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\nOther industry-specific tasks\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 millions was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\n\nEthics\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\nRisks and harm\nPrivacy and copyright\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\nPower needs and environmental impacts\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US). Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\n\nMisinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\n\nAlgorithmic bias and fairness\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\nLack of transparency\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\nBad actors and weaponized AI\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\nTechnological unemployment\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\nExistential risk\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like \"sentience\" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\" He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nOther researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\nEthical machines and alignment\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\nOpen source\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\nFrameworks\nArtificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values\u2014developed by the Alan Turing Institute tests projects in four main areas:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\nRegulation\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\nHistory\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022.\n\nPhilosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\nDefining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\n\nEvaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\nSymbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\nNeat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\nSoft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nNarrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\nMachine consciousness, sentience, and mind\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nConsciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\nComputationalism and functionalism\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\n\nAI welfare and rights\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\nFuture\nSuperintelligence and the singularity\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\nTranshumanism\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\nIn fiction\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\nSee also\nArtificial general intelligence\nArtificial intelligence content detection \u2013 Software to detect AI-generated content\nBehavior selection algorithm \u2013 Algorithm that selects actions for intelligent agents\nBusiness process automation \u2013 Automation of business processes\nCase-based reasoning \u2013 Process of solving new problems based on the solutions of similar past problems\nComputational intelligence \u2013 Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality \u2013 Hypothetical concept of storing a personality in digital form\nEmergent algorithm \u2013 Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies \u2013 Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence \u2013 List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification \u2013 Use of information technology to augment human intelligence\nMind uploading \u2013 Hypothetical process of digitally emulating a brain\nMoravec's paradox \u2013 Observation that perception requires more computation than reasoning\nOrganoid intelligence \u2013 Use of brain cells and brain organoids for intelligent computing\nRobotic process automation \u2013 Form of business process automation technology\nWeak artificial intelligence \u2013 Form of artificial intelligence\nWetware computer \u2013 Computer composed of organic material\nHallucination (artificial intelligence) \u2013 Erroneous material generated by AI\n\nExplanatory notes\nReferences\nAI textbooks\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThese were the four of the most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n\nHistory of AI\nOther sources\nFurther reading\nExternal links\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).", "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"}, {"title": "Global financial system", "text": "The global financial system is the worldwide framework of legal agreements, institutions, and both formal and informal economic action that together facilitate international flows of financial capital for purposes of investment and trade financing. Since emerging in the late 19th century during the first modern wave of economic globalization, its evolution is marked by the establishment of central banks, multilateral treaties, and intergovernmental organizations aimed at improving the transparency, regulation, and effectiveness of international markets.:\u200a74\u200a:\u200a1\u200a In the late 1800s, world migration and communication technology facilitated unprecedented growth in international trade and investment. At the onset of World War I, trade contracted as foreign exchange markets became paralyzed by money market illiquidity. Countries sought to defend against external shocks with protectionist policies and trade virtually halted by 1933, worsening the effects of the global Great Depression until a series of reciprocal trade agreements slowly reduced tariffs worldwide. Efforts to revamp the international monetary system after World War II improved exchange rate stability, fostering record growth in global finance.\nA series of currency devaluations and oil crises in the 1970s led most countries to float their currencies. The world economy became increasingly financially integrated in the 1980s and 1990s due to capital account liberalization and financial deregulation. A series of financial crises in Europe, Asia, and Latin America followed with contagious effects due to greater exposure to volatile capital flows. The 2007\u20132008 financial crisis, which originated in the United States, quickly propagated among other nations and is recognized as the catalyst for the worldwide Great Recession. A market adjustment to Greece's noncompliance with its monetary union in 2009 ignited a sovereign debt crisis among European nations known as the Eurozone crisis. The history of international finance shows a U-shaped pattern in international capital flows: high prior to 1914 and after 1989, but lower in between. The volatility of capital flows has been greater since the 1970s than in previous periods.\nA country's decision to operate an open economy and globalize its financial capital carries monetary implications captured by the balance of payments. It also renders exposure to risks in international finance, such as political deterioration, regulatory changes, foreign exchange controls, and legal uncertainties for property rights and investments. Both individuals and groups may participate in the global financial system. Consumers and international businesses undertake consumption, production, and investment. Governments and intergovernmental bodies act as purveyors of international trade, economic development, and crisis management. Regulatory bodies establish financial regulations and legal procedures, while independent bodies facilitate industry supervision. Research institutes and other associations analyze data, publish reports and policy briefs, and host public discourse on global financial affairs.\nWhile the global financial system is edging toward greater stability, governments must deal with differing regional or national needs. Some nations are trying to systematically discontinue unconventional monetary policies installed to cultivate recovery, while others are expanding their scope and scale. Emerging market policymakers face a challenge of precision as they must carefully institute sustainable macroeconomic policies during extraordinary market sensitivity without provoking investors to retreat their capital to stronger markets. Nations' inability to align interests and achieve international consensus on matters such as banking regulation has perpetuated the risk of future global financial catastrophes.  Initiatives like the United Nations Sustainable Development Goal 10 are aimed at improving regulation and monitoring of global financial systems.\n\nHistory of international financial architecture\nEmergence of financial globalization: 1870\u20131914\nThe world experienced substantial changes in the late 19th century which created an environment favorable to an increase in and development of international financial centers. Principal among such changes were unprecedented growth in capital flows and the resulting rapid financial center integration, as well as faster communication. Before 1870, London and Paris existed as the world's only prominent financial centers.:\u200a1\u200a Soon after, Berlin and New York grew to become major centres providing financial services for their national economies. An array of smaller international financial centers became important as they found market niches, such as Amsterdam, Brussels, Z\u00fcrich, and Geneva. London remained the leading international financial center in the four decades leading up to World War I.:\u200a74\u201375\u200a:\u200a12\u201315\u200a\nThe first modern wave of economic globalization began during the period of 1870\u20131914, marked by transportation expansion, record levels of migration, enhanced communications, trade expansion, and growth in capital transfers.:\u200a75\u200a During the mid-nineteenth century, the passport system in Europe dissolved as rail transport expanded rapidly. Most countries issuing passports did not require they be carried, and so people could travel freely without them. The standardization of international passports would not arise until 1980 under the guidance of the United Nations' International Civil Aviation Organization. From 1870 to 1915, 36 million Europeans migrated away from Europe. Approximately 25 million (or 70%) of these travelers migrated to the United States, while most of the rest reached Canada, Australia and Brazil. Europe itself experienced an influx of foreigners from 1860 to 1910, growing from 0.7% of the population to 1.8%. While the absence of meaningful passport requirements allowed for free travel, migration on such an enormous scale would have been prohibitively difficult if not for technological advances in transportation, particularly the expansion of railway travel and the dominance of steam-powered boats over traditional sailing ships. World railway mileage grew from 205,000 kilometers in 1870 to 925,000 kilometers in 1906, while steamboat cargo tonnage surpassed that of sailboats in the 1890s. Advancements such as the telephone and wireless telegraphy (the precursor to radio) revolutionized telecommunication by providing instantaneous communication. In 1866, the first transatlantic cable was laid beneath the ocean to connect London and New York, while Europe and Asia became connected through new landlines.:\u200a75\u201376\u200a:\u200a5\u200a\nEconomic globalization grew under free trade, starting in 1860 when the United Kingdom entered into a free trade agreement with France known as the Cobden\u2013Chevalier Treaty. However, the golden age of this wave of globalization endured a return to protectionism between 1880 and 1914. In 1879, German Chancellor Otto von Bismarck introduced protective tariffs on agricultural and manufacturing goods, making Germany the first nation to institute new protective trade policies. In 1892, France introduced the M\u00e9line tariff, greatly raising customs duties on both agricultural and manufacturing goods. The United States maintained strong protectionism during most of the nineteenth century, imposing customs duties between 40 and 50% on imported goods. Despite these measures, international trade continued to grow without slowing. Paradoxically, foreign trade grew at a much faster rate during the protectionist phase of the first wave of globalization than during the free trade phase sparked by the United Kingdom.:\u200a76\u201377\u200a\nUnprecedented growth in foreign investment from the 1880s to the 1900s served as the core driver of financial globalization. The worldwide total of capital invested abroad amounted to US$44 billion in 1913 ($1.02 trillion in 2012 dollars), with the greatest share of foreign assets held by the United Kingdom (42%), France (20%), Germany (13%), and the United States (8%). The Netherlands, Belgium, and Switzerland together held foreign investments on par with Germany at around 12%.:\u200a77\u201378\n\nPanic of 1907\nIn October 1907, the United States experienced a bank run on the Knickerbocker Trust Company, forcing the trust to close on October 23, 1907, provoking further reactions. The panic was alleviated when U.S. Secretary of the Treasury George B. Cortelyou and John Pierpont \"J.P.\" Morgan deposited $25 million and $35 million, respectively, into the reserve banks of New York City, enabling withdrawals to be fully covered. The bank run in New York led to a money market crunch which occurred simultaneously as demands for credit heightened from cereal and grain exporters. Since these demands could only be serviced through the purchase of substantial quantities of gold in London, the international markets became exposed to the crisis. The Bank of England had to sustain an artificially high discount lending rate until 1908. To service the flow of gold to the United States, the Bank of England organized a pool from among twenty-four nations, for which the Banque de France temporarily lent \u00a33 million (GBP, 305.6 million in 2012 GBP) in gold.:\u200a123\u2013124\n\nBirth of the U.S. Federal Reserve System: 1913\nThe United States Congress passed the Federal Reserve Act in 1913, giving rise to the Federal Reserve System. Its inception drew influence from the Panic of 1907, underpinning legislators' hesitance in trusting individual investors, such as John Pierpont Morgan, to serve again as a lender of last resort. The system's design also considered the findings of the Pujo Committee's investigation of the possibility of a money trust in which Wall Street's concentration of influence over national financial matters was questioned and in which investment bankers were suspected of unusually deep involvement in the directorates of manufacturing corporations. Although the committee's findings were inconclusive, the very possibility was enough to motivate support for the long-resisted notion of establishing a central bank. The Federal Reserve's overarching aim was to become the sole lender of last resort and to resolve the inelasticity of the United States' money supply during significant shifts in money demand. In addition to addressing the underlying issues that precipitated the international ramifications of the 1907 money market crunch, New York's banks were liberated from the need to maintain their own reserves and began undertaking greater risks. New access to rediscount facilities enabled them to launch foreign branches, bolstering New York's rivalry with London's competitive discount market.:\u200a123\u2013124\u200a:\u200a53\u200a:\u200a18\n\nInterwar period: 1915\u20131944\nEconomists have referred to the onset of World War I as the end of an age of innocence for foreign exchange markets, as it was the first geopolitical conflict to have a destabilizing and paralyzing impact. The United Kingdom declared war on Germany on August 4, 1914 following Germany's invasion of France and Belgium. In the weeks prior, the foreign exchange market in London was the first to exhibit distress. European tensions and increasing political uncertainty motivated investors to chase liquidity, prompting commercial banks to borrow heavily from London's discount market. As the money market tightened, discount lenders began rediscounting their reserves at the Bank of England rather than discounting new pounds sterling. The Bank of England was forced to raise discount rates daily for three days from 3% on July 30 to 10% by August 1. As foreign investors resorted to buying pounds for remittance to London just to pay off their newly maturing securities, the sudden demand for pounds led the pound to appreciate beyond its gold value against most major currencies, yet sharply depreciate against the French franc after French banks began liquidating their London accounts. Remittance to London became increasingly difficult and culminated in a record exchange rate of US$6.50/GBP. Emergency measures were introduced in the form of moratoria and extended bank holidays, but to no effect as financial contracts became informally unable to be negotiated and export embargoes thwarted gold shipments. A week later, the Bank of England began to address the deadlock in the foreign exchange markets by establishing a new channel for transatlantic payments whereby participants could make remittance payments to the U.K. by depositing gold designated for a Bank of England account with Canada's Minister of Finance, and in exchange receive pounds sterling at an exchange rate of $4.90. Approximately US$104 million in remittances flowed through this channel in the next two months. However, pound sterling liquidity ultimately did not improve due to inadequate relief for merchant banks receiving sterling bills. As the pound sterling was the world's reserve currency and leading vehicle currency, market illiquidity and merchant banks' hesitance to accept sterling bills left currency markets paralyzed.:\u200a23\u201324\u200a\nThe U.K. government attempted several measures to revive the London foreign exchange market, the most notable of which were implemented on September 5 to extend the previous moratorium through October and allow the Bank of England to temporarily loan funds to be paid back upon the end of the war in an effort to settle outstanding or unpaid acceptances for currency transactions. By mid-October, the London market began functioning properly as a result of the September measures. The war continued to present unfavorable circumstances for the foreign exchange market, such as the London Stock Exchange's prolonged closure, the redirection of economic resources to support a transition from producing exports to producing military armaments, and myriad disruptions of freight and mail. The pound sterling enjoyed general stability throughout World War I, in large part due to various steps taken by the U.K. government to influence the pound's value in ways that yet provided individuals with the freedom to continue trading currencies. Such measures included open market interventions on foreign exchange, borrowing in foreign currencies rather than in pounds sterling to finance war activities, outbound capital controls, and limited import restrictions.:\u200a25\u201327\u200a\nIn 1930, the Allied powers established the Bank for International Settlements (BIS). The principal purposes of the BIS were to manage the scheduled payment of Germany's reparations imposed by the Treaty of Versailles in 1919, and to function as a bank for central banks around the world. Nations may hold a portion of their reserves as deposits with the institution. It also serves as a forum for central bank cooperation and research on international monetary and financial matters. The BIS also operates as a general trustee and facilitator of financial settlements between nations.:\u200a182\u200a:\u200a531\u2013532\u200a:\u200a56\u201357\u200a:\u200a269\n\nSmoot\u2013Hawley tariff of 1930\nU.S. President Herbert Hoover signed the Smoot\u2013Hawley Tariff Act into law on June 17, 1930. The tariff's aim was to protect agriculture in the United States, but congressional representatives ultimately raised tariffs on a host of manufactured goods resulting in average duties as high as 53% on over a thousand various goods. Twenty-five trading partners responded in kind by introducing new tariffs on a wide range of U.S. goods. Hoover was pressured and compelled to adhere to the Republican Party's 1928 platform, which sought protective tariffs to alleviate market pressures on the nation's struggling agribusinesses and reduce the domestic unemployment rate. The culmination of the Stock Market Crash of 1929 and the onset of the Great Depression heightened fears, further pressuring Hoover to act on protective policies against the advice of Henry Ford and over 1,000 economists who protested by calling for a veto of the act.:\u200a175\u2013176\u200a:\u200a186\u2013187\u200a:\u200a43\u201344\u200a Exports from the United States plummeted 60% from 1930 to 1933.:\u200a118\u200a Worldwide international trade virtually ground to a halt.:\u200a125\u2013126\u200a The international ramifications of the Smoot-Hawley tariff, comprising protectionist and discriminatory trade policies and bouts of economic nationalism, are credited by economists with prolongment and worldwide propagation of the Great Depression.:\u200a2\u200a:\u200a108\u200a:\u200a33\n\nFormal abandonment of the Gold Standard\nThe classical gold standard was established in 1821 by the United Kingdom as the Bank of England enabled redemption of its banknotes for gold bullion. France, Germany, the United States, Russia, and Japan each embraced the standard one by one from 1878 to 1897, marking its international acceptance. The first departure from the standard occurred in August 1914 when these nations erected trade embargoes on gold exports and suspended redemption of gold for banknotes. Following the end of World War I on November 11, 1918, Austria, Hungary, Germany, Russia, and Poland began experiencing hyperinflation. Having informally departed from the standard, most currencies were freed from exchange rate fixing and allowed to float. Most countries throughout this period sought to gain national advantages and bolster exports by depreciating their currency values to predatory levels. A number of countries, including the United States, made unenthusiastic and uncoordinated attempts to restore the former gold standard. The early years of the Great Depression brought about bank runs in the United States, Austria, and Germany, which placed pressures on gold reserves in the United Kingdom to such a degree that the gold standard became unsustainable. Germany became the first nation to formally abandon the post-World War I gold standard when the Dresdner Bank implemented foreign exchange controls and announced bankruptcy on July 15, 1931. In September 1931, the United Kingdom allowed the pound sterling to float freely. By the end of 1931, a host of countries including Austria, Canada, Japan, and Sweden abandoned gold. Following widespread bank failures and a hemorrhaging of gold reserves, the United States broke free of the gold standard in April 1933. France would not follow suit until 1936 as investors fled from the franc due to political concerns over Prime Minister L\u00e9on Blum's government.:\u200a58\u200a:\u200a414\u200a:\u200a32\u201333\n\nTrade liberalization in the United States\nThe disastrous effects of the Smoot\u2013Hawley tariff proved difficult for Herbert Hoover's 1932 re-election campaign. Franklin D. Roosevelt became the 32nd U.S. president and the Democratic Party worked to reverse trade protectionism in favor of trade liberalization. As an alternative to cutting tariffs across all imports, Democrats advocated for trade reciprocity. The U.S. Congress passed the Reciprocal Trade Agreements Act in 1934, aimed at restoring global trade and reducing unemployment. The legislation expressly authorized President Roosevelt to negotiate bilateral trade agreements and reduce tariffs considerably. If a country agreed to cut tariffs on certain commodities, the U.S. would institute corresponding cuts to promote trade between the two nations. Between 1934 and 1947, the U.S. negotiated 29 such agreements and the average tariff rate decreased by approximately one third during this same period. The legislation contained an important most-favored-nation clause, through which tariffs were equalized to all countries, such that trade agreements would not result in preferential or discriminatory tariff rates with certain countries on any particular import, due to the difficulties and inefficiencies associated with differential tariff rates. The clause effectively generalized tariff reductions from bilateral trade agreements, ultimately reducing worldwide tariff rates.:\u200a176\u2013177\u200a:\u200a186\u2013187\u200a:\u200a108\n\nRise of the Bretton Woods financial order: 1945\nAs the inception of the United Nations as an intergovernmental entity slowly began formalizing in 1944, delegates from 44 of its early member states met at a hotel in Bretton Woods, New Hampshire for the United Nations Monetary and Financial Conference, now commonly referred to as the Bretton Woods conference. Delegates remained cognizant of the effects of the Great Depression, struggles to sustain the international gold standard during the 1930s, and related market instabilities. Whereas previous discourse on the international monetary system focused on fixed versus floating exchange rates, Bretton Woods delegates favored pegged exchange rates for their flexibility. Under this system, nations would peg their exchange rates to the U.S. dollar, which would be convertible to gold at US$35 per ounce.:\u200a448\u200a:\u200a34\u200a:\u200a3\u200a:\u200a6\u200a This arrangement is commonly referred to as the Bretton Woods system. Rather than maintaining fixed rates, nations would peg their currencies to the U.S. dollar and allow their exchange rates to fluctuate within a 1% band of the agreed-upon parity. To meet this requirement, central banks would intervene via sales or purchases of their currencies against the dollar.:\u200a491\u2013493\u200a:\u200a296\u200a:\u200a21\u200a \nMembers could adjust their pegs in response to long-run fundamental disequilibria in the balance of payments, but were responsible for correcting imbalances via fiscal and monetary policy tools before resorting to repegging strategies.:\u200a448\u200a:\u200a22\u200a The adjustable pegging enabled greater exchange rate stability for commercial and financial transactions which fostered unprecedented growth in international trade and foreign investment. This feature grew from delegates' experiences in the 1930s when excessively volatile exchange rates and the reactive protectionist exchange controls that followed proved destructive to trade and prolonged the deflationary effects of the Great Depression. Capital mobility faced de facto limits under the system as governments instituted restrictions on capital flows and aligned their monetary policy to support their pegs.:\u200a448\u200a:\u200a38\u200a:\u200a91\u200a:\u200a30\u200a\nAn important component of the Bretton Woods agreements was the creation of two new international financial institutions, the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (IBRD). Collectively referred to as the Bretton Woods institutions, they became operational in 1947 and 1946 respectively. The IMF was established to support the monetary system by facilitating cooperation on international monetary issues, providing advisory and technical assistance to members, and offering emergency lending to nations experiencing repeated difficulties restoring the balance of payments equilibrium. Members would contribute funds to a pool according to their share of gross world product, from which emergency loans could be issued.:\u200a21\u200a:\u200a9\u201310\u200a:\u200a20\u201322\u200a \nMember states were authorized and encouraged to employ capital controls as necessary to manage payments imbalances and meet pegging targets, but prohibited from relying on IMF financing to cover particularly short-term capital hemorrhages.:\u200a38\u200a While the IMF was instituted to guide members and provide a short-term financing window for recurrent balance of payments deficits, the IBRD was established to serve as a type of financial intermediary for channeling global capital toward long-term investment opportunities and postwar reconstruction projects.:\u200a22\u200a The creation of these organizations was a crucial milestone in the evolution of the international financial architecture, and some economists consider it the most significant achievement of multilateral cooperation following World War II.:\u200a39\u200a:\u200a1\u20133\u200a Since the establishment of the International Development Association (IDA) in 1960, the IBRD and IDA are together known as the World Bank. While the IBRD lends to middle-income developing countries, the IDA extends the Bank's lending program by offering concessional loans and grants to the world's poorest nations.\n\nGeneral Agreement on Tariffs and Trade: 1947\nIn 1947, 23 countries concluded the General Agreement on Tariffs and Trade (GATT) at a UN conference in Geneva. Delegates intended the agreement to suffice while member states would negotiate creation of a UN body to be known as the International Trade Organization (ITO). As the ITO never became ratified, GATT became the de facto framework for later multilateral trade negotiations. Members emphasized trade reprocity as an approach to lowering barriers in pursuit of mutual gains.:\u200a46\u200a The agreement's structure enabled its signatories to codify and enforce regulations for trading of goods and services.:\u200a11\u200a GATT was centered on two precepts: trade relations needed to be equitable and nondiscriminatory, and subsidizing non-agricultural exports needed to be prohibited. As such, the agreement's most favored nation clause prohibited members from offering preferential tariff rates to any nation that it would not otherwise offer to fellow GATT members. In the event of any discovery of non-agricultural subsidies, members were authorized to offset such policies by enacting countervailing tariffs.:\u200a460\u200a The agreement provided governments with a transparent structure for managing trade relations and avoiding protectionist pressures.:\u200a108\u200a However, GATT's principles did not extend to financial activity, consistent with the era's rigid discouragement of capital movements.:\u200a70\u201371\u200a The agreement's initial round achieved only limited success in reducing tariffs. While the U.S. reduced its tariffs by one third, other signatories offered much smaller trade concessions.:\u200a99\n\nResurgence of financial globalization\nFlexible exchange rate regimes: 1973\u2013present\nAlthough the exchange rate stability sustained by the Bretton Woods system facilitated expanding international trade, this early success masked its underlying design flaw, wherein there existed no mechanism for increasing the supply of international reserves to support continued growth in trade.:\u200a22\u200a The system began experiencing insurmountable market pressures and deteriorating cohesion among its key participants in the late 1950s and early 1960s. Central banks needed more U.S. dollars to hold as reserves, but were unable to expand their money supplies if doing so meant exceeding their dollar reserves and threatening their exchange rate pegs. To accommodate these needs, the Bretton Woods system depended on the United States to run dollar deficits. As a consequence, the dollar's value began exceeding its gold backing. During the early 1960s, investors could sell gold for a greater dollar exchange rate in London than in the United States, signaling to market participants that the dollar was overvalued. Belgian-American economist Robert Triffin defined this problem now known as the Triffin dilemma, in which a country's national economic interests conflict with its international objectives as the custodian of the world's reserve currency.:\u200a34\u201335\u200a\nFrance voiced concerns over the artificially low price of gold in 1968 and called for returns to the former gold standard. Meanwhile, excess dollars flowed into international markets as the United States expanded its money supply to accommodate the costs of its military campaign in the Vietnam War. Its gold reserves were assaulted by speculative investors following its first current account deficit since the 19th century. In August 1971, President Richard Nixon suspended the exchange of U.S. dollars for gold as part of the Nixon Shock. The closure of the gold window effectively shifted the adjustment burdens of a devalued dollar to other nations. Speculative traders chased other currencies and began selling dollars in anticipation of these currencies being revalued against the dollar. These influxes of capital presented difficulties to foreign central banks, which then faced choosing among inflationary money supplies, largely ineffective capital controls, or floating exchange rates.:\u200a34\u201335\u200a:\u200a14\u201315\u200a Following these woes surrounding the U.S. dollar, the dollar price of gold was raised to US$38 per ounce and the Bretton Woods system was modified to allow fluctuations within an augmented band of 2.25% as part of the Smithsonian Agreement signed by the G-10 members in December 1971. The agreement delayed the system's demise for a further two years.:\u200a6\u20137\u200a The system's erosion was expedited not only by the dollar devaluations that occurred, but also by the oil crises of the 1970s which emphasized the importance of international financial markets in petrodollar recycling and balance of payments financing. Once the world's reserve currency began to float, other nations began adopting floating exchange rate regimes.:\u200a5\u20137\n\nPost-Bretton Woods financial order: 1976\nAs part of the first amendment to its articles of agreement in 1969, the IMF developed a new reserve instrument called special drawing rights (SDRs), which could be held by central banks and exchanged among themselves and the Fund as an alternative to gold. SDRs entered service in 1970 originally as units of a market basket of sixteen major vehicle currencies of countries whose share of total world exports exceeded 1%. The basket's composition changed over time and presently consists of the U.S. dollar, euro, Japanese yen, Chinese yuan, and British pound. Beyond holding them as reserves, nations can denominate transactions among themselves and the Fund in SDRs, although the instrument is not a vehicle for trade. In international transactions, the currency basket's portfolio characteristic affords greater stability against the uncertainties inherent with free floating exchange rates.:\u200a34\u201335\u200a:\u200a50\u201351\u200a:\u200a117\u200a:\u200a10\u200a Special drawing rights were originally equivalent to a specified amount of gold, but were not directly redeemable for gold and instead served as a surrogate in obtaining other currencies that could be exchanged for gold. The Fund initially issued 9.5 billion XDR from 1970 to 1972.:\u200a182\u2013183\u200a\nIMF members signed the Jamaica Agreement in January 1976, which ratified the end of the Bretton Woods system and reoriented the Fund's role in supporting the international monetary system. The agreement officially embraced the flexible exchange rate regimes that emerged after the failure of the Smithsonian Agreement measures. In tandem with floating exchange rates, the agreement endorsed central bank interventions aimed at clearing excessive volatility. The agreement retroactively formalized the abandonment of gold as a reserve instrument and the Fund subsequently demonetized its gold reserves, returning gold to members or selling it to provide poorer nations with relief funding. Developing countries and countries not endowed with oil export resources enjoyed greater access to IMF lending programs as a result. The Fund continued assisting nations experiencing balance of payments deficits and currency crises, but began imposing conditionality on its funding that required countries to adopt policies aimed at reducing deficits through spending cuts and tax increases, reducing protective trade barriers, and contractionary monetary policy.:\u200a36\u200a:\u200a47\u201348\u200a:\u200a12\u201313\u200a\nThe second amendment to the articles of agreement was signed in 1978. It legally formalized the free-floating acceptance and gold demonetization achieved by the Jamaica Agreement, and required members to support stable exchange rates through macroeconomic policy. The post-Bretton Woods system was decentralized in that member states retained autonomy in selecting an exchange rate regime. The amendment also expanded the institution's capacity for oversight and charged members with supporting monetary sustainability by cooperating with the Fund on regime implementation.:\u200a62\u201363\u200a:\u200a138\u200a This role is called IMF surveillance and is recognized as a pivotal point in the evolution of the Fund's mandate, which was extended beyond balance of payments issues to broader concern with internal and external stresses on countries' overall economic policies.:\u200a148\u200a:\u200a10\u201311\u200a\nUnder the dominance of flexible exchange rate regimes, the foreign exchange markets became significantly more volatile. In 1980, newly elected U.S. president Ronald Reagan's administration brought about increasing balance of payments deficits and budget deficits. To finance these deficits, the United States offered artificially high real interest rates to attract large inflows of foreign capital. As foreign investors' demand for U.S. dollars grew, the dollar's value appreciated substantially until reaching its peak in February 1985. The U.S. trade deficit grew to $160 billion in 1985 ($341 billion in 2012 dollars) as a result of the dollar's strong appreciation. The G5 met in September 1985 at the Plaza Hotel in New York City and agreed that the dollar should depreciate against the major currencies to resolve the United States' trade deficit and pledged to support this goal with concerted foreign exchange market interventions, in what became known as the Plaza Accord. The U.S. dollar continued to depreciate, but industrialized nations became increasingly concerned that it would decline too heavily and that exchange rate volatility would increase. To address these concerns, the G7 (now G8) held a summit in Paris in 1987, where they agreed to pursue improved exchange rate stability and better coordinate their macroeconomic policies, in what became known as the Louvre Accord. This accord became the provenance of the managed float regime by which central banks jointly intervene to resolve under- and overvaluations in the foreign exchange market to stabilize otherwise freely floating currencies. Exchange rates stabilized following the embrace of managed floating during the 1990s, with a strong U.S. economic performance from 1997 to 2000 during the Dot-com bubble. After the 2000 stock market correction of the Dot-com bubble the country's trade deficit grew, the September 11 attacks increased political uncertainties, and the dollar began to depreciate in 2001.:\u200a175\u200a:\u200a36\u201337\u200a:\u200a37\u200a:\u200a147\u200a:\u200a16\u201317\n\nEuropean Monetary System: 1979\nFollowing the Smithsonian Agreement, member states of the European Economic Community adopted a narrower currency band of 1.125% for exchange rates among their own currencies, creating a smaller scale fixed exchange rate system known as the snake in the tunnel. The snake proved unsustainable as it did not compel EEC countries to coordinate macroeconomic policies. In 1979, the European Monetary System (EMS) phased out the currency snake. The EMS featured two key components: the European Currency Unit (ECU), an artificial weighted average market basket of European Union members' currencies, and the Exchange Rate Mechanism (ERM), a procedure for managing exchange rate fluctuations in keeping with a calculated parity grid of currencies' par values.:\u200a130\u200a:\u200a42\u201344\u200a:\u200a185\u200a \nThe parity grid was derived from parities each participating country established for its currency with all other currencies in the system, denominated in terms of ECUs. The weights within the ECU changed in response to variances in the values of each currency in its basket. Under the ERM, if an exchange rate reached its upper or lower limit (within a 2.25% band), both nations in that currency pair were obligated to intervene collectively in the foreign exchange market and buy or sell the under- or overvalued currency as necessary to return the exchange rate to its par value according to the parity matrix. The requirement of cooperative market intervention marked a key difference from the Bretton Woods system. Similarly to Bretton Woods however, EMS members could impose capital controls and other monetary policy shifts on countries responsible for exchange rates approaching their bounds, as identified by a divergence indicator which measured deviations from the ECU's value.:\u200a496\u2013497\u200a:\u200a29\u201330\u200a The central exchange rates of the parity grid could be adjusted in exceptional circumstances, and were modified every eight months on average during the systems' initial four years of operation.:\u200a160\u200a During its twenty-year lifespan, these central rates were adjusted over 50 times.:\u200a7\n\nBirth of the World Trade Organization: 1994\nThe Uruguay Round of GATT multilateral trade negotiations took place from 1986 to 1994, with 123 nations becoming party to agreements achieved throughout the negotiations. Among the achievements were trade liberalization in agricultural goods and textiles, the General Agreement on Trade in Services, and agreements on intellectual property rights issues. The key manifestation of this round was the Marrakech Agreement signed in April 1994, which established the World Trade Organization (WTO). The WTO is a chartered multilateral trade organization, charged with continuing the GATT mandate to promote trade, govern trade relations, and prevent damaging trade practices or policies. It became operational in January 1995. Compared with its GATT secretariat predecessor, the WTO features an improved mechanism for settling trade disputes since the organization is membership-based and not dependent on consensus as in traditional trade negotiations. This function was designed to address prior weaknesses, whereby parties in dispute would invoke delays, obstruct negotiations, or fall back on weak enforcement.:\u200a181\u200a:\u200a459\u2013460\u200a:\u200a47\u200a In 1997, WTO members reached an agreement which committed to softer restrictions on commercial financial services, including banking services, securities trading, and insurance services. These commitments entered into force in March 1999, consisting of 70 governments accounting for approximately 95% of worldwide financial services.\n\nFinancial integration and systemic crises: 1980\u2013present\nFinancial integration among industrialized nations grew substantially during the 1980s and 1990s, as did liberalization of their capital accounts.:\u200a15\u200a Integration among financial markets and banks rendered benefits such as greater productivity and the broad sharing of risk in the macroeconomy. The resulting interdependence also carried a substantive cost in terms of shared vulnerabilities and increased exposure to systemic risks.:\u200a440\u2013441\u200a Accompanying financial integration in recent decades was a succession of deregulation, in which countries increasingly abandoned regulations over the behavior of financial intermediaries and simplified requirements of disclosure to the public and to regulatory authorities.:\u200a36\u201337\u200a As economies became more open, nations became increasingly exposed to external shocks. Economists have argued greater worldwide financial integration has resulted in more volatile capital flows, thereby increasing the potential for financial market turbulence. Given greater integration among nations, a systemic crisis in one can easily infect others.:\u200a136\u2013137\u200a \nThe 1980s and 1990s saw a wave of currency crises and sovereign defaults, including the 1987 Black Monday stock market crashes, 1992 European Monetary System crisis, 1994 Mexican peso crisis, 1997 Asian financial crisis, 1998 Russian financial crisis, and the 1998\u20132002 Argentine great depression.:\u200a254\u200a:\u200a498\u200a:\u200a50\u201358\u200a:\u200a6\u20137\u200a:\u200a26\u201328\u200a These crises differed in terms of their breadth, causes, and aggravations, among which were capital flights brought about by speculative attacks on fixed exchange rate currencies perceived to be mispriced given a nation's fiscal policy,:\u200a83\u200a self-fulfilling speculative attacks by investors expecting other investors to follow suit given doubts about a nation's currency peg,:\u200a7\u200a lack of access to developed and functioning domestic capital markets in emerging market countries,:\u200a87\u200a and current account reversals during conditions of limited capital mobility and dysfunctional banking systems.:\u200a99\u200a\nFollowing research of systemic crises that plagued developing countries throughout the 1990s, economists have reached a consensus that liberalization of capital flows carries important prerequisites if these countries are to observe the benefits offered by financial globalization. Such conditions include stable macroeconomic policies, healthy fiscal policy, robust bank regulations, and strong legal protection of property rights. Economists largely favor adherence to an organized sequence of encouraging foreign direct investment, liberalizing domestic equity capital, and embracing capital outflows and short-term capital mobility only once the country has achieved functioning domestic capital markets and established a sound regulatory framework.:\u200a25\u200a:\u200a113\u200a An emerging market economy must develop a credible currency in the eyes of both domestic and international investors to realize benefits of globalization such as greater liquidity, greater savings at higher interest rates, and accelerated economic growth. If a country embraces unrestrained access to foreign capital markets without maintaining a credible currency, it becomes vulnerable to speculative capital flights and sudden stops, which carry serious economic and social costs.:\u200axii\u200a\nCountries sought to improve the sustainability and transparency of the global financial system in response to crises in the 1980s and 1990s. The Basel Committee on Banking Supervision was formed in 1974 by the G-10 members' central bank governors to facilitate cooperation on the supervision and regulation of banking practices. It is headquartered at the Bank for International Settlements in Basel, Switzerland. The committee has held several rounds of deliberation known collectively as the Basel Accords. The first of these accords, known as Basel I, took place in 1988 and emphasized credit risk and the assessment of different asset classes. Basel I was motivated by concerns over whether large multinational banks were appropriately regulated, stemming from observations during the 1980s Latin American debt crisis. Following Basel I, the committee published recommendations on new capital requirements for banks, which the G-10 nations implemented four years later. In 1999, the G-10 established the Financial Stability Forum (reconstituted by the G-20 in 2009 as the Financial Stability Board) to facilitate cooperation among regulatory agencies and promote stability in the global financial system. The Forum was charged with developing and codifying twelve international standards and implementation thereof.:\u200a222\u2013223\u200a:\u200a12\u200a \nThe Basel II accord was set in 2004 and again emphasized capital requirements as a safeguard against systemic risk as well as the need for global consistency in banking regulations so as not to competitively disadvantage banks operating internationally. It was motivated by what were seen as inadequacies of the first accord such as insufficient public disclosure of banks' risk profiles and oversight by regulatory bodies. Members were slow to implement it, with major efforts by the European Union and United States taking place as late as 2007 and 2008.:\u200a153\u200a:\u200a486\u2013488\u200a:\u200a160\u2013162\u200a In 2010, the Basel Committee revised the capital requirements in a set of enhancements to Basel II known as Basel III, which centered on a leverage ratio requirement aimed at restricting excessive leveraging by banks. In addition to strengthening the ratio, Basel III modified the formulas used to weight risk and compute the capital thresholds necessary to mitigate the risks of bank holdings, concluding the capital threshold should be set at 7% of the value of a bank's risk-weighted assets.:\u200a274\n\nBirth of the European Economic and Monetary Union 1992\nIn February 1992, European Union countries signed the Maastricht Treaty which outlined a three-stage plan to accelerate progress toward an Economic and Monetary Union (EMU). The first stage centered on liberalizing capital mobility and aligning macroeconomic policies between countries. The second stage established the European Monetary Institute which was ultimately dissolved in tandem with the establishment in 1998 of the European Central Bank (ECB) and European System of Central Banks. Key to the Maastricht Treaty was the outlining of convergence criteria that EU members would need to satisfy before being permitted to proceed. The third and final stage introduced a common currency for circulation known as the Euro, adopted by eleven of then-fifteen members of the European Union in January 1999. In doing so, they disaggregated their sovereignty in matters of monetary policy. These countries continued to circulate their national legal tenders, exchangeable for euros at fixed rates, until 2002 when the ECB began issuing official Euro coins and notes. As of 2011, the EMU comprises 17 nations which have issued the Euro, and 11 non-Euro states.:\u200a473\u2013474\u200a:\u200a45\u20134\u200a:\u200a7\u200a:\u200a185\u2013186\n\n2007\u20132008 financial crisis\nFollowing the market turbulence of the 1990s financial crises and September 11 attacks on the U.S. in 2001, financial integration intensified among developed nations and emerging markets, with substantial growth in capital flows among banks and in the trading of financial derivatives and structured finance products. Worldwide international capital flows grew from $3 trillion to $11 trillion U.S. dollars from 2002 to 2007, primarily in the form of short-term money market instruments. The United States experienced growth in the size and complexity of firms engaged in a broad range of financial services across borders in the wake of the Gramm\u2013Leach\u2013Bliley Act of 1999 which repealed the Glass\u2013Steagall Act of 1933, ending limitations on commercial banks' investment banking activity. Industrialized nations began relying more on foreign capital to finance domestic investment opportunities, resulting in unprecedented capital flows to advanced economies from developing countries, as reflected by global imbalances which grew to 6% of gross world product in 2007 from 3% in 2001.:\u200a19\u200a:\u200a129\u2013130\u200a\nThe 2007\u20132008 financial crisis shared some of the key features exhibited by the wave of international financial crises in the 1990s, including accelerated capital influxes, weak regulatory frameworks, relaxed monetary policies, herd behavior during investment bubbles, collapsing asset prices, and massive deleveraging. The systemic problems originated in the United States and other advanced nations.:\u200a133\u2013134\u200a Similarly to the 1997 Asian crisis, the global crisis entailed broad lending by banks undertaking unproductive real estate investments as well as poor standards of corporate governance within financial intermediaries. Particularly in the United States, the crisis was characterized by growing securitization of non-performing assets, large fiscal deficits, and excessive financing in the housing sector.:\u200a18\u201320\u200a:\u200a21\u201322\u200a While the real estate bubble in the U.S. triggered the 2007\u20132008 financial crisis, the bubble was financed by foreign capital flowing from many countries. As its contagious effects began infecting other nations, the crisis became a precursor for the Great Recession. In the wake of the crisis, total volume of world trade in goods and services fell 10% from 2008 to 2009 and did not recover until 2011, with an increased concentration in emerging market countries. The 2007\u20132008 financial crisis demonstrated the negative effects of worldwide financial integration, sparking discourse on how and whether some countries should decouple themselves from the system altogether.:\u200a3\n\nEurozone crisis\nIn 2009, a newly elected government in Greece revealed the falsification of its national budget data, and that its fiscal deficit for the year was 12.7% of GDP as opposed to the 3.7% espoused by the previous administration. This news alerted markets to the fact that Greece's deficit exceeded the eurozone's maximum of 3% outlined in the Economic and Monetary Union's Stability and Growth Pact. Investors concerned about a possible sovereign default rapidly sold Greek bonds. Given Greece's prior decision to embrace the euro as its currency, it no longer held monetary policy autonomy and could not intervene to depreciate a national currency to absorb the shock and boost competitiveness, as was the traditional solution to sudden capital flight. The crisis proved contagious when it spread to Portugal, Italy, and Spain (together with Greece these are collectively referred to as the PIGS). Ratings agencies downgraded these countries' debt instruments in 2010 which further increased the costliness of refinancing or repaying their national debts. The crisis continued to spread and soon grew into a European sovereign debt crisis which threatened economic recovery in the wake of the Great Recession. In tandem with the IMF, the European Union members assembled a \u20ac750 billion bailout for Greece and other afflicted nations. Additionally, the ECB pledged to purchase bonds from troubled eurozone nations in an effort to mitigate the risk of a banking system panic. The crisis is recognized by economists as highlighting the depth of financial integration in Europe, contrasted with the lack of fiscal integration and political unification necessary to prevent or decisively respond to crises. During the initial waves of the crisis, the public speculated that the turmoil could result in a disintegration of the eurozone and an abandonment of the euro. German Federal Minister of Finance Wolfgang Sch\u00e4uble called for the expulsion of offending countries from the eurozone. Now commonly referred to as the Eurozone crisis, it has been ongoing since 2009 and most recently began encompassing the 2012\u20132013 Cypriot financial crisis.:\u200a12\u201314\u200a:\u200a579\u2013581\n\nImplications of globalized capital\nBalance of payments\nThe balance of payments accounts summarize payments made to or received from foreign countries. Receipts are considered credit transactions while payments are considered debit transactions. The balance of payments is a function of three components: transactions involving export or import of goods and services form the current account, transactions involving purchase or sale of financial assets form the financial account, and transactions involving unconventional transfers of wealth form the capital account.:\u200a306\u2013307\u200a The current account summarizes three variables: the trade balance, net factor income from abroad, and net unilateral transfers. The financial account summarizes the value of exports versus imports of assets, and the capital account summarizes the value of asset transfers received net of transfers given. The capital account also includes the official reserve account, which summarizes central banks' purchases and sales of domestic currency, foreign exchange, gold, and SDRs for purposes of maintaining or utilizing bank reserves.:\u200a66\u201371\u200a:\u200a169\u2013172\u200a:\u200a32\u201335\u200a\nBecause the balance of payments sums to zero, a current account surplus indicates a deficit in the asset accounts and vice versa. A current account surplus or deficit indicates the extent to which a country is relying on foreign capital to finance its consumption and investments, and whether it is living beyond its means. For example, assuming a capital account balance of zero (thus no asset transfers available for financing), a current account deficit of \u00a31 billion implies a financial account surplus (or net asset exports) of \u00a31 billion. A net exporter of financial assets is known as a borrower, exchanging future payments for current consumption. Further, a net export of financial assets indicates growth in a country's debt. From this perspective, the balance of payments links a nation's income to its spending by indicating the degree to which current account imbalances are financed with domestic or foreign financial capital, which illuminates how a nation's wealth is shaped over time.:\u200a73\u200a:\u200a308\u2013313\u200a:\u200a203\u200a A healthy balance of payments position is important for economic growth. If countries experiencing a growth in demand have trouble sustaining a healthy balance of payments, demand can slow, leading to: unused or excess supply, discouraged foreign investment, and less attractive exports which can further reinforce a negative cycle that intensifies payments imbalances.:\u200a21\u201322\u200a\nA country's external wealth is measured by the value of its foreign assets net of its foreign liabilities. A current account surplus (and corresponding financial account deficit) indicates an increase in external wealth while a deficit indicates a decrease. Aside from current account indications of whether a country is a net buyer or net seller of assets, shifts in a nation's external wealth are influenced by capital gains and capital losses on foreign investments. Having positive external wealth means a country is a net lender (or creditor) in the world economy, while negative external wealth indicates a net borrower (or debtor).:\u200a13,\u200a210\n\nUnique financial risks\nNations and international businesses face an array of financial risks unique to foreign investment activity. Political risk is the potential for losses from a foreign country's political instability or otherwise unfavorable developments, which manifests in different forms. Transfer risk emphasizes uncertainties surrounding a country's capital controls and balance of payments. Operational risk characterizes concerns over a country's regulatory policies and their impact on normal business operations. Control risk is born from uncertainties surrounding property and decision rights in the local operation of foreign direct investments.:\u200a422\u200a Credit risk implies lenders may face an absent or unfavorable regulatory framework that affords little or no legal protection of foreign investments. For example, foreign governments may commit to a sovereign default or otherwise repudiate their debt obligations to international investors without any legal consequence or recourse. Governments may decide to expropriate or nationalize foreign-held assets or enact contrived policy changes following an investor's decision to acquire assets in the host country.:\u200a14\u201317\u200a Country risk encompasses both political risk and credit risk, and represents the potential for unanticipated developments in a host country to threaten its capacity for debt repayment and repatriation of gains from interest and dividends.:\u200a425,\u200a526\u200a:\u200a216\n\nParticipants\nEconomic actors\nEach of the core economic functions, consumption, production, and investment, have become highly globalized in recent decades. While consumers increasingly import foreign goods or purchase domestic goods produced with foreign inputs, businesses continue to expand production internationally to meet an increasingly globalized consumption in the world economy. International financial integration among nations has afforded investors the opportunity to diversify their asset portfolios by investing abroad.:\u200a4\u20135\u200a Consumers, multinational corporations, individual and institutional investors, and financial intermediaries (such as banks) are the key economic actors within the global financial system. Central banks (such as the European Central Bank or the U.S. Federal Reserve System) undertake open market operations in their efforts to realize monetary policy goals.:\u200a13\u201315\u200a:\u200a11\u201313,\u200a76\u200a International financial institutions such as the Bretton Woods institutions, multilateral development banks and other development finance institutions provide emergency financing to countries in crisis, provide risk mitigation tools to prospective foreign investors, and assemble capital for development finance and poverty reduction initiatives.:\u200a243\u200a Trade organizations such as the World Trade Organization, Institute of International Finance, and the World Federation of Exchanges attempt to ease trade, facilitate trade disputes and address economic affairs, promote standards, and sponsor research and statistics publications.\n\nRegulatory bodies\nExplicit goals of financial regulation include countries' pursuits of financial stability and the safeguarding of unsophisticated market players from fraudulent activity, while implicit goals include offering viable and competitive financial environments to world investors.:\u200a57\u200a A single nation with functioning governance, financial regulations, deposit insurance, emergency financing through discount windows, standard accounting practices, and established legal and disclosure procedures, can itself develop and grow a healthy domestic financial system. In a global context however, no central political authority exists which can extend these arrangements globally. Rather, governments have cooperated to establish a host of institutions and practices that have evolved over time and are referred to collectively as the international financial architecture.:\u200axviii\u200a:\u200a2\u200a Within this architecture, regulatory authorities such as national governments and intergovernmental organizations have the capacity to influence international financial markets. National governments may employ their finance ministries, treasuries, and regulatory agencies to impose tariffs and foreign capital controls or may use their central banks to execute a desired intervention in the open markets.:\u200a17\u201321\u200a\nSome degree of self-regulation occurs whereby banks and other financial institutions attempt to operate within guidelines set and published by multilateral organizations such as the International Monetary Fund or the Bank for International Settlements (particularly the Basel Committee on Banking Supervision and the Committee on the Global Financial System).:\u200a33\u201334\u200a Further examples of international regulatory bodies are: the Financial Stability Board (FSB) established to coordinate information and activities among developed countries; the International Organization of Securities Commissions (IOSCO) which coordinates the regulation of financial securities; the International Association of Insurance Supervisors (IAIS) which promotes consistent insurance industry supervision; the Financial Action Task Force on Money Laundering which facilitates collaboration in battling money laundering and terrorism financing; and the International Accounting Standards Board (IASB) which publishes accounting and auditing standards. Public and private arrangements exist to assist and guide countries struggling with sovereign debt payments, such as the Paris Club and London Club.:\u200a22\u200a:\u200a10\u201311\u200a National securities commissions and independent financial regulators maintain oversight of their industries' foreign exchange market activities.:\u200a61\u201364\u200a Two examples of supranational financial regulators in Europe are the European Banking Authority (EBA) which identifies systemic risks and institutional weaknesses and may overrule national regulators, and the European Shadow Financial Regulatory Committee (ESFRC) which reviews financial regulatory issues and publishes policy recommendations.\n\nResearch organizations and other fora\nResearch and academic institutions, professional associations, and think-tanks aim to observe, model, understand, and publish recommendations to improve the transparency and effectiveness of the global financial system. For example, the independent non-partisan World Economic Forum facilitates the Global Agenda Council on the Global Financial System and Global Agenda Council on the International Monetary System, which report on systemic risks and assemble policy recommendations. The Global Financial Markets Association facilitates discussion of global financial issues among members of various professional associations around the world. The Group of Thirty (G30) formed in 1978 as a private, international group of consultants, researchers, and representatives committed to advancing understanding of international economics and global finance.\n\nFuture of the global financial system\nThe IMF has reported that the global financial system is on a path to improved financial stability, but faces a host of transitional challenges borne out by regional vulnerabilities and policy regimes. One challenge is managing the United States' disengagement from its accommodative monetary policy. Doing so in an elegant, orderly manner could be difficult as markets adjust to reflect investors' expectations of a new monetary regime with higher interest rates. Interest rates could rise too sharply if exacerbated by a structural decline in market liquidity from higher interest rates and greater volatility, or by structural deleveraging in short-term securities and in the shadow banking system (particularly the mortgage market and real estate investment trusts). Other central banks are contemplating ways to exit unconventional monetary policies employed in recent years. Some nations however, such as Japan, are attempting stimulus programs at larger scales to combat deflationary pressures. The Eurozone's nations implemented myriad national reforms aimed at strengthening the monetary union and alleviating stress on banks and governments. Yet some European nations such as Portugal, Italy, and Spain continue to struggle with heavily leveraged corporate sectors and fragmented financial markets in which investors face pricing inefficiency and difficulty identifying quality assets. Banks operating in such environments may need stronger provisions in place to withstand corresponding market adjustments and absorb potential losses. Emerging market economies face challenges to greater stability as bond markets indicate heightened sensitivity to monetary easing from external investors flooding into domestic markets, rendering exposure to potential capital flights brought on by heavy corporate leveraging in expansionary credit environments. Policymakers in these economies are tasked with transitioning to more sustainable and balanced financial sectors while still fostering market growth so as not to provoke investor withdrawal.:\u200axi\u2013xiii\u200a\nThe 2007\u20132008 financial crisis and the Great Recession prompted renewed discourse on the architecture of the global financial system. These events called to attention financial integration, inadequacies of global governance, and the emergent systemic risks of financial globalization.:\u200a2\u20139\u200a Since the establishment in 1945 of a formal international monetary system with the IMF empowered as its guardian, the world has undergone extensive changes politically and economically. This has fundamentally altered the paradigm in which international financial institutions operate, increasing the complexities of the IMF and World Bank's mandates.:\u200a1\u20132\u200a The lack of adherence to a formal monetary system has created a void of global constraints on national macroeconomic policies and a deficit of rule-based governance of financial activities.:\u200a4\u200a French economist and Executive Director of the World Economic Forum's Reinventing Bretton Woods Committee, Marc Uzan, has pointed out that some radical proposals such as a \"global central bank or a world financial authority\" have been deemed impractical, leading to further consideration of medium-term efforts to improve transparency and disclosure, strengthen emerging market financial climates, bolster prudential regulatory environments in advanced nations, and better moderate capital account liberalization and exchange rate regime selection in emerging markets. He has also drawn attention to calls for increased participation from the private sector in the management of financial crises and the augmenting of multilateral institutions' resources.:\u200a1\u20132\u200a\nThe Council on Foreign Relations' assessment of global finance notes that excessive institutions with overlapping directives and limited scopes of authority, coupled with difficulty aligning national interests with international reforms, are the two key weaknesses inhibiting global financial reform. Nations do not presently enjoy a comprehensive structure for macroeconomic policy coordination, and global savings imbalances have abounded before and after the 2007\u20132008 financial crisis to the extent that the United States' status as the steward of the world's reserve currency was called into question. Post-crisis efforts to pursue macroeconomic policies aimed at stabilizing foreign exchange markets have yet to be institutionalized. The lack of international consensus on how best to monitor and govern banking and investment activity threatens the world's ability to prevent future financial crises. The slow and often delayed implementation of banking regulations that meet Basel III criteria means most of the standards will not take effect until 2019, rendering continued exposure of global finance to unregulated systemic risks. Despite Basel III and other efforts by the G20 to bolster the Financial Stability Board's capacity to facilitate cooperation and stabilizing regulatory changes, regulation exists predominantly at the national and regional levels.\n\nReform efforts\nFormer World Bank Chief Economist and former chairman of the U.S. Council of Economic Advisers Joseph E. Stiglitz referred in the late 1990s to a growing consensus that something is wrong with a system having the capacity to impose high costs on a great number of people who are hardly even participants in international financial markets, neither speculating on international investments nor borrowing in foreign currencies. He argued that foreign crises have strong worldwide repercussions due in part to the phenomenon of moral hazard, particularly when many multinational firms deliberately invest in highly risky government bonds in anticipation of a national or international bailout. Although crises can be overcome by emergency financing, employing bailouts places a heavy burden on taxpayers living in the afflicted countries, and the high costs damage standards of living. Stiglitz has advocated finding means of stabilizing short-term international capital flows without adversely affecting long-term foreign direct investment which usually carries new knowledge spillover and technological advancements into economies.\nAmerican economist and former chairman of the Federal Reserve Paul Volcker has argued that the lack of global consensus on key issues threatens efforts to reform the global financial system. He has argued that quite possibly the most important issue is a unified approach to addressing failures of systemically important financial institutions, noting public taxpayers and government officials have grown disillusioned with deploying tax revenues to bail out creditors for the sake of stopping contagion and mitigating economic disaster. Volcker has expressed an array of potential coordinated measures: increased policy surveillance by the IMF and commitment from nations to adopt agreed-upon best practices, mandatory consultation from multilateral bodies leading to more direct policy recommendations, stricter controls on national qualification for emergency financing facilities (such as those offered by the IMF or by central banks), and improved incentive structures with financial penalties.\nGovernor of the Bank of England and former governor of the Bank of Canada Mark Carney has described two approaches to global financial reform: shielding financial institutions from cyclic economic effects by strengthening banks individually, and defending economic cycles from banks by improving systemic resiliency. Strengthening financial institutions necessitates stronger capital requirements and liquidity provisions, as well as better measurement and management of risks. The G-20 agreed to new standards presented by the Basel Committee on Banking Supervision at its 2009 summit in Pittsburgh, Pennsylvania. The standards included leverage ratio targets to supplement other capital adequacy requirements established by Basel II. Improving the resiliency of the global financial system requires protections that enable the system to withstand singular institutional and market failures. Carney has argued that policymakers have converged on the view that institutions must bear the burden of financial losses during future financial crises, and such occurrences should be well-defined and pre-planned. He suggested other national regulators follow Canada in establishing staged intervention procedures and require banks to commit to what he termed \"living wills\" which would detail plans for an orderly institutional failure.\nAt its 2010 summit in Seoul, South Korea, the G-20 collectively endorsed a new collection of capital adequacy and liquidity standards for banks recommended by Basel III. Andreas Dombret of the Executive Board of Deutsche Bundesbank has noted a difficulty in identifying institutions that constitute systemic importance via their size, complexity, and degree of interconnectivity within the global financial system, and that efforts should be made to identify a group of 25 to 30 indisputable globally systemic institutions. He has suggested they be held to standards higher than those mandated by Basel III, and that despite the inevitability of institutional failures, such failures should not drag with them the financial systems in which they participate. Dombret has advocated for regulatory reform that extends beyond banking regulations and has argued in favor of greater transparency through increased public disclosure and increased regulation of the shadow banking system.\nPresident of the Federal Reserve Bank of New York and Vice Chairman of the Federal Open Market Committee William C. Dudley has argued that a global financial system regulated on a largely national basis is untenable for supporting a world economy with global financial firms. In 2011, he advocated five pathways to improving the safety and security of the global financial system: a special capital requirement for financial institutions deemed systemically important; a level playing field which discourages exploitation of disparate regulatory environments and beggar thy neighbour policies that serve \"national constituencies at the expense of global financial stability\"; superior cooperation among regional and national regulatory regimes with broader protocols for sharing information such as records for the trade of over-the-counter financial derivatives; improved delineation of \"the responsibilities of the home versus the host country\" when banks encounter trouble; and well-defined procedures for managing emergency liquidity solutions across borders including which parties are responsible for the risk, terms, and funding of such measures.\n\nSee also\nOutline of finance\nCommittee on the Global Financial System\nGlobal Financial Markets Association\n\nReferences\nFurther reading\nJames, Paul; Patom\u00e4ki, Heikki (2007). Globalization and Economy, Vol. 2: Global Finance and the New Global Economy. London: Sage Publications.\nReinhart, Carmen; Rogoff, Kenneth (2009). This Time Is Different: Eight Centuries of Financial Folly. Princeton University Press. ISBN 978-0-691-15264-6.", "url": "https://en.wikipedia.org/wiki/Global_financial_system"}, {"title": "Financial crisis", "text": "A financial crisis is any of a broad variety of situations in which some financial assets suddenly lose a large part of their nominal value. In the 19th and early 20th centuries, many financial crises were associated with banking panics, and many recessions coincided with these panics. Other situations that are often called financial crises include stock market crashes and the bursting of other financial bubbles, currency crises, and sovereign defaults. Financial crises directly result in a loss of paper wealth but do not necessarily result in significant changes in the real economy (for example, the crisis resulting from the famous tulip mania bubble in the 17th century).\nMany economists have offered theories about how financial crises develop and how they could be prevented. There is little consensus and financial crises continue to occur from time to time. It is apparent however that a consistent feature of both economic (and other applied finance disciplines) is the obvious inability to predict and avert financial crises. This realization raises the question as to what is known and also capable of being known (i.e. the epistemology) within economics and applied finance.  It has been argued that the assumptions of unique, well-defined causal chains being present in economic thinking, models and data, could, in part, explain why financial crises are often inherent and unavoidable.\n\nTypes\nBanking crisis\nWhen a bank suffers a sudden rush of withdrawals by depositors, this is called a bank run. Since banks lend out most of the cash they receive in deposits (see fractional-reserve banking), it is difficult for them to quickly pay back all deposits if these are suddenly demanded, so a run renders the bank insolvent, causing customers to lose their deposits, to the extent that they are not covered by deposit insurance. An event in which bank runs are widespread is called a systemic banking crisis or banking panic.\nExamples of bank runs include the run on the Bank of the United States in 1931 and the run on Northern Rock in 2007. Banking crises generally occur after periods of risky lending and resulting loan defaults.\n\nCurrency crisis\nA currency crisis, also called a devaluation crisis, is normally considered as part of a financial crisis. Kaminsky et al. (1998), for instance, define currency crises as occurring when a weighted average of monthly percentage depreciations in the exchange rate and monthly percentage declines in exchange reserves exceeds its mean by more than three standard deviations. Frankel and Rose (1996) define a currency crisis as a nominal depreciation of a currency of at least 25% but it is also defined as at least a 10% increase in the rate of depreciation. In general, a currency crisis can be defined as a situation when the participants in an exchange market come to recognize that a pegged exchange rate is about to fail, causing speculation against the peg that hastens the failure and forces a devaluation.\n\nSpeculative bubbles and crashes\nA speculative bubble (also called a financial bubble or an economic bubble) exists in the event of large, sustained overpricing of some class of assets. One factor that frequently contributes to a bubble is the presence of buyers who purchase an asset based solely on the expectation that they can later resell it at a higher price, rather than calculating the income it will generate in the future. If there is a bubble, there is also a risk of a crash in asset prices: market participants will go on buying only as long as they expect others to buy, and when many decide to sell the price will fall. However, it is difficult to predict whether an asset's price actually equals its fundamental value, so it is hard to detect bubbles reliably. Some economists insist that bubbles never or almost never occur.\n\nWell-known examples of bubbles (or purported bubbles) and crashes in stock prices and other asset prices include the 17th century Dutch tulip mania, the 18th century South Sea Bubble, the Wall Street Crash of 1929, the Japanese property bubble of the 1980s, and the crash of the United States housing bubble during 2006\u20132008. The 2000s sparked a real estate bubble where housing prices were increasing significantly as an asset good.\n\nInternational financial crisis\nWhen a country that maintains a fixed exchange rate is suddenly forced to devalue its currency due to accruing an unsustainable current account deficit, this is called a currency crisis or balance of payments crisis. When a country fails to pay back its sovereign debt, this is called a sovereign default. While devaluation and default could both be voluntary decisions of the government, they are often perceived to be the involuntary results of a change in investor sentiment that leads to a sudden stop in capital inflows or a sudden increase in capital flight.\nSeveral currencies that formed part of the European Exchange Rate Mechanism suffered crises in 1992\u201393 and were forced to devalue or withdraw from the mechanism. Another round of currency crises took place in Asia in 1997\u201398. Many Latin American countries defaulted on their debt in the early 1980s. The 1998 Russian financial crisis resulted in a devaluation of the ruble and default on Russian government bonds.\n\nWider economic crisis\nNegative GDP growth lasting two or more quarters is called a recession. An especially prolonged or severe recession may be called a depression, while a long period of slow but not necessarily negative growth is sometimes called economic stagnation.\n\nSome economists argue that many recessions have been caused in large part by financial crises. One important example is the Great Depression, which was preceded in many countries by bank runs and stock market crashes. The subprime mortgage crisis and the bursting of other real estate bubbles around the world also led to recession in the U.S. and a number of other countries in late 2008 and 2009.\nSome economists argue that financial crises are caused by recessions instead of the other way around, and that even where a financial crisis is the initial shock that sets off a recession, other factors may be more important in prolonging the recession. In particular, Milton Friedman and Anna Schwartz argued that the initial economic decline associated with the crash of 1929 and the bank panics of the 1930s would not have turned into a prolonged depression if it had not been reinforced by monetary policy mistakes on the part of the Federal Reserve, a position supported by Ben Bernanke.\n\nCauses and consequences\nStrategic complementarities in financial markets\nIt is often observed that successful investment requires each investor in a financial market to guess what other investors will do. Reflexivity refers to the circular relationships often evident in social systems between cause and effect - and relates to the property of self-referencing in financial markets. George Soros has been a proponent of the reflexivity paradigm surrounding financial crises. Similarly, John Maynard Keynes compared financial markets to a beauty contest game in which each participant tries to predict which model other participants will consider most beautiful.\nFurthermore, in many cases, investors have incentives to coordinate their choices. For example, someone who thinks other investors want to heavily buy Japanese yen may expect the yen to rise in value, and therefore has an incentive to buy yen, too. Likewise, a depositor in IndyMac Bank who expects other depositors to withdraw their funds may expect the bank to fail, and therefore has an incentive to withdraw, too. Economists call an incentive to mimic the strategies of others strategic complementarity.\nIt has been argued that if people or firms have a sufficiently strong incentive to do the same thing they expect others to do, then self-fulfilling prophecies may occur. For example, if investors expect the value of the yen to rise, this may cause its value to rise; if depositors expect a bank to fail this may cause it to fail. Therefore, financial crises are sometimes viewed as a vicious circle in which investors shun some institution or asset because they expect others to do so.  Reflexivity poses a challenge to the epistemic norms typically assumed within financial economics and all of empirical finance. The possibility of financial crises being beyond the predictive reach of causality is discussed further within Epistemology of finance.\n\nLeverage\nLeverage, which means borrowing to finance investments, is frequently cited as a contributor to financial crises. When a financial institution (or an individual) only invests its own money, it can, in the very worst case, lose its own money. But when it borrows in order to invest more, it can potentially earn more from its investment, but it can also lose more than all it has. Therefore, leverage magnifies the potential returns from investment, but also creates a risk of bankruptcy. Since bankruptcy means that a firm fails to honor all its promised payments to other firms, it may spread financial troubles from one firm to another (see 'Contagion' below).\nFor example, borrowing to finance investment in the stock market (\"margin buying\") became increasingly common prior to the Wall Street Crash of 1929.\n\nAsset-liability mismatch\nAnother factor believed to contribute to financial crises is asset-liability mismatch, a situation in which the risks associated with an institution's debts and assets are not appropriately aligned. For example, commercial banks offer deposit accounts that can be withdrawn at any time, and they use the proceeds to make long-term loans to businesses and homeowners. The mismatch between the banks' short-term liabilities (its deposits) and its long-term assets (its loans) is seen as one of the reasons bank runs occur (when depositors panic and decide to withdraw their funds more quickly than the bank can get back the proceeds of its loans). Likewise, Bear Stearns failed in 2007\u201308 because it was unable to renew the short-term debt it used to finance long-term investments in mortgage securities.\nIn an international context, many emerging market governments are unable to sell bonds denominated in their own currencies, and therefore sell bonds denominated in US dollars instead. This generates a mismatch between the currency denomination of their liabilities (their bonds) and their assets (their local tax revenues), so that they run a risk of sovereign default due to fluctuations in exchange rates.\n\nUncertainty and herd behavior\nMany analyses of financial crises emphasize the role of investment mistakes caused by lack of knowledge or the imperfections of human reasoning. Behavioural finance studies errors in economic and quantitative reasoning. Psychologist Torbjorn K A Eliazon has also analyzed failures of economic reasoning in his concept of '\u0153copathy'.\nHistorians, notably Charles P. Kindleberger, have pointed out that crises often follow soon after major financial or technical innovations that present investors with new types of financial opportunities, which he called \"displacements\" of investors' expectations. Early examples include the South Sea Bubble and Mississippi Bubble of 1720, which occurred when the notion of investment in shares of company stock was itself new and unfamiliar, and the Crash of 1929, which followed the introduction of new electrical and transportation technologies. More recently, many financial crises followed changes in the investment environment brought about by financial deregulation, and the crash of the dot com bubble in 2001 arguably began with \"irrational exuberance\" about Internet technology.\nUnfamiliarity with recent technical and financial innovations may help explain how investors sometimes grossly overestimate asset values. Also, if the first investors in a new class of assets (for example, stock in \"dot com\" companies) profit from rising asset values as other investors learn about the innovation (in our example, as others learn about the potential of the Internet), then still more others may follow their example, driving the price even higher as they rush to buy in hopes of similar profits. If such \"herd behaviour\" causes prices to spiral up far above the true value of the assets, a crash may become inevitable. If for any reason the price briefly falls, so that investors realize that further gains are not assured, then the spiral may go into reverse, with price decreases causing a rush of sales, reinforcing the decrease in prices.\n\nRegulatory failures\nGovernments have attempted to eliminate or mitigate financial crises by regulating the financial sector. One major goal of regulation is transparency: making institutions' financial situations publicly known by requiring regular reporting under standardized accounting procedures. Another goal of regulation is making sure institutions have sufficient assets to meet their contractual obligations, through reserve requirements, capital requirements, and other limits on leverage.\nSome financial crises have been blamed on insufficient regulation, and have led to changes in regulation in order to avoid a repeat. For example, the former Managing Director of the International Monetary Fund, Dominique Strauss-Kahn, has blamed the financial crisis of 2007\u20132008 on 'regulatory failure to guard against excessive risk-taking in the financial system, especially in the US'. Likewise, the New York Times singled out the deregulation of credit default swaps as a cause of the crisis.\nHowever, excessive regulation has also been cited as a possible cause of financial crises. In particular, the Basel II Accord has been criticized for requiring banks to increase their capital when risks rise, which might cause them to decrease lending precisely when capital is scarce, potentially aggravating a financial crisis.\nInternational regulatory convergence has been interpreted in terms of regulatory herding, deepening market herding (discussed above) and so increasing systemic risk. From this perspective, maintaining diverse regulatory regimes would be a safeguard.\nFraud has played a role in the collapse of some financial institutions, when companies have attracted depositors with misleading claims about their investment strategies, or have embezzled the resulting income. Examples include Charles Ponzi's scam in early 20th century Boston, the collapse of the MMM investment fund in Russia in 1994, the scams that led to the Albanian Lottery Uprising of 1997, and the collapse of Madoff Investment Securities in 2008.\nMany rogue traders that have caused large losses at financial institutions have been accused of acting fraudulently in order to hide their trades. Fraud in mortgage financing has also been cited as one possible cause of the 2008 subprime mortgage crisis; government officials stated on 23 September 2008 that the FBI was looking into possible fraud by mortgage financing companies Fannie Mae and Freddie Mac, Lehman Brothers, and insurer American International Group. Likewise it has been argued that many financial companies failed in the recent crisis because their managers failed to carry out their fiduciary duties.\n\nContagion\nContagion refers to the idea that financial crises may spread from one institution to another, as when a bank run spreads from a few banks to many others, or from one country to another, as when currency crises, sovereign defaults, or stock market crashes spread across countries. When the failure of one particular financial institution threatens the stability of many other institutions, this is called systemic risk.\nOne widely cited example of contagion was the spread of the Thai crisis in 1997 to other countries like South Korea. However, economists often debate whether observing crises in many countries around the same time is truly caused by contagion from one market to another, or whether it is instead caused by similar underlying problems that would have affected each country individually even in the absence of international linkages.\n\nInterest rate disparity and capital flows\nThe nineteenth century Banking School theory of crises suggested that crises were caused by flows of investment capital between areas with different rates of interest. Capital could be borrowed in areas with low interest rates and invested in areas of high interest. Using this method a small profit could be made with little or no capital. However, when interest rates changed and the incentive for the flow was removed or reversed sudden changes in capital flows could occur. The subjects of investment might be starved of cash possibly becoming insolvent and creating a credit crunch and the loaning banks would be left with defaulting investors leading to a banking crisis. As Charles Read has pointed out, the modern equivalent of this process involves the Carry Trade, see Carry (investment).\n\nRecessionary effects\nSome financial crises have little effect outside of the financial sector, like the Wall Street crash of 1987, but other crises are believed to have played a role in decreasing growth in the rest of the economy. There are many theories why a financial crisis could have a recessionary effect on the rest of the economy. These theoretical ideas include the 'financial accelerator', 'flight to quality' and 'flight to liquidity', and the Kiyotaki-Moore model. Some 'third generation' models of currency crises explore how currency crises and banking crises together can cause recessions.\n\nTheories\nAustrian theories\nAustrian School economists Ludwig von Mises and Friedrich Hayek discussed the business cycle starting with Mises' Theory of Money and Credit, published in 1912.\n\nMarxist theories\nRecurrent major depressions in the world economy at the pace of 20 and 50 years have been the subject of studies since Jean Charles L\u00e9onard de Sismondi (1773\u20131842) provided the first theory of crisis in a critique of classical political economy's assumption of equilibrium between supply and demand. Developing an economic crisis theory became the central recurring concept throughout Karl Marx's mature work. Marx's law of the tendency for the rate of profit to fall borrowed many features of the presentation of John Stuart Mill's discussion Of the Tendency of Profits to a Minimum (Principles of Political Economy Book IV Chapter IV). The theory is a corollary of the Tendency towards the Centralization of Profits.\nIn a capitalist system, successfully-operating businesses return less money to their workers (in the form of wages) than the value of the goods produced by those workers (i.e. the amount of money the products are sold for). This profit first goes towards covering the initial investment in the business. In the long-run, however, when one considers the combined economic activity of all successfully-operating business, it is clear that less money (in the form of wages) is being returned to the mass of the population (the workers) than is available to them to buy all of these goods being produced. Furthermore, the expansion of businesses in the process of competing for markets leads to an abundance of goods and a general fall in their prices, further exacerbating the tendency for the rate of profit to fall.\nThe viability of this theory depends upon two main factors: firstly, the degree to which profit is taxed by government and returned to the mass of people in the form of welfare, family benefits and health and education spending; and secondly, the proportion of the population who are workers rather than investors/business owners. Given the extraordinary capital expenditure required to enter modern economic sectors like airline transport, the military industry, or chemical production, these sectors are extremely difficult for new businesses to enter and are being concentrated in fewer and fewer hands.\nEmpirical and econometric research continues especially in the world systems theory and in the debate about Nikolai Kondratiev and the so-called 50-years Kondratiev waves. Major figures of world systems theory, like Andre Gunder Frank and Immanuel Wallerstein, consistently warned about the crash that the world economy is now facing. World systems scholars and Kondratiev cycle researchers always implied that Washington Consensus oriented economists never understood the dangers and perils, which leading industrial nations will be facing and are now facing at the end of the long economic cycle which began after the oil crisis of 1973.\n\nMinsky's theory\nHyman Minsky has proposed a post-Keynesian explanation that is most applicable to a closed economy. He theorized that financial fragility is a typical feature of any capitalist economy. High fragility leads to a higher risk of a financial crisis. To facilitate his analysis, Minsky defines three approaches to financing firms may choose, according to their tolerance of risk. They are hedge finance, speculative finance, and Ponzi finance. Ponzi finance leads to the most fragility.\n\nfor hedge finance, income flows are expected to meet financial obligations in every period, including both the principal and the interest on loans.\nfor speculative finance, a firm must roll over debt because income flows are expected to only cover interest costs. None of the principal is paid off.\nfor Ponzi finance, expected income flows will not even cover interest cost, so the firm must borrow more or sell off assets simply to service its debt. The hope is that either the market value of assets or income will rise enough to pay off interest and principal.\nFinancial fragility levels move together with the business cycle. After a recession, firms have lost much financing and choose only hedge, the safest. As the economy grows and expected profits rise, firms tend to believe that they can allow themselves to take on speculative financing. In this case, they know that profits will not cover all the interest all the time. Firms, however, believe that profits will rise and the loans will eventually be repaid without much trouble. More loans lead to more investment, and the economy grows further. Then lenders also start believing that they will get back all the money they lend. Therefore, they are ready to lend to firms without full guarantees of success.\nLenders know that such firms will have problems repaying. Still, they believe these firms will refinance from elsewhere as their expected profits rise. This is Ponzi financing. In this way, the economy has taken on much risky credit. Now it is only a question of time before some big firm actually defaults. Lenders understand the actual risks in the economy and stop giving credit so easily. Refinancing becomes impossible for many, and more firms default. If no new money comes into the economy to allow the refinancing process, a real economic crisis begins. During the recession, firms start to hedge again, and the cycle is closed.\n\nBanking School theory of crises\nThe Banking School theory of crises describes a continuous cycle driven by varying interest rates. It is based on the work of Thomas Tooke, Thomas Attwood, Henry Thornton, William Jevons and a number of bankers opposed to the Bank Charter Act 1844.\nStarting at a time when short-term interest rates are low, frustration builds up among investors who search for a better yield in countries and locations with higher rates, leading to increased capital flows to countries with higher rates. Internally, short-term rates rise above long-term rates causing failures where borrowing at short term rates has been used to invest long-term where the funds cannot be liquidated quickly (a similar mechanism was implicated in the March 2023 failure of SVB Bank). Internationally, arbitrage and the need to stop capital flows, which caused bullion drains in the gold standard of the nineteenth century and drains of foreign capital later, bring interest rates in the low-rate country up to equal those in the country which is the subject of investment.\nThe capital flows reverse or cease suddenly causing the subject of investment to be starved of funds and the remaining investors (often those who are least knowledgeable) to be left with devalued assets. Bankruptcies, defaults and bank failures follow as rates are pushed high. After the crisis governments push short-term interest rates low again to diminish the cost of servicing government borrowing which has been used to overcome the crisis. Funds build up again looking for investment opportunities and the cycle restarts from the beginning.\n\nCoordination games\nMathematical approaches to modeling financial crises have emphasized that there is often positive feedback between market participants' decisions (see strategic complementarity). Positive feedback implies that there may be dramatic changes in asset values in response to small changes in economic fundamentals. For example, some models of currency crises (including that of Paul Krugman) imply that a fixed exchange rate may be stable for a long period of time, but will collapse suddenly in an avalanche of currency sales in response to a sufficient deterioration of government finances or underlying economic conditions.\nAccording to some theories, positive feedback implies that the economy can have more than one equilibrium. There may be an equilibrium in which market participants invest heavily in asset markets because they expect assets to be valuable. This is the type of argument underlying Diamond and Dybvig's model of bank runs, in which savers withdraw their assets from the bank because they expect others to withdraw too. Likewise, in Obstfeld's model of currency crises, when economic conditions are neither too bad nor too good, there are two possible outcomes: speculators may or may not decide to attack the currency depending on what they expect other speculators to do.\n\nHerding models and learning models\nA variety of models have been developed in which asset values may spiral excessively up or down as investors learn from each other. In these models, asset purchases by a few agents encourage others to buy too, not because the true value of the asset increases when many buy (which is called \"strategic complementarity\"), but because investors come to believe the true asset value is high when they observe others buying.\nIn \"herding\" models, it is assumed that investors are fully rational, but only have partial information about the economy. In these models, when a few investors buy some type of asset, this reveals that they have some positive information about that asset, which increases the rational incentive of others to buy the asset too. Even though this is a fully rational decision, it may sometimes lead to mistakenly high asset values (implying, eventually, a crash) since the first investors may, by chance, have been mistaken. Herding models, based on Complexity Science, indicate that it is the internal structure of the market, not external influences, which is primarily responsible for crashes.\nIn \"adaptive learning\" or \"adaptive expectations\" models, investors are assumed to be imperfectly rational, basing their reasoning only on recent experience. In such models, if the price of a given asset rises for some period of time, investors may begin to believe that its price always rises, which increases their tendency to buy and thus drives the price up further. Likewise, observing a few price decreases may give rise to a downward price spiral, so in models of this type, large fluctuations in asset prices may occur. Agent-based models of financial markets often assume investors act on the basis of adaptive learning or adaptive expectations.\n\nGlobal financial crisis\nAs the most recent and most damaging financial crisis event, the Global financial crisis, deserves special attention, as its causes, effects, response, and lessons are most applicable to the current financial system.\n\nHistory\nA noted survey of financial crises is This Time is Different: Eight Centuries of Financial Folly (Reinhart & Rogoff 2009), by economists Carmen Reinhart and Kenneth Rogoff, who are regarded as among the foremost historians of financial crises. In this survey, they trace the history of financial crisis back to sovereign defaults \u2013 default on public debt, \u2013 which were the form of crisis prior to the 18th century and continue, then and now causing private bank failures; crises since the 18th century feature both public debt default and private debt default. Reinhart and Rogoff also class debasement of currency and hyperinflation as being forms of financial crisis, broadly speaking, because they lead to unilateral reduction (repudiation) of debt.\n\nPrior to 19th century\nReinhart and Rogoff trace inflation (to reduce debt) to Dionysius I rule in Syracuse and begin their \"eight centuries\" in 1258; debasement of currency also occurred under the Roman Empire and Byzantine Empire. A financial crisis in 33 A.D. caused by a contraction of money supply had been recorded by several Roman historians.\nAmong the earliest crises Reinhart and Rogoff study is the 1340 default of England, due to setbacks in its war with France (the Hundred Years' War; see details). Further early sovereign defaults include seven defaults by the Spanish Empire, four under Philip II, three under his successors.\nOther global and national financial mania since the 17th century include:\n\n1637: Bursting of tulip mania in the Netherlands \u2013 while tulip mania is popularly reported as an example of a financial crisis, and was a speculative bubble, modern scholarship holds that its broader economic impact was limited to negligible, and that it did not precipitate a financial crisis.\n1720: Bursting of South Sea Bubble (Great Britain) and Mississippi Bubble (France) \u2013 earliest of modern financial crises; in both cases the company assumed the national debt of the country (80\u201385% in Great Britain, 100% in France), and thereupon the bubble burst. The resulting crisis of confidence probably had a deep impact on the financial and political development of France.\nCrisis of 1763 - started in Amsterdam, begun by the collapse of Johann Ernst Gotzkowsky and Leendert Pieter de Neufville's bank, spread to Germany and Scandinavia.\nCrisis of 1772 \u2013 in London and Amsterdam. 20 important banks in London went bankrupt after one banking house defaulted (bankers Neal, James, Fordyce and Down)\nFrance's Financial and Debt Crisis (1783\u20131788)- France severe financial crisis due to the immense debt accrued through the French involvement in the Seven Years' War (1756\u20131763) and the American Revolution (1775\u20131783).\nPanic of 1792 \u2013 run on banks in US precipitated by the expansion of credit by the newly formed Bank of the United States\nPanic of 1796\u20131797 \u2013 British and US credit crisis caused by land speculation bubble\n\n19th century\nDanish state bankruptcy of 1813\nFinancial Crisis of 1818 - in England caused banks to call in loans and curtail new lending, draining specie out of the U.S.\nPanic of 1819: pervasive USA economic recession w/ bank failures; culmination of U.S.'s 1st boom-to-bust economic cycle\nPanic of 1825: pervasive British economic recession in which many British banks failed, & Bank of England nearly failed\nPanic of 1837: pervasive USA economic recession w/ bank failures; a 5-year depression ensued\nPanic of 1847: a collapse of British financial markets associated with the end of the 1840s railway boom. Also see Bank Charter Act of 1844\nPanic of 1857: pervasive USA economic recession w/ bank failures. The world economy was also more interconnected by the 1850s, which also made the Panic of 1857 the first worldwide economic crisis.\nPanic of 1866: the Overend Gurney crisis (primarily British)\nBlack Friday (1869): aka Gold Panic of 1869\nPanic of 1873: pervasive USA economic recession w/ bank failures, known then as the 5 year Great Depression & now as the Long Depression\nPanic of 1884: a panic in the United States centred on New York banks\nPanic of 1890: aka Baring Crisis; near-failure of a major London bank led to corresponding South American financial crises\nPanic of 1893: a panic in the United States marked by the collapse of railroad overbuilding and shaky railroad financing which set off a series of bank failures\nAustralian banking crisis of 1893\nPanic of 1896: an acute economic depression in the United States precipitated by a drop in silver reserves and market concerns on the effects it would have on the gold standard\n\n20th century\nPanic of 1901: limited to crashing of the New York Stock Exchange\nPanic of 1907: pervasive USA economic recession w/ bank failures\nPanic of 1910\u20131911\n1910: Shanghai rubber stock market crisis\n1914: The Great Financial Crisis (see Aldrich-Vreeland Act)\nWall Street Crash of 1929, followed by the Great Depression: the largest and most important economic depression in the 20th century\n1937-1938: an economic downturn that occurred during the Great Depression.\n1973: 1973 oil crisis \u2013 oil prices soared, causing the 1973\u20131974 stock market crash\nSecondary banking crisis of 1973\u20131975: United Kingdom\n1980s: Latin American debt crisis \u2013 beginning in Mexico in 1982 with the Mexican Weekend\n1980s-1990: Savings and loan crisis\nBank stock crisis (Israel 1983)\n1987: Black Monday (1987) \u2013 the largest one-day percentage decline in stock market history\n1988\u20131992 Norwegian banking crisis\n1989\u20131991: United States Savings & Loan crisis\n1990: Japanese asset price bubble collapsed\nEarly 1990s: Scandinavian banking crisis, Swedish banking crisis, Finnish banking crisis of 1990s\nEarly 1990s recession\n1991:  1991 Indian economic crisis\n1992\u20131993: Black Wednesday \u2013 speculative attacks on currencies in the European Exchange Rate Mechanism\n1994\u20131995:  Economic crisis in Mexico \u2013 speculative attack and default on Mexican debt\n1997\u20131998: 1997 Asian Financial Crisis \u2013 devaluations and banking crises across Asia\n1998:  Russian financial crisis\n\n21st century\n2000\u20132001:  2001 Turkish economic crisis\n2000: Early 2000s recession\n1999\u20132002:  Argentine economic crisis (1999-2002)\n2001: Bursting of dot-com bubble\n2007\u20132008: Global financial crisis of 2007\u20132008\n2008\u20132011:  Icelandic financial crisis\n2008\u20132014:  Spanish financial crisis\n2009\u20132010:  European debt crisis\n2010\u20132018:  Greek government-debt crisis\n2013\u2013:  Ongoing Venezuelan economic crisis\n2014:  2014 Brazilian economic crisis\n2014\u20132016:  Russian financial crisis\n2018\u2013:  Ongoing Turkish currency and debt crisis\n2019\u2013:  Ongoing Sri Lankan currency and debt crisis\n2019\u2013:  Ongoing Lebanese liquidity crisis\n2020: 2020 stock market crash (especially Black Monday and Black Thursday)\n2022:  Russian financial crisis\n2022\u2013:  Ongoing Pakistani currency and debt crisis\n\nSee also\nSpecific:\n\nLiterature\nGeneral perspectives\nWalter Bagehot (1873), Lombard Street: A Description of the Money Market.\nCharles P. Kindleberger and Robert Aliber (2005), Manias, Panics, and Crashes: A History of Financial Crises (Palgrave Macmillan, 2005 ISBN 978-1-4039-3651-6).\nGernot Kohler and Emilio Jos\u00e9 Chaves (Editors) \"Globalization: Critical Perspectives\" Hauppauge, New York: Nova Science Publishers ISBN 1-59033-346-2. With contributions by Samir Amin, Christopher Chase Dunn, Andre Gunder Frank, Immanuel Wallerstein\nHyman P. Minsky (1986, 2008), Stabilizing an Unstable Economy.\nReinhart, Carmen; Rogoff, Kenneth (2009). This Time is Different: Eight Centuries of Financial Folly. Princeton University Press. p. 496. ISBN 978-0-691-14216-6.\nFerguson, Niall (2009). The Ascent of Money: A Financial History of the World. Penguin. pp. 448. ISBN 978-0-14-311617-2.\nJoachim Vogt (2014), Fear, Folly, and Financial Crises \u2013 Some Policy Lessons from History, UBS Center Public Papers, Issue 2, UBS International Center of Economics in Society, Zurich.\nRead, Charles (2022). Calming the storms : the carry trade, the banking school and British financial crises since 1825. Cham, Switzerland. ISBN 978-3-031-11914-9. OCLC 1360456914.{{cite book}}:  CS1 maint: location missing publisher (link)\n\nBanking crises\nAllen, Franklin; Gale, Douglas (February 2000). \"Financial Contagion\". Journal of Political Economy. 108 (1): 1\u201333. doi:10.1086/262109. S2CID 222441436.\nFranklin Allen and Douglas Gale (2007), Understanding Financial Crises.\nCharles W. Calomiris and Stephen H. Haber (2014), Fragile by Design: The Political Origins of Banking Crises and Scarce Credit, Princeton, NJ: Princeton University Press.\nJean-Charles Rochet (2008), Why Are There So Many Banking Crises? The Politics and Policy of Bank Regulation.\nR. Glenn Hubbard, ed., (1991) Financial Markets and Financial Crises.\nDiamond, Douglas W.; Dybvig, Philip H. (June 1983). \"Bank Runs, Deposit Insurance, and Liquidity\" (PDF). Journal of Political Economy. 91 (3): 401\u2013419. doi:10.1086/261155. S2CID 14214187.\nLuc Laeven and Fabian Valencia (2008), 'Systemic banking crises: a new database'. International Monetary Fund Working Paper 08/224.\nThomas Marois (2012), States, Banks and Crisis: Emerging Finance Capitalism in Mexico and Turkey, Edward Elgar Publishing Limited, Cheltenham, UK.\n\nBubbles and crashes\nDutton, Roy (2010), Financial Meltdown 2010 (Hardback). Infodial. ISBN 978-0-9556554-3-2\nCharles Mackay (1841), Extraordinary Popular Delusions and the Madness of Crowds\nDidier Sornette (2003), Why Stock Markets Crash, Princeton University Press.\nRobert J. Shiller (1999, 2006), Irrational Exuberance.\nMarkus Brunnermeier (2008), 'Bubbles', New Palgrave Dictionary of Economics, 2nd ed.\nDouglas French (2009) Early Speculative Bubbles and Increases in the Supply of Money\nMarkus K. Brunnermeier (2001), Asset Pricing under Asymmetric Information: Bubbles, Crashes, Technical Analysis, and Herding, Oxford University Press. ISBN 0-19-829698-3.\n\nInternational financial crises\nAcocella, N. Di Bartolomeo, G. and Hughes Hallett, A. [2012], \u2018Central banks and economic policy after the crisis: what have we learned?\u2019, ch. 5 in: Baker, H.K. and Riddick, L.A. (eds.), \u2018Survey of International Finance\u2019, Oxford University Press.\nPaul Krugman (1995), Currencies and Crises.\nCraig Burnside, Martin Eichenbaum, and Sergio Rebelo (2008), 'Currency crisis models', New Palgrave Dictionary of Economics, 2nd ed.\nMaurice Obstfeld (1996), 'Models of currency crises with self-fulfilling features'. European Economic Review 40.\nStephen Morris and Hyun Song Shin (1998), 'Unique equilibrium in a model of self-fulfilling currency attacks'. American Economic Review 88 (3).\nBarry Eichengreen (2004), Capital Flows and Crises.\nCharles Goodhart and P. Delargy (1998), 'Financial crises: plus \u00e7a change, plus c'est la m\u00eame chose'. International Finance 1 (2), pp. 261\u201387.\nJean Tirole (2002), Financial Crises, Liquidity, and the International Monetary System.\nGuillermo Calvo (2005), Emerging Capital Markets in Turmoil: Bad Luck or Bad Policy?\nBarry Eichengreen (2002), Financial Crises: And What to Do about Them.\nCharles Calomiris (1998), 'Blueprints for a new global financial architecture'.\n\nThe Great Depression and earlier banking crises\nMurray Rothbard (1962), The Panic of 1819\nMurray Rothbard (1963), America`s Great Depression.\nMilton Friedman and Anna Schwartz (1971), A Monetary History of the United States.\nBen S. Bernanke (2000), Essays on the Great Depression.\nRobert F. Bruner (2007), The Panic of 1907. Lessons Learned from the Market's Perfect Storm.\n\nRecent international financial crises\nBarry Eichengreen and Peter Lindert, eds., (1992), The International Debt Crisis in Historical Perspective.\nLessons from the Asian financial crisis / edited by Richard Carney.  New York, NY : Routledge, 2009. ISBN 978-0-415-48190-8 (hardback) ISBN 0-415-48190-2 (hardback) ISBN 978-0-203-88477-5 (ebook) ISBN 0-203-88477-9 (ebook)\nRobertson, Justin, 1972\u2013 US-Asia economic relations : a political economy of crisis and the rise of new business actors / Justin Robertson.  Abingdon, Oxon ; New York, NY : Routledge, 2008. ISBN 978-0-415-46951-7 (hbk.) ISBN 978-0-203-89052-3 (ebook)\n\n2007\u20132012 financial crisis\nRobert J. Shiller (2008), The Subprime Solution: How Today's Global Financial Crisis Happened, and What to Do About It. ISBN 0-691-13929-6.\nJC Coffee, \u2018What went wrong? An initial inquiry into the causes of the 2008 financial crisis\u2019 (2009) 9(1) Journal of Corporate Law Studies 1\nMarchionne, Francesco; Fratianni, Michele U. (10 April 2009). \"The Role of Banks in the Subprime Financial Crisis\". SSRN 1383473. {{cite journal}}: Cite journal requires |journal= (help)\nMarkus Brunnermeier (2009), 'Deciphering the liquidity and credit crunch 2007\u20132008'. Journal of Economic Perspectives 23 (1), pp. 77\u2013100.\nPaul Krugman (2008), The Return of Depression Economics and the Crisis of 2008. ISBN 0-393-07101-4.\n\"The myths about the economic crisis, the reformist left and economic democracy\"  by Takis Fotopoulos, The International Journal of Inclusive Democracy, vol 4, no 4, Oct. 2008.\nUnited States. Congress. House. Committee on the Judiciary. Subcommittee on Commercial and Administrative Law. Working families in financial crisis : medical debt and bankruptcy : hearing before the Subcommittee on Commercial and Administrative Law of the Committee on the Judiciary, House of Representatives, One Hundred Tenth Congress, first session, 17 July 2007.  Washington : U.S. G.P.O. : For sale by the Supt. of Docs., U.S. G.P.O., 2008. 277 p. : ISBN 978-0-16-081376-4 ISBN 016081376X [2]\nWilliams, Mark T. (March 2010). Uncontrolled Risk: The Lessons of Lehman Brothers and How Systemic Risk Can Still Bring Down the World Financial System. McGraw Hill Professional. ISBN 9780071749046. {{cite book}}: |work= ignored (help)\nTkac, Paula A.; Dwyer, Gerald P. (August 2009). \"The Financial Crisis of 2008 in Fixed-income Markets\". SSRN 1464891. {{cite journal}}: Cite journal requires |journal= (help)\n\nReferences\nExternal links\nFinancial Crises: Lessons from History. BBC, 3 September 2007.", "url": "https://en.wikipedia.org/wiki/Financial_crisis"}, {"title": "Travel", "text": "Travel is the movement of people between distant geographical locations. Travel can be done by foot, bicycle, automobile, train, boat, bus, airplane, ship or other means, with or without luggage, and can be one way or round trip. Travel can also include relatively short stays between successive movements, as in the case of tourism.\n\nEtymology\nThe origin of the word \"travel\" is most likely lost to history. The term \"travel\" may originate from the Old French word travail, which means 'work'. According to the Merriam-Webster dictionary, the first known use of the word travel was in the 14th century. It also states that the word comes from Middle English travailen, travelen (which means to torment, labor, strive, journey) and earlier from Old French travailler (which means to work strenuously, toil).\nIn English, people still occasionally use the words travail, which means struggle. According to Simon Winchester in his book The Best Travelers' Tales (2004), the words travel and travail both share an even more ancient root: a Roman instrument of torture called the tripalium (in Latin it means \"three stakes\", as in to impale). This link may reflect the extreme difficulty of travel in ancient times. Travel in modern times may or may not be much easier, depending upon the destination. Travel to Mount Everest, the Amazon rainforest, extreme tourism, and adventure travel are more difficult forms of travel. Travel can also be more difficult depending on the method of travel, such as by bus, cruise ship, or even by bullock cart.\n\nPurpose and motivation\nReasons for traveling include recreation, holidays, rejuvenation, tourism or vacationing, research travel, the gathering of information, visiting people, volunteer travel for charity, migration to begin life somewhere else, religious pilgrimages and mission trips, business travel, trade, commuting, obtaining health care, waging or fleeing war, for the enjoyment of traveling, or other reasons. Travelers may use human-powered transport such as walking or bicycling; or vehicles, such as public transport, automobiles, trains, ferries, boats, cruise ships and airplanes.\nMotives for travel include:\n\nPleasure\nRelaxation\nDiscovery and exploration\nAdventure\nIntercultural communications\nTaking personal time for building interpersonal relationships.\nAvoiding stress\nForming memories\n\nHistory\nTravel dates back to antiquity where wealthy Greeks and Romans would travel for leisure to their summer homes and villas in cities such as Pompeii and Baiae. While early travel tended to be slower, more dangerous, and more dominated by trade and migration, cultural and technological advances over many years have tended to mean that travel has become easier and more accessible. Humankind has come a long way in transportation since Christopher Columbus sailed to the New World from Spain in 1492, an expedition which took over 10 weeks to arrive at the final destination; to the 21st century when aircraft allows travel from Spain to the United States overnight.\nTravel in the Middle Ages offered hardships and challenges, though it was important to the economy and to society. The wholesale sector depended (for example) on merchants dealing with/through caravans or sea-voyagers, end-user retailing often demanded the services of many itinerant peddlers wandering from village to hamlet, gyrovagues (wandering monks) and wandering friars brought theology and pastoral support to neglected areas, traveling minstrels toured, and armies ranged far and wide in various crusades and in sundry other wars. Pilgrimages were common in both the European and Islamic world and involved streams of travelers both locally and internationally.\nIn the late 16th century, it became fashionable for young European aristocrats and wealthy upper-class men to travel to significant European cities as part of their education in the arts and literature. This was known as the Grand Tour, and included cities such as London, Paris, Venice, Florence, and Rome. However, the French Revolution brought with it the end of the Grand Tour.\nTravel by water often provided more comfort and speed than land-travel, at least until the advent of a network of railways in the 19th century. Travel for the purpose of tourism is reported to have started around this time when people began to travel for fun as travel was no longer a hard and challenging task. This was capitalized on by people like Thomas Cook selling tourism packages where trains and hotels were booked together. Airships and airplanes took over much of the role of long-distance surface travel in the 20th century, notably after the Second World War where there was a surplus of both aircraft and pilots. Air travel has become so ubiquitous in the 21st century that one woman, Alexis Alford, visited all 196 countries before the age of 21.\n\nGeographic types\nTravel may be local, regional, national (domestic) or international. In some countries, non-local internal travel may require an internal passport, while international travel typically requires a passport and visa. Tours are a common type of travel. Examples of travel tours are expedition cruises, small group tours, and river cruises.\n\nSafety\nAuthorities emphasize the importance of taking precautions to ensure travel safety. When traveling abroad, the odds favor a safe and incident-free trip, however, travelers can be subject to difficulties, crime and violence. Some safety considerations include being aware of one's surroundings, avoiding being the target of a crime, leaving copies of one's passport and itinerary information with trusted people, obtaining medical insurance valid in the country being visited and registering with one's national embassy when arriving in a foreign country. Many countries do not recognize drivers' licenses from other countries; however most countries accept international driving permits. Automobile insurance policies issued in one's own country are often invalid in foreign countries, and it is often a requirement to obtain temporary auto insurance valid in the country being visited. It is also advisable to become oriented with the driving rules and regulations of destination countries. Wearing a seat belt is highly advisable for safety reasons; many countries have penalties for violating seatbelt laws.\nThere are three main statistics which may be used to compare the safety of various forms of travel (based on a Department of the Environment, Transport and the Regions survey in October 2000):\n\nSee also\nReferences\nExternal links\n\n\"Travel\". Merriam-Webster.com Dictionary. Merriam-Webster.", "url": "https://en.wikipedia.org/wiki/Travel"}, {"title": "Economics", "text": "Economics () is a social science that studies the production, distribution, and consumption of goods and services.\nEconomics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact, and factors affecting it: factors of production, such as labour, capital, land, and enterprise, inflation, economic growth, and public policies that have impact on these elements. It also seeks to analyse and describe the global economy.\nOther broad distinctions within economics include those between positive economics, describing \"what is\", and normative economics, advocating \"what ought to be\"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.\nEconomic analysis can be applied throughout society, including business, finance, cybersecurity, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science, and the environment.\n\nDefinitions of economics\nThe earlier term for the discipline was \"political economy\", but since the late 19th century, it has commonly been called \"economics\". The term is ultimately derived from Ancient Greek \u03bf\u1f30\u03ba\u03bf\u03bd\u03bf\u03bc\u03af\u03b1 (oikonomia) which is a term for the \"way (nomos) to run a household (oikos)\", or in other words the know-how of an \u03bf\u1f30\u03ba\u03bf\u03bd\u03bf\u03bc\u03b9\u03ba\u03cc\u03c2 (oikonomikos), or \"household or homestead manager\". Derived terms such as \"economy\" can therefore often mean \"frugal\" or \"thrifty\". By extension then, \"political economy\" was the way to manage a polis or state.\nThere are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as \"an inquiry into the nature and causes of the wealth of nations\", in particular as:\n\na branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the publick services.\nJean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined \"the dismal science\" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) delimited the subject matter further:\n\nThe science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object.\nAlfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level:\n\nEconomics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man.\nLionel Robbins (1932) developed implications of what has been termed \"[p]erhaps the most commonly accepted current definition of the subject\":\n\nEconomics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses.\nRobbins described the definition as not classificatory in \"pick[ing] out certain kinds of behaviour\" but rather analytical in \"focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity.\" He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).\nSome subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.\nGary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as \"combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly.\" One commentary characterises the remark as making economics an approach rather than a subject matter but with great specificity as to the \"choice process and the type of social interaction that [such] analysis involves.\" The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.\nMany economists including nobel prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter. Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, it is not said that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will perform DNA analysis, others might analyse anatomy, and still others might build game theoretic models of animal behaviour. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is peculiar.\n\nHistory of economic thought\nFrom antiquity through the physiocrats\nQuestions regarding distribution of resources are found throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod as the \"first economist\". However, the word Oikos, the Greek word from which the word economy derives, was used for issues regarding how to manage a household (which was understood to be the landowner, his family, and his slaves) rather than to refer to some normative societal system of distribution of resources, which is a more recent phenomenon. Xenophon, the author of the Oeconomicus, is credited by philologues for being the source of the word economy. Joseph Schumpeter described 16th and 17th century scholastic writers, including Tom\u00e1s de Mercado, Luis de Molina, and Juan de Lugo, as \"coming nearer than any other group to being the 'founders' of scientific economics\" as to monetary, interest, and value theory within a natural-law perspective.\nTwo groups, who later were called \"mercantilists\" and \"physiocrats\", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing inexpensive raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.\nPhysiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy.\nAdam Smith (1723\u20131790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system \"with all its imperfections\" as \"perhaps the purest approximation to the truth that has yet been published\" on the subject.\n\nClassical political economy\nThe publication of Adam Smith's The Wealth of Nations in 1776, has been described as \"the effective birth of economics as a separate discipline.\" The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive.\nSmith discusses potential benefits of specialisation by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His \"theorem\" that \"the division of labor is limited by the extent of the market\" has been described as the \"core of a theory of the functions of firm and industry\" and a \"fundamental principle of economic organization.\" To Smith has also been ascribed \"the most important substantive proposition in all of economics\" and foundation of resource-allocation theory\u2014that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).\nIn an argument that includes \"one of the most famous passages in all economics,\" Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this:\n\nHe generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. \nThe Reverend Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Simon has criticised Malthus's conclusions.\nWhile Adam Smith emphasised production and income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first to state and prove the principle of comparative advantage, according to which each country should specialise in producing and exporting goods in that it has a lower relative cost of production, rather relying only on its own production. It has been termed a \"fundamental analytical explanation\" for gains from trade.\nComing at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.\nValue theory was important in classical theory. Smith wrote that the \"real price of every thing ... is the toil and trouble of acquiring it\". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size.\n\nMarxian economics\nMarxist (later, Marxian) economics descends from classical economics and it derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in 1867. Marx focused on the labour theory of value and theory of surplus value. Marx wrote that they were mechanisms used by capital to exploit labour. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production, and the theory of surplus value demonstrated how workers were only paid a proportion of the value their work had created.\nMarxian economics was further developed by Karl Kautsky (1854\u20131938)'s The Economic Doctrines of Karl Marx and The Class Struggle (Erfurt Program), Rudolf Hilferding's (1877\u20131941) Finance Capital, Vladimir Lenin (1870\u20131924)'s The Development of Capitalism in Russia and Imperialism, the Highest Stage of Capitalism, and Rosa Luxemburg (1871\u20131919)'s The Accumulation of Capital.\n\nNeoclassical economics\nAt its inception as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth (1803). These three items were considered only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has survived in part up to the present, modified by substituting the word \"wealth\" for \"goods and services\" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity, which forces people to choose, allocate scarce resources to competing ends, and economise (seeking the greatest welfare while avoiding the wasting of scarce resources). According to Robbins: \"Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\". Robbins' definition eventually became widely accepted by mainstream economists, and found its way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition.\nA body of theory later termed \"neoclassical economics\" formed from about 1870 to 1910. The term \"economics\" was popularised by such neoclassical economists as Alfred Marshall and Mary Paley Marshall as a concise synonym for \"economic science\" and a substitute for the earlier \"political economy\". This corresponded to the influence on the subject of mathematical methods used in the natural sciences.\nNeoclassical economics systematically integrated supply and demand as joint determinants of both price and quantity in market equilibrium, influencing the allocation of output and income distribution. It rejected the classical economics' labour theory of value in favour of a marginal utility theory of value on the demand side and a more comprehensive theory of costs on the supply side. In the 20th century, neoclassical theorists departed from an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behaviour-based relations across individuals.\nIn microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.\nNeoclassical economics is occasionally referred as orthodox economics whether by its critics or sympathisers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalise earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income.\nNeoclassical economics studies the behaviour of individuals, households, and organisations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more players to attain the best possible outcome.\n\nKeynesian economics\nKeynesian economics derives from John Maynard Keynes, in particular his book The General Theory of Employment, Interest and Money (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low \"effective demand\" and why even price flexibility and monetary policy might be unavailing. The term \"revolutionary\" has been applied to the book in its impact on economic analysis.\nDuring the following decades, many economists followed Keynes' ideas and expanded on his works. John Hicks and Alvin Hansen developed the IS\u2013LM model which was a simple formalisation of some of Keynes' insights on the economy's short-run equilibrium. Franco Modigliani and James Tobin developed important theories of private consumption and investment, respectively, two major components of aggregate demand. Lawrence Klein built the first large-scale macroeconometric model, applying the Keynesian thinking systematically to the US economy.\n\nPost-WWII economics\nImmediately after World War II, Keynesian was the dominant economic view of the United States establishment and its allies, Marxian economics was the dominant economic view of the Soviet Union nomenklatura and its allies.\n\nMonetarism\nMonetarism appeared in the 1950s and 1960s, its intellectual leader being Milton Friedman. Monetarists contended that monetary policy and other monetary shocks, as represented by the growth in the money stock, was an important cause of economic fluctuations, and consequently that monetary policy was more important than fiscal policy for purposes of stabilisation. Friedman was also skeptical about the ability of central banks to conduct a sensible active monetary policy in practice, advocating instead using simple rules such as a steady rate of money growth.\nMonetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned because the results were unsatisfactory.\n\nNew classical economics\nA more fundamental challenge to the prevailing Keynesian paradigm came in the 1970s from new classical economists like Robert Lucas, Thomas Sargent and Edward Prescott. They introduced the notion of rational expectations in economics, which had profound implications for many economic discussions, among which were the so-called Lucas critique and the presentation of real business cycle models.\n\nNew Keynesians\nDuring the 1980s, a group of researchers appeared being called New Keynesian economists, including among others George Akerlof, Janet Yellen, Gregory Mankiw and Olivier Blanchard. They adopted the principle of rational expectations and other monetarist or new classical ideas such as building upon models employing micro foundations and optimizing behaviour, but simultaneously emphasised the importance of various market failures for the functioning of the economy, as had Keynes. Not least, they proposed various reasons that potentially explained the empirically observed features of price and wage rigidity, usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones.\n\nNew neoclassical synthesis\nAfter decades of often heated discussions between Keynesians, monetarists, new classical and new Keynesian economists, a synthesis emerged by the 2000s, often given the name the new neoclassical synthesis. It integrated the rational expectations and optimizing framework of the new classical theory with a new Keynesian role for nominal rigidities and other market imperfections like imperfect information in goods, labour and credit markets. The monetarist importance of monetary policy in stabilizing the economy and in particular controlling inflation was recognised as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis led to a new class of applied models, known as dynamic stochastic general equilibrium or DSGE models, descending from real business cycles models, but extended with several new Keynesian and other features. These models proved useful and influential in the design of modern monetary policy and are now standard workhorses in most central banks.\n\nAfter the financial crisis\nAfter the 2007\u20132008 financial crisis, macroeconomic research has put greater emphasis on understanding and integrating the financial system into models of the general economy and shedding light on the ways in which problems in the financial sector can turn into major macroeconomic recessions. In this and other research branches, inspiration from behavioural economics has started playing a more important role in mainstream economic theory. Also, heterogeneity among the economic agents, e.g. differences in income, plays an increasing role in recent economic research.\n\nOther schools and approaches\nOther schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Freiburg School, the School of Lausanne, the Stockholm school and the Chicago school of economics. During the 1970s and 1980s mainstream economics was sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago school approach.\nWithin macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics and the new neoclassical synthesis.\nBeside the mainstream development of economic thought, various alternative or heterodox economic theories have evolved over time, positioning themselves in contrast to mainstream theory. These include:\n\nAustrian School, emphasizing human action, property rights and the freedom to contract and transact to have a thriving and successful economy. It also emphasises that the state should play as small role as possible (if any role) in the regulation of economic activity between two transacting parties. Friedrich Hayek and Ludwig von Mises are the two most prominent representatives of the Austrian school.\nPost-Keynesian economics concentrates on macroeconomic rigidities and adjustment processes. It is generally associated with the University of Cambridge and the work of Joan Robinson.\nEcological economics like environmental economics studies the interactions between human economies and the ecosystems in which they are embedded, but in contrast to environmental economics takes an oppositional position towards general mainstream economic principles. A major difference between the two subdisciplines is their assumptions about the substitution possibilities between human-made and natural capital.\nAdditionally, alternative developments include Marxian economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, econodynamics, feminist economics and biophysical economics.\nFeminist economics emphasises the role that gender plays in economies, challenging analyses that render gender invisible or support gender-oppressive economic systems. The goal is to create economic research and policy analysis that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalised groups.\n\nMethodology\nTheoretical research\nMainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories. While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories.\nIn microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models with microfoundations, in which microeconomic concepts play a major part.\nSometimes an economic hypothesis is only qualitative, not quantitative.\nExpositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, mathematical economics is the application of mathematical methods to represent theories and analyse problems in economics. Paul Samuelson's treatise Foundations of Economic Analysis (1947) exemplifies the method, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called operationally meaningful theorems in economics, which are theorems that can conceivably be refuted by empirical data.\n\nEmpirical research\nEconomic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.\nStatistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance (\"signal strength\") of the hypothesised relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.\nExperimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct.\nIn behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a \"genuine science\".\n\nMicroeconomics\nMicroeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.\nVarious market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a \"price taker\" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.\nForms of imperfect competition include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Firms under imperfect competition have the potential to be \"price makers\", which means that they can influence the prices of their products.\nIn partial equilibrium method of analysis, it is assumed that activity in the market being analysed does not affect other markets. This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across all markets. This method studies both changes in markets and their interactions leading towards equilibrium.\n\nProduction, cost, and efficiency\nIn microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods, and \"guns\" vs \"butter\".\nInputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.\nEconomic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.\nThe production\u2013possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say \"guns\" and \"butter\"). The PPF is a table or graph (as at the right) showing the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.\nScarcity is represented in the figure by people being willing but unable in the aggregate to consume beyond the PPF (such as at X) and by the negative slope of the curve. If production of one good increases along the curve, production of the other good decreases, an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.\nThe slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a real opportunity cost. Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. Along the PPF, scarcity implies that choosing more of one good in the aggregate entails doing with less of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.\nBy construction, each point on the curve shows productive efficiency in maximizing output for given total inputs. A point inside the curve (as at A), is feasible but represents production inefficiency (wasteful use of inputs), in that output of one or both goods could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organisation of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.\nMuch applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organise society for the most efficient use of resources has been described as the \"essence of economics\", where the subject \"makes its unique contribution.\"\n\nSpecialisation\nSpecialisation is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus relatively cheaper, input.\nEven if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialise in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.\nIt has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.\nThe general theory of specialisation applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.\nAn example that combines features above is a country that specialises in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.\nTheory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialisation of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate.\n\nSupply and demand\nPrices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.\nFor a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is \"constrained utility maximisation\" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesised relation of each individual consumer for ranking different commodity bundles as more or less preferred.\nThe law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.\nSupply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesised to be profit maximisers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.\nThat is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The \"Law of Supply\" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.\nMarket equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilise at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.\n\nFirms\nPeople frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through firms. The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organise their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.\nIn perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organisation generalises from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.\nManagerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimise business decisions, including unit-cost minimisation and profit maximisation, given the firm's objectives and constraints imposed by technology and market conditions.\n\nUncertainty and game theory\nUncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.\nGame theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organisation, discussed above, to model different types of firm behaviour, for example in a solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own.\nIn this, it generalises maximisation approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology.\nRisk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation.\nSome market organisations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's \"Market for Lemons\" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a \"lemon\" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving).\nBoth problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market (\"incomplete markets\"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure.\n\nMarket failure\nThe term \"market failure\" encompasses several problems which may undermine standard economic assumptions. Although economists categorise market failures differently, the following categories emerge in the main texts.\nInformation asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.\nNatural monopoly, or the overlapping concepts of \"practical\" and \"technical\" monopoly, is an extreme case of failure of competition as a restraint on producers. Extreme economies of scale are one possible cause.\nPublic goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.\nExternalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidise or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.\nIn many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesised long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.\nSome specialised fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or \"public bads\".\nPolicy options include regulations that reflect cost\u2013benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights.\n\nWelfare\nWelfare economics uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyses social welfare, however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no \"social welfare\" apart from the \"welfare\" associated with its individual units.\n\nMacroeconomics\nMacroeconomics, another branch of economics, examines the economy as a whole to explain broad aggregates and their interactions \"top down\", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.\nSince at least the 1960s, macroeconomics has been characterised by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject.\nMacroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth.\n\nGrowth\nGrowth economics studies factors that explain economic growth \u2013 the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the level of output per capita between countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.\nMuch-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting.\n\nBusiness cycle\nThe economics of a depression were the spur for the creation of \"macroeconomics\" as a separate discipline. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled The General Theory of Employment, Interest and Money outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.\nHe therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilise output over the business cycle. Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of The General Theory.\nOver the years, understanding of the business cycle has branched into various research programmes, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with classical economics, stating that Keynesianism is correct in the short run but qualified by classical-like considerations in the intermediate and long run.\nNew classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and \"rational expectations\" theory, led by Robert Lucas, and real business cycle theory.\nIn contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are \"sticky\", which means they do not adjust instantaneously to changes in economic conditions.\nThus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the \"long run\" may be very long.\n\nUnemployment\nThe amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes.\nClassical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.\nStructural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.\nWhile some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.\n\nMoney and monetary policy\nMoney is a means of final payment for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, \"Money is what money does\" (\"Money is that money does\" in the original).\nAs a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialised producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.\nMonetary policy is the policy that central banks conduct to accomplish their broader objectives. Most central banks in developed countries follow inflation targeting, whereas the main objective for many central banks in development countries is to uphold a fixed exchange rate system. The primary monetary tool is normally the adjustment of interest rates, either directly via administratively changing the central bank's own interest rates or indirectly via open market operations. Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and inflation.\n\nFiscal policy\nGovernments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.\nFor example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.\nThe effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.\nSceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes.\n\nInequality\nEconomic inequality includes income inequality, measured using the distribution of income (the amount of money people receive), and wealth inequality measured using the distribution of wealth (the amount of wealth people own), and other measures such as consumption, land ownership, and human capital. Inequality exists at different extents between countries or states, groups of people, and individuals. There are many methods for measuring inequality, the Gini coefficient being widely used for income differences among individuals. An example measure of inequality between countries is the Inequality-adjusted Human Development Index, a composite index that takes inequality into account. Important concepts of equality include equity, equality of outcome, and equality of opportunity.\nResearch has linked economic inequality to political and social instability, including revolution, democratic breakdown and civil conflict. Research suggests that greater inequality hinders economic growth and macroeconomic stability, and that land and human capital inequality reduce growth more than inequality of income. Inequality is at the centre stage of economic policy debate across the globe, as government tax and spending policies have significant effects on income distribution. In advanced economies, taxes and transfers decrease income inequality by one-third, with most of this being achieved via public social spending (such as pensions and family benefits.)\n\nOther branches of economics\nPublic economics\nPublic economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost\u2013benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.\nMuch of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like.\nWelfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society.\n\nInternational economics\nInternational trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalisation.\n\nLabour economics\nLabour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms.\n\nDevelopment economics\nDevelopment economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.\n\nRelated subjects\nEconomics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, cultural economics, family economics and institutional economics.\nLaw and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.\nPolitical economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.\nEnergy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.\nThe sociological subfield of economic sociology arose, primarily through the work of \u00c9mile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Georg Simmel's The Philosophy of Money (1900). More recently, the works of James S. Coleman, Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field.\nGary Becker in 1974 presented an economic theory of social interactions, whose applications included the family, charity, merit goods and multiperson interactions, and envy and hatred. He and Kevin Murphy authored a book in 2001 that analysed market behaviour in a social environment.\n\nProfession\nThe professionalisation of economics, reflected in the growth of graduate programmes on the subject, has been described as \"the main change in economics since around 1900\". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study. See Bachelor of Economics and Master of Economics.\nIn the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national treasury, central bank or National Bureau of Statistics. See Economic analyst.\nThere are dozens of prizes awarded to economists each year for outstanding intellectual contributions to the field, the most prominent of which is the Nobel Memorial Prize in Economic Sciences, though it is not a Nobel Prize.\nContemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialise in econometrics and mathematical methods.\n\nWomen in economics\nHarriet Martineau (1802\u20131876) was a widely-read populariser of classical economic thought. Mary Paley Marshall (1850\u20131944), the first women lecturer at a British economics faculty, wrote The Economics of Industry with her husband Alfred Marshall. Joan Robinson (1903\u20131983) was an important post-Keynesian economist. The economic historian Anna Schwartz (1915\u20132012) coauthored A Monetary History of the United States, 1867\u20131960 with Milton Friedman. Three women have received the Nobel Prize in Economics: Elinor Ostrom (2009), Esther Duflo (2019) and Claudia Goldin (2023). Five have received the John Bates Clark Medal: Susan Athey (2007), Esther Duflo (2010), Amy Finkelstein (2012), Emi Nakamura (2019) and Melissa Dell (2020).\nWomen's authorship share in prominent economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship. Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation.\n\nSee also\nNotes\nReferences\nSources\nHoover, Kevin D.; Siegler, Mark V. (20 March 2008). \"Sound and Fury: McCloskey and Significance Testing in Economics\". Journal of Economic Methodology. 15 (1): 1\u201337. CiteSeerX 10.1.1.533.7658. doi:10.1080/13501780801913298. S2CID 216137286.\nSamuelson, Paul A; Nordhaus, William D. (2010). Economics. Boston: Irwin McGraw-Hill. ISBN 978-0073511290. OCLC 751033918.\n\nFurther reading\nAnderson, David A. (2019). Survey of Economics. New York: Worth. ISBN 978-1-4292-5956-9.\nBlanchard, Olivier; Amighini, Alessia; Giavazzi, Francesco (2017). Macroeconomics: a European perspective (3rd ed.). Pearson. ISBN 978-1-292-08567-8.\nBlaug, Mark (1985). Economic Theory in Retrospect (4th ed.). Cambridge: Cambridge University Press. ISBN 978-0521316446.\nMcCann, Charles Robert Jr. (2003). The Elgar Dictionary of Economic Quotations. Edward Elgar. ISBN 978-1840648201.\nPost, Louis F. (1927), The Basic Facts of Economics: A Common-Sense Primer for Advanced Students. United States: Columbian Printing Company, Incorporated.\n Economics public domain audiobook at LibriVox.\n\nExternal links\nGeneral information\nInstitutions and organizations\n\n\n=== Study resources ===", "url": "https://en.wikipedia.org/wiki/Economics"}, {"title": "Marathon", "text": "The marathon is a long-distance foot race with a distance of 42km 195m (c. 26mi 385yd), usually run as a road race, but the distance can be covered on trail routes. The marathon can be completed by running or with a run/walk strategy. There are also wheelchair divisions. More than 800 marathons are held worldwide each year, with the vast majority of competitors being recreational athletes, as larger marathons can have tens of thousands of participants.\nA creation of the French philologist Michel Br\u00e9al inspired by a story from Ancient Greece, the marathon was one of the original modern Olympic events in 1896 in Athens. The distance did not become standardized until 1921. The distance is also included in the World Athletics Championships, which began in 1983. It is the only running road race included in both championship competitions (walking races on the roads are also contested in both).\n\nHistory\nOrigin\nThe name Marathon comes from the legend of Pheidippides, the Greek messenger. The legend states that while he was taking part in the Battle of Marathon, which took place in August or September 490 BC, he witnessed a Persian vessel changing its course towards Athens as the battle was near a victorious end for the Greek army. He interpreted this as an attempt by the defeated Persians to rush into the city to claim a false victory or simply raid, hence claiming their authority over Greek land. It was said that he ran the entire distance to Athens without stopping, discarding his weapons and even clothes to lose as much weight as possible, and burst into the assembly, exclaiming \"we have won!\", before collapsing and dying.\nThe account of the run from Marathon to Athens first appeared in Plutarch's On the Glory of Athens in the first century AD, which quoted from Heraclides Ponticus's lost work, giving the runner's name as either Thersipus of Erchius or Eucles. This was the account adopted by Benjamin Haydon for his painting  Eucles Announcing the Victory of Marathon, published as an engraving in 1836 with a poetical illustration by Letitia Elizabeth Landon. Satirist Lucian of Samosata gave one of the earliest accounts similar to the modern version of the story, but its historical veracity is disputed based on its tongue-in-cheek writing and the runner being referred to as Philippides and not Pheidippides.\nThere is debate about the historical accuracy of this legend. The Greek historian Herodotus, the main source for the Greco-Persian Wars, mentioned Philippides as the messenger who ran from Athens to Sparta asking for help, and then ran back, a distance of over 240 kilometres (150 mi) each way. In some Herodotus manuscripts, the name of the runner between Athens and Sparta is given as Philippides. Herodotus makes no mention of a messenger sent from Marathon to Athens and relates that the main part of the Athenian army, having fought and won the grueling battle and fearing a naval raid by the Persian fleet against an undefended Athens, marched quickly back from the battle to Athens, arriving the same day.\n\nIn 1879, Robert Browning wrote the poem Pheidippides. Browning's poem, his composite story, became part of late 19th-century popular culture and was accepted as a historical legend.\nMount Pentelicus stands between Marathon and Athens, which means that Philippides would have had to run around the mountain, either to the north or to the south. The latter and more obvious route is followed by the modern Marathon-Athens highway (EO83\u2013EO54), which follows the lay of the land southwards from Marathon Bay and along the coast, then takes a gentle but protracted climb westwards towards the eastern approach to Athens, between the foothills of Mounts Hymettus and Penteli, and then gently downhill to Athens proper. As it existed when the Olympics were revived in 1896, this route was approximately 40 kilometres (25 mi) long. It was the approximate distance originally used for marathon races. However, there have been suggestions that Philippides might have followed another route: a westward climb along the eastern and northern slopes of Mount Penteli to the pass of Dionysos, and then a straight southward downhill path to Athens. This route is slightly shorter, 35 kilometres (22 mi), but includes a very steep climb over the first 5 kilometres (3.1 mi).\n\nModern Olympic marathon\nWhen the modern Olympics began in 1896, the initiators and organizers were looking for a great popularizing event, recalling the glory of ancient Greece. The idea of a marathon race came from Michel Br\u00e9al, who wanted the event to feature in the first modern Olympic Games in 1896 in Athens. This idea was heavily supported by Pierre de Coubertin, the founder of the modern Olympics, as well as by the Greeks. A selection race for the Olympic marathon was held on 22 March 1896 (Gregorian) that was won by Charilaos Vasilakos in 3 hours and 18 minutes. The winner of the first Olympic marathon, on 10 April 1896 (a male-only race), was Spyridon Louis, a Greek water-carrier, in 2 hours 58 minutes and 50 seconds. The marathon of the 2004 Summer Olympics was run on the traditional route from Marathon to Athens, ending at Panathinaiko Stadium, the venue for the 1896 Summer Olympics. That men's marathon was won by Italian Stefano Baldini in 2 hours 10 minutes and 55 seconds, a record time for this route until the non-Olympics Athens Classic Marathon of 2014 when Felix Kandie lowered the course record to 2 hours 10 minutes and 37 seconds.\n\nThe women's marathon was introduced at the 1984 Summer Olympics (Los Angeles, US) and was won by Joan Benoit of the United States with a time of 2 hours 24 minutes and 52 seconds.\nIt has become a tradition for the men's Olympic marathon to be the last event of the athletics calendar, on the final day of the Olympics. For many years the race finished inside the Olympic stadium; however, at the 2012 Summer Olympics (London), the start and finish were on The Mall, and at the 2016 Summer Olympics (Rio de Janeiro), the start and finish were in the Samb\u00f3dromo, the parade area that serves as a spectator mall for Carnival.\nOften, the men's marathon medals are awarded during the closing ceremony (including the 2004 games, 2012 games, 2016 games and 2020 games ).\nThe Olympic men's record is 2:06:26, set at the 2024 Summer Olympics by Tamirat Tola of Ethiopia.  The Olympic women's record is 2:23:07, set at the 2012 Summer Olympics by Tiki Gelana of Ethiopia. Per capita, the Kalenjin ethnic group of Rift Valley Province in Kenya has produced a highly disproportionate share of marathon and track-and-field winners.\n\nMarathon mania in the US\nThe Boston Marathon began on 19 April 1897 and was inspired by the success of the first marathon competition in the 1896 Summer Olympics. It is the world's oldest annual marathon and ranks as one of the world's most prestigious road racing events. Its course runs from Hopkinton in southern Middlesex County to Copley Square in Boston. Johnny Hayes' victory at the 1908 Summer Olympics also contributed to the early growth of long-distance running and marathoning in the United States. Later that year, races around the holiday season including the Empire City Marathon held on New Year's Day 1909 in Yonkers, New York, marked the early running craze referred to as \"marathon mania\". Following the 1908 Olympics, the first five amateur marathons in New York City were held on days that held special meanings: Thanksgiving Day, the day after Christmas, New Year's Day, Washington's Birthday, and Lincoln's Birthday.\nFrank Shorter's victory in the marathon at the 1972 Summer Olympics would spur national enthusiasm for the sport more intensely than that which followed Hayes' win 64 years earlier. In 2014, an estimated 550,600 runners completed a marathon within the United States. This can be compared to 143,000 in 1980. Today, marathons are held all around the world on a nearly weekly basis.\n\nInclusion of women\nFor a long time after the Olympic marathon started, there were no long-distance races, such as the marathon, for women. Although a few women, such as Stamata Revithi in 1896, had run the marathon distance, they were not included in any official results. Marie-Louise Ledru has been credited as the first woman to complete a marathon, in 1918. Violet Piercy has been credited as the first woman to be officially timed in a marathon, in 1926.\nArlene Pieper became the first woman to officially finish a marathon in the United States when she completed the Pikes Peak Marathon in Manitou Springs, Colorado, in 1959. Kathrine Switzer was the first woman to run the Boston Marathon \"officially\" (with a number), in 1967. However, Switzer's entry, which was accepted through an \"oversight\" in the screening process, was in \"flagrant violation of the rules\", and she was treated as an interloper once the error was discovered. Bobbi Gibb had completed the Boston race unofficially the previous year (1966), and was later recognized by the race organizers as the women's winner for that year, as well as 1967 and 1968.\n\nDistance\nThe length of an Olympic marathon was not precisely fixed at first. Despite this, the marathon races in the first few Olympic Games were about 40 kilometres (25 mi), roughly the distance from Marathon to Athens by the longer, flatter route. The exact length depended on the route established for each venue.\n\n1908 Olympics\nThe International Olympic Committee agreed in 1907 that the distance for the 1908 London Olympic marathon would be about 25 miles or 40 kilometers. The organizers decided on a course of 26 miles from the start at Windsor Castle to the royal entrance to the White City Stadium, followed by a lap (586 yards 2 feet; 536 m) of the track, finishing in front of the Royal Box. The course was later altered to use a different entrance to the stadium, followed by a partial lap of 385 yards to the same finish.\nThe modern 42.195 km (26.219 mi) standard distance for the marathon was set by the International Amateur Athletic Federation (IAAF) in May 1921 directly from the length used at the 1908 Summer Olympics in London.\n\nIAAF and world records\nAn official IAAF marathon course is 42.195 km (42 m tolerance only in excess). Course officials add a short course prevention factor of up to one meter per kilometer to their measurements to reduce the risk of a measuring error producing a length below the minimum distance.\nFor events governed by IAAF rules, the route must be marked so that all competitors can see the distance covered in kilometers. The rules do not mention the use of miles. The IAAF will only recognize world records established at events run under IAAF rules. For major events, it is customary to publish competitors' timings at the midway mark and also at 5 km splits; marathon runners can be credited with world records for lesser distances recognized by the IAAF (such as 20 km, 30 km and so on) if such records are established while the runner is running a marathon, and completes the marathon course.\n\nMarathon races\nAnnually, more than 800 marathons are organized worldwide. Some of these belong to the Association of International Marathons and Distance Races (AIMS) which has grown since its foundation in 1982 to embrace over 300 member events in 83 countries and territories. The marathons of Berlin, Boston, Chicago, London, New York City and Tokyo form the World Marathon Majors series, awarding $500,000 annually to the best overall male and female performers in the series.\nIn 2006, the editors of Runner's World selected a \"World's Top 10 Marathons\", in which the Amsterdam, Honolulu, Paris, Rotterdam, and Stockholm marathons were featured along with the five original World Marathon Majors events (excluding Tokyo). Other notable large marathons include United States Marine Corps Marathon, Los Angeles, and Rome. The Boston Marathon is the world's oldest annual marathon, inspired by the success of the 1896 Olympic marathon and held every year since 1897 to celebrate Patriots' Day, a holiday marking the beginning of the American Revolution, thereby purposely linking Athenian and American struggle for democracy. The oldest annual marathon in Europe is the Ko\u0161ice Peace Marathon, held since 1924 in Ko\u0161ice, Slovakia. The historic Polytechnic Marathon was discontinued in 1996. The Athens Classic Marathon traces the route of the 1896 Olympic course, starting in Marathon on the eastern coast of Attica, site of the Battle of Marathon of 490 BC, and ending at the Panathenaic Stadium in Athens.\n\nThe Midnight Sun Marathon is held in Troms\u00f8, Norway at 70 degrees north. Using unofficial and temporary courses measured by GPS, races of marathon distance are now held at the North Pole, in Antarctica, and over desert terrain. Other unusual marathons include the Great Wall Marathon on The Great Wall of China, the Big Five Marathon among the safari wildlife of South Africa, the Great Tibetan Marathon \u2013 a marathon in an atmosphere of Tibetan Buddhism at an altitude of 3,500 metres (11,500 ft), and the Polar Circle Marathon on the permanent ice cap of Greenland.\nA few marathons cross international and geographical borders. The Istanbul Marathon is the only marathon where participants run over two continents (Europe and Asia) during a single event. In the Detroit Free Press Marathon, participants cross the US/Canada border twice. The Niagara Falls International Marathon includes one international border crossing, via the Peace Bridge from Buffalo, New York, United States to Fort Erie, Ontario, Canada. In the Three Countries Marathon, participants run through Germany, Switzerland and Austria.\nOn 20 March 2018, an indoor Marathon occurred in the Armory in New York City. The 200 m track saw a world record in the women's and men's field. Lindsey Scherf (USA) set the indoor women's world record with 2:40:55. Malcolm Richards (USA) won in 2:19:01 with a male indoor world record.\n\nWheelchair division\nMany marathons feature a wheelchair division. Typically, those in the wheelchair racing division start their races earlier than their running counterparts.\nThe first wheelchair marathon was in 1974 in Toledo, Ohio, and it was won by Bob Hall at 2:54. Hall competed in the 1975 Boston Marathon and finished in 2:58, inaugurating the introduction of wheelchair divisions into the Boston Marathon. From 1977 the race was declared the US National Wheelchair championship. The Boston Marathon awards $10,000 to the winning push-rim athlete. Ernst van Dyk has won the Boston Marathon wheelchair division ten times and holds the world record at 1:18:27, set in Boston in 2004. Jean Driscoll won eight times (seven consecutively) and holds the women's world record at 1:34:22.\nThe New York City Marathon banned wheelchair entrants in 1977, citing safety concerns, but then voluntarily allowed Bob Hall to compete after the state Division of Human Rights ordered the marathon to show cause. The Division ruled in 1979 that the New York City Marathon and New York Road Runners club had to allow wheelchair athletes to compete, and confirmed this at appeal in 1980, but the New York Supreme Court ruled in 1981 that a ban on wheelchair racers was not discriminatory as the marathon was historically a foot race. However, by 1986 14 wheelchair athletes were competing, and an official wheelchair division was added to the marathon in 2000.\nSome of the quickest people to complete a wheel-chair marathon include Thomas Geierpichler (Austria), who won gold in the men's T52-class marathon (no lower limb function) in 1 hr 49 min 7 sec in Beijing, China, on 17 September 2008; and, Heinz Frei (Switzerland) who won the men's T54 marathon (for racers with spinal cord injuries) in a time of 1 hr 20 min and 14 sec in Oita, Japan, 31 October 1999.\n\nStatistics\nWorld records and world's best\nWorld records were not officially recognized by the IAAF, now known as World Athletics, until 1 January 2004; previously, the best times for the marathon were referred to as the 'world best'. Courses must conform to World Athletics standards for a record to be recognized. However, marathon routes still vary greatly in elevation, course, and surface, making exact comparisons impossible. Typically, the fastest times are set over relatively flat courses near sea level, during good weather conditions and with the assistance of pacesetters.\nThe current world record time for men over the distance is 2 hours and 35 seconds, set in the Chicago Marathon by the late Kelvin Kiptum of Kenya on 8 October 2023.\nThe world record for women was set by Ruth Chepng'etich of Kenya in the Chicago Marathon on 13 October 2024, in 2 hours, 9 minutes, and 56 seconds. This broke Tigst Assefa's previous world record of 2 hours 11 minutes and 53 seconds by almost two minutes, and was the first time in history a woman broke the 2:11 and 2:10 barriers in the marathon.\n\nAll-time top 25\nThe data is correct as of 2 November 2024.\n\nMen\nNotes\n\nEliud Kipchoge (Kenya) ran a time of 1:59:40.2 at the Ineos 1:59 Challenge in Vienna on 12 October 2019. This event was run with no other competitors and with the assistance of fuel and hydration on demand and in-out pacemakers. Therefore, the attempt was not eligible for official ratification. This was faster than his previous assisted run of 2:00:25 at the Nike Breaking2 in Monza on 6 May 2017, which was also ineligible.\nTitus Ekiru (Kenya) ran a time of 2:02:57 at the Milano City Marathon on 16 May 2021, but was later disqualified due to doping violations.\nGeoffrey Mutai (Kenya) ran a time of 2:03:02 at the Boston Marathon on 18 April 2011 that was run on an assisted course (in the case of Boston, a point-to-point, net downhill course in excess of the standards) and is therefore ineligible for record purposes per IAAF rule 260.28\nMoses Mosop (Kenya) ran a time of 2:03:06 at the Boston Marathon on 18 April 2011 that was run on an assisted course and is therefore ineligible for record purposes per IAAF rule 260.28\n\nWomen\nSeason's bests\nOldest marathoner\nFauja Singh, then 100, finished the Toronto Waterfront Marathon, becoming the first centenarian ever to officially complete that distance. Singh, a British citizen, finished the race on 16 October 2011 with a time of 8:11:05.9, making him the oldest marathoner. Because Singh could not produce a birth certificate from rural 1911 Colonial India, the place of his birth, his age could not be verified and his record was not accepted by the official governing body World Masters Athletics.\nJohnny Kelley ran his last full Boston Marathon at the documented age of 84 in 1992. He previously had won the Boston Marathon in both 1935 and 1945 respectively. Between 1934 and 1950, Johnny finished in the top five 15 times, consistently running in the 2:30s and finishing in second place a record seven times at Boston. A fixture at Boston for more than a half century, his 1992 61st start and 58th finish in Boston is a record which still stands today.\nGladys Burrill, a 92-year-old Prospect, Oregon woman and part-time resident of Hawaii, previously held the Guinness World Records title of oldest person to complete a marathon with her 9 hours 53 minutes performance at the 2010 Honolulu Marathon. The records of the Association of Road Racing Statisticians, at that time, however, suggested that Singh was overall the oldest marathoner, completing the 2004 London Marathon at the age of 93 years and 17 days, and that Burrill was the oldest female marathoner, completing the 2010 Honolulu Marathon at the age of 92 years and 19 days. Singh's age was also reported to be 93 by other sources.\nIn 2015, 92-year-old Harriette Thompson of Charlotte, North Carolina, completed the Rock 'n' Roll San Diego Marathon in 7 hours 24 minutes 36 seconds, thus becoming the oldest woman to complete a marathon. While Gladys Burrill was 92 years and 19 days old when she completed her record-setting marathon, Harriette Thompson was 92 years and 65 days old when she completed hers.\nEnglish born Canadian Ed Whitlock is the oldest to complete a marathon in under 3 hours at age 74, and under 4 hours at age 85.\n\nYoungest marathoner\nBudhia Singh, a boy from Odisha, India, completed his first marathon at age five. He trained under the coach Biranchi Das, who saw potential in him. In May 2006, Budhia was temporarily banned from running by the ministers of child welfare, as his life could be at risk. His coach was also arrested for exploiting and cruelty to a child and was later murdered in an unrelated incident. Budhia is now at a state-run sports academy.\nThe youngest under 4 hours is Mary Etta Boitano at age 7 years, 284 days; under 3 hours Julie Mullin at 10 years 180 days; and under 2:50 Carrie Garritson at 11 years 116 days.\n\nParticipation\nIn 2016, Running USA estimated that there were approximately 507,600 marathon finishers in the United States, while other sources reported greater than 550,000 finishers. The chart below from Running USA provides the estimated U.S. Marathon Finisher totals going back to 1976.\nMarathon running has become an obsession in China, with 22 marathon races in 2011 increasing to 400 in 2017. In 2015, 75 Chinese runners participated in the Boston Marathon and this increased to 278 in 2017.\n\nMultiple marathons\nAs marathon running has become more popular, some athletes have undertaken challenges involving running a series of marathons.\nThe 100 Marathon Club is intended to provide a focal point for all runners, particularly from the United Kingdom or Ireland, who have completed 100 or more races of marathon distance or longer. At least 10 of these events must be United Kingdom or Ireland Road Marathons. Club chairman Roger Biggs has run more than 700 marathons or ultras. Brian Mills completed his 800th marathon on 17 September 2011.\nSteve Edwards, a member of the 100 Marathon Club, set the world record for running 500 marathons in the fastest average finish time of 3 hours 15 minutes, at the same time becoming the first man to run 500 marathons with an official time below 3 hours 30 minutes, on 11 November 2012 at Milton Keynes, England. The records took 24 years to achieve. Edwards was 49 at the time.\nOver 350 individuals have completed a marathon in each state of the United States plus Washington, D.C., and some have done it as many as eight times. Beverly Paquin, a 22-year-old nurse from Iowa, was the youngest woman to run a marathon in all 50 states in 2010. A few weeks later, still in 2010, Morgan Cummings (also 22) became the youngest woman to complete a marathon in all 50 states and DC. In 2004, Chuck Bryant of Miami, Florida, who lost his right leg below the knee, became the first amputee to finish this circuit. Bryant has completed a total of 59 marathons on his prosthesis. Twenty-seven people have run a marathon on each of the seven continents, and 31 people have run a marathon in each of the Canadian provinces. In 1980, in what was termed the Marathon of Hope, Terry Fox, who had lost a leg to cancer and so ran with one artificial leg, attained 5,373 km (3,339 mi) of his proposed cross-Canada cancer fundraising run, maintaining an average of over 37 km (23 mi), close to the planned marathon distance, for each of 143 consecutive days.\n\nOn 25 September 2011, Patrick Finney of Grapevine, Texas became the first person with multiple sclerosis to finish a marathon in each state of the United States. In 2004, \"the disease had left him unable to walk. But unwilling to endure a life of infirmity, Finney managed to regain his ability to balance on two feet, to walk \u2013 and eventually to run \u2013 through extensive rehabilitation therapy and new medications.\"\nIn 2003, British adventurer Sir Ranulph Fiennes completed seven marathons on seven continents in seven days. He completed this feat despite suffering from a heart attack and undergoing a double heart bypass operation just four months before. This feat has since been eclipsed by Irish ultramarathon runner Richard Donovan who in 2009 completed seven marathons on seven continents in under 132 hours (five and a half days). Starting 1 February 2012 he improved on this by completing the 7 on 7 in under 120 hours or in less than five days.\nOn 30 November 2013, 69-year-old Larry Macon set a Guinness World Record for Most Marathons Run in a Year by Man by running 238 marathons. Larry Macon celebrated his 1,000th career marathon at the Cowtown Marathon in Ft. Worth on 24 February 2013.\nOther goals are to attempt to run marathons on a series of consecutive weekends (Richard Worley on 159 weekends), or to run the most marathons during a particular year or the most in a lifetime. A pioneer in running multiple marathons was Sy Mah of Toledo, Ohio, who ran 524 before he died in 1988. As of 30 June 2007, Horst Preisler of Germany had successfully completed 1214 marathons plus 347 ultramarathons, a total of 1561 events at marathon distance or longer. Sigrid Eichner, Christian Hottas and Hans-Joachim Meyer have also all completed over 1000 marathons each. Norm Frank of the United States is credited with 945 marathons.\nChristian Hottas is meanwhile the first runner who ever completed 2000 marathons. He ran his 2000th at TUI Marathon Hannover on 5 May 2013 together with a group of more than 80 friends from 11 countries, including 8 officers from the 100 Marathons Clubs U.K., North-America, Germany, Denmark, Austria and Italy. Hottas completed his 2500th marathon on 4 December 2016.\nIn 2010, Stefaan Engels, a Belgian, set out to run the marathon distance every day of the year. Because of a foot injury he had to resort to a handcycle near the end of January 2010. However, on 5 February he was fully recovered and decided to reset the counter back to zero. By 30 March he broke the existing record of Akinori Kusuda, from Japan, who completed 52 marathons in a row in 2009. On 5 February 2011, Engels had run 365 marathon distances in as many days.\nRicardo Abad Mart\u00ednez, from Spain, later ran 150 marathons in 150 consecutive days in 2009, and subsequently 500 marathons in a row, from October 2010 to February 2012.\nSome runners compete to run the same marathons for the most consecutive years. For example, Johnny Kelley completed 58 Boston Marathons (he entered the race 61 times). Currently, the longest consecutive streak of Boston Marathon finishes\u201445 in a row\u2014is held by Bennett Beach, of Bethesda, Maryland.\n\nOlympic medalists\nMen\nWomen\nWorld Championships medalists\nMen\nWomen\nGeneral participation\nMost participants do not run a marathon to win. More important for most runners is their personal finishing time and their placement within their specific gender and age group, though some runners just want to finish. Strategies for completing a marathon include running the whole distance and a run\u2013walk strategy. In 2005, the average marathon time in the U.S. was 4 hours 32 minutes 8 seconds for men, 5 hours 6 minutes 8 seconds for women. In 2015, the men's and women's median marathon times were 4 hours 20 minutes 13 seconds and 4 hours 45 minutes 30 seconds respectively.\nA goal many runners aim for is to break certain time barriers. For example, recreational first-timers often try to run the marathon under four hours; more competitive runners may attempt to finish under three hours. Other benchmarks are the qualifying times for major marathons. The Boston Marathon, the oldest marathon in the United States, requires a qualifying time for all non-professional runners. The New York City Marathon also requires a qualifying time for guaranteed entry, at a significantly faster pace than Boston's.\nTypically, there is a maximum allowed time of about six hours after which the marathon route is closed, although some larger marathons keep the course open considerably longer (eight hours or more). Many marathons around the world have such time limits by which all runners must have crossed the finish line. Anyone slower than the limit will be picked up by a sweeper bus. In many cases the marathon organizers are required to reopen the roads to the public so that traffic can return to normal.\nWith the growth in popularity of marathon-running, many marathons across the United States and the world have been filling to capacity faster than ever before. When the Boston Marathon opened up registration for its 2011 running, the field capacity was filled within eight hours.\n\nTraining\nThe long run is an important element in marathon training. Recreational runners commonly try to reach a maximum of about 32 km (20 mi) in their longest weekly run and a total of about 64 km (40 mi) a week when training for the marathon, but wide variability exists in practice and in recommendations. More experienced marathoners may run a longer distance during the week. Greater weekly training mileages can offer greater results in terms of distance and endurance, but also carry a greater risk of training injury. Most male elite marathon runners will complete weekly distances of over 160 km (100 mi). It is recommended that those new to running should get a checkup from their doctor, as there are certain warning signs and risk factors that should be evaluated before undertaking any new workout program, especially marathon training.\nMany training programs last a minimum of five or six months, with a gradual increase in the distance run and finally, for recovery, a period of tapering in the one to three weeks preceding the race. For beginners wishing to merely finish a marathon, a minimum of four months of running four days a week is recommended. Many trainers recommend a weekly increase in mileage of no more than 10%. It is also often advised to maintain a consistent running program for six weeks or so before beginning a marathon training program, to allow the body to adapt to the new stresses. The marathon training program itself would suppose variation between hard and easy training, with a periodization of the general plan.\nTraining programs can be found at the websites of Runner's World, Hal Higdon, Jeff Galloway, and the Boston Athletic Association, and in numerous other published sources, including the websites of specific marathons.\nThe last long training run might be undertaken up to two weeks prior to the event. Many marathon runners also \"carbo-load\" (increase carbohydrate intake while holding total caloric intake constant) during the week before the marathon to allow their bodies to store more glycogen.\n\nGlycogen and \"the wall\"\nCarbohydrates that a person eats are converted by the liver and muscles into glycogen for storage. Glycogen burns rapidly to provide quick energy. Runners can store about 8 MJ or 2,000 kcal worth of glycogen in their bodies, enough for about 30 km/18\u201320 miles of running. Many runners report that running becomes noticeably more difficult at that point. When glycogen runs low, the body must then obtain energy by burning stored fat, which does not burn as readily. When this happens, the runner will experience dramatic fatigue and is said to \"hit the wall\". The aim of training for the marathon, according to many coaches, is to maximize the limited glycogen available so that the fatigue of the \"wall\" is not as dramatic. This is accomplished in part by utilizing a higher percentage of energy from burned fat even during the early phase of the race, thus conserving glycogen.\nCarbohydrate-based \"energy gels\" are used by runners to avoid or reduce the effect of \"hitting the wall\", as they provide easy to digest energy during the run. Energy gels usually contain varying amounts of sodium and potassium and some also contain caffeine. They need to be consumed with a certain amount of water. Recommendations for how often to take an energy gel during the race range widely.\n\nAlternatives to gels include various forms of concentrated sugars, and foods high in simple carbohydrates that can be digested easily. Many runners experiment with consuming energy supplements during training runs to determine what works best for them. Consumption of food while running sometimes makes the runner sick. Runners are advised not to ingest a new food or medicine just prior to or during a race. It is also important to refrain from taking any of the non-steroidal anti-inflammatory class of pain relievers (NSAIDs, e.g., aspirin, ibuprofen, naproxen), as these drugs may change the way the kidneys regulate their blood flow and may lead to serious kidney problems, especially in cases involving moderate to severe dehydration. NSAIDS block the COX-2 enzyme pathway to prevent the production of prostaglandins. These prostaglandins may act as inflammation factors throughout the body, but they also play a crucial role in maintenance of water retention. In less than 5% of the whole population that take NSAIDS, individuals may be more negatively sensitive to renal prostaglandin synthesis inhibition.\n\nTemperature\nA study of the performance of 1.8 million participants in the Berlin, London, Paris, Boston, Chicago, and New York marathons during the years from 2001 to 2010 found that runners recorded their fastest times when the temperature was around 6 \u00b0C (43 \u00b0F), with an increase of 10 \u00b0C (18 \u00b0F) leading to a 1.5% reduction in speed. A July 2020 study found that increasing temperatures affected faster runners' performance more than slower ones.\n\nAfter a marathon\nMarathon participation may result in various medical, musculoskeletal, and dermatological complaints. Delayed onset muscle soreness (DOMS) is a common condition affecting runners during the first week following a marathon. Various types of mild exercise or massage have been recommended to alleviate pain secondary to DOMS. Dermatological issues frequently include \"jogger's nipple\", \"jogger's toe\", and blisters.\nThe immune system is reportedly suppressed for a short time. Changes to the blood chemistry, such as elevated Cardiac Troponin T, may lead physicians to mistakenly diagnose heart malfunction.\nAfter long training runs and the marathon itself, consuming carbohydrates to replace glycogen stores and protein to aid muscle recovery is commonly recommended. In addition, soaking the lower half of the body for approximately 20 minutes in cold or ice water may force blood through the leg muscles to speed recovery.\n\nHealth risks\nMarathon running has various health risks, though these can be diminished with preparation and care. Training and the races themselves can put runners under stress. While very rare, even death is a possibility during a race.\nCommon minor health risks include blisters, tendonitis, fatigue, knee or ankle sprain, dehydration (electrolyte imbalance), and other conditions. Many are categorised as overuse injuries.\n\nCardiac health\nIn 2016, a systematic medical review found that the risk of sudden cardiac death during or immediately after a marathon was between 0.6 and 1.9 deaths per 100,000 participants, varying across the specific studies and the methods used, and not controlling for age or gender. Since the risk is small, cardiac screening programs for marathons are uncommon. However, this review was not an attempt to assess the overall cardiac health impact of marathon running.\nA 2006 study of non-elite Boston Marathon participants tested runners for certain proteins that indicate heart damage or dysfunction (see Troponin) and gave them echocardiogram scans, before and after the marathon. The study revealed that, in that sample of 60 people, runners who had averaged fewer than 56 km (35 mi) of weekly training in the 4 months before the race were most likely to show some heart damage or dysfunction, while runners who had done more than 72 km (45 mi) of weekly training showed few or no heart problems.\nAccording to a Canadian study presented in 2010, running a marathon can temporarily result in decreased function of more than half the muscle segments in the heart's main pumping chamber, but neighboring segments are generally able to compensate. Full recovery is reached within three months. The fitter the runner, the less the effect. The runners with decreased left ventricle function had an average peak weekly training distance of 55.1 km (34.2 mi), while those who did not averaged 69.1 km (42.9 mi). The marathon was held in 35 \u00b0C (95 \u00b0F) weather. According to one of the researchers: \"Regular exercise reduces cardiovascular risk by a factor of two or three in the long run, but while we're doing vigorous exercise such as marathon running, our cardiac risk increases by seven.\"\n\nHydration\nOverconsumption is the most significant concern associated with water consumption during marathons. Drinking excessive amounts of fluid during a race can lead to dilution of sodium in the blood, a condition called exercise-associated hyponatremia, which may result in vomiting, seizures, coma and even death. Dr. Lewis G. Maharam, medical director for the New York City Marathon, stated in 2005: \"There are no reported cases of dehydration causing death in the history of world running, but there are plenty of cases of people dying of hyponatremia.\"\nFor example, Dr. Cynthia Lucero died at the age of 28 while participating in the 2002 Boston Marathon. It was Lucero's second marathon. At mile 22, Lucero complained of feeling \"dehydrated and rubber-legged.\" She soon wobbled and collapsed to the ground, and was unconscious by the time the paramedics reached her. Lucero was admitted to Brigham and Women's Hospital and died two days later.\nLucero's cause of death was determined to be hyponatremic encephalopathy, a condition that causes swelling of the brain due to an imbalance of sodium in the blood known as exercise-associated hyponatremia (EAH). While EAH is sometimes referred to as \"water intoxication\", Lucero drank large amounts of Gatorade during the race, demonstrating that runners who consume sodium-containing sports drinks in excess of thirst can still develop EAH. Because hyponatremia is caused by excessive water retention, and not just loss of sodium, consumption of sports drinks or salty foods may not prevent hyponatremia.\nWomen are more prone to hyponatremia than men. A study in the New England Journal of Medicine found that 13% of runners completing the 2002 Boston Marathon had hyponatremia.\nThe International Marathon Medical Directors Association (IMMDA) advised in 2006 that fluid intake should be adjusted individually according to factors such as body weight, sex, climate, pace, fitness (VO2 max), and sweat rate, as fluid requirements can vary between people depending on these variables. The IMMDA also recommended sports drinks that include carbohydrates and electrolytes instead of plain water and said that runners should \"drink to thirst\", trying to refrain from drinking at every fluid station before feeling thirsty. Heat exposure leads to diminished thirst drive and thirst may not be a sufficient incentive to drink in many situations. The IMMDA and HSL Harpur Hill give recommendations to drink fluid in small volumes frequently at an approximate rate falling between 100\u2013250 ml (3.4\u20138.5 US fl oz) every 15 minutes. A patient suffering hyponatremia can be given a small volume of a concentrated salt solution intravenously to raise sodium concentrations in the blood. Some runners weigh themselves before running and write the results on their bibs. If anything goes wrong, first aid workers can use the weight information to tell if the patient had consumed too much water.\n\nBody temperature\nExertional heat stroke is an emergency condition in which thermoregulation fails and the body temperature rises dangerously above 40 \u00b0C (104 \u00b0F). It becomes a greater risk in warm and humid weather, even for young and fit individuals. Treatment requires rapid physical cooling of the body.\n\nCharity involvement\nSome charities seek to associate with various races. Some marathon organizers set aside a portion of their limited entry slots for charity organizations to sell to members in exchange for donations. Runners are given the option to sign up to run particular races, especially when marathon entries are no longer available to the general public. In some cases, charities organize their own marathon as a fund-raiser, gaining funds via entry fees or sponsorships.\n\nCulture\nMars rover marathon\n\nIn 2015 the Mars rover Opportunity attained the distance of a marathon from its starting location on Mars, and the valley where it achieved this distance was called Marathon Valley, which was then explored.\n\nSee also\nRecords\n\nMarathon world record progression\nNational records in the marathon\nLists\n\nList of marathon races\nList of marathoners\nList of marathon national champions (men)\nList of non-professional marathon runners\nRelated races\n\nEkiden (marathon relays)\nHalf marathon\nUltramarathon\nOther endurance races\n\nIronman Triathlon\nMountain marathon\nMulti-day race\nSki marathon\nOrganizations\n\n100 Marathon Club\nWorld Peace Marathon\nNotable races\n\nIneos 1:59 Challenge\nMan versus Horse Marathon\nMarathons at the Paralympics\nOther related topics\n\nPacemaker (running)\nPhysiology of marathons\n\nNotes\nReferences\nBibliography\nHans-Joachim Gehrke, \"From Athenian identity to European ethnicity: The cultural biography of the myth of Marathon,\" in Ton Derks, Nico Roymans (ed.), Ethnic Constructs in Antiquity: The Role of Power and Tradition (Amsterdam, Amsterdam University Press, 2009) (Amsterdam Archaeological Studies, 13), 85\u2013100.\nHans W. Giessen: Mythos Marathon. Von Herodot \u00fcber Br\u00e9al bis zur Gegenwart. (= Landauer Schriften zur Kommunikations- und Kulturwissenschaft. Band 17). Verlag Empirische P\u00e4dagogik, Landau 2010\nTom Derderian, Boston Marathon: History of the World's Premier Running Event, Human Kinetics, 1994, 1996\n\nExternal links\n\nIAAF list of marathon records in XML", "url": "https://en.wikipedia.org/wiki/Marathon"}]